Sample 1824 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 696, 51, 32, 519, 19, 4860, 544, 4, 50118, 45443, 6990, 7, 2067, 15, 10, 2903, 8, 582, 10, 4029, 77, 70, 5, 22893, 8, 2104, 13, 8809, 16, 259, 8, 6990, 7, 2067, 15, 10, 2903, 8, 582, 10, 4029, 77, 70, 5, 22893, 8, 2104, 13, 8809, 16, 259, 4, 50118, 45443, 6990, 7, 304, 5, 3104, 874, 7, 1649, 15, 5, 6720, 4, 2, 2, 0, 1039, 15314, 27204, 42, 696, 38, 437, 519, 16, 19, 4860, 544, 2, 0, 1039, 16925, 36647, 653, 18, 164, 15, 19, 110, 4860, 544, 116, 166, 40, 28, 1372, 7, 3991, 4, 2, 0, 1039, 21119, 30397, 38873, 57, 667, 7, 2937, 544, 187, 5, 389, 212, 9, 94, 353, 734, 12186, 41, 4916, 13, 80, 360, 536, 734, 2903, 393, 36, 134, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 239, 253, 3018, 734, 5, 129, 1219, 127, 544, 938, 75, 11764, 12012, 142, 38, 21, 1088, 10196, 30820, 2888, 122, 38, 33, 36, 246, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 969, 734, 553, 7, 28, 14418, 4560, 53, 7, 117, 22345, 38, 202, 33, 7, 2067, 454, 294, 24, 1302, 734, 38, 437, 10, 370, 565, 17968, 8, 10, 36, 176, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 2111, 544, 18278, 14, 31204, 16, 15135, 162, 86, 38, 240, 13, 569, 5390, 734, 45, 7, 4521, 4356, 421, 7, 36, 401, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 2903, 6, 5389, 11, 8869, 878, 31, 633, 19006, 734, 98, 122, 38, 437, 396, 544, 528, 7, 457, 1769, 2903, 29, 8, 457, 1769, 36, 245, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 7, 2067, 15, 10, 2903, 8, 582, 10, 4029, 77, 70, 5, 22893, 8, 2104, 13, 8809, 16, 259, 8, 38, 437, 10, 9785, 46213, 620, 36, 306, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 582, 127, 1087, 15, 86, 53, 38, 33, 7, 3264, 628, 544, 31, 5, 138, 255, 3293, 32191, 2604, 2562, 8041, 510, 46859, 734, 18895, 4248, 37051, 38, 256, 42417, 36, 406, 73, 398, 43, 2, 0, 1039, 21119, 30397, 38873, 38, 108, 448, 83, 468, 3935, 24394, 30540, 487, 108, 565, 14895, 717, 10002, 21695, 2068, 10760, 15421, 347, 34494, 6569, 10470, 10674, 36, 398, 73, 398, 43, 2, 0, 1039, 16925, 36647, 3945, 47, 6475, 7, 6662, 110, 2078, 116, 1793, 47, 32, 45, 562, 5, 2078, 110, 32, 6397, 196, 13, 116, 2, 0, 1039, 16925, 36647, 166, 1346, 110, 2212, 53, 103, 2104, 189, 33, 7, 28, 7084, 6122, 15, 110, 2069, 4, 2, 0, 1039, 21119, 30397, 38873, 14, 630, 75, 10525, 3486, 3922, 596, 787, 1225, 4390, 1244, 630, 75, 33, 10, 8134, 4823, 6720, 734, 70, 38, 348, 57, 4997, 7, 36, 134, 73, 176, 43, 2, 0, 1039, 21119, 30397, 38873, 32, 1199, 544, 6569, 10470, 10674, 849, 1193, 4344, 2650, 36, 176, 73, 176, 43, 2, 0, 1039, 16925, 36647, 3401, 304, 5, 3104, 874, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 9, 10, -1, 0, 4, 5, 7, 12, 14]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 21975
Loss at step 10: 2.4833
Loss at step 20: 2.2580
Loss at step 30: 3.2666
Loss at step 40: 1.6998
Loss at step 50: 3.1219
Loss at step 60: 2.2508
Loss at step 70: 1.9665
Loss at step 80: 2.7948
Loss at step 90: 1.6809
Loss at step 100: 2.0146
Loss at step 110: 2.3214
Loss at step 120: 11.6063
Loss at step 130: 2.1432
Loss at step 140: 2.3546
Loss at step 150: 1.8160
Loss at step 160: 2.8965
Loss at step 170: 4.5263
Loss at step 180: 8.5704
Loss at step 190: 7.2242
Loss at step 200: 2.8458
Loss at step 210: 1.9204
Loss at step 220: 1.4885
Loss at step 230: 6.0701
Loss at step 240: 2.5830
Loss at step 250: 2.5977
Loss at step 260: 2.3162
Loss at step 270: 3.0016
Loss at step 280: 2.3683
Loss at step 290: 1.8948
Loss at step 300: 2.6196
Loss at step 310: 1.8330
Loss at step 320: 1.8582
Loss at step 330: 2.1749
Loss at step 340: 1.7219
Loss at step 350: 2.0946
Loss at step 360: 3.4069
Loss at step 370: 2.2874
Loss at step 380: 3.0724
Loss at step 390: 2.4627
Loss at step 400: 1.4402
Loss at step 410: 1.5678
Loss at step 420: 2.6516
Loss at step 430: 1.7604
Loss at step 440: 2.8640
Loss at step 450: 1.5097
Loss at step 460: 1.6242
Loss at step 470: 1.7630
Loss at step 480: 2.5306
Loss at step 490: 2.0845
Loss at step 500: 2.1018
Loss at step 510: 2.1369
Loss at step 520: 2.4937
Loss at step 530: 2.6645
Loss at step 540: 2.3104
Loss at step 550: 1.9789
Loss at step 560: 2.5338
Loss at step 570: 2.2006
Loss at step 580: 1.4939
Loss at step 590: 2.3182
Loss at step 600: 1.2248
Loss at step 610: 2.2059
Loss at step 620: 1.5667
Loss at step 630: 2.2682
Loss at step 640: 2.0281
Loss at step 650: 1.8634
Loss at step 660: 1.9135
Loss at step 670: 2.2292
Loss at step 680: 4.7326
Loss at step 690: 2.0853
Loss at step 700: 1.7503
Loss at step 710: 1.7561
Loss at step 720: 1.6515
Loss at step 730: 1.8322
Loss at step 740: 2.4938
Loss at step 750: 3.5509
Loss at step 760: 2.6019
Loss at step 770: 1.2561
Loss at step 780: 1.4258
Loss at step 790: 1.7983
Loss at step 800: 1.7387
Loss at step 810: 1.7130
Loss at step 820: 1.4322
Loss at step 830: 1.8155
Loss at step 840: 1.0537
Loss at step 850: 1.2422
Loss at step 860: 1.7372
Loss at step 870: 2.3174
Loss at step 880: 1.3689
Loss at step 890: 1.6279
Loss at step 900: 2.8145
Loss at step 910: 1.7879
Loss at step 920: 1.6649
Loss at step 930: 1.6054
Loss at step 940: 1.8635
Loss at step 950: 1.9972
Loss at step 960: 1.6121
Loss at step 970: 1.6549
Loss at step 980: 1.5691
Loss at step 990: 1.9918
Loss at step 1000: 1.7201
Loss at step 1010: 1.3864
Loss at step 1020: 2.4790
Loss at step 1030: 1.6469
Loss at step 1040: 1.8841
Loss at step 1050: 1.1733
Loss at step 1060: 1.5600
Loss at step 1070: 1.6824
Loss at step 1080: 2.1599
Loss at step 1090: 1.8968
Loss at step 1100: 1.7209
Loss at step 1110: 1.0327
Loss at step 1120: 1.7035
Loss at step 1130: 1.5416
Loss at step 1140: 1.6960
Loss at step 1150: 2.6090
Loss at step 1160: 1.4192
Loss at step 1170: 1.5390
Loss at step 1180: 1.6688
Loss at step 1190: 3.3453
Loss at step 1200: 1.5867
Loss at step 1210: 1.7036
Loss at step 1220: 1.4930
Loss at step 1230: 1.6135
Loss at step 1240: 1.3470
Loss at step 1250: 1.7957
Loss at step 1260: 2.2167
Loss at step 1270: 1.4652
Loss at step 1280: 1.2038
Loss at step 1290: 1.5241
Loss at step 1300: 1.0996
Loss at step 1310: 2.2607
Loss at step 1320: 2.6107
Loss at step 1330: 1.3907
Loss at step 1340: 3.0695
Loss at step 1350: 2.1671
Loss at step 1360: 1.1722
Loss at step 1370: 1.4150
Loss at step 1380: 1.3512
Loss at step 1390: 1.1889
Loss at step 1400: 1.5289
Loss at step 1410: 1.5330
Loss at step 1420: 2.0713
Loss at step 1430: 0.8706
Loss at step 1440: 2.5371
Loss at step 1450: 1.3597
Loss at step 1460: 1.1269
Loss at step 1470: 1.6260
Loss at step 1480: 1.3890
Loss at step 1490: 1.8243
Loss at step 1500: 1.2981
Loss at step 1510: 1.5925
Loss at step 1520: 1.0075
Loss at step 1530: 1.2619
Loss at step 1540: 2.5129
Loss at step 1550: 1.5711
Loss at step 1560: 2.9646
Loss at step 1570: 1.1103
Loss at step 1580: 0.9075
Loss at step 1590: 1.2304
Loss at step 1600: 1.2273
Loss at step 1610: 2.4418
Loss at step 1620: 1.5839
Loss at step 1630: 1.4182
Loss at step 1640: 1.1930
Loss at step 1650: 1.6326
Loss at step 1660: 0.9303
Loss at step 1670: 1.4443
Loss at step 1680: 1.2819
Loss at step 1690: 0.6099
Loss at step 1700: 2.5668
Loss at step 1710: 1.3533
Loss at step 1720: 1.6151
Loss at step 1730: 1.4931
Loss at step 1740: 1.5009
Loss at step 1750: 2.8372
Loss at step 1760: 1.7209
Loss at step 1770: 1.3610
Loss at step 1780: 1.9653
Loss at step 1790: 1.8866
Loss at step 1800: 1.4061
Loss at step 1810: 1.7456
Loss at step 1820: 1.5780
Loss at step 1830: 1.9351
Loss at step 1840: 1.1387
Loss at step 1850: 1.2001
Loss at step 1860: 0.8319
Loss at step 1870: 1.4748
Loss at step 1880: 0.8087
Loss at step 1890: 2.0145
Loss at step 1900: 1.4392
Loss at step 1910: 1.5760
Loss at step 1920: 1.5358
Loss at step 1930: 1.5631
Loss at step 1940: 1.5690
Loss at step 1950: 1.3337
Loss at step 1960: 1.7591
Loss at step 1970: 1.4920
Loss at step 1980: 1.3441
Loss at step 1990: 1.3108
Loss at step 2000: 1.5847
Loss at step 2010: 1.3279
Loss at step 2020: 1.3915
Loss at step 2030: 1.7170
Loss at step 2040: 0.6844
Loss at step 2050: 1.2238
Loss at step 2060: 1.8739
Loss at step 2070: 1.2952
Loss at step 2080: 0.9307
Loss at step 2090: 1.5115
Loss at step 2100: 1.5002
Loss at step 2110: 1.5574
Loss at step 2120: 1.6240
Loss at step 2130: 1.0666
Loss at step 2140: 3.2720
Loss at step 2150: 0.5598
Loss at step 2160: 1.9193
Loss at step 2170: 1.4868
Loss at step 2180: 1.4841
Loss at step 2190: 1.7651
Loss at step 2200: 1.9700
Loss at step 2210: 1.6127
Loss at step 2220: 1.3349
Loss at step 2230: 1.2981
Loss at step 2240: 1.0502
Loss at step 2250: 1.0854
Loss at step 2260: 1.3340
Loss at step 2270: 1.1676
Loss at step 2280: 1.1632
Loss at step 2290: 0.6301
Loss at step 2300: 1.0722
Loss at step 2310: 1.3075
Loss at step 2320: 0.9112
Loss at step 2330: 1.2156
Loss at step 2340: 1.0362
Loss at step 2350: 0.9618
Loss at step 2360: 0.7972
Loss at step 2370: 1.7961
Loss at step 2380: 2.2929
Loss at step 2390: 1.0266
Loss at step 2400: 1.2325
Loss at step 2410: 1.2534
Loss at step 2420: 1.0581
Loss at step 2430: 1.0435
Loss at step 2440: 1.3038
Loss at step 2450: 0.7424
Loss at step 2460: 1.1245
Loss at step 2470: 1.9319
Loss at step 2480: 1.1434
Loss at step 2490: 0.8721
Loss at step 2500: 0.8922
Loss at step 2510: 0.5868
Loss at step 2520: 1.0322
Loss at step 2530: 1.0944
Loss at step 2540: 1.2999
Loss at step 2550: 0.9427
Loss at step 2560: 1.1589
Loss at step 2570: 0.8815
Loss at step 2580: 1.0703
Loss at step 2590: 0.5492
Loss at step 2600: 0.7426
Loss at step 2610: 0.9527
Loss at step 2620: 1.4375
Loss at step 2630: 1.8359
Loss at step 2640: 0.9161
Loss at step 2650: 0.7526
Loss at step 2660: 1.3015
Loss at step 2670: 1.2146
Loss at step 2680: 1.2483
Loss at step 2690: 2.0648
Loss at step 2700: 0.9763
Loss at step 2710: 1.3209
Loss at step 2720: 1.1013
Loss at step 2730: 1.0116
Loss at step 2740: 0.6666
Loss at step 2750: 1.0393
Loss at step 2760: 1.0397
Loss at step 2770: 0.8171
Loss at step 2780: 0.9085
Loss at step 2790: 0.7628
Loss at step 2800: 1.6889
Loss at step 2810: 0.7459
Loss at step 2820: 0.8151
Loss at step 2830: 0.8018
Loss at step 2840: 0.2897
Loss at step 2850: 1.6005
Loss at step 2860: 0.6363
Loss at step 2870: 0.7069
Loss at step 2880: 0.5122
Loss at step 2890: 0.8374
Loss at step 2900: 2.0583
Loss at step 2910: 0.9087
Loss at step 2920: 1.2855
Loss at step 2930: 1.1508
Loss at step 2940: 0.7412
Loss at step 2950: 0.9953
Loss at step 2960: 0.7817
Loss at step 2970: 1.3586
Loss at step 2980: 0.7308
Loss at step 2990: 0.9504
Loss at step 3000: 1.0535
Loss at step 3010: 1.0602
Loss at step 3020: 1.0188
Loss at step 3030: 0.8168
Loss at step 3040: 1.0458
Loss at step 3050: 1.0853
Loss at step 3060: 0.7093
Loss at step 3070: 1.9887
Loss at step 3080: 0.7683
Loss at step 3090: 0.8313
Loss at step 3100: 1.0824
Loss at step 3110: 0.7464
Loss at step 3120: 1.1519
Loss at step 3130: 2.4311
Loss at step 3140: 0.8265
Loss at step 3150: 1.1378
Loss at step 3160: 0.8783
Loss at step 3170: 0.7239
Loss at step 3180: 1.0070
Loss at step 3190: 1.0990
Loss at step 3200: 0.8278
Loss at step 3210: 1.0135
Loss at step 3220: 1.0573
Loss at step 3230: 0.9338
Loss at step 3240: 0.6901
Loss at step 3250: 1.2249
Loss at step 3260: 1.1314
Loss at step 3270: 1.2213
Loss at step 3280: 0.6765
Loss at step 3290: 0.4061
Loss at step 3300: 0.9371
Loss at step 3310: 0.9421
Loss at step 3320: 0.6794
Loss at step 3330: 0.8001
Loss at step 3340: 0.9157
Loss at step 3350: 1.0688
Loss at step 3360: 1.2243
Loss at step 3370: 0.7562
Loss at step 3380: 1.4028
Loss at step 3390: 0.8657
Loss at step 3400: 0.5967
Loss at step 3410: 0.4903
Loss at step 3420: 1.2702
Loss at step 3430: 1.1261
Loss at step 3440: 0.7573
Loss at step 3450: 0.9459
Loss at step 3460: 0.7861
Loss at step 3470: 0.7934
Loss at step 3480: 0.8792
Loss at step 3490: 0.5417
Loss at step 3500: 0.6865
Loss at step 3510: 1.2671
Loss at step 3520: 0.8600
Loss at step 3530: 1.3790
Loss at step 3540: 1.5382
Loss at step 3550: 0.8296
Loss at step 3560: 0.6669
Loss at step 3570: 0.5315
Loss at step 3580: 0.8530
Loss at step 3590: 0.6019
Loss at step 3600: 0.7316
Loss at step 3610: 0.9599
Loss at step 3620: 0.3403
Loss at step 3630: 0.6096
Loss at step 3640: 0.8472
Loss at step 3650: 0.7180
Loss at step 3660: 0.7969
Loss at step 3670: 1.5858
Loss at step 3680: 0.9030
Loss at step 3690: 1.1686
Loss at step 3700: 1.9641
Loss at step 3710: 0.8267
Loss at step 3720: 0.8521
Loss at step 3730: 0.8308
Loss at step 3740: 1.2799
Loss at step 3750: 1.5454
Loss at step 3760: 0.9851
Loss at step 3770: 0.5894
Loss at step 3780: 0.7356
Loss at step 3790: 0.9601
Loss at step 3800: 1.1957
Loss at step 3810: 2.3976
Loss at step 3820: 0.2487
Loss at step 3830: 0.7276
Loss at step 3840: 1.1857
Loss at step 3850: 0.6170
Loss at step 3860: 0.5172
Loss at step 3870: 0.5752
Loss at step 3880: 0.5526
Loss at step 3890: 0.8016
Loss at step 3900: 1.3298
Loss at step 3910: 0.8320
Loss at step 3920: 1.5137
Loss at step 3930: 1.2488
Loss at step 3940: 1.1052
Loss at step 3950: 0.4831
Loss at step 3960: 0.5524
Loss at step 3970: 0.5605
Loss at step 3980: 1.0097
Loss at step 3990: 0.7509
Loss at step 4000: 0.2511
Loss at step 4010: 0.7978
Loss at step 4020: 0.7147
Loss at step 4030: 0.9742
Loss at step 4040: 0.5201
Loss at step 4050: 0.7984
Loss at step 4060: 0.5908
Loss at step 4070: 0.7082
Loss at step 4080: 0.5769
Loss at step 4090: 0.7308
Loss at step 4100: 0.6473
Loss at step 4110: 0.8372
Loss at step 4120: 0.6762
Loss at step 4130: 1.1843
Loss at step 4140: 0.7633
Loss at step 4150: 0.6235
Loss at step 4160: 1.0362
Loss at step 4170: 0.9098
Loss at step 4180: 0.8755
Loss at step 4190: 1.0010
Loss at step 4200: 1.0516
Loss at step 4210: 0.6224
Loss at step 4220: 1.3416
Loss at step 4230: 1.8100
Loss at step 4240: 0.7886
Loss at step 4250: 0.7539
Loss at step 4260: 0.6921
Loss at step 4270: 0.8665
Loss at step 4280: 1.4441
Loss at step 4290: 1.4023
Loss at step 4300: 1.0823
Loss at step 4310: 0.8987
Loss at step 4320: 0.8484
Loss at step 4330: 0.4764
Loss at step 4340: 0.8131
Loss at step 4350: 0.4699
Loss at step 4360: 0.5187
Loss at step 4370: 2.0523
Loss at step 4380: 0.5664
Loss at step 4390: 0.7756
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.537271, 'precision': [0.587784, 0.473527, 0.560976], 'recall': [0.683719, 0.515422, 0.359583], 'f1': [0.632132, 0.493587, 0.43825]}
{'accuracy': 0.760745, 'precision': 0.560976, 'recall': 0.359583, 'f1': 0.43825, 'WordR': 0.177521}
Loss at step 4400: 1.5464
Loss at step 4410: 0.6202
Loss at step 4420: 1.0241
Loss at step 4430: 0.7629
Loss at step 4440: 0.8032
Loss at step 4450: 0.5951
Loss at step 4460: 0.7694
Loss at step 4470: 0.7076
Loss at step 4480: 0.5508
Loss at step 4490: 0.9117
Loss at step 4500: 0.5771
Loss at step 4510: 0.4993
Loss at step 4520: 0.3551
Loss at step 4530: 0.7202
Loss at step 4540: 0.8811
Loss at step 4550: 0.8874
Loss at step 4560: 0.3294
Loss at step 4570: 1.2954
Loss at step 4580: 0.4760
Loss at step 4590: 0.1494
Loss at step 4600: 0.8941
Loss at step 4610: 0.3997
Loss at step 4620: 0.6686
Loss at step 4630: 0.5590
Loss at step 4640: 0.9200
Loss at step 4650: 0.5781
Loss at step 4660: 1.0434
Loss at step 4670: 1.0960
Loss at step 4680: 0.3640
Loss at step 4690: 0.3281
Loss at step 4700: 0.7259
Loss at step 4710: 0.7732
Loss at step 4720: 0.3272
Loss at step 4730: 0.4971
Loss at step 4740: 0.6151
Loss at step 4750: 0.5337
Loss at step 4760: 0.2521
Loss at step 4770: 0.4983
Loss at step 4780: 0.5492
Loss at step 4790: 0.7127
Loss at step 4800: 0.5670
Loss at step 4810: 0.9048
Loss at step 4820: 0.4727
Loss at step 4830: 0.4639
Loss at step 4840: 0.1979
Loss at step 4850: 0.3412
Loss at step 4860: 0.4723
Loss at step 4870: 0.3255
Loss at step 4880: 0.3233
Loss at step 4890: 0.6072
Loss at step 4900: 0.5661
Loss at step 4910: 0.6176
Loss at step 4920: 0.9945
Loss at step 4930: 0.4434
Loss at step 4940: 0.3646
Loss at step 4950: 0.7800
Loss at step 4960: 1.2633
Loss at step 4970: 0.6881
Loss at step 4980: 0.5746
Loss at step 4990: 0.3740
Loss at step 5000: 0.4185
Loss at step 5010: 0.5949
Loss at step 5020: 0.4177
Loss at step 5030: 0.1994
Loss at step 5040: 0.4133
Loss at step 5050: 0.7825
Loss at step 5060: 0.6203
Loss at step 5070: 0.6849
Loss at step 5080: 0.3847
Loss at step 5090: 0.9456
Loss at step 5100: 0.2101
Loss at step 5110: 0.8579
Loss at step 5120: 0.4862
Loss at step 5130: 0.6042
Loss at step 5140: 1.1224
Loss at step 5150: 0.7224
Loss at step 5160: 0.8532
Loss at step 5170: 0.5964
Loss at step 5180: 0.3883
Loss at step 5190: 0.1630
Loss at step 5200: 0.9074
Loss at step 5210: 0.4466
Loss at step 5220: 0.8144
Loss at step 5230: 0.4056
Loss at step 5240: 0.4136
Loss at step 5250: 0.8488
Loss at step 5260: 0.3878
Loss at step 5270: 0.3241
Loss at step 5280: 0.3949
Loss at step 5290: 0.4301
Loss at step 5300: 0.3928
Loss at step 5310: 1.7408
Loss at step 5320: 0.4511
Loss at step 5330: 0.7721
Loss at step 5340: 0.7641
Loss at step 5350: 0.7070
Loss at step 5360: 0.8587
Loss at step 5370: 0.4944
Loss at step 5380: 0.5678
Loss at step 5390: 2.2921
Loss at step 5400: 0.5270
Loss at step 5410: 0.4139
Loss at step 5420: 0.2427
Loss at step 5430: 0.5166
Loss at step 5440: 0.2255
Loss at step 5450: 0.5033
Loss at step 5460: 0.4727
Loss at step 5470: 0.5219
Loss at step 5480: 0.4893
Loss at step 5490: 0.9562
Loss at step 5500: 1.0950
Loss at step 5510: 0.3346
Loss at step 5520: 0.1857
Loss at step 5530: 0.7218
Loss at step 5540: 0.6981
Loss at step 5550: 0.9919
Loss at step 5560: 0.3545
Loss at step 5570: 0.3687
Loss at step 5580: 0.3704
Loss at step 5590: 1.1146
Loss at step 5600: 0.6088
Loss at step 5610: 1.1334
Loss at step 5620: 0.4092
Loss at step 5630: 0.4232
Loss at step 5640: 0.3743
Loss at step 5650: 0.3602
Loss at step 5660: 1.7643
Loss at step 5670: 0.6275
Loss at step 5680: 0.1200
Loss at step 5690: 0.7040
Loss at step 5700: 0.7826
Loss at step 5710: 0.3463
Loss at step 5720: 0.6371
Loss at step 5730: 0.5940
Loss at step 5740: 0.5575
Loss at step 5750: 0.7288
Loss at step 5760: 0.3566
Loss at step 5770: 0.8237
Loss at step 5780: 0.2222
Loss at step 5790: 0.3534
Loss at step 5800: 0.6641
Loss at step 5810: 0.4939
Loss at step 5820: 0.6663
Loss at step 5830: 0.2703
Loss at step 5840: 0.1045
Loss at step 5850: 0.4060
Loss at step 5860: 0.0969
Loss at step 5870: 1.0958
Loss at step 5880: 0.2268
Loss at step 5890: 0.5894
Loss at step 5900: 0.5834
Loss at step 5910: 0.2796
Loss at step 5920: 0.3780
Loss at step 5930: 0.2829
Loss at step 5940: 0.4325
Loss at step 5950: 0.1806
Loss at step 5960: 1.0366
Loss at step 5970: 0.6077
Loss at step 5980: 0.3849
Loss at step 5990: 0.7368
Loss at step 6000: 0.4005
Loss at step 6010: 1.6658
Loss at step 6020: 1.5862
Loss at step 6030: 0.5352
Loss at step 6040: 0.2635
Loss at step 6050: 0.4952
Loss at step 6060: 0.7351
Loss at step 6070: 0.3159
Loss at step 6080: 0.6452
Loss at step 6090: 0.1811
Loss at step 6100: 0.2338
Loss at step 6110: 1.6191
Loss at step 6120: 0.0302
Loss at step 6130: 0.9322
Loss at step 6140: 0.4269
Loss at step 6150: 0.9311
Loss at step 6160: 0.6827
Loss at step 6170: 0.1886
Loss at step 6180: 0.3256
Loss at step 6190: 0.8416
Loss at step 6200: 0.2409
Loss at step 6210: 0.9043
Loss at step 6220: 0.1530
Loss at step 6230: 0.0751
Loss at step 6240: 0.2758
Loss at step 6250: 0.8602
Loss at step 6260: 0.3008
Loss at step 6270: 0.2879
Loss at step 6280: 0.3902
Loss at step 6290: 0.0744
Loss at step 6300: 0.9311
Loss at step 6310: 0.7597
Loss at step 6320: 0.5680
Loss at step 6330: 0.2768
Loss at step 6340: 0.3567
Loss at step 6350: 0.0287
Loss at step 6360: 0.2824
Loss at step 6370: 0.4370
Loss at step 6380: 0.1609
Loss at step 6390: 0.9263
Loss at step 6400: 0.1713
Loss at step 6410: 0.4969
Loss at step 6420: 0.4612
Loss at step 6430: 0.4765
Loss at step 6440: 0.2445
Loss at step 6450: 0.0448
Loss at step 6460: 0.4485
Loss at step 6470: 0.6247
Loss at step 6480: 0.5755
Loss at step 6490: 0.4373
Loss at step 6500: 0.3093
Loss at step 6510: 0.4021
Loss at step 6520: 0.6051
Loss at step 6530: 0.2236
Loss at step 6540: 1.3164
Loss at step 6550: 0.7757
Loss at step 6560: 0.5140
Loss at step 6570: 0.5845
Loss at step 6580: 0.2463
Loss at step 6590: 0.2619
Loss at step 6600: 0.3195
Loss at step 6610: 0.4344
Loss at step 6620: 0.2338
Loss at step 6630: 0.2734
Loss at step 6640: 0.2278
Loss at step 6650: 1.0975
Loss at step 6660: 0.5684
Loss at step 6670: 0.4087
Loss at step 6680: 0.1160
Loss at step 6690: 0.2450
Loss at step 6700: 0.4327
Loss at step 6710: 0.6445
Loss at step 6720: 0.2519
Loss at step 6730: 0.2470
Loss at step 6740: 0.0689
Loss at step 6750: 0.7354
Loss at step 6760: 0.4527
Loss at step 6770: 1.7515
Loss at step 6780: 1.5791
Loss at step 6790: 0.2806
Loss at step 6800: 0.2571
Loss at step 6810: 0.4972
Loss at step 6820: 0.3787
Loss at step 6830: 1.1969
Loss at step 6840: 1.0489
Loss at step 6850: 0.2869
Loss at step 6860: 0.3303
Loss at step 6870: 0.3213
Loss at step 6880: 0.0975
Loss at step 6890: 0.6244
Loss at step 6900: 0.1000
Loss at step 6910: 1.1471
Loss at step 6920: 0.7945
Loss at step 6930: 0.0323
Loss at step 6940: 0.7190
Loss at step 6950: 0.2139
Loss at step 6960: 0.7153
Loss at step 6970: 1.3049
Loss at step 6980: 0.8955
Loss at step 6990: 1.1543
Loss at step 7000: 0.8383
Loss at step 7010: 0.0840
Loss at step 7020: 0.5478
Loss at step 7030: 1.3784
Loss at step 7040: 1.1699
Loss at step 7050: 1.2165
Loss at step 7060: 0.7089
Loss at step 7070: 0.3125
Loss at step 7080: 0.5051
Loss at step 7090: 0.4819
Loss at step 7100: 0.7086
Loss at step 7110: 0.5036
Loss at step 7120: 0.5488
Loss at step 7130: 0.3600
Loss at step 7140: 0.7232
Loss at step 7150: 1.0632
Loss at step 7160: 0.1900
Loss at step 7170: 0.8532
Loss at step 7180: 1.1235
Loss at step 7190: 0.1294
Loss at step 7200: 0.1266
Loss at step 7210: 0.6915
Loss at step 7220: 0.9023
Loss at step 7230: 0.2968
Loss at step 7240: 0.5988
Loss at step 7250: 0.2520
Loss at step 7260: 0.3986
Loss at step 7270: 0.0897
Loss at step 7280: 1.3414
Loss at step 7290: 0.1630
Loss at step 7300: 0.1805
Loss at step 7310: 0.8925
Loss at step 7320: 0.3692
Loss at step 7330: 0.1997
Loss at step 7340: 0.3342
Loss at step 7350: 0.2394
Loss at step 7360: 0.2621
Loss at step 7370: 0.2901
Loss at step 7380: 0.3323
Loss at step 7390: 0.6205
Loss at step 7400: 1.0421
Loss at step 7410: 0.3957
Loss at step 7420: 0.3713
Loss at step 7430: 0.4298
Loss at step 7440: 0.3042
Loss at step 7450: 0.1243
Loss at step 7460: 0.0856
Loss at step 7470: 0.4439
Loss at step 7480: 0.2608
Loss at step 7490: 0.3825
Loss at step 7500: 1.1449
Loss at step 7510: 0.1885
Loss at step 7520: 0.5406
Loss at step 7530: 1.3082
Loss at step 7540: 0.2057
Loss at step 7550: 0.6896
Loss at step 7560: 0.3090
Loss at step 7570: 0.8072
Loss at step 7580: 0.1569
Loss at step 7590: 0.3459
Loss at step 7600: 0.4048
Loss at step 7610: 0.1214
Loss at step 7620: 0.3098
Loss at step 7630: 0.3438
Loss at step 7640: 0.3271
Loss at step 7650: 0.2285
Loss at step 7660: 0.3470
Loss at step 7670: 0.7793
Loss at step 7680: 0.4116
Loss at step 7690: 0.5308
Loss at step 7700: 0.1036
Loss at step 7710: 1.2827
Loss at step 7720: 0.2354
Loss at step 7730: 0.0670
Loss at step 7740: 0.6671
Loss at step 7750: 0.7092
Loss at step 7760: 0.3695
Loss at step 7770: 0.2685
Loss at step 7780: 0.0584
Loss at step 7790: 1.0137
Loss at step 7800: 0.5298
Loss at step 7810: 0.2616
Loss at step 7820: 0.2072
Loss at step 7830: 0.9417
Loss at step 7840: 0.7892
Loss at step 7850: 0.3504
Loss at step 7860: 0.0673
Loss at step 7870: 1.1334
Loss at step 7880: 0.5767
Loss at step 7890: 1.0406
Loss at step 7900: 0.2365
Loss at step 7910: 0.1175
Loss at step 7920: 0.3588
Loss at step 7930: 0.2677
Loss at step 7940: 0.0297
Loss at step 7950: 0.1464
Loss at step 7960: 0.1049
Loss at step 7970: 0.5486
Loss at step 7980: 0.2042
Loss at step 7990: 0.5015
Loss at step 8000: 0.4873
Loss at step 8010: 0.0327
Loss at step 8020: 0.0561
Loss at step 8030: 0.5913
Loss at step 8040: 0.2493
Loss at step 8050: 0.2119
Loss at step 8060: 0.6005
Loss at step 8070: 0.2560
Loss at step 8080: 0.1277
Loss at step 8090: 0.7416
Loss at step 8100: 1.2290
Loss at step 8110: 0.0825
Loss at step 8120: 0.3252
Loss at step 8130: 0.4005
Loss at step 8140: 0.1303
Loss at step 8150: 0.1930
Loss at step 8160: 0.0265
Loss at step 8170: 0.0846
Loss at step 8180: 0.8365
Loss at step 8190: 0.1798
Loss at step 8200: 0.3389
Loss at step 8210: 0.1738
Loss at step 8220: 0.1379
Loss at step 8230: 0.2478
Loss at step 8240: 1.3701
Loss at step 8250: 0.2697
Loss at step 8260: 0.4706
Loss at step 8270: 1.4310
Loss at step 8280: 0.2161
Loss at step 8290: 0.1602
Loss at step 8300: 0.3305
Loss at step 8310: 0.2088
Loss at step 8320: 0.0807
Loss at step 8330: 0.0822
Loss at step 8340: 0.2350
Loss at step 8350: 0.3076
Loss at step 8360: 0.2614
Loss at step 8370: 0.3953
Loss at step 8380: 0.8705
Loss at step 8390: 0.2714
Loss at step 8400: 0.2056
Loss at step 8410: 0.1848
Loss at step 8420: 0.2302
Loss at step 8430: 0.3753
Loss at step 8440: 0.4052
Loss at step 8450: 0.0978
Loss at step 8460: 0.2181
Loss at step 8470: 0.5361
Loss at step 8480: 0.3364
Loss at step 8490: 0.0766
Loss at step 8500: 0.4300
Loss at step 8510: 0.7372
Loss at step 8520: 0.0968
Loss at step 8530: 0.0292
Loss at step 8540: 0.4100
Loss at step 8550: 0.2611
Loss at step 8560: 0.2474
Loss at step 8570: 0.1951
Loss at step 8580: 0.2036
Loss at step 8590: 0.3546
Loss at step 8600: 0.2748
Loss at step 8610: 0.0967
Loss at step 8620: 0.4799
Loss at step 8630: 0.0854
Loss at step 8640: 0.2289
Loss at step 8650: 0.4358
Loss at step 8660: 0.1049
Loss at step 8670: 0.3271
Loss at step 8680: 2.0760
Loss at step 8690: 0.2986
Loss at step 8700: 0.0938
Loss at step 8710: 0.2844
Loss at step 8720: 0.4162
Loss at step 8730: 0.8187
Loss at step 8740: 0.5296
Loss at step 8750: 1.0328
Loss at step 8760: 0.0844
Loss at step 8770: 0.9052
Loss at step 8780: 0.1137
Loss at step 8790: 0.5980
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.56312, 'precision': [0.710779, 0.494578, 0.529452], 'recall': [0.540804, 0.666396, 0.447597], 'f1': [0.614249, 0.567773, 0.485096]}
{'accuracy': 0.753381, 'precision': 0.529452, 'recall': 0.447597, 'f1': 0.485096, 'WordR': 0.337194}
Loss at step 8800: 0.0791
Loss at step 8810: 0.6398
Loss at step 8820: 0.0150
Loss at step 8830: 0.0144
Loss at step 8840: 0.2336
Loss at step 8850: 0.0796
Loss at step 8860: 0.4702
Loss at step 8870: 0.7819
Loss at step 8880: 0.2550
Loss at step 8890: 0.9744
Loss at step 8900: 0.4170
Loss at step 8910: 0.0532
Loss at step 8920: 0.0425
Loss at step 8930: 0.1736
Loss at step 8940: 0.3207
Loss at step 8950: 0.8622
Loss at step 8960: 0.2246
Loss at step 8970: 0.7054
Loss at step 8980: 0.9113
Loss at step 8990: 0.1894
Loss at step 9000: 0.5151
Loss at step 9010: 0.2266
Loss at step 9020: 0.0911
Loss at step 9030: 0.0345
Loss at step 9040: 0.0508
Loss at step 9050: 0.1211
Loss at step 9060: 0.2134
Loss at step 9070: 0.0129
Loss at step 9080: 0.4594
Loss at step 9090: 0.4785
Loss at step 9100: 1.6490
Loss at step 9110: 0.0678
Loss at step 9120: 0.4488
Loss at step 9130: 0.6176
Loss at step 9140: 0.1157
Loss at step 9150: 1.1470
Loss at step 9160: 0.2955
Loss at step 9170: 0.3050
Loss at step 9180: 0.3970
Loss at step 9190: 0.2066
Loss at step 9200: 0.5053
Loss at step 9210: 0.0011
Loss at step 9220: 2.2860
Loss at step 9230: 0.6127
Loss at step 9240: 0.0920
Loss at step 9250: 0.1083
Loss at step 9260: 0.5777
Loss at step 9270: 1.0504
Loss at step 9280: 0.4448
Loss at step 9290: 0.1662
Loss at step 9300: 1.1474
Loss at step 9310: 0.6968
Loss at step 9320: 0.8430
Loss at step 9330: 0.1473
Loss at step 9340: 0.2798
Loss at step 9350: 1.7292
Loss at step 9360: 0.5065
Loss at step 9370: 0.2993
Loss at step 9380: 0.0111
Loss at step 9390: 0.5613
Loss at step 9400: 0.0055
Loss at step 9410: 0.0019
Loss at step 9420: 0.1073
Loss at step 9430: 0.0151
Loss at step 9440: 0.0873
Loss at step 9450: 0.2401
Loss at step 9460: 0.0310
Loss at step 9470: 1.5292
Loss at step 9480: 0.0955
Loss at step 9490: 0.1295
Loss at step 9500: 0.5248
Loss at step 9510: 0.9768
Loss at step 9520: 0.3732
Loss at step 9530: 0.0606
Loss at step 9540: 0.1378
Loss at step 9550: 1.2588
Loss at step 9560: 0.7476
Loss at step 9570: 0.2036
Loss at step 9580: 0.0398
Loss at step 9590: 0.3016
Loss at step 9600: 0.2733
Loss at step 9610: 0.6459
Loss at step 9620: 0.4496
Loss at step 9630: 0.6490
Loss at step 9640: 0.4856
Loss at step 9650: 0.6299
Loss at step 9660: 0.0016
Loss at step 9670: 0.0025
Loss at step 9680: 0.0068
Loss at step 9690: 0.3559
Loss at step 9700: 0.5717
Loss at step 9710: 0.0476
Loss at step 9720: 0.1642
Loss at step 9730: 0.1684
Loss at step 9740: 0.3752
Loss at step 9750: 0.1059
Loss at step 9760: 0.0293
Loss at step 9770: 0.1717
Loss at step 9780: 0.0609
Loss at step 9790: 0.4075
Loss at step 9800: 0.2222
Loss at step 9810: 0.2088
Loss at step 9820: 0.8333
Loss at step 9830: 0.0079
Loss at step 9840: 0.0589
Loss at step 9850: 0.3754
Loss at step 9860: 0.0534
Loss at step 9870: 0.1470
Loss at step 9880: 0.7234
Loss at step 9890: 0.3991
Loss at step 9900: 0.5490
Loss at step 9910: 0.5872
Loss at step 9920: 0.1887
Loss at step 9930: 0.0718
Loss at step 9940: 0.3732
Loss at step 9950: 0.1823
Loss at step 9960: 0.0974
Loss at step 9970: 0.0030
Loss at step 9980: 1.9184
Loss at step 9990: 1.1186
Loss at step 10000: 0.0020
Loss at step 10010: 0.0096
Loss at step 10020: 0.1069
Loss at step 10030: 0.5979
Loss at step 10040: 0.5042
Loss at step 10050: 0.0069
Loss at step 10060: 1.4616
Loss at step 10070: 0.0036
Loss at step 10080: 1.0786
Loss at step 10090: 0.2845
Loss at step 10100: 0.3277
Loss at step 10110: 0.2904
Loss at step 10120: 0.5252
Loss at step 10130: 0.2345
Loss at step 10140: 0.1555
Loss at step 10150: 0.6036
Loss at step 10160: 0.0040
Loss at step 10170: 0.0128
Loss at step 10180: 0.0086
Loss at step 10190: 0.6248
Loss at step 10200: 0.4131
Loss at step 10210: 0.1347
Loss at step 10220: 1.5724
Loss at step 10230: 0.8708
Loss at step 10240: 0.0037
Loss at step 10250: 0.0357
Loss at step 10260: 0.1293
Loss at step 10270: 0.6906
Loss at step 10280: 1.3217
Loss at step 10290: 0.1456
Loss at step 10300: 0.1673
Loss at step 10310: 0.0514
Loss at step 10320: 0.3166
Loss at step 10330: 0.1835
Loss at step 10340: 0.2456
Loss at step 10350: 0.0187
Loss at step 10360: 0.7050
Loss at step 10370: 0.5351
Loss at step 10380: 0.2718
Loss at step 10390: 0.8090
Loss at step 10400: 0.1011
Loss at step 10410: 0.0071
Loss at step 10420: 0.8762
Loss at step 10430: 1.2313
Loss at step 10440: 0.2219
Loss at step 10450: 0.2142
Loss at step 10460: 0.1942
Loss at step 10470: 0.3959
Loss at step 10480: 0.0292
Loss at step 10490: 0.3266
Loss at step 10500: 0.2476
Loss at step 10510: 0.3547
Loss at step 10520: 1.1836
Loss at step 10530: 0.0122
Loss at step 10540: 0.2142
Loss at step 10550: 0.1374
Loss at step 10560: 0.2492
Loss at step 10570: 0.7981
Loss at step 10580: 0.0091
Loss at step 10590: 0.0815
Loss at step 10600: 0.0333
Loss at step 10610: 0.0420
Loss at step 10620: 0.2107
Loss at step 10630: 0.0011
Loss at step 10640: 0.7649
Loss at step 10650: 0.0542
Loss at step 10660: 0.0668
Loss at step 10670: 1.4711
Loss at step 10680: 0.4613
Loss at step 10690: 0.5695
Loss at step 10700: 0.0119
Loss at step 10710: 0.1704
Loss at step 10720: 0.0430
Loss at step 10730: 0.2142
Loss at step 10740: 0.0121
Loss at step 10750: 0.3351
Loss at step 10760: 1.4492
Loss at step 10770: 0.8188
Loss at step 10780: 0.2441
Loss at step 10790: 0.0604
Loss at step 10800: 0.8000
Loss at step 10810: 0.2668
Loss at step 10820: 0.7514
Loss at step 10830: 0.0003
Loss at step 10840: 0.4905
Loss at step 10850: 0.9953
Loss at step 10860: 3.3628
Loss at step 10870: 0.2533
Loss at step 10880: 0.0027
Loss at step 10890: 0.1099
Loss at step 10900: 0.0002
Loss at step 10910: 0.2382
Loss at step 10920: 0.4562
Loss at step 10930: 0.0717
Loss at step 10940: 0.1823
Loss at step 10950: 0.9445
Loss at step 10960: 0.6358
Loss at step 10970: 1.0775
Loss at step 10980: 0.0472
Loss at step 10990: 0.7561
Loss at step 11000: 0.0757
Loss at step 11010: 0.1313
Loss at step 11020: 0.1511
Loss at step 11030: 0.0062
Loss at step 11040: 0.2150
Loss at step 11050: 0.4000
Loss at step 11060: 0.0983
Loss at step 11070: 0.9487
Loss at step 11080: 0.1541
Loss at step 11090: 0.5614
Loss at step 11100: 0.1254
Loss at step 11110: 0.0036
Loss at step 11120: 0.3218
Loss at step 11130: 0.5732
Loss at step 11140: 0.0545
Loss at step 11150: 0.4396
Loss at step 11160: 0.0002
Loss at step 11170: 0.1097
Loss at step 11180: 0.3644
Loss at step 11190: 0.1751
Loss at step 11200: 0.2416
Loss at step 11210: 0.0103
Loss at step 11220: 0.6818
Loss at step 11230: 0.2722
Loss at step 11240: 0.0088
Loss at step 11250: 0.2551
Loss at step 11260: 0.0110
Loss at step 11270: 0.0794
Loss at step 11280: 0.0871
Loss at step 11290: 0.5394
Loss at step 11300: 0.0104
Loss at step 11310: 0.6616
Loss at step 11320: 0.5550
Loss at step 11330: 0.1443
Loss at step 11340: 0.3027
Loss at step 11350: 0.0434
Loss at step 11360: 0.0452
Loss at step 11370: 0.0117
Loss at step 11380: 0.0728
Loss at step 11390: 0.0078
Loss at step 11400: 0.2426
Loss at step 11410: 0.0247
Loss at step 11420: 0.5394
Loss at step 11430: 0.0483
Loss at step 11440: 0.4032
Loss at step 11450: 1.0771
Loss at step 11460: 0.0156
Loss at step 11470: 0.0119
Loss at step 11480: 0.5666
Loss at step 11490: 0.6759
Loss at step 11500: 0.1937
Loss at step 11510: 0.0175
Loss at step 11520: 0.0245
Loss at step 11530: 0.8121
Loss at step 11540: 2.1051
Loss at step 11550: 0.7025
Loss at step 11560: 0.0580
Loss at step 11570: 0.0296
Loss at step 11580: 0.4602
Loss at step 11590: 0.0992
Loss at step 11600: 0.9007
Loss at step 11610: 0.3677
Loss at step 11620: 0.0139
Loss at step 11630: 0.1878
Loss at step 11640: 0.2744
Loss at step 11650: 0.3384
Loss at step 11660: 0.0034
Loss at step 11670: 0.1878
Loss at step 11680: 0.4736
Loss at step 11690: 0.9618
Loss at step 11700: 0.0298
Loss at step 11710: 0.4597
Loss at step 11720: 0.0106
Loss at step 11730: 0.0285
Loss at step 11740: 0.0115
Loss at step 11750: 0.2516
Loss at step 11760: 0.2324
Loss at step 11770: 0.0022
Loss at step 11780: 0.1411
Loss at step 11790: 0.2304
Loss at step 11800: 0.0255
Loss at step 11810: 0.0657
Loss at step 11820: 1.1894
Loss at step 11830: 0.0126
Loss at step 11840: 0.0056
Loss at step 11850: 0.9272
Loss at step 11860: 0.0084
Loss at step 11870: 0.3492
Loss at step 11880: 0.7411
Loss at step 11890: 0.7486
Loss at step 11900: 0.0027
Loss at step 11910: 0.0837
Loss at step 11920: 0.0227
Loss at step 11930: 0.0421
Loss at step 11940: 0.0047
Loss at step 11950: 0.0016
Loss at step 11960: 0.4275
Loss at step 11970: 0.1083
Loss at step 11980: 0.1591
Loss at step 11990: 0.0215
Loss at step 12000: 0.4616
Loss at step 12010: 0.4198
Loss at step 12020: 0.2474
Loss at step 12030: 0.0971
Loss at step 12040: 0.3024
Loss at step 12050: 0.1756
Loss at step 12060: 0.8128
Loss at step 12070: 0.0140
Loss at step 12080: 0.6650
Loss at step 12090: 0.1218
Loss at step 12100: 0.0005
Loss at step 12110: 0.0030
Loss at step 12120: 0.9967
Loss at step 12130: 0.9707
Loss at step 12140: 0.1544
Loss at step 12150: 0.2232
Loss at step 12160: 0.6282
Loss at step 12170: 0.0307
Loss at step 12180: 0.2411
Loss at step 12190: 0.0143
Loss at step 12200: 0.3365
Loss at step 12210: 0.2976
Loss at step 12220: 0.0044
Loss at step 12230: 0.0012
Loss at step 12240: 0.0828
Loss at step 12250: 1.1182
Loss at step 12260: 0.2991
Loss at step 12270: 0.0385
Loss at step 12280: 0.0029
Loss at step 12290: 0.0776
Loss at step 12300: 0.0211
Loss at step 12310: 0.0037
Loss at step 12320: 0.0180
Loss at step 12330: 1.9823
Loss at step 12340: 0.9377
Loss at step 12350: 0.0018
Loss at step 12360: 0.4401
Loss at step 12370: 0.0091
Loss at step 12380: 1.7163
Loss at step 12390: 0.1473
Loss at step 12400: 0.0465
Loss at step 12410: 0.0908
Loss at step 12420: 0.1094
Loss at step 12430: 0.0056
Loss at step 12440: 0.0523
Loss at step 12450: 0.0185
Loss at step 12460: 0.0278
Loss at step 12470: 1.0892
Loss at step 12480: 0.6668
Loss at step 12490: 0.0295
Loss at step 12500: 0.1519
Loss at step 12510: 0.0978
Loss at step 12520: 0.6042
Loss at step 12530: 0.0057
Loss at step 12540: 1.1799
Loss at step 12550: 0.0372
Loss at step 12560: 0.2428
Loss at step 12570: 0.7250
Loss at step 12580: 0.3856
Loss at step 12590: 0.2185
Loss at step 12600: 0.3506
Loss at step 12610: 0.0410
Loss at step 12620: 0.3675
Loss at step 12630: 0.1889
Loss at step 12640: 0.0656
Loss at step 12650: 0.0012
Loss at step 12660: 0.7681
Loss at step 12670: 0.0001
Loss at step 12680: 0.0003
Loss at step 12690: 0.0130
Loss at step 12700: 0.2673
Loss at step 12710: 0.0086
Loss at step 12720: 0.2708
Loss at step 12730: 1.2091
Loss at step 12740: 0.1604
Loss at step 12750: 0.0009
Loss at step 12760: 0.6792
Loss at step 12770: 0.0036
Loss at step 12780: 0.1259
Loss at step 12790: 0.9763
Loss at step 12800: 0.2911
Loss at step 12810: 0.0051
Loss at step 12820: 0.5981
Loss at step 12830: 0.0069
Loss at step 12840: 0.2530
Loss at step 12850: 0.7417
Loss at step 12860: 0.0065
Loss at step 12870: 0.0693
Loss at step 12880: 0.0006
Loss at step 12890: 0.0197
Loss at step 12900: 0.3408
Loss at step 12910: 0.0305
Loss at step 12920: 0.2696
Loss at step 12930: 0.1091
Loss at step 12940: 0.1169
Loss at step 12950: 0.5434
Loss at step 12960: 0.0846
Loss at step 12970: 0.1449
Loss at step 12980: 0.3765
Loss at step 12990: 0.0007
Loss at step 13000: 0.2996
Loss at step 13010: 0.0034
Loss at step 13020: 0.0025
Loss at step 13030: 0.3647
Loss at step 13040: 0.1746
Loss at step 13050: 0.2053
Loss at step 13060: 0.7394
Loss at step 13070: 0.0241
Loss at step 13080: 0.0032
Loss at step 13090: 0.5990
Loss at step 13100: 0.1043
Loss at step 13110: 0.0014
Loss at step 13120: 0.0883
Loss at step 13130: 0.0006
Loss at step 13140: 0.8601
Loss at step 13150: 0.0013
Loss at step 13160: 0.5347
Loss at step 13170: 0.1224
Loss at step 13180: 0.0003
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.570183, 'precision': [0.715719, 0.494565, 0.572581], 'recall': [0.521315, 0.701705, 0.452229], 'f1': [0.603242, 0.580201, 0.505338]}
{'accuracy': 0.770213, 'precision': 0.572581, 'recall': 0.452229, 'f1': 0.505338, 'WordR': 0.495451}
Loss at step 13190: 0.0041
Loss at step 13200: 0.3052
Loss at step 13210: 0.0153
Loss at step 13220: 0.0028
Loss at step 13230: 0.0929
Loss at step 13240: 0.0065
Loss at step 13250: 0.2205
Loss at step 13260: 0.0006
Loss at step 13270: 0.0012
Loss at step 13280: 0.0001
Loss at step 13290: 0.0001
Loss at step 13300: 0.7650
Loss at step 13310: 0.2860
Loss at step 13320: 0.0268
Loss at step 13330: 0.1045
Loss at step 13340: 0.0217
Loss at step 13350: 0.3107
Loss at step 13360: 0.0002
Loss at step 13370: 0.0004
Loss at step 13380: 0.0003
Loss at step 13390: 0.0231
Loss at step 13400: 0.0449
Loss at step 13410: 0.1644
Loss at step 13420: 0.0126
Loss at step 13430: 0.0454
Loss at step 13440: 0.1511
Loss at step 13450: 1.5896
Loss at step 13460: 0.0071
Loss at step 13470: 0.1705
Loss at step 13480: 0.0038
Loss at step 13490: 0.0186
Loss at step 13500: 2.1486
Loss at step 13510: 0.0019
Loss at step 13520: 0.0110
Loss at step 13530: 0.1194
Loss at step 13540: 0.5319
Loss at step 13550: 0.1366
Loss at step 13560: 0.0012
Loss at step 13570: 0.0004
Loss at step 13580: 0.3413
Loss at step 13590: 0.3727
Loss at step 13600: 0.0951
Loss at step 13610: 0.0103
Loss at step 13620: 2.9873
Loss at step 13630: 0.0013
Loss at step 13640: 0.1301
Loss at step 13650: 0.5983
Loss at step 13660: 0.0176
Loss at step 13670: 0.0001
Loss at step 13680: 0.0252
Loss at step 13690: 0.0001
Loss at step 13700: 0.0039
Loss at step 13710: 0.6975
Loss at step 13720: 0.0601
Loss at step 13730: 0.0001
Loss at step 13740: 0.4239
Loss at step 13750: 0.4726
Loss at step 13760: 0.0003
Loss at step 13770: 0.0304
Loss at step 13780: 0.0259
Loss at step 13790: 0.1156
Loss at step 13800: 0.0010
Loss at step 13810: 0.3708
Loss at step 13820: 1.1309
Loss at step 13830: 0.0074
Loss at step 13840: 0.1851
Loss at step 13850: 0.0317
Loss at step 13860: 0.0011
Loss at step 13870: 0.6251
Loss at step 13880: 0.0001
Loss at step 13890: 0.3874
Loss at step 13900: 0.0001
Loss at step 13910: 0.0320
Loss at step 13920: 0.1593
Loss at step 13930: 0.0034
Loss at step 13940: 0.0001
Loss at step 13950: 0.4421
Loss at step 13960: 0.0061
Loss at step 13970: 0.6293
Loss at step 13980: 0.0009
Loss at step 13990: 0.0053
Loss at step 14000: 0.3293
Loss at step 14010: 0.0018
Loss at step 14020: 0.5963
Loss at step 14030: 0.0565
Loss at step 14040: 0.1357
Loss at step 14050: 0.2531
Loss at step 14060: 0.0549
Loss at step 14070: 0.0011
Loss at step 14080: 0.0096
Loss at step 14090: 0.0803
Loss at step 14100: 0.5234
Loss at step 14110: 0.0524
Loss at step 14120: 0.1008
Loss at step 14130: 0.0054
Loss at step 14140: 0.3069
Loss at step 14150: 2.2156
Loss at step 14160: 0.7032
Loss at step 14170: 0.1359
Loss at step 14180: 0.3238
Loss at step 14190: 0.0255
Loss at step 14200: 0.0126
Loss at step 14210: 0.0022
Loss at step 14220: 0.0026
Loss at step 14230: 0.0862
Loss at step 14240: 0.0032
Loss at step 14250: 0.0151
Loss at step 14260: 0.0002
Loss at step 14270: 0.1161
Loss at step 14280: 0.2798
Loss at step 14290: 0.0316
Loss at step 14300: 0.1574
Loss at step 14310: 0.0011
Loss at step 14320: 0.0001
Loss at step 14330: 0.6711
Loss at step 14340: 0.0022
Loss at step 14350: 0.0022
Loss at step 14360: 0.0011
Loss at step 14370: 0.0349
Loss at step 14380: 0.0028
Loss at step 14390: 1.1757
Loss at step 14400: 0.1345
Loss at step 14410: 0.0107
Loss at step 14420: 0.0012
Loss at step 14430: 0.0007
Loss at step 14440: 0.0006
Loss at step 14450: 0.3194
Loss at step 14460: 0.2665
Loss at step 14470: 0.0215
Loss at step 14480: 0.0559
Loss at step 14490: 0.0006
Loss at step 14500: 0.0872
Loss at step 14510: 0.0022
Loss at step 14520: 1.5542
Loss at step 14530: 0.1212
Loss at step 14540: 0.0001
Loss at step 14550: 0.3199
Loss at step 14560: 0.0150
Loss at step 14570: 0.0632
Loss at step 14580: 0.2531
Loss at step 14590: 0.0328
Loss at step 14600: 0.0002
Loss at step 14610: 1.0387
Loss at step 14620: 0.4401
Loss at step 14630: 0.0001
Loss at step 14640: 0.3569
Loss at step 14650: 0.1879
Loss at step 14660: 0.5506
Loss at step 14670: 0.0087
Loss at step 14680: 0.0294
Loss at step 14690: 0.0200
Loss at step 14700: 0.0003
Loss at step 14710: 0.0001
Loss at step 14720: 0.0408
Loss at step 14730: 0.0002
Loss at step 14740: 0.5916
Loss at step 14750: 0.2531
Loss at step 14760: 0.0078
Loss at step 14770: 0.2620
Loss at step 14780: 0.5097
Loss at step 14790: 0.3015
Loss at step 14800: 0.0027
Loss at step 14810: 0.0713
Loss at step 14820: 0.0058
Loss at step 14830: 0.0035
Loss at step 14840: 1.3967
Loss at step 14850: 2.7083
Loss at step 14860: 0.0058
Loss at step 14870: 0.4624
Loss at step 14880: 0.0159
Loss at step 14890: 0.0001
Loss at step 14900: 0.0128
Loss at step 14910: 0.0227
Loss at step 14920: 0.2027
Loss at step 14930: 0.0018
Loss at step 14940: 0.0066
Loss at step 14950: 0.0004
Loss at step 14960: 0.0070
Loss at step 14970: 0.1587
Loss at step 14980: 0.0015
Loss at step 14990: 0.0053
Loss at step 15000: 0.4771
Loss at step 15010: 0.3432
Loss at step 15020: 0.0004
Loss at step 15030: 0.0001
Loss at step 15040: 0.0018
Loss at step 15050: 0.0001
Loss at step 15060: 0.0002
Loss at step 15070: 0.2906
Loss at step 15080: 0.0003
Loss at step 15090: 0.0002
Loss at step 15100: 0.4564
Loss at step 15110: 0.0000
Loss at step 15120: 0.0792
Loss at step 15130: 0.0037
Loss at step 15140: 0.0053
Loss at step 15150: 0.3954
Loss at step 15160: 0.0007
Loss at step 15170: 0.0009
Loss at step 15180: 0.5548
Loss at step 15190: 0.1406
Loss at step 15200: 0.0818
Loss at step 15210: 0.0002
Loss at step 15220: 0.0009
Loss at step 15230: 0.1096
Loss at step 15240: 0.0008
Loss at step 15250: 0.8593
Loss at step 15260: 0.0001
Loss at step 15270: 0.0004
Loss at step 15280: 1.4027
Loss at step 15290: 0.0026
Loss at step 15300: 0.0284
Loss at step 15310: 0.0162
Loss at step 15320: 0.8357
Loss at step 15330: 0.0172
Loss at step 15340: 0.0744
Loss at step 15350: 0.0113
Loss at step 15360: 0.0012
Loss at step 15370: 0.3557
Loss at step 15380: 0.0027
Loss at step 15390: 0.0258
Loss at step 15400: 0.0002
Loss at step 15410: 0.1476
Loss at step 15420: 0.8906
Loss at step 15430: 0.4035
Loss at step 15440: 0.0456
Loss at step 15450: 0.0245
Loss at step 15460: 0.0123
Loss at step 15470: 0.1670
Loss at step 15480: 0.6635
Loss at step 15490: 0.5508
Loss at step 15500: 0.8970
Loss at step 15510: 0.0152
Loss at step 15520: 0.0906
Loss at step 15530: 0.0023
Loss at step 15540: 0.0083
Loss at step 15550: 0.4650
Loss at step 15560: 0.0004
Loss at step 15570: 0.3812
Loss at step 15580: 0.0043
Loss at step 15590: 0.3839
Loss at step 15600: 0.0007
Loss at step 15610: 0.1436
Loss at step 15620: 0.0311
Loss at step 15630: 0.0002
Loss at step 15640: 0.0007
Loss at step 15650: 0.0013
Loss at step 15660: 0.0019
Loss at step 15670: 0.0343
Loss at step 15680: 0.0226
Loss at step 15690: 0.0011
Loss at step 15700: 0.0020
Loss at step 15710: 0.0001
Loss at step 15720: 0.0124
Loss at step 15730: 0.0076
Loss at step 15740: 0.0014
Loss at step 15750: 0.1422
Loss at step 15760: 0.0022
Loss at step 15770: 0.0003
Loss at step 15780: 0.0003
Loss at step 15790: 0.0962
Loss at step 15800: 0.0033
Loss at step 15810: 0.0003
Loss at step 15820: 0.4435
Loss at step 15830: 0.0017
Loss at step 15840: 0.0007
Loss at step 15850: 0.0013
Loss at step 15860: 1.9861
Loss at step 15870: 0.1884
Loss at step 15880: 0.0003
Loss at step 15890: 0.0000
Loss at step 15900: 0.0133
Loss at step 15910: 0.2187
Loss at step 15920: 0.9797
Loss at step 15930: 0.1495
Loss at step 15940: 0.0000
Loss at step 15950: 0.0001
Loss at step 15960: 0.0050
Loss at step 15970: 0.7988
Loss at step 15980: 0.0000
Loss at step 15990: 1.0965
Loss at step 16000: 0.0001
Loss at step 16010: 0.0000
Loss at step 16020: 0.0014
Loss at step 16030: 0.0039
Loss at step 16040: 0.0001
Loss at step 16050: 0.0014
Loss at step 16060: 0.1791
Loss at step 16070: 0.0006
Loss at step 16080: 0.0004
Loss at step 16090: 0.0000
Loss at step 16100: 0.0000
Loss at step 16110: 0.0613
Loss at step 16120: 0.0713
Loss at step 16130: 0.1823
Loss at step 16140: 0.3030
Loss at step 16150: 0.7028
Loss at step 16160: 0.0053
Loss at step 16170: 0.0008
Loss at step 16180: 0.0015
Loss at step 16190: 0.0006
Loss at step 16200: 0.0007
Loss at step 16210: 0.0008
Loss at step 16220: 0.0007
Loss at step 16230: 0.0001
Loss at step 16240: 0.0005
Loss at step 16250: 0.0015
Loss at step 16260: 0.1955
Loss at step 16270: 0.0026
Loss at step 16280: 0.0994
Loss at step 16290: 0.0006
Loss at step 16300: 0.3543
Loss at step 16310: 0.0009
Loss at step 16320: 0.5678
Loss at step 16330: 0.0097
Loss at step 16340: 0.0002
Loss at step 16350: 1.2889
Loss at step 16360: 0.0203
Loss at step 16370: 1.5221
Loss at step 16380: 0.0005
Loss at step 16390: 0.0011
Loss at step 16400: 0.9114
Loss at step 16410: 0.2535
Loss at step 16420: 0.0019
Loss at step 16430: 1.5044
Loss at step 16440: 0.0011
Loss at step 16450: 0.0252
Loss at step 16460: 0.0012
Loss at step 16470: 0.0948
Loss at step 16480: 0.4082
Loss at step 16490: 0.1106
Loss at step 16500: 0.0002
Loss at step 16510: 0.0000
Loss at step 16520: 0.0010
Loss at step 16530: 0.0004
Loss at step 16540: 0.0031
Loss at step 16550: 0.5385
Loss at step 16560: 0.0000
Loss at step 16570: 0.3413
Loss at step 16580: 0.0000
Loss at step 16590: 0.0007
Loss at step 16600: 0.0001
Loss at step 16610: 0.0000
Loss at step 16620: 0.0246
Loss at step 16630: 0.0025
Loss at step 16640: 0.0003
Loss at step 16650: 0.0013
Loss at step 16660: 0.0008
Loss at step 16670: 0.0857
Loss at step 16680: 0.0030
Loss at step 16690: 0.0006
Loss at step 16700: 0.0042
Loss at step 16710: 0.2569
Loss at step 16720: 0.0228
Loss at step 16730: 0.0001
Loss at step 16740: 0.0001
Loss at step 16750: 0.0084
Loss at step 16760: 0.5467
Loss at step 16770: 0.0784
Loss at step 16780: 0.0001
Loss at step 16790: 0.0038
Loss at step 16800: 0.0026
Loss at step 16810: 0.3719
Loss at step 16820: 0.4060
Loss at step 16830: 0.0019
Loss at step 16840: 0.0000
Loss at step 16850: 0.0011
Loss at step 16860: 0.0154
Loss at step 16870: 0.0000
Loss at step 16880: 0.0149
Loss at step 16890: 0.0024
Loss at step 16900: 0.0000
Loss at step 16910: 0.0052
Loss at step 16920: 0.0001
Loss at step 16930: 0.6428
Loss at step 16940: 0.1016
Loss at step 16950: 0.0064
Loss at step 16960: 0.0186
Loss at step 16970: 0.0011
Loss at step 16980: 0.0001
Loss at step 16990: 1.3627
Loss at step 17000: 0.9533
Loss at step 17010: 0.0005
Loss at step 17020: 0.0004
Loss at step 17030: 0.5403
Loss at step 17040: 0.0016
Loss at step 17050: 0.5984
Loss at step 17060: 0.0026
Loss at step 17070: 0.0014
Loss at step 17080: 0.0017
Loss at step 17090: 0.0999
Loss at step 17100: 0.0007
Loss at step 17110: 0.2076
Loss at step 17120: 0.4952
Loss at step 17130: 0.0002
Loss at step 17140: 0.0002
Loss at step 17150: 0.7653
Loss at step 17160: 0.0031
Loss at step 17170: 0.0217
Loss at step 17180: 0.1081
Loss at step 17190: 0.0009
Loss at step 17200: 0.0214
Loss at step 17210: 0.0069
Loss at step 17220: 0.1150
Loss at step 17230: 0.4504
Loss at step 17240: 0.0001
Loss at step 17250: 0.6073
Loss at step 17260: 0.0018
Loss at step 17270: 0.0224
Loss at step 17280: 0.0004
Loss at step 17290: 0.0003
Loss at step 17300: 0.0360
Loss at step 17310: 0.0145
Loss at step 17320: 0.0005
Loss at step 17330: 0.3333
Loss at step 17340: 0.0060
Loss at step 17350: 0.0037
Loss at step 17360: 0.0006
Loss at step 17370: 0.0000
Loss at step 17380: 0.0126
Loss at step 17390: 0.0007
Loss at step 17400: 0.0040
Loss at step 17410: 0.0746
Loss at step 17420: 0.0057
Loss at step 17430: 0.0007
Loss at step 17440: 0.0000
Loss at step 17450: 0.0000
Loss at step 17460: 0.0066
Loss at step 17470: 0.1018
Loss at step 17480: 0.0097
Loss at step 17490: 0.0012
Loss at step 17500: 0.0000
Loss at step 17510: 0.0044
Loss at step 17520: 0.0006
Loss at step 17530: 7.0421
Loss at step 17540: 0.0001
Loss at step 17550: 0.5544
Loss at step 17560: 0.0003
Loss at step 17570: 0.0629
Loss at step 17580: 0.0597
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.581605, 'precision': [0.730638, 0.517305, 0.549338], 'recall': [0.520909, 0.679383, 0.528662], 'f1': [0.608201, 0.587368, 0.538802]}
{'accuracy': 0.765104, 'precision': 0.549338, 'recall': 0.528662, 'f1': 0.538802, 'WordR': 0.495451}
Loss at step 17590: 0.0001
Loss at step 17600: 0.0204
Loss at step 17610: 0.0163
Loss at step 17620: 0.0056
Loss at step 17630: 0.6013
Loss at step 17640: 1.0246
Loss at step 17650: 0.0000
Loss at step 17660: 0.0001
Loss at step 17670: 0.0004
Loss at step 17680: 0.0005
Loss at step 17690: 0.0014
Loss at step 17700: 0.0227
Loss at step 17710: 0.0003
Loss at step 17720: 0.0015
Loss at step 17730: 0.0001
Loss at step 17740: 0.0167
Loss at step 17750: 0.0005
Loss at step 17760: 0.2824
Loss at step 17770: 0.0004
Loss at step 17780: 0.0372
Loss at step 17790: 0.0322
Loss at step 17800: 0.0002
Loss at step 17810: 0.5595
Loss at step 17820: 0.0001
Loss at step 17830: 0.1205
Loss at step 17840: 0.0004
Loss at step 17850: 0.3564
Loss at step 17860: 0.0002
Loss at step 17870: 0.1873
Loss at step 17880: 0.0001
Loss at step 17890: 0.1558
Loss at step 17900: 0.1877
Loss at step 17910: 0.0000
Loss at step 17920: 0.0954
Loss at step 17930: 0.0007
Loss at step 17940: 0.0002
Loss at step 17950: 0.5152
Loss at step 17960: 0.0001
Loss at step 17970: 0.0001
Loss at step 17980: 0.0001
Loss at step 17990: 0.3046
Loss at step 18000: 0.0395
Loss at step 18010: 0.0000
Loss at step 18020: 0.0001
Loss at step 18030: 0.0002
Loss at step 18040: 0.0000
Loss at step 18050: 0.3374
Loss at step 18060: 0.0003
Loss at step 18070: 0.0055
Loss at step 18080: 0.0041
Loss at step 18090: 0.0001
Loss at step 18100: 0.0001
Loss at step 18110: 0.7160
Loss at step 18120: 0.0002
Loss at step 18130: 0.0001
Loss at step 18140: 0.4918
Loss at step 18150: 0.0004
Loss at step 18160: 0.2990
Loss at step 18170: 0.0010
Loss at step 18180: 0.0053
Loss at step 18190: 0.0051
Loss at step 18200: 0.0000
Loss at step 18210: 0.0001
Loss at step 18220: 0.2920
Loss at step 18230: 0.3949
Loss at step 18240: 0.0011
Loss at step 18250: 0.3862
Loss at step 18260: 0.0010
Loss at step 18270: 0.0001
Loss at step 18280: 0.2245
Loss at step 18290: 0.0082
Loss at step 18300: 0.0004
Loss at step 18310: 0.0106
Loss at step 18320: 0.8650
Loss at step 18330: 0.1613
Loss at step 18340: 0.0004
Loss at step 18350: 0.0008
Loss at step 18360: 0.0463
Loss at step 18370: 0.7316
Loss at step 18380: 0.0002
Loss at step 18390: 0.0001
Loss at step 18400: 0.0004
Loss at step 18410: 0.0006
Loss at step 18420: 0.0175
Loss at step 18430: 0.0003
Loss at step 18440: 0.0006
Loss at step 18450: 0.0036
Loss at step 18460: 0.8371
Loss at step 18470: 0.0015
Loss at step 18480: 0.0005
Loss at step 18490: 0.3790
Loss at step 18500: 0.0000
Loss at step 18510: 0.0001
Loss at step 18520: 0.0179
Loss at step 18530: 0.0021
Loss at step 18540: 0.8930
Loss at step 18550: 0.0003
Loss at step 18560: 0.0001
Loss at step 18570: 0.2597
Loss at step 18580: 0.8143
Loss at step 18590: 0.0002
Loss at step 18600: 0.0006
Loss at step 18610: 0.0001
Loss at step 18620: 0.0000
Loss at step 18630: 0.0008
Loss at step 18640: 0.7091
Loss at step 18650: 0.0026
Loss at step 18660: 0.0038
Loss at step 18670: 0.0021
Loss at step 18680: 0.0004
Loss at step 18690: 0.0000
Loss at step 18700: 0.0000
Loss at step 18710: 0.0297
Loss at step 18720: 0.0152
Loss at step 18730: 0.0022
Loss at step 18740: 0.0095
Loss at step 18750: 0.8165
Loss at step 18760: 0.0114
Loss at step 18770: 0.0006
Loss at step 18780: 0.0000
Loss at step 18790: 0.0000
Loss at step 18800: 0.0024
Loss at step 18810: 0.0000
Loss at step 18820: 0.0001
Loss at step 18830: 0.0001
Loss at step 18840: 0.3375
Loss at step 18850: 0.0096
Loss at step 18860: 0.2799
Loss at step 18870: 0.0001
Loss at step 18880: 0.0001
Loss at step 18890: 0.0000
Loss at step 18900: 0.0185
Loss at step 18910: 0.0303
Loss at step 18920: 0.7643
Loss at step 18930: 0.0000
Loss at step 18940: 0.0002
Loss at step 18950: 0.0002
Loss at step 18960: 0.0000
Loss at step 18970: 1.2233
Loss at step 18980: 0.1564
Loss at step 18990: 0.6547
Loss at step 19000: 0.0001
Loss at step 19010: 0.0111
Loss at step 19020: 0.0000
Loss at step 19030: 0.1479
Loss at step 19040: 0.0146
Loss at step 19050: 0.0005
Loss at step 19060: 0.0005
Loss at step 19070: 0.0023
Loss at step 19080: 0.0000
Loss at step 19090: 0.0015
Loss at step 19100: 0.0017
Loss at step 19110: 0.0003
Loss at step 19120: 0.0019
Loss at step 19130: 0.0000
Loss at step 19140: 0.0003
Loss at step 19150: 0.0005
Loss at step 19160: 0.0000
Loss at step 19170: 0.0000
Loss at step 19180: 0.0007
Loss at step 19190: 0.0083
Loss at step 19200: 0.0022
Loss at step 19210: 0.0000
Loss at step 19220: 0.0000
Loss at step 19230: 0.0016
Loss at step 19240: 0.0020
Loss at step 19250: 0.0001
Loss at step 19260: 0.0006
Loss at step 19270: 0.0002
Loss at step 19280: 0.0015
Loss at step 19290: 0.0003
Loss at step 19300: 0.0001
Loss at step 19310: 0.0220
Loss at step 19320: 0.0008
Loss at step 19330: 0.0000
Loss at step 19340: 0.2031
Loss at step 19350: 0.0002
Loss at step 19360: 0.7390
Loss at step 19370: 0.0010
Loss at step 19380: 0.0090
Loss at step 19390: 0.0001
Loss at step 19400: 0.1401
Loss at step 19410: 0.0000
Loss at step 19420: 0.2588
Loss at step 19430: 0.0000
Loss at step 19440: 0.0006
Loss at step 19450: 0.0001
Loss at step 19460: 0.0021
Loss at step 19470: 0.0000
Loss at step 19480: 0.0007
Loss at step 19490: 0.4484
Loss at step 19500: 0.0119
Loss at step 19510: 0.0005
Loss at step 19520: 0.0000
Loss at step 19530: 0.5166
Loss at step 19540: 0.0000
Loss at step 19550: 0.0001
Loss at step 19560: 0.1901
Loss at step 19570: 0.0000
Loss at step 19580: 0.0005
Loss at step 19590: 0.0000
Loss at step 19600: 0.0000
Loss at step 19610: 0.3185
Loss at step 19620: 0.1034
Loss at step 19630: 0.0001
Loss at step 19640: 0.0002
Loss at step 19650: 0.1874
Loss at step 19660: 0.0022
Loss at step 19670: 0.0003
Loss at step 19680: 0.1590
Loss at step 19690: 0.0657
Loss at step 19700: 0.0000
Loss at step 19710: 0.0716
Loss at step 19720: 0.0004
Loss at step 19730: 0.0000
Loss at step 19740: 0.0020
Loss at step 19750: 14.4030
Loss at step 19760: 0.0101
Loss at step 19770: 0.0000
Loss at step 19780: 0.0000
Loss at step 19790: 0.0000
Loss at step 19800: 0.0001
Loss at step 19810: 0.0001
Loss at step 19820: 0.0000
Loss at step 19830: 0.0172
Loss at step 19840: 0.0010
Loss at step 19850: 0.0187
Loss at step 19860: 0.0000
Loss at step 19870: 0.0001
Loss at step 19880: 0.0000
Loss at step 19890: 0.1473
Loss at step 19900: 0.0296
Loss at step 19910: 0.0000
Loss at step 19920: 0.0000
Loss at step 19930: 0.1718
Loss at step 19940: 0.0031
Loss at step 19950: 0.0003
Loss at step 19960: 0.3687
Loss at step 19970: 0.0050
Loss at step 19980: 0.0002
Loss at step 19990: 0.0595
Loss at step 20000: 0.0000
Loss at step 20010: 0.3502
Loss at step 20020: 0.1729
Loss at step 20030: 0.6478
Loss at step 20040: 0.0012
Loss at step 20050: 0.0001
Loss at step 20060: 0.0044
Loss at step 20070: 0.0007
Loss at step 20080: 0.0629
Loss at step 20090: 0.0000
Loss at step 20100: 0.0008
Loss at step 20110: 0.3915
Loss at step 20120: 0.0048
Loss at step 20130: 0.0000
Loss at step 20140: 0.0000
Loss at step 20150: 0.0007
Loss at step 20160: 0.1197
Loss at step 20170: 0.0001
Loss at step 20180: 0.0000
Loss at step 20190: 0.0001
Loss at step 20200: 0.0003
Loss at step 20210: 0.0117
Loss at step 20220: 0.0000
Loss at step 20230: 0.0035
Loss at step 20240: 0.2210
Loss at step 20250: 0.0075
Loss at step 20260: 0.3586
Loss at step 20270: 0.0002
Loss at step 20280: 1.5567
Loss at step 20290: 0.0152
Loss at step 20300: 0.6857
Loss at step 20310: 0.0090
Loss at step 20320: 0.0022
Loss at step 20330: 0.0008
Loss at step 20340: 0.0002
Loss at step 20350: 0.3245
Loss at step 20360: 0.0005
Loss at step 20370: 0.0629
Loss at step 20380: 0.0005
Loss at step 20390: 0.0055
Loss at step 20400: 0.0685
Loss at step 20410: 0.0030
Loss at step 20420: 0.0064
Loss at step 20430: 0.0093
Loss at step 20440: 0.0001
Loss at step 20450: 0.0004
Loss at step 20460: 0.0001
Loss at step 20470: 0.0090
Loss at step 20480: 0.0011
Loss at step 20490: 0.0243
Loss at step 20500: 0.0018
Loss at step 20510: 0.0162
Loss at step 20520: 0.0000
Loss at step 20530: 0.0000
Loss at step 20540: 0.0003
Loss at step 20550: 0.0480
Loss at step 20560: 0.0000
Loss at step 20570: 0.0001
Loss at step 20580: 0.0000
Loss at step 20590: 0.0001
Loss at step 20600: 0.0002
Loss at step 20610: 1.6276
Loss at step 20620: 0.3051
Loss at step 20630: 0.0001
Loss at step 20640: 1.1360
Loss at step 20650: 0.0001
Loss at step 20660: 0.0001
Loss at step 20670: 0.0007
Loss at step 20680: 0.0001
Loss at step 20690: 0.0000
Loss at step 20700: 0.0000
Loss at step 20710: 0.0031
Loss at step 20720: 0.0061
Loss at step 20730: 0.0003
Loss at step 20740: 0.0002
Loss at step 20750: 0.0004
Loss at step 20760: 0.0002
Loss at step 20770: 0.0000
Loss at step 20780: 0.0001
Loss at step 20790: 0.0001
Loss at step 20800: 0.0002
Loss at step 20810: 0.0000
Loss at step 20820: 0.0105
Loss at step 20830: 0.0000
Loss at step 20840: 0.0023
Loss at step 20850: 0.0002
Loss at step 20860: 0.0958
Loss at step 20870: 0.2616
Loss at step 20880: 0.0311
Loss at step 20890: 0.0000
Loss at step 20900: 0.0003
Loss at step 20910: 0.0000
Loss at step 20920: 0.0001
Loss at step 20930: 0.0004
Loss at step 20940: 0.0567
Loss at step 20950: 0.0000
Loss at step 20960: 0.1969
Loss at step 20970: 0.0054
Loss at step 20980: 0.0000
Loss at step 20990: 0.0001
Loss at step 21000: 0.0001
Loss at step 21010: 0.0001
Loss at step 21020: 0.0001
Loss at step 21030: 0.0000
Loss at step 21040: 0.0001
Loss at step 21050: 0.0026
Loss at step 21060: 0.0002
Loss at step 21070: 0.0001
Loss at step 21080: 0.1257
Loss at step 21090: 0.0001
Loss at step 21100: 0.0004
Loss at step 21110: 0.1379
Loss at step 21120: 0.0002
Loss at step 21130: 0.0001
Loss at step 21140: 0.0001
Loss at step 21150: 0.0075
Loss at step 21160: 0.0004
Loss at step 21170: 0.8064
Loss at step 21180: 0.0001
Loss at step 21190: 0.0004
Loss at step 21200: 0.0015
Loss at step 21210: 0.0001
Loss at step 21220: 0.0000
Loss at step 21230: 0.7671
Loss at step 21240: 0.0004
Loss at step 21250: 0.5048
Loss at step 21260: 0.0000
Loss at step 21270: 0.5573
Loss at step 21280: 0.0002
Loss at step 21290: 0.0001
Loss at step 21300: 0.0000
Loss at step 21310: 0.4703
Loss at step 21320: 0.0002
Loss at step 21330: 0.0003
Loss at step 21340: 0.0000
Loss at step 21350: 0.0009
Loss at step 21360: 0.0530
Loss at step 21370: 0.0002
Loss at step 21380: 0.1321
Loss at step 21390: 0.0000
Loss at step 21400: 0.4753
Loss at step 21410: 0.0000
Loss at step 21420: 0.0022
Loss at step 21430: 0.0000
Loss at step 21440: 0.0004
Loss at step 21450: 0.0003
Loss at step 21460: 0.4004
Loss at step 21470: 0.0003
Loss at step 21480: 0.0344
Loss at step 21490: 0.0001
Loss at step 21500: 0.0000
Loss at step 21510: 0.0001
Loss at step 21520: 0.0019
Loss at step 21530: 1.0359
Loss at step 21540: 0.0134
Loss at step 21550: 0.0375
Loss at step 21560: 0.0000
Loss at step 21570: 0.0002
Loss at step 21580: 0.0000
Loss at step 21590: 0.0000
Loss at step 21600: 0.0000
Loss at step 21610: 0.0001
Loss at step 21620: 0.0001
Loss at step 21630: 0.1894
Loss at step 21640: 0.0014
Loss at step 21650: 0.0006
Loss at step 21660: 0.0002
Loss at step 21670: 0.0900
Loss at step 21680: 0.9250
Loss at step 21690: 0.0003
Loss at step 21700: 0.0006
Loss at step 21710: 0.0002
Loss at step 21720: 0.0002
Loss at step 21730: 0.0001
Loss at step 21740: 0.0008
Loss at step 21750: 0.2318
Loss at step 21760: 0.2763
Loss at step 21770: 0.0001
Loss at step 21780: 0.2520
Loss at step 21790: 0.0018
Loss at step 21800: 0.0004
Loss at step 21810: 0.0001
Loss at step 21820: 0.0001
Loss at step 21830: 0.0000
Loss at step 21840: 0.0008
Loss at step 21850: 0.0001
Loss at step 21860: 0.0000
Loss at step 21870: 0.0000
Loss at step 21880: 0.0000
Loss at step 21890: 0.0001
Loss at step 21900: 0.0001
Loss at step 21910: 0.3091
Loss at step 21920: 0.0002
Loss at step 21930: 0.0001
Loss at step 21940: 0.0081
Loss at step 21950: 0.0001
Loss at step 21960: 0.0007
Loss at step 21970: 0.0001
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/accuracy/bbddc2dafac9b46b0aeeb39c145af710c55e03b223eae89dfe86388f40d9d157 (last modified on Fri Dec 13 14:33:35 2024) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Fri Dec 13 14:33:36 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Fri Dec 13 14:33:38 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Fri Dec 13 14:33:39 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.577397, 'precision': [0.710682, 0.510638, 0.555486], 'recall': [0.537556, 0.662338, 0.513028], 'f1': [0.612113, 0.576678, 0.533414]}
{'accuracy': 0.767057, 'precision': 0.555486, 'recall': 0.513028, 'f1': 0.533414, 'WordR': 0.495451}
