Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x732b2c5afb80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.9618
Loss at step 20: 0.8469
Loss at step 30: 1.0853
Loss at step 40: 0.7949
Loss at step 50: 0.9862
Loss at step 60: 0.9193
Loss at step 70: 1.0093
Loss at step 80: 0.8311
Loss at step 90: 0.7005
Loss at step 100: 0.6766
Loss at step 110: 0.9313
Loss at step 120: 0.7806
Loss at step 130: 0.6531
Loss at step 140: 0.6194
Loss at step 150: 0.6204
Loss at step 160: 0.6911
Loss at step 170: 0.6423
Loss at step 180: 0.7314
Loss at step 190: 0.9085
Loss at step 200: 0.6625
Loss at step 210: 0.5666
Loss at step 220: 0.5469
Loss at step 230: 0.7742
Loss at step 240: 0.5933
Loss at step 250: 0.6156
Loss at step 260: 0.8471
Loss at step 270: 0.8222
Loss at step 280: 0.5842
Loss at step 290: 0.4718
Loss at step 300: 0.5226
Loss at step 310: 0.7503
Loss at step 320: 0.8177
Loss at step 330: 0.8003
Loss at step 340: 0.8121
Loss at step 350: 0.6208
Loss at step 360: 0.4835
Loss at step 370: 0.5998
Loss at step 380: 0.7386
Loss at step 390: 0.7808
Loss at step 400: 0.8078
Loss at step 410: 0.6450
Loss at step 420: 0.5993
Loss at step 430: 0.5761
Loss at step 440: 0.5857
Loss at step 450: 0.4714
Loss at step 460: 0.4760
Loss at step 470: 0.6971
Loss at step 480: 0.5773
Loss at step 490: 0.6079
Loss at step 500: 1.2694
Loss at step 510: 0.5731
Loss at step 520: 0.6714
Loss at step 530: 0.5197
Loss at step 540: 0.7025
Loss at step 550: 0.9478
Loss at step 560: 0.5675
Loss at step 570: 0.5766
Loss at step 580: 0.6903
Loss at step 590: 0.6419
Loss at step 600: 0.6873
Loss at step 610: 0.4903
Loss at step 620: 0.5600
Loss at step 630: 0.6452
Loss at step 640: 0.6980
Loss at step 650: 0.6319
Loss at step 660: 0.4380
Loss at step 670: 0.5265
Loss at step 680: 0.5830
Loss at step 690: 0.7806
Loss at step 700: 0.6871
Loss at step 710: 0.4765
Loss at step 720: 0.5825
Loss at step 730: 0.4785
Loss at step 740: 0.7760
Loss at step 750: 0.6589
Loss at step 760: 0.4539
Loss at step 770: 0.3491
Loss at step 780: 0.4346
Loss at step 790: 0.4812
Loss at step 800: 0.5899
Loss at step 810: 0.4728
Loss at step 820: 0.5215
Loss at step 830: 0.6680
Loss at step 840: 0.5074
Loss at step 850: 0.5132
Loss at step 860: 0.4923
Loss at step 870: 0.5038
Loss at step 880: 0.6806
Loss at step 890: 0.4543
Loss at step 900: 0.5485
Loss at step 910: 0.4745
Loss at step 920: 0.5199
Loss at step 930: 0.4123
Loss at step 940: 0.9690
Loss at step 950: 0.5029
Loss at step 960: 0.4916
Loss at step 970: 0.5344
Loss at step 980: 0.5364
Loss at step 990: 0.4012
Loss at step 1000: 0.4212
Loss at step 1010: 0.4353
Loss at step 1020: 0.6773
Loss at step 1030: 0.5845
Loss at step 1040: 0.4989
Loss at step 1050: 0.5568
Loss at step 1060: 0.4224
Loss at step 1070: 0.6560
Loss at step 1080: 0.4730
Loss at step 1090: 0.6201
Loss at step 1100: 0.5915
Loss at step 1110: 0.7006
Loss at step 1120: 0.3839
Loss at step 1130: 0.4244
Loss at step 1140: 0.4466
Loss at step 1150: 0.4976
Loss at step 1160: 0.5902
Loss at step 1170: 0.4678
Loss at step 1180: 0.5405
Loss at step 1190: 0.3352
Loss at step 1200: 0.5518
Loss at step 1210: 0.5827
Loss at step 1220: 0.3050
Loss at step 1230: 0.4109
Loss at step 1240: 0.4297
Loss at step 1250: 0.6993
Loss at step 1260: 0.5555
Loss at step 1270: 0.8614
Loss at step 1280: 1.0023
Loss at step 1290: 0.2491
Loss at step 1300: 0.4262
Loss at step 1310: 0.7158
Loss at step 1320: 0.3992
Loss at step 1330: 0.4362
Loss at step 1340: 0.4110
Loss at step 1350: 0.4710
Loss at step 1360: 0.6474
Loss at step 1370: 0.5540
Loss at step 1380: 0.4526
Loss at step 1390: 0.6467
Loss at step 1400: 0.2324
Loss at step 1410: 0.2863
Loss at step 1420: 0.5997
Loss at step 1430: 0.4820
Loss at step 1440: 0.3420
Loss at step 1450: 0.3595
Loss at step 1460: 0.3696
Loss at step 1470: 0.5778
Loss at step 1480: 0.3685
Loss at step 1490: 0.3909
Loss at step 1500: 0.5168
Loss at step 1510: 0.7093
Loss at step 1520: 0.3368
Loss at step 1530: 0.4209
Loss at step 1540: 0.4965
Loss at step 1550: 0.4761
Loss at step 1560: 0.3970
Loss at step 1570: 0.7104
Loss at step 1580: 0.3122
Loss at step 1590: 0.3674
Loss at step 1600: 0.3404
Loss at step 1610: 0.4811
Loss at step 1620: 0.4571
Loss at step 1630: 0.2181
Loss at step 1640: 0.7198
Loss at step 1650: 0.5136
Loss at step 1660: 0.5047
Loss at step 1670: 0.3721
Loss at step 1680: 0.3403
Loss at step 1690: 0.5101
Loss at step 1700: 0.2616
Loss at step 1710: 0.5992
Loss at step 1720: 0.2942
Loss at step 1730: 0.3108
Loss at step 1740: 0.5605
Loss at step 1750: 0.3219
Loss at step 1760: 0.3369
Loss at step 1770: 0.3700
Loss at step 1780: 0.3107
Loss at step 1790: 0.5056
Loss at step 1800: 0.2565
Loss at step 1810: 0.3311
Loss at step 1820: 0.4105
Loss at step 1830: 0.2586
Loss at step 1840: 0.2923
Loss at step 1850: 0.6068
Loss at step 1860: 0.3520
Loss at step 1870: 0.1934
Loss at step 1880: 0.3853
Loss at step 1890: 0.4943
Loss at step 1900: 0.2774
Loss at step 1910: 0.3918
Loss at step 1920: 0.2429
Loss at step 1930: 0.4397
Loss at step 1940: 0.3346
Loss at step 1950: 0.3927
Loss at step 1960: 0.3239
Loss at step 1970: 0.3519
Loss at step 1980: 0.3344
Loss at step 1990: 0.5549
Loss at step 2000: 0.3038
Loss at step 2010: 0.6846
Loss at step 2020: 0.7299
Loss at step 2030: 0.1051
Loss at step 2040: 0.4494
Loss at step 2050: 0.2015
Loss at step 2060: 0.4469
Loss at step 2070: 0.4581
Loss at step 2080: 0.2310
Loss at step 2090: 0.2507
Loss at step 2100: 0.6081
Loss at step 2110: 0.4757
Loss at step 2120: 0.3100
Loss at step 2130: 0.1056
Loss at step 2140: 0.3832
Loss at step 2150: 0.7144
Loss at step 2160: 0.3335
Loss at step 2170: 0.3987
Loss at step 2180: 0.4051
Loss at step 2190: 0.4441
Loss at step 2200: 0.2653
Loss at step 2210: 0.6626
Loss at step 2220: 0.4351
Loss at step 2230: 0.2653
Loss at step 2240: 0.3103
Loss at step 2250: 0.4678
Loss at step 2260: 0.2145
Loss at step 2270: 0.2281
Loss at step 2280: 0.4866
Loss at step 2290: 0.3712
Loss at step 2300: 0.2762
Loss at step 2310: 0.1987
Loss at step 2320: 0.1234
Loss at step 2330: 0.3004
Loss at step 2340: 0.2996
Loss at step 2350: 0.4632
Loss at step 2360: 0.2996
Loss at step 2370: 0.5521
Loss at step 2380: 0.1668
Loss at step 2390: 0.2094
Loss at step 2400: 0.3121
Loss at step 2410: 0.4069
Loss at step 2420: 0.4175
Loss at step 2430: 0.2423
Loss at step 2440: 0.2650
Loss at step 2450: 0.1925
Loss at step 2460: 0.3868
Loss at step 2470: 0.3262
Loss at step 2480: 0.4587
Loss at step 2490: 0.3299
Loss at step 2500: 0.2646
Loss at step 2510: 0.4147
Loss at step 2520: 0.2707
Loss at step 2530: 0.2306
Loss at step 2540: 0.2993
Loss at step 2550: 0.3501
Loss at step 2560: 0.3158
Loss at step 2570: 0.3219
Loss at step 2580: 0.5833
Loss at step 2590: 0.0687
Loss at step 2600: 0.3507
Loss at step 2610: 0.2222
Loss at step 2620: 0.4045
Loss at step 2630: 0.1354
Loss at step 2640: 0.3464
Loss at step 2650: 0.2853
Loss at step 2660: 0.2573
Loss at step 2670: 0.2737
Loss at step 2680: 0.2403
Loss at step 2690: 0.2726
Loss at step 2700: 0.1697
Loss at step 2710: 0.1512
Loss at step 2720: 0.2004
Loss at step 2730: 0.5910
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.698332, 'precision': [0.873877, 0.384237, 0.479179], 'recall': [0.812, 0.477645, 0.481212], 'f1': [0.841803, 0.42588, 0.480194]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x721cd3b70b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.9618
Loss at step 20: 0.8469
Loss at step 30: 1.0853
Loss at step 40: 0.7949
Loss at step 50: 0.9862
Loss at step 60: 0.9193
Loss at step 70: 1.0093
Loss at step 80: 0.8311
Loss at step 90: 0.7005
Loss at step 100: 0.6766
Loss at step 110: 0.9313
Loss at step 120: 0.7806
Loss at step 130: 0.6531
Loss at step 140: 0.6194
Loss at step 150: 0.6204
Loss at step 160: 0.6911
Loss at step 170: 0.6423
Loss at step 180: 0.7314
Loss at step 190: 0.9085
Loss at step 200: 0.6625
Loss at step 210: 0.5666
Loss at step 220: 0.5469
Loss at step 230: 0.7742
Loss at step 240: 0.5933
Loss at step 250: 0.6156
Loss at step 260: 0.8471
Loss at step 270: 0.8222
Loss at step 280: 0.5842
Loss at step 290: 0.4718
Loss at step 300: 0.5226
Loss at step 310: 0.7503
Loss at step 320: 0.8177
Loss at step 330: 0.8003
Loss at step 340: 0.8121
Loss at step 350: 0.6208
Loss at step 360: 0.4835
Loss at step 370: 0.5998
Loss at step 380: 0.7386
Loss at step 390: 0.7808
Loss at step 400: 0.8078
Loss at step 410: 0.6450
Loss at step 420: 0.5993
Loss at step 430: 0.5761
Loss at step 440: 0.5857
Loss at step 450: 0.4714
Loss at step 460: 0.4760
Loss at step 470: 0.6971
Loss at step 480: 0.5773
Loss at step 490: 0.6079
Loss at step 500: 1.2694
Loss at step 510: 0.5731
Loss at step 520: 0.6714
Loss at step 530: 0.5197
Loss at step 540: 0.7025
Loss at step 550: 0.9478
Loss at step 560: 0.5675
Loss at step 570: 0.5766
Loss at step 580: 0.6903
Loss at step 590: 0.6419
Loss at step 600: 0.6873
Loss at step 610: 0.4903
Loss at step 620: 0.5600
Loss at step 630: 0.6452
Loss at step 640: 0.6980
Loss at step 650: 0.6319
Loss at step 660: 0.4380
Loss at step 670: 0.5265
Loss at step 680: 0.5830
Loss at step 690: 0.7806
Loss at step 700: 0.6871
Loss at step 710: 0.4765
Loss at step 720: 0.5825
Loss at step 730: 0.4785
Loss at step 740: 0.7760
Loss at step 750: 0.6589
Loss at step 760: 0.4539
Loss at step 770: 0.3491
Loss at step 780: 0.4346
Loss at step 790: 0.4812
Loss at step 800: 0.5899
Loss at step 810: 0.4728
Loss at step 820: 0.5215
Loss at step 830: 0.6680
Loss at step 840: 0.5074
Loss at step 850: 0.5132
Loss at step 860: 0.4923
Loss at step 870: 0.5038
Loss at step 880: 0.6806
Loss at step 890: 0.4543
Loss at step 900: 0.5485
Loss at step 910: 0.4745
Loss at step 920: 0.5199
Loss at step 930: 0.4123
Loss at step 940: 0.9690
Loss at step 950: 0.5029
Loss at step 960: 0.4916
Loss at step 970: 0.5344
Loss at step 980: 0.5364
Loss at step 990: 0.4012
Loss at step 1000: 0.4212
Loss at step 1010: 0.4353
Loss at step 1020: 0.6773
Loss at step 1030: 0.5845
Loss at step 1040: 0.4989
Loss at step 1050: 0.5568
Loss at step 1060: 0.4224
Loss at step 1070: 0.6560
Loss at step 1080: 0.4730
Loss at step 1090: 0.6201
Loss at step 1100: 0.5915
Loss at step 1110: 0.7006
Loss at step 1120: 0.3839
Loss at step 1130: 0.4244
Loss at step 1140: 0.4466
Loss at step 1150: 0.4976
Loss at step 1160: 0.5902
Loss at step 1170: 0.4678
Loss at step 1180: 0.5405
Loss at step 1190: 0.3352
Loss at step 1200: 0.5518
Loss at step 1210: 0.5827
Loss at step 1220: 0.3050
Loss at step 1230: 0.4109
Loss at step 1240: 0.4297
Loss at step 1250: 0.6993
Loss at step 1260: 0.5555
Loss at step 1270: 0.8614
Loss at step 1280: 1.0023
Loss at step 1290: 0.2491
Loss at step 1300: 0.4262
Loss at step 1310: 0.7158
Loss at step 1320: 0.3992
Loss at step 1330: 0.4362
Loss at step 1340: 0.4110
Loss at step 1350: 0.4710
Loss at step 1360: 0.6474
Loss at step 1370: 0.5540
Loss at step 1380: 0.4526
Loss at step 1390: 0.6467
Loss at step 1400: 0.2324
Loss at step 1410: 0.2863
Loss at step 1420: 0.5997
Loss at step 1430: 0.4820
Loss at step 1440: 0.3420
Loss at step 1450: 0.3595
Loss at step 1460: 0.3696
Loss at step 1470: 0.5778
Loss at step 1480: 0.3685
Loss at step 1490: 0.3909
Loss at step 1500: 0.5168
Loss at step 1510: 0.7093
Loss at step 1520: 0.3368
Loss at step 1530: 0.4209
Loss at step 1540: 0.4965
Loss at step 1550: 0.4761
Loss at step 1560: 0.3970
Loss at step 1570: 0.7104
Loss at step 1580: 0.3122
Loss at step 1590: 0.3674
Loss at step 1600: 0.3404
Loss at step 1610: 0.4811
Loss at step 1620: 0.4571
Loss at step 1630: 0.2181
Loss at step 1640: 0.7198
Loss at step 1650: 0.5136
Loss at step 1660: 0.5047
Loss at step 1670: 0.3721
Loss at step 1680: 0.3403
Loss at step 1690: 0.5101
Loss at step 1700: 0.2616
Loss at step 1710: 0.5992
Loss at step 1720: 0.2942
Loss at step 1730: 0.3108
Loss at step 1740: 0.5605
Loss at step 1750: 0.3219
Loss at step 1760: 0.3369
Loss at step 1770: 0.3700
Loss at step 1780: 0.3107
Loss at step 1790: 0.5056
Loss at step 1800: 0.2565
Loss at step 1810: 0.3311
Loss at step 1820: 0.4105
Loss at step 1830: 0.2586
Loss at step 1840: 0.2923
Loss at step 1850: 0.6068
Loss at step 1860: 0.3520
Loss at step 1870: 0.1934
Loss at step 1880: 0.3853
Loss at step 1890: 0.4943
Loss at step 1900: 0.2774
Loss at step 1910: 0.3918
Loss at step 1920: 0.2429
Loss at step 1930: 0.4397
Loss at step 1940: 0.3346
Loss at step 1950: 0.3927
Loss at step 1960: 0.3239
Loss at step 1970: 0.3519
Loss at step 1980: 0.3344
Loss at step 1990: 0.5549
Loss at step 2000: 0.3038
Loss at step 2010: 0.6846
Loss at step 2020: 0.7299
Loss at step 2030: 0.1051
Loss at step 2040: 0.4494
Loss at step 2050: 0.2015
Loss at step 2060: 0.4469
Loss at step 2070: 0.4581
Loss at step 2080: 0.2310
Loss at step 2090: 0.2507
Loss at step 2100: 0.6081
Loss at step 2110: 0.4757
Loss at step 2120: 0.3100
Loss at step 2130: 0.1056
Loss at step 2140: 0.3832
Loss at step 2150: 0.7144
Loss at step 2160: 0.3335
Loss at step 2170: 0.3987
Loss at step 2180: 0.4051
Loss at step 2190: 0.4441
Loss at step 2200: 0.2653
Loss at step 2210: 0.6626
Loss at step 2220: 0.4351
Loss at step 2230: 0.2653
Loss at step 2240: 0.3103
Loss at step 2250: 0.4678
Loss at step 2260: 0.2145
Loss at step 2270: 0.2281
Loss at step 2280: 0.4866
Loss at step 2290: 0.3712
Loss at step 2300: 0.2762
Loss at step 2310: 0.1987
Loss at step 2320: 0.1234
Loss at step 2330: 0.3004
Loss at step 2340: 0.2996
Loss at step 2350: 0.4632
Loss at step 2360: 0.2996
Loss at step 2370: 0.5521
Loss at step 2380: 0.1668
Loss at step 2390: 0.2094
Loss at step 2400: 0.3121
Loss at step 2410: 0.4069
Loss at step 2420: 0.4175
Loss at step 2430: 0.2423
Loss at step 2440: 0.2650
Loss at step 2450: 0.1925
Loss at step 2460: 0.3868
Loss at step 2470: 0.3262
Loss at step 2480: 0.4587
Loss at step 2490: 0.3299
Loss at step 2500: 0.2646
Loss at step 2510: 0.4147
Loss at step 2520: 0.2707
Loss at step 2530: 0.2306
Loss at step 2540: 0.2993
Loss at step 2550: 0.3501
Loss at step 2560: 0.3158
Loss at step 2570: 0.3219
Loss at step 2580: 0.5833
Loss at step 2590: 0.0687
Loss at step 2600: 0.3507
Loss at step 2610: 0.2222
Loss at step 2620: 0.4045
Loss at step 2630: 0.1354
Loss at step 2640: 0.3464
Loss at step 2650: 0.2853
Loss at step 2660: 0.2573
Loss at step 2670: 0.2737
Loss at step 2680: 0.2403
Loss at step 2690: 0.2726
Loss at step 2700: 0.1697
Loss at step 2710: 0.1512
Loss at step 2720: 0.2004
Loss at step 2730: 0.5910
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.698332, 'precision': [0.873877, 0.384237, 0.479179], 'recall': [0.812, 0.477645, 0.481212], 'f1': [0.841803, 0.42588, 0.480194]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x75c45d7efb80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.9618
Loss at step 20: 0.8469
Loss at step 30: 1.0853
Loss at step 40: 0.7949
Loss at step 50: 0.9862
Loss at step 60: 0.9193
Loss at step 70: 1.0093
Loss at step 80: 0.8311
Loss at step 90: 0.7005
Loss at step 100: 0.6766
Loss at step 110: 0.9313
Loss at step 120: 0.7806
Loss at step 130: 0.6531
Loss at step 140: 0.6194
Loss at step 150: 0.6204
Loss at step 160: 0.6911
Loss at step 170: 0.6423
Loss at step 180: 0.7314
Loss at step 190: 0.9085
Loss at step 200: 0.6625
Loss at step 210: 0.5666
Loss at step 220: 0.5469
Loss at step 230: 0.7742
Loss at step 240: 0.5933
Loss at step 250: 0.6156
Loss at step 260: 0.8471
Loss at step 270: 0.8222
Loss at step 280: 0.5842
Loss at step 290: 0.4718
Loss at step 300: 0.5226
Loss at step 310: 0.7503
Loss at step 320: 0.8177
Loss at step 330: 0.8003
Loss at step 340: 0.8121
Loss at step 350: 0.6208
Loss at step 360: 0.4835
Loss at step 370: 0.5998
Loss at step 380: 0.7386
Loss at step 390: 0.7808
Loss at step 400: 0.8078
Loss at step 410: 0.6450
Loss at step 420: 0.5993
Loss at step 430: 0.5761
Loss at step 440: 0.5857
Loss at step 450: 0.4714
Loss at step 460: 0.4760
Loss at step 470: 0.6971
Loss at step 480: 0.5773
Loss at step 490: 0.6079
Loss at step 500: 1.2694
Loss at step 510: 0.5731
Loss at step 520: 0.6714
Loss at step 530: 0.5197
Loss at step 540: 0.7025
Loss at step 550: 0.9478
Loss at step 560: 0.5675
Loss at step 570: 0.5766
Loss at step 580: 0.6903
Loss at step 590: 0.6419
Loss at step 600: 0.6873
Loss at step 610: 0.4903
Loss at step 620: 0.5600
Loss at step 630: 0.6452
Loss at step 640: 0.6980
Loss at step 650: 0.6319
Loss at step 660: 0.4380
Loss at step 670: 0.5265
Loss at step 680: 0.5830
Loss at step 690: 0.7806
Loss at step 700: 0.6871
Loss at step 710: 0.4765
Loss at step 720: 0.5825
Loss at step 730: 0.4785
Loss at step 740: 0.7760
Loss at step 750: 0.6589
Loss at step 760: 0.4539
Loss at step 770: 0.3491
Loss at step 780: 0.4346
Loss at step 790: 0.4812
Loss at step 800: 0.5899
Loss at step 810: 0.4728
Loss at step 820: 0.5215
Loss at step 830: 0.6680
Loss at step 840: 0.5074
Loss at step 850: 0.5132
Loss at step 860: 0.4923
Loss at step 870: 0.5038
Loss at step 880: 0.6806
Loss at step 890: 0.4543
Loss at step 900: 0.5485
Loss at step 910: 0.4745
Loss at step 920: 0.5199
Loss at step 930: 0.4123
Loss at step 940: 0.9690
Loss at step 950: 0.5029
Loss at step 960: 0.4916
Loss at step 970: 0.5344
Loss at step 980: 0.5364
Loss at step 990: 0.4012
Loss at step 1000: 0.4212
Loss at step 1010: 0.4353
Loss at step 1020: 0.6773
Loss at step 1030: 0.5845
Loss at step 1040: 0.4989
Loss at step 1050: 0.5568
Loss at step 1060: 0.4224
Loss at step 1070: 0.6560
Loss at step 1080: 0.4730
Loss at step 1090: 0.6201
Loss at step 1100: 0.5915
Loss at step 1110: 0.7006
Loss at step 1120: 0.3839
Loss at step 1130: 0.4244
Loss at step 1140: 0.4466
Loss at step 1150: 0.4976
Loss at step 1160: 0.5902
Loss at step 1170: 0.4678
Loss at step 1180: 0.5405
Loss at step 1190: 0.3352
Loss at step 1200: 0.5518
Loss at step 1210: 0.5827
Loss at step 1220: 0.3050
Loss at step 1230: 0.4109
Loss at step 1240: 0.4297
Loss at step 1250: 0.6993
Loss at step 1260: 0.5555
Loss at step 1270: 0.8614
Loss at step 1280: 1.0023
Loss at step 1290: 0.2491
Loss at step 1300: 0.4262
Loss at step 1310: 0.7158
Loss at step 1320: 0.3992
Loss at step 1330: 0.4362
Loss at step 1340: 0.4110
Loss at step 1350: 0.4710
Loss at step 1360: 0.6474
Loss at step 1370: 0.5540
Loss at step 1380: 0.4526
Loss at step 1390: 0.6467
Loss at step 1400: 0.2324
Loss at step 1410: 0.2863
Loss at step 1420: 0.5997
Loss at step 1430: 0.4820
Loss at step 1440: 0.3420
Loss at step 1450: 0.3595
Loss at step 1460: 0.3696
Loss at step 1470: 0.5778
Loss at step 1480: 0.3685
Loss at step 1490: 0.3909
Loss at step 1500: 0.5168
Loss at step 1510: 0.7093
Loss at step 1520: 0.3368
Loss at step 1530: 0.4209
Loss at step 1540: 0.4965
Loss at step 1550: 0.4761
Loss at step 1560: 0.3970
Loss at step 1570: 0.7104
Loss at step 1580: 0.3122
Loss at step 1590: 0.3674
Loss at step 1600: 0.3404
Loss at step 1610: 0.4811
Loss at step 1620: 0.4571
Loss at step 1630: 0.2181
Loss at step 1640: 0.7198
Loss at step 1650: 0.5136
Loss at step 1660: 0.5047
Loss at step 1670: 0.3721
Loss at step 1680: 0.3403
Loss at step 1690: 0.5101
Loss at step 1700: 0.2616
Loss at step 1710: 0.5992
Loss at step 1720: 0.2942
Loss at step 1730: 0.3108
Loss at step 1740: 0.5605
Loss at step 1750: 0.3219
Loss at step 1760: 0.3369
Loss at step 1770: 0.3700
Loss at step 1780: 0.3107
Loss at step 1790: 0.5056
Loss at step 1800: 0.2565
Loss at step 1810: 0.3311
Loss at step 1820: 0.4105
Loss at step 1830: 0.2586
Loss at step 1840: 0.2923
Loss at step 1850: 0.6068
Loss at step 1860: 0.3520
Loss at step 1870: 0.1934
Loss at step 1880: 0.3853
Loss at step 1890: 0.4943
Loss at step 1900: 0.2774
Loss at step 1910: 0.3918
Loss at step 1920: 0.2429
Loss at step 1930: 0.4397
Loss at step 1940: 0.3346
Loss at step 1950: 0.3927
Loss at step 1960: 0.3239
Loss at step 1970: 0.3519
Loss at step 1980: 0.3344
Loss at step 1990: 0.5549
Loss at step 2000: 0.3038
Loss at step 2010: 0.6846
Loss at step 2020: 0.7299
Loss at step 2030: 0.1051
Loss at step 2040: 0.4494
Loss at step 2050: 0.2015
Loss at step 2060: 0.4469
Loss at step 2070: 0.4581
Loss at step 2080: 0.2310
Loss at step 2090: 0.2507
Loss at step 2100: 0.6081
Loss at step 2110: 0.4757
Loss at step 2120: 0.3100
Loss at step 2130: 0.1056
Loss at step 2140: 0.3832
Loss at step 2150: 0.7144
Loss at step 2160: 0.3335
Loss at step 2170: 0.3987
Loss at step 2180: 0.4051
Loss at step 2190: 0.4441
Loss at step 2200: 0.2653
Loss at step 2210: 0.6626
Loss at step 2220: 0.4351
Loss at step 2230: 0.2653
Loss at step 2240: 0.3103
Loss at step 2250: 0.4678
Loss at step 2260: 0.2145
Loss at step 2270: 0.2281
Loss at step 2280: 0.4866
Loss at step 2290: 0.3712
Loss at step 2300: 0.2762
Loss at step 2310: 0.1987
Loss at step 2320: 0.1234
Loss at step 2330: 0.3004
Loss at step 2340: 0.2996
Loss at step 2350: 0.4632
Loss at step 2360: 0.2996
Loss at step 2370: 0.5521
Loss at step 2380: 0.1668
Loss at step 2390: 0.2094
Loss at step 2400: 0.3121
Loss at step 2410: 0.4069
Loss at step 2420: 0.4175
Loss at step 2430: 0.2423
Loss at step 2440: 0.2650
Loss at step 2450: 0.1925
Loss at step 2460: 0.3868
Loss at step 2470: 0.3262
Loss at step 2480: 0.4587
Loss at step 2490: 0.3299
Loss at step 2500: 0.2646
Loss at step 2510: 0.4147
Loss at step 2520: 0.2707
Loss at step 2530: 0.2306
Loss at step 2540: 0.2993
Loss at step 2550: 0.3501
Loss at step 2560: 0.3158
Loss at step 2570: 0.3219
Loss at step 2580: 0.5833
Loss at step 2590: 0.0687
Loss at step 2600: 0.3507
Loss at step 2610: 0.2222
Loss at step 2620: 0.4045
Loss at step 2630: 0.1354
Loss at step 2640: 0.3464
Loss at step 2650: 0.2853
Loss at step 2660: 0.2573
Loss at step 2670: 0.2737
Loss at step 2680: 0.2403
Loss at step 2690: 0.2726
Loss at step 2700: 0.1697
Loss at step 2710: 0.1512
Loss at step 2720: 0.2004
Loss at step 2730: 0.5910
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.698332, 'precision': [0.873877, 0.384237, 0.479179], 'recall': [0.812, 0.477645, 0.481212], 'f1': [0.841803, 0.42588, 0.480194]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x735dd61afc10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x77a4811efb80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 2738
Loss at step 10: 0.9617
Loss at step 20: 0.8470
Loss at step 30: 1.0852
Loss at step 40: 0.7961
Loss at step 50: 0.9881
Loss at step 60: 0.9222
Loss at step 70: 0.9946
Loss at step 80: 0.8542
Loss at step 90: 0.7061
Loss at step 100: 0.6793
Loss at step 110: 0.9365
Loss at step 120: 0.7747
Loss at step 130: 0.7160
Loss at step 140: 0.6834
Loss at step 150: 0.6410
Loss at step 160: 0.6934
Loss at step 170: 0.5814
Loss at step 180: 0.7753
Loss at step 190: 0.8491
Loss at step 200: 0.6695
Loss at step 210: 0.6148
Loss at step 220: 0.5486
Loss at step 230: 0.7939
Loss at step 240: 0.6316
Loss at step 250: 0.7175
Loss at step 260: 0.8587
Loss at step 270: 0.7741
Loss at step 280: 0.5207
Loss at step 290: 0.4899
Loss at step 300: 0.5244
Loss at step 310: 0.7409
Loss at step 320: 0.7666
Loss at step 330: 0.7704
Loss at step 340: 0.7924
Loss at step 350: 0.6377
Loss at step 360: 0.4824
Loss at step 370: 0.6133
Loss at step 380: 0.6978
Loss at step 390: 0.7564
Loss at step 400: 0.7623
Loss at step 410: 0.6939
Loss at step 420: 0.6330
Loss at step 430: 0.6089
Loss at step 440: 0.5137
Loss at step 450: 0.4724
Loss at step 460: 0.4802
Loss at step 470: 0.7110
Loss at step 480: 0.5547
Loss at step 490: 0.6595
Loss at step 500: 1.0663
Loss at step 510: 0.5552
Loss at step 520: 0.6816
Loss at step 530: 0.5138
Loss at step 540: 0.6322
Loss at step 550: 0.9795
Loss at step 560: 0.5687
Loss at step 570: 0.6014
Loss at step 580: 0.7378
Loss at step 590: 0.6328
Loss at step 600: 0.6502
Loss at step 610: 0.4847
Loss at step 620: 0.5277
Loss at step 630: 0.6451
Loss at step 640: 0.6322
Loss at step 650: 0.5989
Loss at step 660: 0.4345
Loss at step 670: 0.4459
Loss at step 680: 0.6002
Loss at step 690: 0.7521
Loss at step 700: 0.5996
Loss at step 710: 0.4941
Loss at step 720: 0.5978
Loss at step 730: 0.5014
Loss at step 740: 0.7659
Loss at step 750: 0.8007
Loss at step 760: 0.4813
Loss at step 770: 0.3907
Loss at step 780: 0.5410
Loss at step 790: 0.4793
Loss at step 800: 0.5932
Loss at step 810: 0.4084
Loss at step 820: 0.5083
Loss at step 830: 0.6396
Loss at step 840: 0.4960
Loss at step 850: 0.4987
Loss at step 860: 0.5192
Loss at step 870: 0.4678
Loss at step 880: 0.5194
Loss at step 890: 0.4629
Loss at step 900: 0.4854
Loss at step 910: 0.4830
Loss at step 920: 0.5160
Loss at step 930: 0.4119
Loss at step 940: 0.9770
Loss at step 950: 0.5285
Loss at step 960: 0.4849
Loss at step 970: 0.5571
Loss at step 980: 0.5319
Loss at step 990: 0.3941
Loss at step 1000: 0.4849
Loss at step 1010: 0.4109
Loss at step 1020: 0.7137
Loss at step 1030: 0.6098
Loss at step 1040: 0.5738
Loss at step 1050: 0.6839
Loss at step 1060: 0.4123
Loss at step 1070: 0.6526
Loss at step 1080: 0.4753
Loss at step 1090: 0.6458
Loss at step 1100: 0.5328
Loss at step 1110: 0.6493
Loss at step 1120: 0.4214
Loss at step 1130: 0.4507
Loss at step 1140: 0.4830
Loss at step 1150: 0.5375
Loss at step 1160: 0.5133
Loss at step 1170: 0.5839
Loss at step 1180: 0.4455
Loss at step 1190: 0.4115
Loss at step 1200: 0.5119
Loss at step 1210: 0.5612
Loss at step 1220: 0.3252
Loss at step 1230: 0.4030
Loss at step 1240: 0.3953
Loss at step 1250: 0.7052
Loss at step 1260: 0.5216
Loss at step 1270: 0.8535
Loss at step 1280: 0.9684
Loss at step 1290: 0.3642
Loss at step 1300: 0.2897
Loss at step 1310: 0.7366
Loss at step 1320: 0.4475
Loss at step 1330: 0.4028
Loss at step 1340: 0.4157
Loss at step 1350: 0.5787
Loss at step 1360: 0.7355
Loss at step 1370: 0.4650
Loss at step 1380: 0.4518
Loss at step 1390: 0.7191
Loss at step 1400: 0.2739
Loss at step 1410: 0.2282
Loss at step 1420: 0.6609
Loss at step 1430: 0.5635
Loss at step 1440: 0.3013
Loss at step 1450: 0.3134
Loss at step 1460: 0.3010
Loss at step 1470: 0.4998
Loss at step 1480: 0.3241
Loss at step 1490: 0.4392
Loss at step 1500: 0.5380
Loss at step 1510: 0.6226
Loss at step 1520: 0.3190
Loss at step 1530: 0.4554
Loss at step 1540: 0.5965
Loss at step 1550: 0.5588
Loss at step 1560: 0.5531
Loss at step 1570: 0.8039
Loss at step 1580: 0.2972
Loss at step 1590: 0.3630
Loss at step 1600: 0.4107
Loss at step 1610: 0.4599
Loss at step 1620: 0.3162
Loss at step 1630: 0.2502
Loss at step 1640: 0.7004
Loss at step 1650: 0.4406
Loss at step 1660: 0.5612
Loss at step 1670: 0.4002
Loss at step 1680: 0.3401
Loss at step 1690: 0.5636
Loss at step 1700: 0.2599
Loss at step 1710: 0.4950
Loss at step 1720: 0.3461
Loss at step 1730: 0.2983
Loss at step 1740: 0.5654
Loss at step 1750: 0.3477
Loss at step 1760: 0.4219
Loss at step 1770: 0.3423
Loss at step 1780: 0.3012
Loss at step 1790: 0.4207
Loss at step 1800: 0.3518
Loss at step 1810: 0.2677
Loss at step 1820: 0.5297
Loss at step 1830: 0.3168
Loss at step 1840: 0.3573
Loss at step 1850: 0.6856
Loss at step 1860: 0.3718
Loss at step 1870: 0.2453
Loss at step 1880: 0.3085
Loss at step 1890: 0.4357
Loss at step 1900: 0.2707
Loss at step 1910: 0.3898
Loss at step 1920: 0.2507
Loss at step 1930: 0.5400
Loss at step 1940: 0.3106
Loss at step 1950: 0.4032
Loss at step 1960: 0.3530
Loss at step 1970: 0.3253
Loss at step 1980: 0.3987
Loss at step 1990: 0.5393
Loss at step 2000: 0.3886
Loss at step 2010: 0.8796
Loss at step 2020: 0.5733
Loss at step 2030: 0.0947
Loss at step 2040: 0.4237
Loss at step 2050: 0.2477
Loss at step 2060: 0.3082
Loss at step 2070: 0.3729
Loss at step 2080: 0.2834
Loss at step 2090: 0.3046
Loss at step 2100: 0.4762
Loss at step 2110: 0.5091
Loss at step 2120: 0.3620
Loss at step 2130: 0.2491
Loss at step 2140: 0.3362
Loss at step 2150: 0.5984
Loss at step 2160: 0.2990
Loss at step 2170: 0.4936
Loss at step 2180: 0.2865
Loss at step 2190: 0.3099
Loss at step 2200: 0.3461
Loss at step 2210: 0.4944
Loss at step 2220: 0.3588
Loss at step 2230: 0.2443
Loss at step 2240: 0.2922
Loss at step 2250: 0.4870
Loss at step 2260: 0.2462
Loss at step 2270: 0.3456
Loss at step 2280: 0.6220
Loss at step 2290: 0.3273
Loss at step 2300: 0.2387
Loss at step 2310: 0.2232
Loss at step 2320: 0.1822
Loss at step 2330: 0.3391
Loss at step 2340: 0.2641
Loss at step 2350: 0.3738
Loss at step 2360: 0.2746
Loss at step 2370: 0.6024
Loss at step 2380: 0.1653
Loss at step 2390: 0.3351
Loss at step 2400: 0.2590
Loss at step 2410: 0.5325
Loss at step 2420: 0.2539
Loss at step 2430: 0.3505
Loss at step 2440: 0.2226
Loss at step 2450: 0.2235
Loss at step 2460: 0.3540
Loss at step 2470: 0.3069
Loss at step 2480: 0.3673
Loss at step 2490: 0.2198
Loss at step 2500: 0.2583
Loss at step 2510: 0.3013
Loss at step 2520: 0.3193
Loss at step 2530: 0.2673
Loss at step 2540: 0.3575
Loss at step 2550: 0.4036
Loss at step 2560: 0.3777
Loss at step 2570: 0.2711
Loss at step 2580: 0.4838
Loss at step 2590: 0.1360
Loss at step 2600: 0.3310
Loss at step 2610: 0.3223
Loss at step 2620: 0.3286
Loss at step 2630: 0.1518
Loss at step 2640: 0.2661
Loss at step 2650: 0.2803
Loss at step 2660: 0.2446
Loss at step 2670: 0.2496
Loss at step 2680: 0.3518
Loss at step 2690: 0.3477
Loss at step 2700: 0.2516
Loss at step 2710: 0.1855
Loss at step 2720: 0.1912
Loss at step 2730: 0.6342
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.72647, 'precision': [0.871225, 0.420236, 0.518005], 'recall': [0.858386, 0.469827, 0.475152], 'f1': [0.864758, 0.44365, 0.495654]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7b562536fb80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 2738
Loss at step 10: 0.9617
Loss at step 20: 0.8470
Loss at step 30: 1.0852
Loss at step 40: 0.7961
Loss at step 50: 0.9881
Loss at step 60: 0.9222
Loss at step 70: 0.9946
Loss at step 80: 0.8542
Loss at step 90: 0.7061
Loss at step 100: 0.6793
Loss at step 110: 0.9365
Loss at step 120: 0.7747
Loss at step 130: 0.7160
Loss at step 140: 0.6834
Loss at step 150: 0.6410
Loss at step 160: 0.6934
Loss at step 170: 0.5814
Loss at step 180: 0.7753
Loss at step 190: 0.8491
Loss at step 200: 0.6695
Loss at step 210: 0.6148
Loss at step 220: 0.5486
Loss at step 230: 0.7939
Loss at step 240: 0.6316
Loss at step 250: 0.7175
Loss at step 260: 0.8587
Loss at step 270: 0.7741
Loss at step 280: 0.5207
Loss at step 290: 0.4899
Loss at step 300: 0.5244
Loss at step 310: 0.7409
Loss at step 320: 0.7666
Loss at step 330: 0.7704
Loss at step 340: 0.7924
Loss at step 350: 0.6377
Loss at step 360: 0.4824
Loss at step 370: 0.6133
Loss at step 380: 0.6978
Loss at step 390: 0.7564
Loss at step 400: 0.7623
Loss at step 410: 0.6939
Loss at step 420: 0.6330
Loss at step 430: 0.6089
Loss at step 440: 0.5137
Loss at step 450: 0.4724
Loss at step 460: 0.4802
Loss at step 470: 0.7110
Loss at step 480: 0.5547
Loss at step 490: 0.6595
Loss at step 500: 1.0663
Loss at step 510: 0.5552
Loss at step 520: 0.6816
Loss at step 530: 0.5138
Loss at step 540: 0.6322
Loss at step 550: 0.9795
Loss at step 560: 0.5687
Loss at step 570: 0.6014
Loss at step 580: 0.7378
Loss at step 590: 0.6328
Loss at step 600: 0.6502
Loss at step 610: 0.4847
Loss at step 620: 0.5277
Loss at step 630: 0.6451
Loss at step 640: 0.6322
Loss at step 650: 0.5989
Loss at step 660: 0.4345
Loss at step 670: 0.4459
Loss at step 680: 0.6002
Loss at step 690: 0.7521
Loss at step 700: 0.5996
Loss at step 710: 0.4941
Loss at step 720: 0.5978
Loss at step 730: 0.5014
Loss at step 740: 0.7659
Loss at step 750: 0.8007
Loss at step 760: 0.4813
Loss at step 770: 0.3907
Loss at step 780: 0.5410
Loss at step 790: 0.4793
Loss at step 800: 0.5932
Loss at step 810: 0.4084
Loss at step 820: 0.5083
Loss at step 830: 0.6396
Loss at step 840: 0.4960
Loss at step 850: 0.4987
Loss at step 860: 0.5192
Loss at step 870: 0.4678
Loss at step 880: 0.5194
Loss at step 890: 0.4629
Loss at step 900: 0.4854
Loss at step 910: 0.4830
Loss at step 920: 0.5160
Loss at step 930: 0.4119
Loss at step 940: 0.9770
Loss at step 950: 0.5285
Loss at step 960: 0.4849
Loss at step 970: 0.5571
Loss at step 980: 0.5319
Loss at step 990: 0.3941
Loss at step 1000: 0.4849
Loss at step 1010: 0.4109
Loss at step 1020: 0.7137
Loss at step 1030: 0.6098
Loss at step 1040: 0.5738
Loss at step 1050: 0.6839
Loss at step 1060: 0.4123
Loss at step 1070: 0.6526
Loss at step 1080: 0.4753
Loss at step 1090: 0.6458
Loss at step 1100: 0.5328
Loss at step 1110: 0.6493
Loss at step 1120: 0.4214
Loss at step 1130: 0.4507
Loss at step 1140: 0.4830
Loss at step 1150: 0.5375
Loss at step 1160: 0.5133
Loss at step 1170: 0.5839
Loss at step 1180: 0.4455
Loss at step 1190: 0.4115
Loss at step 1200: 0.5119
Loss at step 1210: 0.5612
Loss at step 1220: 0.3252
Loss at step 1230: 0.4030
Loss at step 1240: 0.3953
Loss at step 1250: 0.7052
Loss at step 1260: 0.5216
Loss at step 1270: 0.8535
Loss at step 1280: 0.9684
Loss at step 1290: 0.3642
Loss at step 1300: 0.2897
Loss at step 1310: 0.7366
Loss at step 1320: 0.4475
Loss at step 1330: 0.4028
Loss at step 1340: 0.4157
Loss at step 1350: 0.5787
Loss at step 1360: 0.7355
Loss at step 1370: 0.4650
Loss at step 1380: 0.4518
Loss at step 1390: 0.7191
Loss at step 1400: 0.2739
Loss at step 1410: 0.2282
Loss at step 1420: 0.6609
Loss at step 1430: 0.5635
Loss at step 1440: 0.3013
Loss at step 1450: 0.3134
Loss at step 1460: 0.3010
Loss at step 1470: 0.4998
Loss at step 1480: 0.3241
Loss at step 1490: 0.4392
Loss at step 1500: 0.5380
Loss at step 1510: 0.6226
Loss at step 1520: 0.3190
Loss at step 1530: 0.4554
Loss at step 1540: 0.5965
Loss at step 1550: 0.5588
Loss at step 1560: 0.5531
Loss at step 1570: 0.8039
Loss at step 1580: 0.2972
Loss at step 1590: 0.3630
Loss at step 1600: 0.4107
Loss at step 1610: 0.4599
Loss at step 1620: 0.3162
Loss at step 1630: 0.2502
Loss at step 1640: 0.7004
Loss at step 1650: 0.4406
Loss at step 1660: 0.5612
Loss at step 1670: 0.4002
Loss at step 1680: 0.3401
Loss at step 1690: 0.5636
Loss at step 1700: 0.2599
Loss at step 1710: 0.4950
Loss at step 1720: 0.3461
Loss at step 1730: 0.2983
Loss at step 1740: 0.5654
Loss at step 1750: 0.3477
Loss at step 1760: 0.4219
Loss at step 1770: 0.3423
Loss at step 1780: 0.3012
Loss at step 1790: 0.4207
Loss at step 1800: 0.3518
Loss at step 1810: 0.2677
Loss at step 1820: 0.5297
Loss at step 1830: 0.3168
Loss at step 1840: 0.3573
Loss at step 1850: 0.6856
Loss at step 1860: 0.3718
Loss at step 1870: 0.2453
Loss at step 1880: 0.3085
Loss at step 1890: 0.4357
Loss at step 1900: 0.2707
Loss at step 1910: 0.3898
Loss at step 1920: 0.2507
Loss at step 1930: 0.5400
Loss at step 1940: 0.3106
Loss at step 1950: 0.4032
Loss at step 1960: 0.3530
Loss at step 1970: 0.3253
Loss at step 1980: 0.3987
Loss at step 1990: 0.5393
Loss at step 2000: 0.3886
Loss at step 2010: 0.8796
Loss at step 2020: 0.5733
Loss at step 2030: 0.0947
Loss at step 2040: 0.4237
Loss at step 2050: 0.2477
Loss at step 2060: 0.3082
Loss at step 2070: 0.3729
Loss at step 2080: 0.2834
Loss at step 2090: 0.3046
Loss at step 2100: 0.4762
Loss at step 2110: 0.5091
Loss at step 2120: 0.3620
Loss at step 2130: 0.2491
Loss at step 2140: 0.3362
Loss at step 2150: 0.5984
Loss at step 2160: 0.2990
Loss at step 2170: 0.4936
Loss at step 2180: 0.2865
Loss at step 2190: 0.3099
Loss at step 2200: 0.3461
Loss at step 2210: 0.4944
Loss at step 2220: 0.3588
Loss at step 2230: 0.2443
Loss at step 2240: 0.2922
Loss at step 2250: 0.4870
Loss at step 2260: 0.2462
Loss at step 2270: 0.3456
Loss at step 2280: 0.6220
Loss at step 2290: 0.3273
Loss at step 2300: 0.2387
Loss at step 2310: 0.2232
Loss at step 2320: 0.1822
Loss at step 2330: 0.3391
Loss at step 2340: 0.2641
Loss at step 2350: 0.3738
Loss at step 2360: 0.2746
Loss at step 2370: 0.6024
Loss at step 2380: 0.1653
Loss at step 2390: 0.3351
Loss at step 2400: 0.2590
Loss at step 2410: 0.5325
Loss at step 2420: 0.2539
Loss at step 2430: 0.3505
Loss at step 2440: 0.2226
Loss at step 2450: 0.2235
Loss at step 2460: 0.3540
Loss at step 2470: 0.3069
Loss at step 2480: 0.3673
Loss at step 2490: 0.2198
Loss at step 2500: 0.2583
Loss at step 2510: 0.3013
Loss at step 2520: 0.3193
Loss at step 2530: 0.2673
Loss at step 2540: 0.3575
Loss at step 2550: 0.4036
Loss at step 2560: 0.3777
Loss at step 2570: 0.2711
Loss at step 2580: 0.4838
Loss at step 2590: 0.1360
Loss at step 2600: 0.3310
Loss at step 2610: 0.3223
Loss at step 2620: 0.3286
Loss at step 2630: 0.1518
Loss at step 2640: 0.2661
Loss at step 2650: 0.2803
Loss at step 2660: 0.2446
Loss at step 2670: 0.2496
Loss at step 2680: 0.3518
Loss at step 2690: 0.3477
Loss at step 2700: 0.2516
Loss at step 2710: 0.1855
Loss at step 2720: 0.1912
Loss at step 2730: 0.6342
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.72647, 'precision': [0.871225, 0.420236, 0.518005], 'recall': [0.858386, 0.469827, 0.475152], 'f1': [0.864758, 0.44365, 0.495654]}
{'accuracy': 0.852562, 'precision': 0.518005, 'recall': 0.475152, 'f1': 0.495654, 'WordR': 0.131316}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7899fcdf0430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [101, 8065, 1039, 6727, 1996, 2136, 2008, 1996, 19329, 28084, 2818, 3259, 2018, 2042, 3970, 1998, 1996, 3034, 2052, 2202, 2173, 1999, 9779, 20850, 21759, 5842, 1999, 2244, 1012, 1996, 2136, 2001, 2437, 5082, 2006, 2006, 1011, 2240, 3671, 3989, 1010, 2437, 2009, 2052, 3499, 2005, 1996, 2951, 1012, 102, 101, 2054, 2106, 8065, 1040, 2228, 2055, 6948, 2278, 1029, 1001, 1001, 102, 101, 8065, 1040, 1024, 2096, 2256, 2291, 2003, 2747, 2012, 2698, 3867, 1012, 8529, 1010, 2021, 2054, 6433, 2036, 2003, 2008, 2065, 1045, 4952, 2000, 1996, 1010, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2512, 6767, 9289, 6499, 8630, 1065, 1037, 2128, 1011, 23572, 2544, 1997, 1996, 4613, 1998, 1063, 8724, 1065, 1045, 2128, 1011, 23572, 2023, 2478, 1037, 2317, 5005, 2008, 1005, 1055, 21839, 2011, 1037, 6948, 2278, 1010, 7910, 1010, 11307, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 8529, 1010, 2092, 1010, 2017, 2064, 7475, 1010, 2008, 1010, 7910, 1063, 4487, 22747, 10665, 2121, 1065, 2008, 2023, 2003, 2025, 4613, 1010, 102, 101, 2934, 1038, 1024, 3398, 1012, 102, 101, 8065, 1040, 1024, 2061, 1996, 4540, 2003, 2025, 4738, 2000, 6807, 2023, 1012, 2021, 1055, 2941, 2009, 2614, 2066, 1063, 8724, 1065, 13550, 1010, 2061, 2057, 2024, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2092, 1010, 1045, 2812, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 8065, 1040, 1024, 15501, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2045, 1005, 1055, 2048, 3471, 2045, 1012, 1045, 2812, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 2812, 1010, 2061, 1063, 4487, 22747, 10665, 2121, 1065, 2061, 1996, 2034, 2003, 1063, 2955, 28819, 1065, 2008, 2011, 2725, 6948, 2278, 1011, 4376, 2007, 23572, 4613, 1059, 2066, 2017, 1005, 2128, 3038, 1010, 7910, 1010, 2009, 1005, 1055, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 1045, 1045, 2017, 1005, 2128, 1063, 4487, 22747, 10665, 2121, 1065, 2017, 1005, 2128, 5815, 2060, 16627, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2157, 1029, 2061, 2009, 1005, 1055, 2025, 2074, 1996, 5005, 2021, 2017, 1005, 2128, 5815, 1999, 2755, 2070, 16627, 2138, 2009, 1005, 1055, 2069, 2019, 20167, 1012, 8529, 1010, 1998, 1996, 2117, 2518, 2003, 1063, 4487, 22747, 10665, 2121, 1065, 2029, 2003, 1049, 2672, 2062, 5875, 1063, 4487, 22747, 10665, 2121, 1065, 2003, 2008, 1010, 8529, 1010, 1063, 7615, 1065, 1063, 2955, 28819, 1065, 2065, 2017, 2079, 2009, 2007, 3990, 4613, 1010, 2017, 2131, 2023, 2193, 1012, 2054, 2065, 2017, 2018, 1063, 8724, 1065, 2589, 4106, 1063, 7615, 1065, 2128, 1011, 10752, 1998, 2579, 1996, 6510, 2004, 2092, 1029, 10303, 1029, 2061, 2085, 2017, 2404, 1996, 6510, 1999, 1012, 102, 101, 8065, 1040, 1024, 7910, 1011, 9616, 1012, 102, 101, 2934, 1038, 1024, 2054, 2052, 1996, 7017, 2022, 2059, 1029, 102, 101, 8065, 1040, 1024, 8529, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 2934, 1038, 1024, 2156, 1010, 2008, 1005, 1055, 1996, 3160, 1012, 2061, 1010, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.9618
Loss at step 20: 0.8469
Loss at step 30: 1.0853
Loss at step 40: 0.7949
Loss at step 50: 0.9862
Loss at step 60: 0.9193
Loss at step 70: 1.0093
Loss at step 80: 0.8311
Loss at step 90: 0.7005
Loss at step 100: 0.6766
Loss at step 110: 0.9313
Loss at step 120: 0.7806
Loss at step 130: 0.6531
Loss at step 140: 0.6194
Loss at step 150: 0.6204
Loss at step 160: 0.6911
Loss at step 170: 0.6423
Loss at step 180: 0.7314
Loss at step 190: 0.9085
Loss at step 200: 0.6625
Loss at step 210: 0.5666
Loss at step 220: 0.5469
Loss at step 230: 0.7742
Loss at step 240: 0.5933
Loss at step 250: 0.6156
Loss at step 260: 0.8471
Loss at step 270: 0.8222
Loss at step 280: 0.5842
Loss at step 290: 0.4718
Loss at step 300: 0.5226
Loss at step 310: 0.7503
Loss at step 320: 0.8177
Loss at step 330: 0.8003
Loss at step 340: 0.8121
Loss at step 350: 0.6208
Loss at step 360: 0.4835
Loss at step 370: 0.5998
Loss at step 380: 0.7386
Loss at step 390: 0.7808
Loss at step 400: 0.8078
Loss at step 410: 0.6450
Loss at step 420: 0.5993
Loss at step 430: 0.5761
Loss at step 440: 0.5857
Loss at step 450: 0.4714
Loss at step 460: 0.4760
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7ebf407b0430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [101, 1996, 2136, 6936, 1996, 4972, 1997, 1996, 6556, 2491, 1012, 2009, 2323, 2022, 1037, 4800, 1011, 8360, 11721, 24291, 2008, 2052, 2491, 2035, 2115, 4398, 7910, 7910, 6681, 10468, 1012, 1996, 2136, 3530, 2008, 2000, 3006, 2242, 5147, 2009, 2323, 2079, 2070, 2062, 2477, 1012, 1996, 2136, 2056, 2008, 1996, 3371, 2009, 2001, 2428, 5023, 1010, 1999, 2060, 2616, 1010, 2065, 2009, 2001, 2428, 1999, 1037, 2601, 3962, 2008, 2009, 2435, 2041, 1037, 2614, 2030, 1037, 4742, 1012, 102, 101, 2054, 2106, 1996, 2136, 6848, 2055, 1996, 4972, 1997, 1996, 6556, 1029, 1001, 1001, 102, 101, 5310, 8278, 1024, 2748, 1010, 1045, 1045, 2514, 2008, 2035, 1996, 6556, 2323, 2022, 2200, 9233, 1012, 102, 101, 3919, 5859, 1024, 3398, 1012, 102, 101, 2622, 3208, 1024, 2235, 1010, 2157, 1012, 3461, 1011, 17012, 1012, 102, 101, 5310, 8278, 1024, 3398, 1010, 2216, 2029, 2057, 2131, 2182, 13367, 2009, 1005, 1055, 2200, 2146, 1012, 102, 101, 2622, 3208, 1024, 3461, 1011, 17012, 1012, 102, 101, 5310, 8278, 1024, 1998, 8529, 1998, 2009, 2323, 2031, 4800, 1011, 3800, 1012, 2066, 7910, 1996, 6556, 2491, 2029, 2057, 2224, 2005, 1056, 1035, 1058, 1035, 1010, 2009, 26822, 2226, 7910, 2009, 2323, 2022, 2109, 1042, 7910, 2005, 2070, 2060, 3800, 2036, 1010, 2066, 9756, 1996, 7910, 4860, 2503, 1996, 2160, 2030, 2005, 2250, 1011, 4650, 2545, 1010, 2030, 2005, 10808, 2291, 1012, 102, 101, 2622, 3208, 1024, 3461, 1011, 17012, 1012, 102, 101, 3919, 5859, 1024, 5746, 2447, 1012, 2821, 1012, 3100, 1012, 102, 101, 2622, 3208, 1024, 2061, 2009, 2323, 2022, 1037, 4800, 1011, 8360, 7910, 11721, 24291, 2008, 2052, 8529, 2491, 2035, 2115, 4398, 7910, 7910, 6681, 10468, 1012, 102, 101, 5310, 8278, 1024, 2748, 1010, 3599, 2748, 1012, 102, 101, 3919, 5859, 1024, 17012, 1012, 102, 101, 5310, 8278, 1024, 2748, 1012, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 20487, 2149, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 5310, 8278, 1024, 3398, 1012, 3599, 1012, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 3398, 1012, 3398, 1012, 102, 101, 5821, 1024, 1063, 2955, 28819, 1065, 2012, 8529, 4376, 5595, 19329, 2566, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 102, 101, 5310, 8278, 1024, 1063, 2955, 28819, 1065, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 102, 101, 2622, 3208, 1024, 2092, 1012, 102, 101, 5821, 1024, 2092, 2040, 4282, 2065, 2057, 2131, 1037, 2428, 2204, 5859, 2672, 2057, 2064, 2079, 2008, 1012, 102, 101, 3919, 5859, 1024, 3398, 1012, 102, 101, 5821, 1024, 1063, 2955, 28819, 1065, 2057, 5121, 2064, 3046, 2000, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 5993, 2007, 2014, 2008, 2000, 3006, 2242, 5147, 2009, 2323, 2079, 2070, 2062, 2477, 1012, 102, 101, 2622, 3208, 1024, 2009, 2323, 2022, 2242, 2047, 1063, 4487, 22747, 10665, 2121, 1065, 2009, 2323, 2022, 1055, 2009, 2009, 2323, 2079, 2242, 2367, 2084, 2084, 2074, 2054, 2057, 2031, 1012, 102, 101, 5821, 1024, 2008, 1005, 1055, 2157, 1012, 102, 101, 5821, 1024, 1045, 2228, 2178, 2518, 2008, 2052, 2393, 2003, 8529, 2065, 2009, 10506, 4523, 2043, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 1, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1369
Loss at step 10: 0.8839
Loss at step 20: 0.9068
Loss at step 30: 0.8859
Loss at step 40: 0.8657
Loss at step 50: 0.8433
Loss at step 60: 1.0693
Loss at step 70: 0.8224
Loss at step 80: 0.8375
Loss at step 90: 0.8604
Loss at step 100: 0.6320
Loss at step 110: 0.6257
Loss at step 120: 0.5477
Loss at step 130: 0.8993
Loss at step 140: 0.6194
Loss at step 150: 0.6317
Loss at step 160: 0.7577
Loss at step 170: 0.7752
Loss at step 180: 0.5574
Loss at step 190: 0.5909
Loss at step 200: 0.5562
Loss at step 210: 0.5892
Loss at step 220: 0.5579
Loss at step 230: 0.6191
Loss at step 240: 0.7036
Loss at step 250: 0.7058
Loss at step 260: 0.7205
Loss at step 270: 0.7174
Loss at step 280: 0.6269
Loss at step 290: 0.5184
Loss at step 300: 0.6757
Loss at step 310: 0.6081
Loss at step 320: 0.7739
Loss at step 330: 0.5870
Loss at step 340: 0.5524
Loss at step 350: 0.5143
Loss at step 360: 0.6854
Loss at step 370: 0.5481
Loss at step 380: 0.4257
Loss at step 390: 0.5316
Loss at step 400: 0.5515
Loss at step 410: 0.3775
Loss at step 420: 0.5719
Loss at step 430: 0.6069
Loss at step 440: 0.6556
Loss at step 450: 0.5908
Loss at step 460: 0.4819
Loss at step 470: 0.9102
Loss at step 480: 0.5471
Loss at step 490: 0.5938
Loss at step 500: 0.5074
Loss at step 510: 0.6415
Loss at step 520: 0.5278
Loss at step 530: 0.4768
Loss at step 540: 0.4484
Loss at step 550: 0.5709
Loss at step 560: 0.5188
Loss at step 570: 0.4673
Loss at step 580: 0.5062
Loss at step 590: 0.5324
Loss at step 600: 0.5210
Loss at step 610: 0.3827
Loss at step 620: 0.5135
Loss at step 630: 0.5432
Loss at step 640: 0.6093
Loss at step 650: 0.5294
Loss at step 660: 0.5429
Loss at step 670: 0.5233
Loss at step 680: 0.5304
Loss at step 690: 0.4971
Loss at step 700: 0.3565
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x772b77201310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 710: 0.7861
Loss at step 720: 0.5599
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 730: 0.3935
Loss at step 740: 0.6543
Loss at step 750: 0.5476
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 760: 0.3768
Loss at step 770: 0.6347
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1099
Loss at step 780: 0.5414
Loss at step 790: 0.5157
Loss at step 10: 1.1041
Loss at step 800: 0.4053
Loss at step 20: 1.0819
Loss at step 30: 1.0799
Loss at step 810: 0.5035
Loss at step 40: 1.0927
Loss at step 820: 0.6593
Loss at step 50: 1.0409
Loss at step 830: 0.5109
Loss at step 60: 1.0296
Loss at step 840: 0.4955
Loss at step 70: 1.0509
Loss at step 850: 0.3674
Loss at step 80: 1.0839
Loss at step 860: 0.4427
Loss at step 90: 1.0242
Loss at step 870: 0.5916
Loss at step 100: 1.0358
Loss at step 880: 0.3420
Loss at step 110: 1.0720
Loss at step 890: 0.3425
Loss at step 120: 0.8917
Loss at step 900: 0.3673
Loss at step 130: 0.9449
Loss at step 910: 0.5455
Loss at step 140: 0.9836
Loss at step 920: 0.3648
Loss at step 150: 0.9955
Loss at step 930: 0.4327
Loss at step 160: 1.0560
Loss at step 940: 0.4971
Loss at step 170: 0.9167
Loss at step 950: 0.4598
Loss at step 180: 0.9217
Loss at step 190: 0.8924
Loss at step 960: 0.3771
Loss at step 200: 0.8956
Loss at step 970: 0.4181
Loss at step 210: 0.9402
Loss at step 980: 0.4887
Loss at step 220: 0.8860
Loss at step 990: 0.5775
Loss at step 230: 0.8987
Loss at step 1000: 0.4525
Loss at step 240: 0.8053
Loss at step 1010: 0.6144
Loss at step 250: 0.9253
Loss at step 1020: 0.4652
Loss at step 260: 0.8656
Loss at step 1030: 0.5496
Loss at step 270: 0.8838
Loss at step 1040: 0.4205
Loss at step 280: 0.8768
Loss at step 1050: 0.3715
Loss at step 290: 0.7354
Loss at step 1060: 0.4330
Loss at step 300: 0.7345
Loss at step 1070: 0.4329
Loss at step 310: 0.8806
Loss at step 1080: 0.4470
Loss at step 320: 0.8272
Loss at step 1090: 0.2987
Loss at step 330: 0.8351
Loss at step 1100: 0.4308
Loss at step 340: 0.7850
Loss at step 1110: 0.4110
Loss at step 350: 0.7521
Loss at step 1120: 0.3559
Loss at step 360: 0.7865
Loss at step 1130: 0.3953
Loss at step 370: 0.6874
Loss at step 380: 0.6863
Loss at step 1140: 0.4869
Loss at step 390: 0.9417
Loss at step 1150: 0.3637
Loss at step 400: 0.8511
Loss at step 1160: 0.3255
Loss at step 410: 0.6857
Loss at step 1170: 0.3822
Loss at step 420: 0.7347
Loss at step 1180: 0.2557
Loss at step 430: 0.5992
Loss at step 1190: 0.2374
Loss at step 440: 0.8823
Loss at step 1200: 0.4340
Loss at step 450: 0.6339
Loss at step 1210: 0.3977
Loss at step 460: 0.6345
Loss at step 1220: 0.2345
Loss at step 470: 0.7894
Loss at step 1230: 0.3729
Loss at step 480: 0.4748
Loss at step 1240: 0.5095
Loss at step 490: 0.5932
Loss at step 1250: 0.4411
Loss at step 500: 0.6626
Loss at step 1260: 0.3469
Loss at step 510: 0.5865
Loss at step 1270: 0.4833
Loss at step 520: 0.6394
Loss at step 1280: 0.3938
Loss at step 530: 0.7747
Loss at step 1290: 0.4143
Loss at step 540: 0.5425
Loss at step 1300: 0.3611
Loss at step 550: 0.5304
Loss at step 1310: 0.4111
Loss at step 560: 0.6026
Loss at step 1320: 0.4375
Loss at step 570: 0.6134
Loss at step 580: 0.8432
Loss at step 1330: 0.4478
Loss at step 590: 0.6073
Loss at step 1340: 0.5205
Loss at step 600: 0.5531
Loss at step 1350: 0.4029
Loss at step 610: 0.5420
Loss at step 1360: 0.3318
Loss at step 620: 0.3994
Loss at step 630: 0.4358
Loss at step 640: 0.4919
Loss at step 650: 0.7211
Loss at step 660: 0.5504
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Loss at step 670: 0.5161
Loss at step 680: 0.5287
Loss at step 690: 0.3586
Loss at step 700: 0.5418
Loss at step 710: 0.4766
Loss at step 720: 0.3613
Loss at step 730: 0.6535
{'accuracy': 0.72855, 'precision': [0.868352, 0.437813, 0.505785], 'recall': [0.858175, 0.448082, 0.516667], 'f1': [0.863234, 0.442888, 0.511168]}
Loss at step 740: 0.5293
Loss at step 750: 0.5143
Loss at step 760: 0.4896
{'accuracy': 0.849328, 'precision': 0.505785, 'recall': 0.516667, 'f1': 0.511168, 'WordR': 0.131316}
Loss at step 770: 0.4898
Loss at step 780: 0.5097
Loss at step 790: 0.3140
Loss at step 800: 0.4504
Loss at step 810: 0.5941
Loss at step 820: 0.4387
Loss at step 830: 0.4954
Loss at step 840: 0.4106
Loss at step 850: 0.3812
Loss at step 860: 0.3226
Loss at step 870: 0.4066
Loss at step 880: 0.4489
Loss at step 890: 0.3861
Loss at step 900: 0.4346
Loss at step 910: 0.3417
Loss at step 920: 0.4379
Loss at step 930: 0.4237
Loss at step 940: 0.4507
Loss at step 950: 0.3243
Loss at step 960: 0.3659
Loss at step 970: 0.4119
Loss at step 980: 0.2668
Loss at step 990: 0.2900
Loss at step 1000: 0.4082
Loss at step 1010: 0.3607
Loss at step 1020: 0.4192
Loss at step 1030: 0.4673
Loss at step 1040: 0.4303
Loss at step 1050: 0.3402
Loss at step 1060: 0.3135
Loss at step 1070: 0.3940
Loss at step 1080: 0.3208
Loss at step 1090: 0.3151
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.559166, 'precision': [0.690524, 0.504463, 0.5], 'recall': [0.561475, 0.621589, 0.466899], 'f1': [0.619349, 0.556934, 0.482883]}
{'accuracy': 0.739761, 'precision': 0.5, 'recall': 0.466899, 'f1': 0.482883, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7c18549a1430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [101, 1996, 2136, 6936, 1996, 4972, 1997, 1996, 6556, 2491, 1012, 2009, 2323, 2022, 1037, 4800, 1011, 8360, 11721, 24291, 2008, 2052, 2491, 2035, 2115, 4398, 7910, 7910, 6681, 10468, 1012, 1996, 2136, 3530, 2008, 2000, 3006, 2242, 5147, 2009, 2323, 2079, 2070, 2062, 2477, 1012, 1996, 2136, 2056, 2008, 1996, 3371, 2009, 2001, 2428, 5023, 1010, 1999, 2060, 2616, 1010, 2065, 2009, 2001, 2428, 1999, 1037, 2601, 3962, 2008, 2009, 2435, 2041, 1037, 2614, 2030, 1037, 4742, 1012, 102, 101, 2054, 2106, 1996, 2136, 6848, 2055, 1996, 4972, 1997, 1996, 6556, 1029, 1001, 1001, 102, 101, 5310, 8278, 1024, 2748, 1010, 1045, 1045, 2514, 2008, 2035, 1996, 6556, 2323, 2022, 2200, 9233, 1012, 102, 101, 3919, 5859, 1024, 3398, 1012, 102, 101, 2622, 3208, 1024, 2235, 1010, 2157, 1012, 3461, 1011, 17012, 1012, 102, 101, 5310, 8278, 1024, 3398, 1010, 2216, 2029, 2057, 2131, 2182, 13367, 2009, 1005, 1055, 2200, 2146, 1012, 102, 101, 2622, 3208, 1024, 3461, 1011, 17012, 1012, 102, 101, 5310, 8278, 1024, 1998, 8529, 1998, 2009, 2323, 2031, 4800, 1011, 3800, 1012, 2066, 7910, 1996, 6556, 2491, 2029, 2057, 2224, 2005, 1056, 1035, 1058, 1035, 1010, 2009, 26822, 2226, 7910, 2009, 2323, 2022, 2109, 1042, 7910, 2005, 2070, 2060, 3800, 2036, 1010, 2066, 9756, 1996, 7910, 4860, 2503, 1996, 2160, 2030, 2005, 2250, 1011, 4650, 2545, 1010, 2030, 2005, 10808, 2291, 1012, 102, 101, 2622, 3208, 1024, 3461, 1011, 17012, 1012, 102, 101, 3919, 5859, 1024, 5746, 2447, 1012, 2821, 1012, 3100, 1012, 102, 101, 2622, 3208, 1024, 2061, 2009, 2323, 2022, 1037, 4800, 1011, 8360, 7910, 11721, 24291, 2008, 2052, 8529, 2491, 2035, 2115, 4398, 7910, 7910, 6681, 10468, 1012, 102, 101, 5310, 8278, 1024, 2748, 1010, 3599, 2748, 1012, 102, 101, 3919, 5859, 1024, 17012, 1012, 102, 101, 5310, 8278, 1024, 2748, 1012, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 20487, 2149, 1063, 4487, 22747, 10665, 2121, 1065, 102, 101, 5310, 8278, 1024, 3398, 1012, 3599, 1012, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 3398, 1012, 3398, 1012, 102, 101, 5821, 1024, 1063, 2955, 28819, 1065, 2012, 8529, 4376, 5595, 19329, 2566, 1063, 4487, 22747, 10665, 2121, 1065, 1063, 2955, 28819, 1065, 102, 101, 5310, 8278, 1024, 1063, 2955, 28819, 1065, 102, 101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 102, 101, 2622, 3208, 1024, 2092, 1012, 102, 101, 5821, 1024, 2092, 2040, 4282, 2065, 2057, 2131, 1037, 2428, 2204, 5859, 2672, 2057, 2064, 2079, 2008, 1012, 102, 101, 3919, 5859, 1024, 3398, 1012, 102, 101, 5821, 1024, 1063, 2955, 28819, 1065, 2057, 5121, 2064, 3046, 2000, 1063, 4487, 22747, 10665, 2121, 1065, 1045, 5993, 2007, 2014, 2008, 2000, 3006, 2242, 5147, 2009, 2323, 2079, 2070, 2062, 2477, 1012, 102, 101, 2622, 3208, 1024, 2009, 2323, 2022, 2242, 2047, 1063, 4487, 22747, 10665, 2121, 1065, 2009, 2323, 2022, 1055, 2009, 2009, 2323, 2079, 2242, 2367, 2084, 2084, 2074, 2054, 2057, 2031, 1012, 102, 101, 5821, 1024, 2008, 1005, 1055, 2157, 1012, 102, 101, 5821, 1024, 1045, 2228, 2178, 2518, 2008, 2052, 2393, 2003, 8529, 2065, 2009, 10506, 4523, 2043, 2017, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 1, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1369
Loss at step 10: 0.8839
Loss at step 20: 0.9068
Loss at step 30: 0.8859
Loss at step 40: 0.8657
Loss at step 50: 0.8433
Loss at step 60: 1.0693
Loss at step 70: 0.8224
Loss at step 80: 0.8375
Loss at step 90: 0.8604
Loss at step 100: 0.6320
Loss at step 110: 0.6257
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7a8d5a9c03a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 120: 0.5477
Loss at step 130: 0.8993
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 140: 0.6194
Loss at step 150: 0.6317
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 160: 0.7577
Loss at step 170: 0.7752
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1099
Loss at step 180: 0.5574
Loss at step 10: 1.1041
Loss at step 190: 0.5909
Loss at step 20: 1.0819
Loss at step 200: 0.5562
Loss at step 30: 1.0799
Loss at step 210: 0.5892
Loss at step 40: 1.0927
Loss at step 220: 0.5579
Loss at step 50: 1.0409
Loss at step 230: 0.6191
Loss at step 60: 1.0296
Loss at step 240: 0.7036
Loss at step 70: 1.0509
Loss at step 250: 0.7058
Loss at step 80: 1.0839
Loss at step 260: 0.7205
Loss at step 90: 1.0242
Loss at step 270: 0.7174
Loss at step 100: 1.0358
Loss at step 280: 0.6269
Loss at step 110: 1.0720
Loss at step 290: 0.5184
Loss at step 120: 0.8917
Loss at step 130: 0.9449
Loss at step 300: 0.6757
Loss at step 140: 0.9836
Loss at step 310: 0.6081
Loss at step 150: 0.9955
Loss at step 320: 0.7739
Loss at step 160: 1.0560
Loss at step 330: 0.5870
Loss at step 170: 0.9167
Loss at step 340: 0.5524
Loss at step 180: 0.9217
Loss at step 350: 0.5143
Loss at step 190: 0.8924
Loss at step 360: 0.6854
Loss at step 200: 0.8956
Loss at step 370: 0.5481
Loss at step 210: 0.9402
Loss at step 380: 0.4257
Loss at step 220: 0.8860
Loss at step 390: 0.5316
Loss at step 230: 0.8987
Loss at step 400: 0.5515
Loss at step 240: 0.8053
Loss at step 410: 0.3775
Loss at step 250: 0.9253
Loss at step 420: 0.5719
Loss at step 260: 0.8656
Loss at step 430: 0.6069
Loss at step 270: 0.8838
Loss at step 280: 0.8768
Loss at step 440: 0.6556
Loss at step 290: 0.7354
Loss at step 450: 0.5908
Loss at step 300: 0.7345
Loss at step 460: 0.4819
Loss at step 310: 0.8806
Loss at step 470: 0.9102
Loss at step 320: 0.8272
Loss at step 480: 0.5471
Loss at step 330: 0.8351
Loss at step 490: 0.5938
Loss at step 340: 0.7850
Loss at step 500: 0.5074
Loss at step 350: 0.7521
Loss at step 510: 0.6415
Loss at step 360: 0.7865
Loss at step 520: 0.5278
Loss at step 370: 0.6874
Loss at step 530: 0.4768
Loss at step 380: 0.6863
Loss at step 540: 0.4484
Loss at step 390: 0.9417
Loss at step 550: 0.5709
Loss at step 400: 0.8511
Loss at step 560: 0.5188
Loss at step 410: 0.6857
Loss at step 570: 0.4673
Loss at step 420: 0.7347
Loss at step 580: 0.5062
Loss at step 430: 0.5992
Loss at step 590: 0.5324
Loss at step 440: 0.8823
Loss at step 600: 0.5210
Loss at step 450: 0.6339
Loss at step 460: 0.6345
Loss at step 610: 0.3827
Loss at step 470: 0.7894
Loss at step 620: 0.5135
Loss at step 480: 0.4748
Loss at step 630: 0.5432
Loss at step 490: 0.5932
Loss at step 640: 0.6093
Loss at step 500: 0.6626
Loss at step 650: 0.5294
Loss at step 510: 0.5865
Loss at step 660: 0.5429
Loss at step 520: 0.6394
Loss at step 670: 0.5233
Loss at step 530: 0.7747
Loss at step 680: 0.5304
Loss at step 540: 0.5425
Loss at step 690: 0.4971
Loss at step 550: 0.5304
Loss at step 700: 0.3565
Loss at step 560: 0.6026
Loss at step 710: 0.7861
Loss at step 570: 0.6134
Loss at step 720: 0.5599
Loss at step 580: 0.8432
Loss at step 730: 0.3935
Loss at step 590: 0.6073
Loss at step 740: 0.6543
Loss at step 600: 0.5531
Loss at step 750: 0.5476
Loss at step 610: 0.5420
Loss at step 760: 0.3768
Loss at step 620: 0.3994
Loss at step 630: 0.4358
Loss at step 770: 0.6347
Loss at step 640: 0.4919
Loss at step 780: 0.5414
Loss at step 650: 0.7211
Loss at step 790: 0.5157
Loss at step 660: 0.5504
Loss at step 800: 0.4053
Loss at step 670: 0.5161
Loss at step 810: 0.5035
Loss at step 680: 0.5287
Loss at step 820: 0.6593
Loss at step 690: 0.3586
Loss at step 830: 0.5109
Loss at step 700: 0.5418
Loss at step 840: 0.4955
Loss at step 710: 0.4766
Loss at step 850: 0.3674
Loss at step 720: 0.3613
Loss at step 860: 0.4427
Loss at step 730: 0.6535
Loss at step 870: 0.5916
Loss at step 740: 0.5293
Loss at step 880: 0.3420
Loss at step 750: 0.5143
Loss at step 890: 0.3425
Loss at step 760: 0.4896
Loss at step 900: 0.3673
Loss at step 770: 0.4898
Loss at step 910: 0.5455
Loss at step 780: 0.5097
Loss at step 790: 0.3140
Loss at step 920: 0.3648
Loss at step 800: 0.4504
Loss at step 930: 0.4327
Loss at step 810: 0.5941
Loss at step 940: 0.4971
Loss at step 820: 0.4387
Loss at step 950: 0.4598
Loss at step 830: 0.4954
Loss at step 960: 0.3771
Loss at step 840: 0.4106
Loss at step 970: 0.4181
Loss at step 850: 0.3812
Loss at step 980: 0.4887
Loss at step 860: 0.3226
Loss at step 990: 0.5775
Loss at step 870: 0.4066
Loss at step 1000: 0.4525
Loss at step 880: 0.4489
Loss at step 1010: 0.6144
Loss at step 890: 0.3861
Loss at step 1020: 0.4652
Loss at step 900: 0.4346
Loss at step 1030: 0.5496
Loss at step 910: 0.3417
Loss at step 1040: 0.4205
Loss at step 920: 0.4379
Loss at step 1050: 0.3715
Loss at step 930: 0.4237
Loss at step 1060: 0.4330
Loss at step 940: 0.4507
Loss at step 1070: 0.4329
Loss at step 950: 0.3243
Loss at step 1080: 0.4470
Loss at step 960: 0.3659
Loss at step 970: 0.4119
Loss at step 1090: 0.2987
Loss at step 980: 0.2668
Loss at step 1100: 0.4308
Loss at step 990: 0.2900
Loss at step 1110: 0.4110
Loss at step 1000: 0.4082
Loss at step 1120: 0.3559
Loss at step 1010: 0.3607
Loss at step 1130: 0.3953
Loss at step 1020: 0.4192
Loss at step 1140: 0.4869
Loss at step 1030: 0.4673
Loss at step 1150: 0.3637
Loss at step 1040: 0.4303
Loss at step 1160: 0.3255
Loss at step 1050: 0.3402
Loss at step 1170: 0.3822
Loss at step 1060: 0.3135
Loss at step 1180: 0.2557
Loss at step 1070: 0.3940
Loss at step 1190: 0.2374
Loss at step 1080: 0.3208
Loss at step 1200: 0.4340
Loss at step 1090: 0.3151
Loss at step 1210: 0.3977
Loss at step 1220: 0.2345
Loss at step 1230: 0.3729
Loss at step 1240: 0.5095
Loss at step 1250: 0.4411
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Loss at step 1260: 0.3469
Loss at step 1270: 0.4833
{'accuracy': 0.559166, 'precision': [0.690524, 0.504463, 0.5], 'recall': [0.561475, 0.621589, 0.466899], 'f1': [0.619349, 0.556934, 0.482883]}
Loss at step 1280: 0.3938
{'accuracy': 0.739761, 'precision': 0.5, 'recall': 0.466899, 'f1': 0.482883, 'WordR': 0.018182}
Loss at step 1290: 0.4143
Loss at step 1300: 0.3611
Loss at step 1310: 0.4111
Loss at step 1320: 0.4375
Loss at step 1330: 0.4478
Loss at step 1340: 0.5205
Loss at step 1350: 0.4029
Loss at step 1360: 0.3318
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.72855, 'precision': [0.868352, 0.437813, 0.505785], 'recall': [0.858175, 0.448082, 0.516667], 'f1': [0.863234, 0.442888, 0.511168]}
{'accuracy': 0.849328, 'precision': 0.505785, 'recall': 0.516667, 'f1': 0.511168, 'WordR': 0.131316}
