Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x742707746160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [0, 1039, 21119, 30397, 38873, 10861, 544, 416, 18821, 5357, 5, 30577, 8, 200, 86, 5, 2888, 1411, 66, 2, 2, 44799, 16, 13689, 59, 5, 2888, 544, 61, 16, 45, 447, 4, 50118, 45443, 982, 14, 42, 696, 16, 2121, 8, 5034, 13, 5, 2170, 9, 5, 1316, 8, 67, 982, 14, 51, 33, 10, 2789, 356, 88, 42, 696, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 1.1145
Loss at step 20: 1.1054
Loss at step 30: 1.0878
Loss at step 40: 1.1247
Loss at step 50: 1.0863
Loss at step 60: 1.0000
Loss at step 70: 1.0238
Loss at step 80: 1.0215
Loss at step 90: 0.9705
Loss at step 100: 1.1627
Loss at step 110: 0.9779
Loss at step 120: 1.0033
Loss at step 130: 0.9511
Loss at step 140: 1.0283
Loss at step 150: 0.9236
Loss at step 160: 0.9682
Loss at step 170: 1.2602
Loss at step 180: 1.0983
Loss at step 190: 1.0980
Loss at step 200: 1.0919
Loss at step 210: 1.0624
Loss at step 220: 1.0743
Loss at step 230: 1.0685
Loss at step 240: 1.0781
Loss at step 250: 1.0567
Loss at step 260: 1.1414
Loss at step 270: 1.0637
Loss at step 280: 1.1159
Loss at step 290: 1.1172
Loss at step 300: 1.1863
Loss at step 310: 0.9797
Loss at step 320: 1.0253
Loss at step 330: 0.9044
Loss at step 340: 0.9784
Loss at step 350: 1.1795
Loss at step 360: 0.9027
Loss at step 370: 0.8707
Loss at step 380: 1.3017
Loss at step 390: 1.2873
Loss at step 400: 0.8802
Loss at step 410: 1.0948
Loss at step 420: 1.2023
Loss at step 430: 1.0633
Loss at step 440: 0.8857
Loss at step 450: 1.0843
Loss at step 460: 1.1789
Loss at step 470: 0.9663
Loss at step 480: 0.8874
Loss at step 490: 1.0456
Loss at step 500: 1.0316
Loss at step 510: 1.0380
Loss at step 520: 1.1116
Loss at step 530: 1.0330
Loss at step 540: 0.9837
Loss at step 550: 0.9607
Loss at step 560: 1.0777
Loss at step 570: 1.0652
Loss at step 580: 1.1434
Loss at step 590: 1.0173
Loss at step 600: 0.9535
Loss at step 610: 0.9372
Loss at step 620: 0.7506
Loss at step 630: 0.9220
Loss at step 640: 1.1593
Loss at step 650: 1.0085
Loss at step 660: 0.8644
Loss at step 670: 0.9539
Loss at step 680: 0.9401
Loss at step 690: 1.0402
Loss at step 700: 1.1203
Loss at step 710: 1.1539
Loss at step 720: 1.0773
Loss at step 730: 1.0250
Loss at step 740: 0.9023
Loss at step 750: 1.0400
Loss at step 760: 0.8633
Loss at step 770: 1.0697
Loss at step 780: 0.9160
Loss at step 790: 0.9105
Loss at step 800: 0.8665
Loss at step 810: 0.7174
Loss at step 820: 1.0127
Loss at step 830: 0.8837
Loss at step 840: 0.9009
Loss at step 850: 0.7905
Loss at step 860: 0.9793
Loss at step 870: 0.8989
Loss at step 880: 0.8827
Loss at step 890: 0.7183
Loss at step 900: 0.9347
Loss at step 910: 0.7778
Loss at step 920: 1.1002
Loss at step 930: 0.8535
Loss at step 940: 0.9193
Loss at step 950: 0.8555
Loss at step 960: 0.8571
Loss at step 970: 0.9464
Loss at step 980: 0.9312
Loss at step 990: 0.9160
Loss at step 1000: 0.8292
Loss at step 1010: 0.9100
Loss at step 1020: 0.9483
Loss at step 1030: 1.0223
Loss at step 1040: 0.9203
Loss at step 1050: 0.9768
Loss at step 1060: 0.9005
Loss at step 1070: 0.9167
Loss at step 1080: 0.5384
Loss at step 1090: 0.9518
Loss at step 1100: 0.8258
Loss at step 1110: 0.6917
Loss at step 1120: 0.8910
Loss at step 1130: 0.8789
Loss at step 1140: 1.0922
Loss at step 1150: 0.7330
Loss at step 1160: 0.8169
Loss at step 1170: 0.8538
Loss at step 1180: 0.8521
Loss at step 1190: 0.9018
Loss at step 1200: 0.8078
Loss at step 1210: 0.9019
Loss at step 1220: 0.6455
Loss at step 1230: 0.7711
Loss at step 1240: 0.8670
Loss at step 1250: 0.9289
Loss at step 1260: 0.7112
Loss at step 1270: 0.8459
Loss at step 1280: 0.6969
Loss at step 1290: 0.8523
Loss at step 1300: 0.7428
Loss at step 1310: 0.8867
Loss at step 1320: 0.7732
Loss at step 1330: 0.5408
Loss at step 1340: 0.7898
Loss at step 1350: 0.8592
Loss at step 1360: 0.6834
Loss at step 1370: 0.6738
Loss at step 1380: 0.5165
Loss at step 1390: 0.8784
Loss at step 1400: 1.1592
Loss at step 1410: 0.7847
Loss at step 1420: 0.6790
Loss at step 1430: 0.5431
Loss at step 1440: 0.6812
Loss at step 1450: 0.6220
Loss at step 1460: 0.6159
Loss at step 1470: 0.6861
Loss at step 1480: 0.6696
Loss at step 1490: 0.7251
Loss at step 1500: 0.6373
Loss at step 1510: 0.5731
Loss at step 1520: 0.9088
Loss at step 1530: 0.6336
Loss at step 1540: 0.6255
Loss at step 1550: 0.7207
Loss at step 1560: 0.7798
Loss at step 1570: 0.5266
Loss at step 1580: 0.6380
Loss at step 1590: 0.5914
Loss at step 1600: 0.8724
Loss at step 1610: 1.0020
Loss at step 1620: 0.5672
Loss at step 1630: 0.8504
Loss at step 1640: 1.0403
Loss at step 1650: 0.6618
Loss at step 1660: 0.6177
Loss at step 1670: 0.6913
Loss at step 1680: 0.6536
Loss at step 1690: 0.8474
Loss at step 1700: 0.6075
Loss at step 1710: 0.5837
Loss at step 1720: 0.5267
Loss at step 1730: 0.4637
Loss at step 1740: 0.8601
Loss at step 1750: 0.6022
Loss at step 1760: 0.4984
Loss at step 1770: 0.6458
Loss at step 1780: 0.3477
Loss at step 1790: 0.6347
Loss at step 1800: 0.3409
Loss at step 1810: 0.7203
Loss at step 1820: 0.6113
Loss at step 1830: 0.5865
Loss at step 1840: 0.5780
Loss at step 1850: 0.5923
Loss at step 1860: 0.8763
Loss at step 1870: 0.4967
Loss at step 1880: 0.5961
Loss at step 1890: 0.6958
Loss at step 1900: 0.4741
Loss at step 1910: 0.5430
Loss at step 1920: 0.6646
Loss at step 1930: 0.7872
Loss at step 1940: 0.6314
Loss at step 1950: 0.3869
Loss at step 1960: 0.4849
Loss at step 1970: 0.5079
Loss at step 1980: 0.3365
Loss at step 1990: 0.4297
Loss at step 2000: 0.4949
Loss at step 2010: 0.5739
Loss at step 2020: 0.3675
Loss at step 2030: 0.6518
Loss at step 2040: 0.4295
Loss at step 2050: 0.3914
Loss at step 2060: 0.4186
Loss at step 2070: 0.3915
Loss at step 2080: 0.3668
Loss at step 2090: 0.4608
Loss at step 2100: 0.3291
Loss at step 2110: 0.5290
Loss at step 2120: 0.3846
Loss at step 2130: 0.5426
Loss at step 2140: 0.6918
Loss at step 2150: 0.7283
Loss at step 2160: 0.2925
Loss at step 2170: 0.3859
Loss at step 2180: 0.5616
Loss at step 2190: 0.4686
Loss at step 2200: 0.4666
Loss at step 2210: 0.4804
Loss at step 2220: 0.4254
Loss at step 2230: 0.5790
Loss at step 2240: 0.5174
Loss at step 2250: 0.3846
Loss at step 2260: 0.2891
Loss at step 2270: 0.3260
Loss at step 2280: 0.5686
Loss at step 2290: 0.4631
Loss at step 2300: 0.3796
Loss at step 2310: 0.2402
Loss at step 2320: 0.7661
Loss at step 2330: 0.4000
Loss at step 2340: 0.5864
Loss at step 2350: 0.6408
Loss at step 2360: 0.3656
Loss at step 2370: 0.5896
Loss at step 2380: 0.4161
Loss at step 2390: 0.3352
Loss at step 2400: 0.3635
Loss at step 2410: 0.5924
Loss at step 2420: 0.3522
Loss at step 2430: 0.4260
Loss at step 2440: 0.2613
Loss at step 2450: 0.4117
Loss at step 2460: 0.3297
Loss at step 2470: 0.4224
Loss at step 2480: 0.6009
Loss at step 2490: 0.2740
Loss at step 2500: 0.3077
Loss at step 2510: 0.5155
Loss at step 2520: 0.4958
Loss at step 2530: 0.5415
Loss at step 2540: 0.5963
Loss at step 2550: 0.5152
Loss at step 2560: 0.4428
Loss at step 2570: 0.3664
Loss at step 2580: 0.6791
Loss at step 2590: 0.3089
Loss at step 2600: 0.2166
Loss at step 2610: 0.5463
Loss at step 2620: 0.4384
Loss at step 2630: 0.4695
Loss at step 2640: 0.2872
Loss at step 2650: 0.2738
Loss at step 2660: 0.2966
Loss at step 2670: 0.4979
Loss at step 2680: 0.2049
Loss at step 2690: 0.2529
Loss at step 2700: 0.4295
Loss at step 2710: 0.2832
Loss at step 2720: 0.5199
Loss at step 2730: 0.2137
Loss at step 2740: 0.1611
Loss at step 2750: 0.1889
Loss at step 2760: 0.5277
Loss at step 2770: 0.1602
Loss at step 2780: 0.2108
Loss at step 2790: 0.3347
Loss at step 2800: 0.2171
Loss at step 2810: 0.2016
Loss at step 2820: 0.3109
Loss at step 2830: 0.3804
Loss at step 2840: 0.2151
Loss at step 2850: 0.3930
Loss at step 2860: 0.3629
Loss at step 2870: 0.3796
Loss at step 2880: 0.1668
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.522192, 'precision': [0.685961, 0.468526, 0.473962], 'recall': [0.446314, 0.710145, 0.362168], 'f1': [0.540777, 0.56457, 0.410592]}
{'accuracy': 0.731457, 'precision': 0.473962, 'recall': 0.362168, 'f1': 0.410592, 'WordR': 0.271323}
Loss at step 2890: 0.2261
Loss at step 2900: 0.0743
Loss at step 2910: 0.1654
Loss at step 2920: 0.2643
Loss at step 2930: 0.1406
Loss at step 2940: 0.3406
Loss at step 2950: 0.2704
Loss at step 2960: 0.3673
Loss at step 2970: 0.2393
Loss at step 2980: 0.1737
Loss at step 2990: 0.2267
Loss at step 3000: 0.3933
Loss at step 3010: 0.2812
Loss at step 3020: 0.4613
Loss at step 3030: 0.5576
Loss at step 3040: 0.1367
Loss at step 3050: 0.3670
Loss at step 3060: 0.7651
Loss at step 3070: 0.4018
Loss at step 3080: 0.1460
Loss at step 3090: 0.2702
Loss at step 3100: 0.1018
Loss at step 3110: 0.1473
Loss at step 3120: 0.0812
Loss at step 3130: 0.4681
Loss at step 3140: 0.0778
Loss at step 3150: 0.3102
Loss at step 3160: 0.4339
Loss at step 3170: 0.2083
Loss at step 3180: 0.1795
Loss at step 3190: 0.0830
Loss at step 3200: 0.0522
Loss at step 3210: 0.2571
Loss at step 3220: 0.1660
Loss at step 3230: 0.2527
Loss at step 3240: 0.2638
Loss at step 3250: 0.2673
Loss at step 3260: 0.2732
Loss at step 3270: 0.4609
Loss at step 3280: 0.2711
Loss at step 3290: 0.1585
Loss at step 3300: 0.3114
Loss at step 3310: 0.1752
Loss at step 3320: 0.1394
Loss at step 3330: 0.1205
Loss at step 3340: 0.1229
Loss at step 3350: 0.2563
Loss at step 3360: 0.2836
Loss at step 3370: 0.4373
Loss at step 3380: 0.3999
Loss at step 3390: 0.2026
Loss at step 3400: 0.3505
Loss at step 3410: 0.2107
Loss at step 3420: 0.1855
Loss at step 3430: 0.2008
Loss at step 3440: 0.1478
Loss at step 3450: 0.1844
Loss at step 3460: 0.2786
Loss at step 3470: 0.2610
Loss at step 3480: 0.1555
Loss at step 3490: 0.1424
Loss at step 3500: 0.2166
Loss at step 3510: 0.2962
Loss at step 3520: 0.2423
Loss at step 3530: 0.2058
Loss at step 3540: 0.2421
Loss at step 3550: 0.2104
Loss at step 3560: 0.1452
Loss at step 3570: 0.0845
Loss at step 3580: 0.2548
Loss at step 3590: 0.0230
Loss at step 3600: 0.2112
Loss at step 3610: 0.3681
Loss at step 3620: 0.1286
Loss at step 3630: 0.1183
Loss at step 3640: 0.1030
Loss at step 3650: 0.3830
Loss at step 3660: 0.1478
Loss at step 3670: 0.3360
Loss at step 3680: 0.3947
Loss at step 3690: 0.0217
Loss at step 3700: 0.2452
Loss at step 3710: 0.0917
Loss at step 3720: 0.0741
Loss at step 3730: 0.1236
Loss at step 3740: 0.2240
Loss at step 3750: 0.6212
Loss at step 3760: 0.1456
Loss at step 3770: 0.0612
Loss at step 3780: 0.0933
Loss at step 3790: 0.1576
Loss at step 3800: 0.3297
Loss at step 3810: 0.1766
Loss at step 3820: 0.1570
Loss at step 3830: 0.1163
Loss at step 3840: 0.3406
Loss at step 3850: 0.0898
Loss at step 3860: 0.2275
Loss at step 3870: 0.1388
Loss at step 3880: 0.0561
Loss at step 3890: 0.0289
Loss at step 3900: 0.1313
Loss at step 3910: 0.0350
Loss at step 3920: 0.3351
Loss at step 3930: 0.0942
Loss at step 3940: 0.3827
Loss at step 3950: 0.0753
Loss at step 3960: 0.4273
Loss at step 3970: 0.3864
Loss at step 3980: 0.4539
Loss at step 3990: 0.3321
Loss at step 4000: 0.1903
Loss at step 4010: 0.1418
Loss at step 4020: 0.1174
Loss at step 4030: 0.2371
Loss at step 4040: 0.1501
Loss at step 4050: 0.0554
Loss at step 4060: 0.3173
Loss at step 4070: 0.1138
Loss at step 4080: 0.3399
Loss at step 4090: 0.1163
Loss at step 4100: 0.1116
Loss at step 4110: 0.1178
Loss at step 4120: 0.0835
Loss at step 4130: 0.2476
Loss at step 4140: 0.2213
Loss at step 4150: 0.0485
Loss at step 4160: 0.0707
Loss at step 4170: 0.1111
Loss at step 4180: 0.2750
Loss at step 4190: 0.1688
Loss at step 4200: 0.3741
Loss at step 4210: 0.3356
Loss at step 4220: 0.0840
Loss at step 4230: 0.0302
Loss at step 4240: 0.0270
Loss at step 4250: 0.0760
Loss at step 4260: 0.1151
Loss at step 4270: 0.1375
Loss at step 4280: 0.0271
Loss at step 4290: 0.0306
Loss at step 4300: 0.3677
Loss at step 4310: 0.3880
Loss at step 4320: 0.2924
Loss at step 4330: 0.3304
Loss at step 4340: 0.2896
Loss at step 4350: 0.1135
Loss at step 4360: 0.1697
Loss at step 4370: 0.4096
Loss at step 4380: 0.1833
Loss at step 4390: 0.0550
Loss at step 4400: 0.0288
Loss at step 4410: 0.3179
Loss at step 4420: 0.1605
Loss at step 4430: 0.2004
Loss at step 4440: 0.2061
Loss at step 4450: 0.0765
Loss at step 4460: 0.3754
Loss at step 4470: 0.2288
Loss at step 4480: 0.1095
Loss at step 4490: 0.0190
Loss at step 4500: 0.4045
Loss at step 4510: 0.1274
Loss at step 4520: 0.0205
Loss at step 4530: 0.4781
Loss at step 4540: 0.1155
Loss at step 4550: 0.2452
Loss at step 4560: 0.0305
Loss at step 4570: 0.2118
Loss at step 4580: 0.0528
Loss at step 4590: 0.1553
Loss at step 4600: 0.1514
Loss at step 4610: 0.1190
Loss at step 4620: 0.3964
Loss at step 4630: 0.1744
Loss at step 4640: 0.0280
Loss at step 4650: 0.0230
Loss at step 4660: 0.0885
Loss at step 4670: 0.4658
Loss at step 4680: 0.0480
Loss at step 4690: 0.2698
Loss at step 4700: 0.6499
Loss at step 4710: 0.2551
Loss at step 4720: 0.2875
Loss at step 4730: 0.1117
Loss at step 4740: 0.1713
Loss at step 4750: 0.4216
Loss at step 4760: 0.0946
Loss at step 4770: 0.5681
Loss at step 4780: 0.0539
Loss at step 4790: 0.2909
Loss at step 4800: 0.2122
Loss at step 4810: 0.2357
Loss at step 4820: 0.1504
Loss at step 4830: 0.0960
Loss at step 4840: 0.0530
Loss at step 4850: 0.2207
Loss at step 4860: 0.0676
Loss at step 4870: 0.2934
Loss at step 4880: 0.0558
Loss at step 4890: 0.1321
Loss at step 4900: 0.1434
Loss at step 4910: 0.1727
Loss at step 4920: 0.1781
Loss at step 4930: 0.1839
Loss at step 4940: 0.2213
Loss at step 4950: 0.1235
Loss at step 4960: 0.1857
Loss at step 4970: 0.4365
Loss at step 4980: 0.0553
Loss at step 4990: 0.0235
Loss at step 5000: 0.0369
Loss at step 5010: 0.1650
Loss at step 5020: 0.2088
Loss at step 5030: 0.0435
Loss at step 5040: 0.0481
Loss at step 5050: 0.0146
Loss at step 5060: 0.0137
Loss at step 5070: 0.1698
Loss at step 5080: 0.0775
Loss at step 5090: 0.0322
Loss at step 5100: 0.2845
Loss at step 5110: 0.1677
Loss at step 5120: 0.1069
Loss at step 5130: 0.1422
Loss at step 5140: 0.1429
Loss at step 5150: 0.0832
Loss at step 5160: 0.3009
Loss at step 5170: 0.2736
Loss at step 5180: 0.2419
Loss at step 5190: 0.1556
Loss at step 5200: 0.0108
Loss at step 5210: 0.0408
Loss at step 5220: 0.4101
Loss at step 5230: 0.3389
Loss at step 5240: 0.3573
Loss at step 5250: 0.1527
Loss at step 5260: 0.0320
Loss at step 5270: 0.3627
Loss at step 5280: 0.2011
Loss at step 5290: 0.1019
Loss at step 5300: 0.0212
Loss at step 5310: 0.2000
Loss at step 5320: 0.3086
Loss at step 5330: 0.0081
Loss at step 5340: 0.0710
Loss at step 5350: 0.0188
Loss at step 5360: 0.2914
Loss at step 5370: 0.0996
Loss at step 5380: 0.3547
Loss at step 5390: 0.1010
Loss at step 5400: 0.1052
Loss at step 5410: 0.0265
Loss at step 5420: 0.0320
Loss at step 5430: 0.0126
Loss at step 5440: 0.2395
Loss at step 5450: 0.0152
Loss at step 5460: 0.0796
Loss at step 5470: 0.1024
Loss at step 5480: 0.1233
Loss at step 5490: 0.0140
Loss at step 5500: 0.2488
Loss at step 5510: 0.1758
Loss at step 5520: 0.2285
Loss at step 5530: 0.0261
Loss at step 5540: 0.0995
Loss at step 5550: 0.2292
Loss at step 5560: 0.0847
Loss at step 5570: 0.3844
Loss at step 5580: 0.1062
Loss at step 5590: 0.0535
Loss at step 5600: 0.1739
Loss at step 5610: 0.0122
Loss at step 5620: 0.1523
Loss at step 5630: 0.3181
Loss at step 5640: 0.0099
Loss at step 5650: 0.0114
Loss at step 5660: 0.3219
Loss at step 5670: 0.0335
Loss at step 5680: 0.0158
Loss at step 5690: 0.1051
Loss at step 5700: 0.1126
Loss at step 5710: 0.0076
Loss at step 5720: 0.0192
Loss at step 5730: 0.0608
Loss at step 5740: 0.1768
Loss at step 5750: 0.0260
Loss at step 5760: 0.0744
Loss at step 5770: 0.0873
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.535597, 'precision': [0.621199, 0.522546, 0.44388], 'recall': [0.572917, 0.555153, 0.453864], 'f1': [0.596082, 0.538356, 0.448817]}
{'accuracy': 0.712094, 'precision': 0.44388, 'recall': 0.453864, 'f1': 0.448817, 'WordR': 0.271323}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7ce1cbdb0160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [0, 1039, 21119, 30397, 38873, 10861, 544, 416, 18821, 5357, 5, 30577, 8, 200, 86, 5, 2888, 1411, 66, 2, 2, 44799, 16, 13689, 59, 5, 2888, 544, 61, 16, 45, 447, 4, 50118, 45443, 982, 14, 42, 696, 16, 2121, 8, 5034, 13, 5, 2170, 9, 5, 1316, 8, 67, 982, 14, 51, 33, 10, 2789, 356, 88, 42, 696, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 1.1145
Loss at step 20: 1.1054
Loss at step 30: 1.0878
Loss at step 40: 1.1247
Loss at step 50: 1.0863
Loss at step 60: 1.0000
Loss at step 70: 1.0238
Loss at step 80: 1.0215
Loss at step 90: 0.9705
Loss at step 100: 1.1627
Loss at step 110: 0.9779
Loss at step 120: 1.0033
Loss at step 130: 0.9511
Loss at step 140: 1.0283
Loss at step 150: 0.9236
Loss at step 160: 0.9682
Loss at step 170: 1.2602
Loss at step 180: 1.0983
Loss at step 190: 1.0980
Loss at step 200: 1.0919
Loss at step 210: 1.0624
Loss at step 220: 1.0743
Loss at step 230: 1.0685
Loss at step 240: 1.0781
Loss at step 250: 1.0567
Loss at step 260: 1.1414
Loss at step 270: 1.0637
Loss at step 280: 1.1159
Loss at step 290: 1.1172
Loss at step 300: 1.1863
Loss at step 310: 0.9797
Loss at step 320: 1.0253
Loss at step 330: 0.9044
Loss at step 340: 0.9784
Loss at step 350: 1.1795
Loss at step 360: 0.9027
Loss at step 370: 0.8707
Loss at step 380: 1.3017
Loss at step 390: 1.2873
Loss at step 400: 0.8802
Loss at step 410: 1.0948
Loss at step 420: 1.2023
Loss at step 430: 1.0633
Loss at step 440: 0.8857
Loss at step 450: 1.0843
Loss at step 460: 1.1789
Loss at step 470: 0.9663
Loss at step 480: 0.8874
Loss at step 490: 1.0456
Loss at step 500: 1.0316
Loss at step 510: 1.0380
Loss at step 520: 1.1116
Loss at step 530: 1.0330
Loss at step 540: 0.9837
Loss at step 550: 0.9607
Loss at step 560: 1.0777
Loss at step 570: 1.0652
Loss at step 580: 1.1434
Loss at step 590: 1.0173
Loss at step 600: 0.9535
Loss at step 610: 0.9372
Loss at step 620: 0.7506
Loss at step 630: 0.9220
Loss at step 640: 1.1593
Loss at step 650: 1.0085
Loss at step 660: 0.8644
Loss at step 670: 0.9539
Loss at step 680: 0.9401
Loss at step 690: 1.0402
Loss at step 700: 1.1203
Loss at step 710: 1.1539
Loss at step 720: 1.0773
Loss at step 730: 1.0250
Loss at step 740: 0.9023
Loss at step 750: 1.0400
Loss at step 760: 0.8633
Loss at step 770: 1.0697
Loss at step 780: 0.9160
Loss at step 790: 0.9105
Loss at step 800: 0.8665
Loss at step 810: 0.7174
Loss at step 820: 1.0127
Loss at step 830: 0.8837
Loss at step 840: 0.9009
Loss at step 850: 0.7905
Loss at step 860: 0.9793
Loss at step 870: 0.8989
Loss at step 880: 0.8827
Loss at step 890: 0.7183
Loss at step 900: 0.9347
Loss at step 910: 0.7778
Loss at step 920: 1.1002
Loss at step 930: 0.8535
Loss at step 940: 0.9193
Loss at step 950: 0.8555
Loss at step 960: 0.8571
Loss at step 970: 0.9464
Loss at step 980: 0.9312
Loss at step 990: 0.9160
Loss at step 1000: 0.8292
Loss at step 1010: 0.9100
Loss at step 1020: 0.9483
Loss at step 1030: 1.0223
Loss at step 1040: 0.9203
Loss at step 1050: 0.9768
Loss at step 1060: 0.9005
Loss at step 1070: 0.9167
Loss at step 1080: 0.5384
Loss at step 1090: 0.9518
Loss at step 1100: 0.8258
Loss at step 1110: 0.6917
Loss at step 1120: 0.8910
Loss at step 1130: 0.8789
Loss at step 1140: 1.0922
Loss at step 1150: 0.7330
Loss at step 1160: 0.8169
Loss at step 1170: 0.8538
Loss at step 1180: 0.8521
Loss at step 1190: 0.9018
Loss at step 1200: 0.8078
Loss at step 1210: 0.9019
Loss at step 1220: 0.6455
Loss at step 1230: 0.7711
Loss at step 1240: 0.8670
Loss at step 1250: 0.9289
Loss at step 1260: 0.7112
Loss at step 1270: 0.8459
Loss at step 1280: 0.6969
Loss at step 1290: 0.8523
Loss at step 1300: 0.7428
Loss at step 1310: 0.8867
Loss at step 1320: 0.7732
Loss at step 1330: 0.5408
Loss at step 1340: 0.7898
Loss at step 1350: 0.8592
Loss at step 1360: 0.6834
Loss at step 1370: 0.6738
Loss at step 1380: 0.5165
Loss at step 1390: 0.8784
Loss at step 1400: 1.1592
Loss at step 1410: 0.7847
Loss at step 1420: 0.6790
Loss at step 1430: 0.5431
Loss at step 1440: 0.6812
Loss at step 1450: 0.6220
Loss at step 1460: 0.6159
Loss at step 1470: 0.6861
Loss at step 1480: 0.6696
Loss at step 1490: 0.7251
Loss at step 1500: 0.6373
Loss at step 1510: 0.5731
Loss at step 1520: 0.9088
Loss at step 1530: 0.6336
Loss at step 1540: 0.6255
Loss at step 1550: 0.7207
Loss at step 1560: 0.7798
Loss at step 1570: 0.5266
Loss at step 1580: 0.6380
Loss at step 1590: 0.5914
Loss at step 1600: 0.8724
Loss at step 1610: 1.0020
Loss at step 1620: 0.5672
Loss at step 1630: 0.8504
Loss at step 1640: 1.0403
Loss at step 1650: 0.6618
Loss at step 1660: 0.6177
Loss at step 1670: 0.6913
Loss at step 1680: 0.6536
Loss at step 1690: 0.8474
Loss at step 1700: 0.6075
Loss at step 1710: 0.5837
Loss at step 1720: 0.5267
Loss at step 1730: 0.4637
Loss at step 1740: 0.8601
Loss at step 1750: 0.6022
Loss at step 1760: 0.4984
Loss at step 1770: 0.6458
Loss at step 1780: 0.3477
Loss at step 1790: 0.6347
Loss at step 1800: 0.3409
Loss at step 1810: 0.7203
Loss at step 1820: 0.6113
Loss at step 1830: 0.5865
Loss at step 1840: 0.5780
Loss at step 1850: 0.5923
Loss at step 1860: 0.8763
Loss at step 1870: 0.4967
Loss at step 1880: 0.5961
Loss at step 1890: 0.6958
Loss at step 1900: 0.4741
Loss at step 1910: 0.5430
Loss at step 1920: 0.6646
Loss at step 1930: 0.7872
Loss at step 1940: 0.6314
Loss at step 1950: 0.3869
Loss at step 1960: 0.4849
Loss at step 1970: 0.5079
Loss at step 1980: 0.3365
Loss at step 1990: 0.4297
Loss at step 2000: 0.4949
Loss at step 2010: 0.5739
Loss at step 2020: 0.3675
Loss at step 2030: 0.6518
Loss at step 2040: 0.4295
Loss at step 2050: 0.3914
Loss at step 2060: 0.4186
Loss at step 2070: 0.3915
Loss at step 2080: 0.3668
Loss at step 2090: 0.4608
Loss at step 2100: 0.3291
Loss at step 2110: 0.5290
Loss at step 2120: 0.3846
Loss at step 2130: 0.5426
Loss at step 2140: 0.6918
Loss at step 2150: 0.7283
Loss at step 2160: 0.2925
Loss at step 2170: 0.3859
Loss at step 2180: 0.5616
Loss at step 2190: 0.4686
Loss at step 2200: 0.4666
Loss at step 2210: 0.4804
Loss at step 2220: 0.4254
Loss at step 2230: 0.5790
Loss at step 2240: 0.5174
Loss at step 2250: 0.3846
Loss at step 2260: 0.2891
Loss at step 2270: 0.3260
Loss at step 2280: 0.5686
Loss at step 2290: 0.4631
Loss at step 2300: 0.3796
Loss at step 2310: 0.2402
Loss at step 2320: 0.7661
Loss at step 2330: 0.4000
Loss at step 2340: 0.5864
Loss at step 2350: 0.6408
Loss at step 2360: 0.3656
Loss at step 2370: 0.5896
Loss at step 2380: 0.4161
Loss at step 2390: 0.3352
Loss at step 2400: 0.3635
Loss at step 2410: 0.5924
Loss at step 2420: 0.3522
Loss at step 2430: 0.4260
Loss at step 2440: 0.2613
Loss at step 2450: 0.4117
Loss at step 2460: 0.3297
Loss at step 2470: 0.4224
Loss at step 2480: 0.6009
Loss at step 2490: 0.2740
Loss at step 2500: 0.3077
Loss at step 2510: 0.5155
Loss at step 2520: 0.4958
Loss at step 2530: 0.5415
Loss at step 2540: 0.5963
Loss at step 2550: 0.5152
Loss at step 2560: 0.4428
Loss at step 2570: 0.3664
Loss at step 2580: 0.6791
Loss at step 2590: 0.3089
Loss at step 2600: 0.2166
Loss at step 2610: 0.5463
Loss at step 2620: 0.4384
Loss at step 2630: 0.4695
Loss at step 2640: 0.2872
Loss at step 2650: 0.2738
Loss at step 2660: 0.2966
Loss at step 2670: 0.4979
Loss at step 2680: 0.2049
Loss at step 2690: 0.2529
Loss at step 2700: 0.4295
Loss at step 2710: 0.2832
Loss at step 2720: 0.5199
Loss at step 2730: 0.2137
Loss at step 2740: 0.1611
Loss at step 2750: 0.1889
Loss at step 2760: 0.5277
Loss at step 2770: 0.1602
Loss at step 2780: 0.2108
Loss at step 2790: 0.3347
Loss at step 2800: 0.2171
Loss at step 2810: 0.2016
Loss at step 2820: 0.3109
Loss at step 2830: 0.3804
Loss at step 2840: 0.2151
Loss at step 2850: 0.3930
Loss at step 2860: 0.3629
Loss at step 2870: 0.3796
Loss at step 2880: 0.1668
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.522192, 'precision': [0.685961, 0.468526, 0.473962], 'recall': [0.446314, 0.710145, 0.362168], 'f1': [0.540777, 0.56457, 0.410592]}
{'accuracy': 0.731457, 'precision': 0.473962, 'recall': 0.362168, 'f1': 0.410592, 'WordR': 0.271323}
Loss at step 2890: 0.2261
Loss at step 2900: 0.0743
Loss at step 2910: 0.1654
Loss at step 2920: 0.2643
Loss at step 2930: 0.1406
Loss at step 2940: 0.3406
Loss at step 2950: 0.2704
Loss at step 2960: 0.3673
Loss at step 2970: 0.2393
Loss at step 2980: 0.1737
Loss at step 2990: 0.2267
Loss at step 3000: 0.3933
Loss at step 3010: 0.2812
Loss at step 3020: 0.4613
Loss at step 3030: 0.5576
Loss at step 3040: 0.1367
Loss at step 3050: 0.3670
Loss at step 3060: 0.7651
Loss at step 3070: 0.4018
Loss at step 3080: 0.1460
Loss at step 3090: 0.2702
Loss at step 3100: 0.1018
Loss at step 3110: 0.1473
Loss at step 3120: 0.0812
Loss at step 3130: 0.4681
Loss at step 3140: 0.0778
Loss at step 3150: 0.3102
Loss at step 3160: 0.4339
Loss at step 3170: 0.2083
Loss at step 3180: 0.1795
Loss at step 3190: 0.0830
Loss at step 3200: 0.0522
Loss at step 3210: 0.2571
Loss at step 3220: 0.1660
Loss at step 3230: 0.2527
Loss at step 3240: 0.2638
Loss at step 3250: 0.2673
Loss at step 3260: 0.2732
Loss at step 3270: 0.4609
Loss at step 3280: 0.2711
Loss at step 3290: 0.1585
Loss at step 3300: 0.3114
Loss at step 3310: 0.1752
Loss at step 3320: 0.1394
Loss at step 3330: 0.1205
Loss at step 3340: 0.1229
Loss at step 3350: 0.2563
Loss at step 3360: 0.2836
Loss at step 3370: 0.4373
Loss at step 3380: 0.3999
Loss at step 3390: 0.2026
Loss at step 3400: 0.3505
Loss at step 3410: 0.2107
Loss at step 3420: 0.1855
Loss at step 3430: 0.2008
Loss at step 3440: 0.1478
Loss at step 3450: 0.1844
Loss at step 3460: 0.2786
Loss at step 3470: 0.2610
Loss at step 3480: 0.1555
Loss at step 3490: 0.1424
Loss at step 3500: 0.2166
Loss at step 3510: 0.2962
Loss at step 3520: 0.2423
Loss at step 3530: 0.2058
Loss at step 3540: 0.2421
Loss at step 3550: 0.2104
Loss at step 3560: 0.1452
Loss at step 3570: 0.0845
Loss at step 3580: 0.2548
Loss at step 3590: 0.0230
Loss at step 3600: 0.2112
Loss at step 3610: 0.3681
Loss at step 3620: 0.1286
Loss at step 3630: 0.1183
Loss at step 3640: 0.1030
Loss at step 3650: 0.3830
Loss at step 3660: 0.1478
Loss at step 3670: 0.3360
Loss at step 3680: 0.3947
Loss at step 3690: 0.0217
Loss at step 3700: 0.2452
Loss at step 3710: 0.0917
Loss at step 3720: 0.0741
Loss at step 3730: 0.1236
Loss at step 3740: 0.2240
Loss at step 3750: 0.6212
Loss at step 3760: 0.1456
Loss at step 3770: 0.0612
Loss at step 3780: 0.0933
Loss at step 3790: 0.1576
Loss at step 3800: 0.3297
Loss at step 3810: 0.1766
Loss at step 3820: 0.1570
Loss at step 3830: 0.1163
Loss at step 3840: 0.3406
Loss at step 3850: 0.0898
Loss at step 3860: 0.2275
Loss at step 3870: 0.1388
Loss at step 3880: 0.0561
Loss at step 3890: 0.0289
Loss at step 3900: 0.1313
Loss at step 3910: 0.0350
Loss at step 3920: 0.3351
Loss at step 3930: 0.0942
Loss at step 3940: 0.3827
Loss at step 3950: 0.0753
Loss at step 3960: 0.4273
Loss at step 3970: 0.3864
Loss at step 3980: 0.4539
Loss at step 3990: 0.3321
Loss at step 4000: 0.1903
Loss at step 4010: 0.1418
Loss at step 4020: 0.1174
Loss at step 4030: 0.2371
Loss at step 4040: 0.1501
Loss at step 4050: 0.0554
Loss at step 4060: 0.3173
Loss at step 4070: 0.1138
Loss at step 4080: 0.3399
Loss at step 4090: 0.1163
Loss at step 4100: 0.1116
Loss at step 4110: 0.1178
Loss at step 4120: 0.0835
Loss at step 4130: 0.2476
Loss at step 4140: 0.2213
Loss at step 4150: 0.0485
Loss at step 4160: 0.0707
Loss at step 4170: 0.1111
Loss at step 4180: 0.2750
Loss at step 4190: 0.1688
Loss at step 4200: 0.3741
Loss at step 4210: 0.3356
Loss at step 4220: 0.0840
Loss at step 4230: 0.0302
Loss at step 4240: 0.0270
Loss at step 4250: 0.0760
Loss at step 4260: 0.1151
Loss at step 4270: 0.1375
Loss at step 4280: 0.0271
Loss at step 4290: 0.0306
Loss at step 4300: 0.3677
Loss at step 4310: 0.3880
Loss at step 4320: 0.2924
Loss at step 4330: 0.3304
Loss at step 4340: 0.2896
Loss at step 4350: 0.1135
Loss at step 4360: 0.1697
Loss at step 4370: 0.4096
Loss at step 4380: 0.1833
Loss at step 4390: 0.0550
Loss at step 4400: 0.0288
Loss at step 4410: 0.3179
Loss at step 4420: 0.1605
Loss at step 4430: 0.2004
Loss at step 4440: 0.2061
Loss at step 4450: 0.0765
Loss at step 4460: 0.3754
Loss at step 4470: 0.2288
Loss at step 4480: 0.1095
Loss at step 4490: 0.0190
Loss at step 4500: 0.4045
Loss at step 4510: 0.1274
Loss at step 4520: 0.0205
Loss at step 4530: 0.4781
Loss at step 4540: 0.1155
Loss at step 4550: 0.2452
Loss at step 4560: 0.0305
Loss at step 4570: 0.2118
Loss at step 4580: 0.0528
Loss at step 4590: 0.1553
Loss at step 4600: 0.1514
Loss at step 4610: 0.1190
Loss at step 4620: 0.3964
Loss at step 4630: 0.1744
Loss at step 4640: 0.0280
Loss at step 4650: 0.0230
Loss at step 4660: 0.0885
Loss at step 4670: 0.4658
Loss at step 4680: 0.0480
Loss at step 4690: 0.2698
Loss at step 4700: 0.6499
Loss at step 4710: 0.2551
Loss at step 4720: 0.2875
Loss at step 4730: 0.1117
Loss at step 4740: 0.1713
Loss at step 4750: 0.4216
Loss at step 4760: 0.0946
Loss at step 4770: 0.5681
Loss at step 4780: 0.0539
Loss at step 4790: 0.2909
Loss at step 4800: 0.2122
Loss at step 4810: 0.2357
Loss at step 4820: 0.1504
Loss at step 4830: 0.0960
Loss at step 4840: 0.0530
Loss at step 4850: 0.2207
Loss at step 4860: 0.0676
Loss at step 4870: 0.2934
Loss at step 4880: 0.0558
Loss at step 4890: 0.1321
Loss at step 4900: 0.1434
Loss at step 4910: 0.1727
Loss at step 4920: 0.1781
Loss at step 4930: 0.1839
Loss at step 4940: 0.2213
Loss at step 4950: 0.1235
Loss at step 4960: 0.1857
Loss at step 4970: 0.4365
Loss at step 4980: 0.0553
Loss at step 4990: 0.0235
Loss at step 5000: 0.0369
Loss at step 5010: 0.1650
Loss at step 5020: 0.2088
Loss at step 5030: 0.0435
Loss at step 5040: 0.0481
Loss at step 5050: 0.0146
Loss at step 5060: 0.0137
Loss at step 5070: 0.1698
Loss at step 5080: 0.0775
Loss at step 5090: 0.0322
Loss at step 5100: 0.2845
Loss at step 5110: 0.1677
Loss at step 5120: 0.1069
Loss at step 5130: 0.1422
Loss at step 5140: 0.1429
Loss at step 5150: 0.0832
Loss at step 5160: 0.3009
Loss at step 5170: 0.2736
Loss at step 5180: 0.2419
Loss at step 5190: 0.1556
Loss at step 5200: 0.0108
Loss at step 5210: 0.0408
Loss at step 5220: 0.4101
Loss at step 5230: 0.3389
Loss at step 5240: 0.3573
Loss at step 5250: 0.1527
Loss at step 5260: 0.0320
Loss at step 5270: 0.3627
Loss at step 5280: 0.2011
Loss at step 5290: 0.1019
Loss at step 5300: 0.0212
Loss at step 5310: 0.2000
Loss at step 5320: 0.3086
Loss at step 5330: 0.0081
Loss at step 5340: 0.0710
Loss at step 5350: 0.0188
Loss at step 5360: 0.2914
Loss at step 5370: 0.0996
Loss at step 5380: 0.3547
Loss at step 5390: 0.1010
Loss at step 5400: 0.1052
Loss at step 5410: 0.0265
Loss at step 5420: 0.0320
Loss at step 5430: 0.0126
Loss at step 5440: 0.2395
Loss at step 5450: 0.0152
Loss at step 5460: 0.0796
Loss at step 5470: 0.1024
Loss at step 5480: 0.1233
Loss at step 5490: 0.0140
Loss at step 5500: 0.2488
Loss at step 5510: 0.1758
Loss at step 5520: 0.2285
Loss at step 5530: 0.0261
Loss at step 5540: 0.0995
Loss at step 5550: 0.2292
Loss at step 5560: 0.0847
Loss at step 5570: 0.3844
Loss at step 5580: 0.1062
Loss at step 5590: 0.0535
Loss at step 5600: 0.1739
Loss at step 5610: 0.0122
Loss at step 5620: 0.1523
Loss at step 5630: 0.3181
Loss at step 5640: 0.0099
Loss at step 5650: 0.0114
Loss at step 5660: 0.3219
Loss at step 5670: 0.0335
Loss at step 5680: 0.0158
Loss at step 5690: 0.1051
Loss at step 5700: 0.1126
Loss at step 5710: 0.0076
Loss at step 5720: 0.0192
Loss at step 5730: 0.0608
Loss at step 5740: 0.1768
Loss at step 5750: 0.0260
Loss at step 5760: 0.0744
Loss at step 5770: 0.0873
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.535597, 'precision': [0.621199, 0.522546, 0.44388], 'recall': [0.572917, 0.555153, 0.453864], 'f1': [0.596082, 0.538356, 0.448817]}
{'accuracy': 0.712094, 'precision': 0.44388, 'recall': 0.453864, 'f1': 0.448817, 'WordR': 0.271323}
Loss at step 5780: 0.1216
Loss at step 5790: 0.0370
Loss at step 5800: 0.3326
Loss at step 5810: 0.0285
Loss at step 5820: 0.0668
Loss at step 5830: 0.1097
Loss at step 5840: 0.1090
Loss at step 5850: 0.0137
Loss at step 5860: 0.0746
Loss at step 5870: 0.0153
Loss at step 5880: 0.1870
Loss at step 5890: 0.0789
Loss at step 5900: 0.1465
Loss at step 5910: 0.0703
Loss at step 5920: 0.1076
Loss at step 5930: 0.1539
Loss at step 5940: 0.0172
Loss at step 5950: 0.0270
Loss at step 5960: 0.0603
Loss at step 5970: 0.0667
Loss at step 5980: 0.1202
Loss at step 5990: 0.1764
Loss at step 6000: 0.0035
Loss at step 6010: 0.0165
Loss at step 6020: 0.1435
Loss at step 6030: 0.1123
Loss at step 6040: 0.2808
Loss at step 6050: 0.1248
Loss at step 6060: 0.0516
Loss at step 6070: 0.0582
Loss at step 6080: 0.0076
Loss at step 6090: 0.0085
Loss at step 6100: 0.1220
Loss at step 6110: 0.1997
Loss at step 6120: 0.0033
Loss at step 6130: 0.0374
Loss at step 6140: 0.2757
Loss at step 6150: 0.2475
Loss at step 6160: 0.0420
Loss at step 6170: 0.0442
Loss at step 6180: 0.1514
Loss at step 6190: 0.0064
Loss at step 6200: 0.0040
Loss at step 6210: 0.0791
Loss at step 6220: 0.0630
Loss at step 6230: 0.1321
Loss at step 6240: 0.0076
Loss at step 6250: 0.2767
Loss at step 6260: 0.0049
Loss at step 6270: 0.1570
Loss at step 6280: 0.3871
Loss at step 6290: 0.4630
Loss at step 6300: 0.0089
Loss at step 6310: 0.0169
Loss at step 6320: 0.2183
Loss at step 6330: 0.0439
Loss at step 6340: 0.1717
Loss at step 6350: 0.0171
Loss at step 6360: 0.1624
Loss at step 6370: 0.1186
Loss at step 6380: 0.0121
Loss at step 6390: 0.0256
Loss at step 6400: 0.0095
Loss at step 6410: 0.0123
Loss at step 6420: 0.3170
Loss at step 6430: 0.0043
Loss at step 6440: 0.3041
Loss at step 6450: 0.0039
Loss at step 6460: 0.0067
Loss at step 6470: 0.1418
Loss at step 6480: 0.1478
Loss at step 6490: 0.1399
Loss at step 6500: 0.3820
Loss at step 6510: 0.0203
Loss at step 6520: 0.0163
Loss at step 6530: 0.2721
Loss at step 6540: 0.1262
Loss at step 6550: 0.0083
Loss at step 6560: 0.0053
Loss at step 6570: 0.0143
Loss at step 6580: 0.1804
Loss at step 6590: 0.2804
Loss at step 6600: 0.0022
Loss at step 6610: 0.0057
Loss at step 6620: 0.3048
Loss at step 6630: 0.0665
Loss at step 6640: 0.0266
Loss at step 6650: 0.0619
Loss at step 6660: 0.0245
Loss at step 6670: 0.2761
Loss at step 6680: 0.1057
Loss at step 6690: 0.3183
Loss at step 6700: 0.0040
Loss at step 6710: 0.1314
Loss at step 6720: 0.0068
Loss at step 6730: 0.1819
Loss at step 6740: 0.0270
Loss at step 6750: 0.0199
Loss at step 6760: 0.2100
Loss at step 6770: 0.0219
Loss at step 6780: 0.1464
Loss at step 6790: 0.2896
Loss at step 6800: 0.1402
Loss at step 6810: 0.0038
Loss at step 6820: 0.0105
Loss at step 6830: 0.0206
Loss at step 6840: 0.0241
Loss at step 6850: 0.1535
Loss at step 6860: 0.1848
Loss at step 6870: 0.2031
Loss at step 6880: 0.0095
Loss at step 6890: 0.0054
Loss at step 6900: 0.1125
Loss at step 6910: 0.0168
Loss at step 6920: 0.0236
Loss at step 6930: 0.0041
Loss at step 6940: 0.0636
Loss at step 6950: 0.0285
Loss at step 6960: 0.2198
Loss at step 6970: 0.0030
Loss at step 6980: 0.1694
Loss at step 6990: 0.1823
Loss at step 7000: 0.0043
Loss at step 7010: 0.2843
Loss at step 7020: 0.1890
Loss at step 7030: 0.1814
Loss at step 7040: 0.3188
Loss at step 7050: 0.3250
Loss at step 7060: 0.0816
Loss at step 7070: 0.0053
Loss at step 7080: 0.1223
Loss at step 7090: 0.0045
Loss at step 7100: 0.1276
Loss at step 7110: 0.1204
Loss at step 7120: 0.0213
Loss at step 7130: 0.0196
Loss at step 7140: 0.0035
Loss at step 7150: 0.1055
Loss at step 7160: 0.0046
Loss at step 7170: 0.2015
Loss at step 7180: 0.0151
Loss at step 7190: 0.0091
Loss at step 7200: 0.0683
Loss at step 7210: 0.1219
Loss at step 7220: 0.3394
Loss at step 7230: 0.1599
Loss at step 7240: 0.0421
Loss at step 7250: 0.0313
Loss at step 7260: 0.0044
Loss at step 7270: 0.1492
Loss at step 7280: 0.0064
Loss at step 7290: 0.0555
Loss at step 7300: 0.0777
Loss at step 7310: 0.1707
Loss at step 7320: 0.3635
Loss at step 7330: 0.0706
Loss at step 7340: 0.1923
Loss at step 7350: 0.0899
Loss at step 7360: 0.2586
Loss at step 7370: 0.0054
Loss at step 7380: 0.1506
Loss at step 7390: 0.0056
Loss at step 7400: 0.0786
Loss at step 7410: 0.0048
Loss at step 7420: 0.1092
Loss at step 7430: 0.2775
Loss at step 7440: 0.0048
Loss at step 7450: 0.0038
Loss at step 7460: 0.0031
Loss at step 7470: 0.4016
Loss at step 7480: 0.0851
Loss at step 7490: 0.1178
Loss at step 7500: 0.0073
Loss at step 7510: 0.0035
Loss at step 7520: 0.0025
Loss at step 7530: 0.1558
Loss at step 7540: 0.0038
Loss at step 7550: 0.0022
Loss at step 7560: 0.2383
Loss at step 7570: 0.0067
Loss at step 7580: 0.0034
Loss at step 7590: 0.0053
Loss at step 7600: 0.0306
Loss at step 7610: 0.0032
Loss at step 7620: 0.1373
Loss at step 7630: 0.1476
Loss at step 7640: 0.1041
Loss at step 7650: 0.0044
Loss at step 7660: 0.1912
Loss at step 7670: 0.0032
Loss at step 7680: 0.0876
Loss at step 7690: 0.0067
Loss at step 7700: 0.0151
Loss at step 7710: 0.0048
Loss at step 7720: 0.0025
Loss at step 7730: 0.0053
Loss at step 7740: 0.1054
Loss at step 7750: 0.0716
Loss at step 7760: 0.2754
Loss at step 7770: 0.1141
Loss at step 7780: 0.0673
Loss at step 7790: 0.1297
Loss at step 7800: 0.0040
Loss at step 7810: 0.0041
Loss at step 7820: 0.0043
Loss at step 7830: 0.0059
Loss at step 7840: 0.0358
Loss at step 7850: 0.0841
Loss at step 7860: 0.0026
Loss at step 7870: 0.0060
Loss at step 7880: 0.2140
Loss at step 7890: 0.0059
Loss at step 7900: 0.0032
Loss at step 7910: 0.0215
Loss at step 7920: 0.0967
Loss at step 7930: 0.0042
Loss at step 7940: 0.0040
Loss at step 7950: 0.0121
Loss at step 7960: 0.0223
Loss at step 7970: 0.0039
Loss at step 7980: 0.0099
Loss at step 7990: 0.0024
Loss at step 8000: 0.4244
Loss at step 8010: 0.0040
Loss at step 8020: 0.0787
Loss at step 8030: 0.0046
Loss at step 8040: 0.1031
Loss at step 8050: 0.1758
Loss at step 8060: 0.3545
Loss at step 8070: 0.1156
Loss at step 8080: 0.2343
Loss at step 8090: 0.0039
Loss at step 8100: 0.1067
Loss at step 8110: 0.1351
Loss at step 8120: 0.2688
Loss at step 8130: 0.0336
Loss at step 8140: 0.0043
Loss at step 8150: 0.0028
Loss at step 8160: 0.0365
Loss at step 8170: 0.0039
Loss at step 8180: 0.0916
Loss at step 8190: 0.1392
Loss at step 8200: 0.2773
Loss at step 8210: 0.0044
Loss at step 8220: 0.2180
Loss at step 8230: 0.0037
Loss at step 8240: 0.0298
Loss at step 8250: 0.2637
Loss at step 8260: 0.0119
Loss at step 8270: 0.0045
Loss at step 8280: 0.0032
Loss at step 8290: 0.0380
Loss at step 8300: 0.0052
Loss at step 8310: 0.1677
Loss at step 8320: 0.0058
Loss at step 8330: 0.0027
Loss at step 8340: 0.0033
Loss at step 8350: 0.0095
Loss at step 8360: 0.0203
Loss at step 8370: 0.0019
Loss at step 8380: 0.0032
Loss at step 8390: 0.0182
Loss at step 8400: 0.1556
Loss at step 8410: 0.1760
Loss at step 8420: 0.0082
Loss at step 8430: 0.1369
Loss at step 8440: 0.1411
Loss at step 8450: 0.1552
Loss at step 8460: 0.1610
Loss at step 8470: 0.1820
Loss at step 8480: 0.1502
Loss at step 8490: 0.0568
Loss at step 8500: 0.0040
Loss at step 8510: 0.0039
Loss at step 8520: 0.0027
Loss at step 8530: 0.0565
Loss at step 8540: 0.0033
Loss at step 8550: 0.0104
Loss at step 8560: 0.1201
Loss at step 8570: 0.0117
Loss at step 8580: 0.0021
Loss at step 8590: 0.1217
Loss at step 8600: 0.0041
Loss at step 8610: 0.1155
Loss at step 8620: 0.2715
Loss at step 8630: 0.2361
Loss at step 8640: 0.0795
Loss at step 8650: 0.0050
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.532618, 'precision': [0.638728, 0.506382, 0.449684], 'recall': [0.53125, 0.590982, 0.45098], 'f1': [0.580052, 0.545421, 0.450331]}
{'accuracy': 0.715669, 'precision': 0.449684, 'recall': 0.45098, 'f1': 0.450331, 'WordR': 0.271323}
