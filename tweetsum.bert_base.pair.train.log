Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x739371cb28b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7983989e9700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x755dd0969700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x71fe935ea700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7127c6cf35e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7684df9354c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7c3b8afafaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7bbea1d71af0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7372ad5afaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Sample 83810 of the training set: {'input_ids': [101, 1030, 3962, 8757, 16302, 2015, 2134, 1521, 1056, 4025, 2000, 2022, 2151, 5724, 2008, 1045, 2071, 2424, 1999, 1996, 16380, 10439, 102, 2043, 2017, 2123, 1521, 1056, 3477, 2005, 3962, 8757, 1996, 3325, 2003, 2428, 4485, 1010, 4788, 2084, 1996, 2557, 1012, 2065, 2017, 2215, 2033, 2000, 3477, 2769, 2005, 2091, 16307, 2075, 2059, 13417, 1037, 2172, 1008, 2025, 4658, 1008, 3325, 3531, 2265, 2033, 2129, 2000, 2507, 2017, 2026, 2769, 2153, 1012, 3531, 2292, 2149, 2113, 2065, 2017, 2412, 2342, 2149, 2153, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 1.0634
Loss at step 20: 0.9048
Loss at step 30: 0.8914
Loss at step 40: 0.9212
Loss at step 50: 1.0366
Loss at step 60: 1.0975
Loss at step 70: 1.0460
Loss at step 80: 0.9045
Loss at step 90: 1.0749
Loss at step 100: 0.8807
Loss at step 110: 0.8759
Loss at step 120: 0.8930
Loss at step 130: 0.7009
Loss at step 140: 1.0178
Loss at step 150: 0.9867
Loss at step 160: 0.9237
Loss at step 170: 0.8827
Loss at step 180: 0.7794
Loss at step 190: 0.8033
Loss at step 200: 0.8115
Loss at step 210: 1.0096
Loss at step 220: 0.9839
Loss at step 230: 0.8685
Loss at step 240: 0.9606
Loss at step 250: 0.7633
Loss at step 260: 0.6514
Loss at step 270: 0.9740
Loss at step 280: 0.8179
Loss at step 290: 0.6406
Loss at step 300: 0.8608
Loss at step 310: 0.6261
Loss at step 320: 0.7014
Loss at step 330: 0.8005
Loss at step 340: 0.6603
Loss at step 350: 0.6982
Loss at step 360: 0.7162
Loss at step 370: 0.9608
Loss at step 380: 0.9046
Loss at step 390: 0.7838
Loss at step 400: 0.6143
Loss at step 410: 0.7650
Loss at step 420: 0.6317
Loss at step 430: 0.8178
Loss at step 440: 0.6717
Loss at step 450: 0.7481
Loss at step 460: 0.7277
Loss at step 470: 0.7940
Loss at step 480: 0.9109
Loss at step 490: 1.0107
Loss at step 500: 0.5995
Loss at step 510: 0.7032
Loss at step 520: 0.6716
Loss at step 530: 0.5796
Loss at step 540: 0.6570
Loss at step 550: 0.8580
Loss at step 560: 0.4740
Loss at step 570: 0.6721
Loss at step 580: 0.7462
Loss at step 590: 0.7453
Loss at step 600: 0.6968
Loss at step 610: 0.8203
Loss at step 620: 0.7824
Loss at step 630: 0.8322
Loss at step 640: 0.6238
Loss at step 650: 0.5888
Loss at step 660: 0.4632
Loss at step 670: 0.7313
Loss at step 680: 0.7522
Loss at step 690: 0.5820
Loss at step 700: 0.4838
Loss at step 710: 0.5335
Loss at step 720: 0.6053
Loss at step 730: 0.4306
Loss at step 740: 0.6523
Loss at step 750: 0.6282
Loss at step 760: 0.4242
Loss at step 770: 0.3449
Loss at step 780: 0.3363
Loss at step 790: 0.5048
Loss at step 800: 0.5098
Loss at step 810: 0.6090
Loss at step 820: 0.5794
Loss at step 830: 0.3485
Loss at step 840: 0.4859
Loss at step 850: 0.7137
Loss at step 860: 0.8376
Loss at step 870: 0.5978
Loss at step 880: 0.4497
Loss at step 890: 0.3761
Loss at step 900: 0.5802
Loss at step 910: 0.4431
Loss at step 920: 0.6728
Loss at step 930: 0.4304
Loss at step 940: 0.4900
Loss at step 950: 0.3464
Loss at step 960: 0.5366
Loss at step 970: 0.2647
Loss at step 980: 0.3655
Loss at step 990: 0.5287
Loss at step 1000: 0.4062
Loss at step 1010: 0.5166
Loss at step 1020: 0.4301
Loss at step 1030: 0.4714
Loss at step 1040: 0.4873
Loss at step 1050: 0.3380
Loss at step 1060: 0.3901
Loss at step 1070: 0.5108
Loss at step 1080: 0.2481
Loss at step 1090: 0.3549
Loss at step 1100: 0.4772
Loss at step 1110: 0.2382
Loss at step 1120: 0.6061
Loss at step 1130: 0.4386
Loss at step 1140: 0.2987
Loss at step 1150: 0.1946
Loss at step 1160: 0.2950
Loss at step 1170: 0.4279
Loss at step 1180: 0.3447
Loss at step 1190: 0.2072
Loss at step 1200: 0.4270
Loss at step 1210: 0.3291
Loss at step 1220: 0.4416
Loss at step 1230: 0.6348
Loss at step 1240: 1.0037
Loss at step 1250: 0.4931
Loss at step 1260: 0.3531
Loss at step 1270: 0.3864
Loss at step 1280: 0.2218
Loss at step 1290: 0.3549
Loss at step 1300: 0.1733
Loss at step 1310: 0.4453
Loss at step 1320: 0.2741
Loss at step 1330: 0.4575
Loss at step 1340: 0.3158
Loss at step 1350: 0.3788
Loss at step 1360: 0.3638
Loss at step 1370: 0.2694
Loss at step 1380: 0.4487
Loss at step 1390: 0.2048
Loss at step 1400: 0.1376
Loss at step 1410: 0.2202
Loss at step 1420: 0.3804
Loss at step 1430: 0.2990
Loss at step 1440: 0.3122
Loss at step 1450: 0.3485
Loss at step 1460: 0.5460
Loss at step 1470: 0.2928
Loss at step 1480: 0.1251
Loss at step 1490: 0.1069
Loss at step 1500: 0.3270
Loss at step 1510: 0.3661
Loss at step 1520: 0.2345
Loss at step 1530: 0.2532
Loss at step 1540: 0.1629
Loss at step 1550: 0.2661
Loss at step 1560: 0.2249
Loss at step 1570: 0.1286
Loss at step 1580: 0.3860
Loss at step 1590: 0.1437
Loss at step 1600: 0.2844
Loss at step 1610: 0.2439
Loss at step 1620: 0.1539
Loss at step 1630: 0.3837
Loss at step 1640: 0.2479
Loss at step 1650: 0.2538
Loss at step 1660: 0.2204
Loss at step 1670: 0.3860
Loss at step 1680: 0.1650
Loss at step 1690: 0.1275
Loss at step 1700: 0.6788
Loss at step 1710: 0.0941
Loss at step 1720: 0.0845
Loss at step 1730: 0.2215
Loss at step 1740: 0.2088
Loss at step 1750: 0.1702
Loss at step 1760: 0.0806
Loss at step 1770: 0.0872
Loss at step 1780: 0.2328
Loss at step 1790: 0.1310
Loss at step 1800: 0.1418
Loss at step 1810: 0.1003
Loss at step 1820: 0.2220
Loss at step 1830: 0.3791
Loss at step 1840: 0.1132
Loss at step 1850: 0.1782
Loss at step 1860: 0.6307
Loss at step 1870: 0.1948
Loss at step 1880: 0.1861
Loss at step 1890: 0.3328
Loss at step 1900: 0.0412
Loss at step 1910: 0.2837
Loss at step 1920: 0.2248
Loss at step 1930: 0.5660
Loss at step 1940: 0.1534
Loss at step 1950: 0.1993
Loss at step 1960: 0.3060
Loss at step 1970: 0.3778
Loss at step 1980: 0.2386
Loss at step 1990: 0.2502
Loss at step 2000: 0.0330
Loss at step 2010: 0.1591
Loss at step 2020: 0.5607
Loss at step 2030: 0.1034
Loss at step 2040: 0.1946
Loss at step 2050: 0.1256
Loss at step 2060: 0.0977
Loss at step 2070: 0.2086
Loss at step 2080: 0.4296
Loss at step 2090: 0.1636
Loss at step 2100: 0.2094
Loss at step 2110: 0.2412
Loss at step 2120: 0.3593
Loss at step 2130: 0.2880
Loss at step 2140: 0.2347
Loss at step 2150: 0.3511
Loss at step 2160: 0.3918
Loss at step 2170: 0.1690
Loss at step 2180: 0.0879
Loss at step 2190: 0.0390
Loss at step 2200: 0.1487
Loss at step 2210: 0.0986
Loss at step 2220: 0.1165
Loss at step 2230: 0.2233
Loss at step 2240: 0.2615
Loss at step 2250: 0.1110
Loss at step 2260: 0.4232
Loss at step 2270: 0.0552
Loss at step 2280: 0.1159
Loss at step 2290: 0.0537
Loss at step 2300: 0.1467
Loss at step 2310: 0.1718
Loss at step 2320: 0.2851
Loss at step 2330: 0.3960
Loss at step 2340: 0.5117
Loss at step 2350: 0.0769
Loss at step 2360: 0.0715
Loss at step 2370: 0.1296
Loss at step 2380: 0.1437
Loss at step 2390: 0.0661
Loss at step 2400: 0.0540
Loss at step 2410: 0.2749
Loss at step 2420: 0.1983
Loss at step 2430: 0.0367
Loss at step 2440: 0.2625
Loss at step 2450: 0.3368
Loss at step 2460: 0.0813
Loss at step 2470: 0.0921
Loss at step 2480: 0.2290
Loss at step 2490: 0.2092
Loss at step 2500: 0.1941
Loss at step 2510: 0.2218
Loss at step 2520: 0.1840
Loss at step 2530: 0.1140
Loss at step 2540: 0.0923
Loss at step 2550: 0.1501
Loss at step 2560: 0.2017
Loss at step 2570: 0.1815
Loss at step 2580: 0.1436
Loss at step 2590: 0.1388
Loss at step 2600: 0.2317
Loss at step 2610: 0.1147
Loss at step 2620: 0.0375
Loss at step 2630: 0.0888
Loss at step 2640: 0.1226
Loss at step 2650: 0.2862
Loss at step 2660: 0.1327
Loss at step 2670: 0.1678
Loss at step 2680: 0.0565
Loss at step 2690: 0.0723
Loss at step 2700: 0.2783
Loss at step 2710: 0.0714
Loss at step 2720: 0.2938
Loss at step 2730: 0.1018
Loss at step 2740: 0.0180
Loss at step 2750: 0.4856
Loss at step 2760: 0.1732
Loss at step 2770: 0.4798
Loss at step 2780: 0.1439
Loss at step 2790: 0.2813
Loss at step 2800: 0.1320
Loss at step 2810: 0.0830
Loss at step 2820: 0.0415
Loss at step 2830: 0.0513
Loss at step 2840: 0.0429
Loss at step 2850: 0.3474
Loss at step 2860: 0.0857
Loss at step 2870: 0.2023
Loss at step 2880: 0.1424
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/precision/bfadb1cf35fe89242263de7dc028b248827c08ba075659c0e812d0fc6e5237c9 (last modified on Sat Nov 30 13:38:15 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.554364, 'precision': [0.626842, 0.595863, 0.435389], 'recall': [0.630609, 0.475443, 0.55767], 'f1': [0.62872, 0.528885, 0.489001]}
Sample 83810 of the training set: {'input_ids': [101, 1030, 3962, 8757, 16302, 2015, 2134, 1521, 1056, 4025, 2000, 2022, 2151, 5724, 2008, 1045, 2071, 2424, 1999, 1996, 16380, 10439, 102, 2043, 2017, 2123, 1521, 1056, 3477, 2005, 3962, 8757, 1996, 3325, 2003, 2428, 4485, 1010, 4788, 2084, 1996, 2557, 1012, 2065, 2017, 2215, 2033, 2000, 3477, 2769, 2005, 2091, 16307, 2075, 2059, 13417, 1037, 2172, 1008, 2025, 4658, 1008, 3325, 3531, 2265, 2033, 2129, 2000, 2507, 2017, 2026, 2769, 2153, 1012, 3531, 2292, 2149, 2113, 2065, 2017, 2412, 2342, 2149, 2153, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 1.0634
Loss at step 20: 0.9048
Loss at step 30: 0.8915
Loss at step 40: 0.9210
Loss at step 50: 1.0370
Loss at step 60: 1.1007
Loss at step 70: 1.0456
Loss at step 80: 0.9096
Loss at step 90: 1.0839
Loss at step 100: 0.8535
Loss at step 110: 0.8849
Loss at step 120: 0.8916
Loss at step 130: 0.7160
Loss at step 140: 1.0233
Loss at step 150: 0.9965
Loss at step 160: 0.9600
Loss at step 170: 0.9081
Loss at step 180: 0.7773
Loss at step 190: 0.8352
Loss at step 200: 0.8136
Loss at step 210: 1.1767
Loss at step 220: 0.9984
Loss at step 230: 0.8750
Loss at step 240: 0.9893
Loss at step 250: 0.7527
Loss at step 260: 0.6286
Loss at step 270: 0.9884
Loss at step 280: 0.8698
Loss at step 290: 0.6523
Loss at step 300: 0.9037
Loss at step 310: 0.6245
Loss at step 320: 0.6874
Loss at step 330: 0.7525
Loss at step 340: 0.6576
Loss at step 350: 0.7024
Loss at step 360: 0.7106
Loss at step 370: 0.9571
Loss at step 380: 0.9818
Loss at step 390: 0.8068
Loss at step 400: 0.6903
Loss at step 410: 0.7904
Loss at step 420: 0.5956
Loss at step 430: 0.8648
Loss at step 440: 0.6789
Loss at step 450: 0.7807
Loss at step 460: 0.7328
Loss at step 470: 0.7418
Loss at step 480: 0.9422
Loss at step 490: 0.9917
Loss at step 500: 0.5999
Loss at step 510: 0.6794
Loss at step 520: 0.7097
Loss at step 530: 0.5264
Loss at step 540: 0.7132
Loss at step 550: 0.8559
Loss at step 560: 0.4478
Loss at step 570: 0.6146
Loss at step 580: 0.7237
Loss at step 590: 0.6321
Loss at step 600: 0.8255
Loss at step 610: 0.7741
Loss at step 620: 0.8169
Loss at step 630: 0.8114
Loss at step 640: 0.5479
Loss at step 650: 0.5012
Loss at step 660: 0.5423
Loss at step 670: 0.7460
Loss at step 680: 0.6474
Loss at step 690: 0.5844
Loss at step 700: 0.6832
Loss at step 710: 0.6029
Loss at step 720: 0.5796
Loss at step 730: 0.3704
Loss at step 740: 0.5870
Loss at step 750: 0.6458
Loss at step 760: 0.5291
Loss at step 770: 0.4512
Loss at step 780: 0.3520
Loss at step 790: 0.5876
Loss at step 800: 0.5381
Loss at step 810: 0.5689
Loss at step 820: 0.5218
Loss at step 830: 0.4135
Loss at step 840: 0.5746
Loss at step 850: 0.7216
Loss at step 860: 0.8307
Loss at step 870: 0.6034
Loss at step 880: 0.4440
Loss at step 890: 0.4514
Loss at step 900: 0.7186
Loss at step 910: 0.4458
Loss at step 920: 0.7372
Loss at step 930: 0.3612
Loss at step 940: 0.4237
Loss at step 950: 0.3682
Loss at step 960: 0.5663
Loss at step 970: 0.3137
Loss at step 980: 0.3579
Loss at step 990: 0.5324
Loss at step 1000: 0.3719
Loss at step 1010: 0.5412
Loss at step 1020: 0.4675
Loss at step 1030: 0.5655
Loss at step 1040: 0.5794
Loss at step 1050: 0.3332
Loss at step 1060: 0.5606
Loss at step 1070: 0.6946
Loss at step 1080: 0.2932
Loss at step 1090: 0.3287
Loss at step 1100: 0.3812
Loss at step 1110: 0.2834
Loss at step 1120: 0.7634
Loss at step 1130: 0.5363
Loss at step 1140: 0.3701
Loss at step 1150: 0.2715
Loss at step 1160: 0.3284
Loss at step 1170: 0.4737
Loss at step 1180: 0.3890
Loss at step 1190: 0.2917
Loss at step 1200: 0.4608
Loss at step 1210: 0.2879
Loss at step 1220: 0.3297
Loss at step 1230: 0.5719
Loss at step 1240: 0.6427
Loss at step 1250: 0.4582
Loss at step 1260: 0.5043
Loss at step 1270: 0.4293
Loss at step 1280: 0.2281
Loss at step 1290: 0.5254
Loss at step 1300: 0.2547
Loss at step 1310: 0.6359
Loss at step 1320: 0.1516
Loss at step 1330: 0.3070
Loss at step 1340: 0.3425
Loss at step 1350: 0.3092
Loss at step 1360: 0.5344
Loss at step 1370: 0.2994
Loss at step 1380: 0.4537
Loss at step 1390: 0.3470
Loss at step 1400: 0.1704
Loss at step 1410: 0.2392
Loss at step 1420: 0.3375
Loss at step 1430: 0.1658
Loss at step 1440: 0.3179
Loss at step 1450: 0.2124
Loss at step 1460: 0.6802
Loss at step 1470: 0.3062
Loss at step 1480: 0.2057
Loss at step 1490: 0.2161
Loss at step 1500: 0.6868
Loss at step 1510: 0.2561
Loss at step 1520: 0.4449
Loss at step 1530: 0.1996
Loss at step 1540: 0.1843
Loss at step 1550: 0.2037
Loss at step 1560: 0.1417
Loss at step 1570: 0.2116
Loss at step 1580: 0.2590
Loss at step 1590: 0.1525
Loss at step 1600: 0.3669
Loss at step 1610: 0.2027
Loss at step 1620: 0.1059
Loss at step 1630: 0.3498
Loss at step 1640: 0.2000
Loss at step 1650: 0.3033
Loss at step 1660: 0.2253
Loss at step 1670: 0.2376
Loss at step 1680: 0.3302
Loss at step 1690: 0.2838
Loss at step 1700: 0.4169
Loss at step 1710: 0.0602
Loss at step 1720: 0.1345
Loss at step 1730: 0.1579
Loss at step 1740: 0.0759
Loss at step 1750: 0.3896
Loss at step 1760: 0.1424
Loss at step 1770: 0.0948
Loss at step 1780: 0.1846
Loss at step 1790: 0.2570
Loss at step 1800: 0.1501
Loss at step 1810: 0.0801
Loss at step 1820: 0.3002
Loss at step 1830: 0.3325
Loss at step 1840: 0.0835
Loss at step 1850: 0.2133
Loss at step 1860: 0.5762
Loss at step 1870: 0.3982
Loss at step 1880: 0.1466
Loss at step 1890: 0.2145
Loss at step 1900: 0.0425
Loss at step 1910: 0.3325
Loss at step 1920: 0.1234
Loss at step 1930: 0.4012
Loss at step 1940: 0.0967
Loss at step 1950: 0.2786
Loss at step 1960: 0.4349
Loss at step 1970: 0.4687
Loss at step 1980: 0.2703
Loss at step 1990: 0.3084
Loss at step 2000: 0.0692
Loss at step 2010: 0.2458
Loss at step 2020: 0.5888
Loss at step 2030: 0.2994
Loss at step 2040: 0.1771
Loss at step 2050: 0.2363
Loss at step 2060: 0.2011
Loss at step 2070: 0.1777
Loss at step 2080: 0.2990
Loss at step 2090: 0.2057
Loss at step 2100: 0.1779
Loss at step 2110: 0.2279
Loss at step 2120: 0.2749
Loss at step 2130: 0.2818
Loss at step 2140: 0.2011
Loss at step 2150: 0.4340
Loss at step 2160: 0.3736
Loss at step 2170: 0.1326
Loss at step 2180: 0.1958
Loss at step 2190: 0.0485
Loss at step 2200: 0.2757
Loss at step 2210: 0.1647
Loss at step 2220: 0.2334
Loss at step 2230: 0.1284
Loss at step 2240: 0.2872
Loss at step 2250: 0.1390
Loss at step 2260: 0.5005
Loss at step 2270: 0.0775
Loss at step 2280: 0.1178
Loss at step 2290: 0.0767
Loss at step 2300: 0.0942
Loss at step 2310: 0.1537
Loss at step 2320: 0.2028
Loss at step 2330: 0.5357
Loss at step 2340: 0.0916
Loss at step 2350: 0.0956
Loss at step 2360: 0.0916
Loss at step 2370: 0.1441
Loss at step 2380: 0.1528
Loss at step 2390: 0.0482
Loss at step 2400: 0.1845
Loss at step 2410: 0.0820
Loss at step 2420: 0.3899
Loss at step 2430: 0.1854
Loss at step 2440: 0.1306
Loss at step 2450: 0.2447
Loss at step 2460: 0.0186
Loss at step 2470: 0.3006
Loss at step 2480: 0.2780
Loss at step 2490: 0.2979
Loss at step 2500: 0.1023
Loss at step 2510: 0.1056
Loss at step 2520: 0.3291
Loss at step 2530: 0.0840
Loss at step 2540: 0.1642
Loss at step 2550: 0.1578
Loss at step 2560: 0.2436
Loss at step 2570: 0.1871
Loss at step 2580: 0.1754
Loss at step 2590: 0.1582
Loss at step 2600: 0.1658
Loss at step 2610: 0.1629
Loss at step 2620: 0.0583
Loss at step 2630: 0.0471
Loss at step 2640: 0.1863
Loss at step 2650: 0.2943
Loss at step 2660: 0.0444
Loss at step 2670: 0.1119
Loss at step 2680: 0.1170
Loss at step 2690: 0.0950
Loss at step 2700: 0.1629
Loss at step 2710: 0.0477
Loss at step 2720: 0.3087
Loss at step 2730: 0.1102
Loss at step 2740: 0.0355
Loss at step 2750: 0.2179
Loss at step 2760: 0.1151
Loss at step 2770: 0.3293
Loss at step 2780: 0.0297
Loss at step 2790: 0.1848
Loss at step 2800: 0.1451
Loss at step 2810: 0.0648
Loss at step 2820: 0.0814
Loss at step 2830: 0.0963
Loss at step 2840: 0.0149
Loss at step 2850: 0.3371
Loss at step 2860: 0.1214
Loss at step 2870: 0.2781
Loss at step 2880: 0.0441
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.551981, 'precision': [0.673565, 0.527799, 0.451194], 'recall': [0.573718, 0.56562, 0.501153], 'f1': [0.619645, 0.546055, 0.474863]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x744c865efa60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x72b4935b0a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x783b3d970a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x74f40cdb0a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7b3f52faea60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 8655
Loss at step 10: 0.9841
Loss at step 20: 1.0290
Loss at step 30: 0.8958
Loss at step 40: 1.1063
Loss at step 50: 0.7665
Loss at step 60: 1.1717
Loss at step 70: 0.7787
Loss at step 80: 0.8785
Loss at step 90: 0.9307
Loss at step 100: 0.9444
Loss at step 110: 0.8200
Loss at step 120: 0.9162
Loss at step 130: 1.0273
Loss at step 140: 0.9849
Loss at step 150: 0.8466
Loss at step 160: 0.9728
Loss at step 170: 0.9802
Loss at step 180: 0.9027
Loss at step 190: 0.8232
Loss at step 200: 1.0056
Loss at step 210: 0.7596
Loss at step 220: 0.9751
Loss at step 230: 0.8198
Loss at step 240: 0.9254
Loss at step 250: 0.9247
Loss at step 260: 0.9077
Loss at step 270: 0.8100
Loss at step 280: 0.8513
Loss at step 290: 0.9319
Loss at step 300: 0.7880
Loss at step 310: 0.7348
Loss at step 320: 0.7221
Loss at step 330: 0.9254
Loss at step 340: 0.6225
Loss at step 350: 0.7834
Loss at step 360: 0.7118
Loss at step 370: 0.6483
Loss at step 380: 0.8001
Loss at step 390: 0.8113
Loss at step 400: 1.0467
Loss at step 410: 0.6732
Loss at step 420: 0.7971
Loss at step 430: 0.6171
Loss at step 440: 0.7005
Loss at step 450: 0.6471
Loss at step 460: 0.7324
Loss at step 470: 0.7721
Loss at step 480: 0.7328
Loss at step 490: 0.7172
Loss at step 500: 0.7507
Loss at step 510: 0.4797
Loss at step 520: 0.5074
Loss at step 530: 0.8072
Loss at step 540: 0.5815
Loss at step 550: 0.6806
Loss at step 560: 0.4999
Loss at step 570: 0.6190
Loss at step 580: 0.8453
Loss at step 590: 0.7843
Loss at step 600: 0.6234
Loss at step 610: 0.6992
Loss at step 620: 0.6448
Loss at step 630: 0.5057
Loss at step 640: 0.7299
Loss at step 650: 0.7094
Loss at step 660: 0.5062
Loss at step 670: 0.6774
Loss at step 680: 0.6283
Loss at step 690: 0.6846
Loss at step 700: 0.5493
Loss at step 710: 0.5691
Loss at step 720: 0.9779
Loss at step 730: 0.6309
Loss at step 740: 0.4806
Loss at step 750: 0.5491
Loss at step 760: 0.5017
Loss at step 770: 0.5622
Loss at step 780: 0.4795
Loss at step 790: 0.7014
Loss at step 800: 0.4830
Loss at step 810: 0.5776
Loss at step 820: 0.6509
Loss at step 830: 0.5568
Loss at step 840: 0.4010
Loss at step 850: 0.4974
Loss at step 860: 0.5387
Loss at step 870: 0.5582
Loss at step 880: 0.5453
Loss at step 890: 0.6176
Loss at step 900: 0.5763
Loss at step 910: 0.4497
Loss at step 920: 0.3930
Loss at step 930: 0.7440
Loss at step 940: 0.5648
Loss at step 950: 0.3751
Loss at step 960: 0.4530
Loss at step 970: 0.3602
Loss at step 980: 0.4863
Loss at step 990: 0.4477
Loss at step 1000: 0.6402
Loss at step 1010: 0.4505
Loss at step 1020: 0.6069
Loss at step 1030: 0.3506
Loss at step 1040: 0.3192
Loss at step 1050: 0.3252
Loss at step 1060: 0.4955
Loss at step 1070: 0.2392
Loss at step 1080: 0.6029
Loss at step 1090: 0.3922
Loss at step 1100: 0.4412
Loss at step 1110: 0.2713
Loss at step 1120: 0.5940
Loss at step 1130: 0.3809
Loss at step 1140: 0.2848
Loss at step 1150: 0.3968
Loss at step 1160: 0.4733
Loss at step 1170: 0.4110
Loss at step 1180: 0.3930
Loss at step 1190: 0.3940
Loss at step 1200: 0.3136
Loss at step 1210: 0.3323
Loss at step 1220: 0.4394
Loss at step 1230: 0.5120
Loss at step 1240: 0.5331
Loss at step 1250: 0.2209
Loss at step 1260: 0.5174
Loss at step 1270: 0.3539
Loss at step 1280: 0.3272
Loss at step 1290: 0.2755
Loss at step 1300: 0.2823
Loss at step 1310: 0.3093
Loss at step 1320: 0.5923
Loss at step 1330: 0.2910
Loss at step 1340: 0.3607
Loss at step 1350: 0.3962
Loss at step 1360: 0.2842
Loss at step 1370: 0.2095
Loss at step 1380: 0.4533
Loss at step 1390: 0.6763
Loss at step 1400: 0.5128
Loss at step 1410: 0.3425
Loss at step 1420: 0.2530
Loss at step 1430: 0.2287
Loss at step 1440: 0.5867
Loss at step 1450: 0.4016
Loss at step 1460: 0.3813
Loss at step 1470: 0.2885
Loss at step 1480: 0.3369
Loss at step 1490: 0.2600
Loss at step 1500: 0.1841
Loss at step 1510: 0.2779
Loss at step 1520: 0.3581
Loss at step 1530: 0.2951
Loss at step 1540: 0.2065
Loss at step 1550: 0.1987
Loss at step 1560: 0.3485
Loss at step 1570: 0.2698
Loss at step 1580: 0.3865
Loss at step 1590: 0.3896
Loss at step 1600: 0.2466
Loss at step 1610: 0.3812
Loss at step 1620: 0.3473
Loss at step 1630: 0.4037
Loss at step 1640: 0.1446
Loss at step 1650: 0.1553
Loss at step 1660: 0.3325
Loss at step 1670: 0.2351
Loss at step 1680: 0.2893
Loss at step 1690: 0.3029
Loss at step 1700: 0.3171
Loss at step 1710: 0.2751
Loss at step 1720: 0.6470
Loss at step 1730: 0.1657
Loss at step 1740: 0.3221
Loss at step 1750: 0.2411
Loss at step 1760: 0.0865
Loss at step 1770: 0.4187
Loss at step 1780: 0.2692
Loss at step 1790: 0.1600
Loss at step 1800: 0.2243
Loss at step 1810: 0.0409
Loss at step 1820: 0.3599
Loss at step 1830: 0.0302
Loss at step 1840: 0.3276
Loss at step 1850: 0.0949
Loss at step 1860: 0.2553
Loss at step 1870: 0.2002
Loss at step 1880: 0.3865
Loss at step 1890: 0.2003
Loss at step 1900: 0.3068
Loss at step 1910: 0.4065
Loss at step 1920: 0.2804
Loss at step 1930: 0.1097
Loss at step 1940: 0.1051
Loss at step 1950: 0.3668
Loss at step 1960: 0.1804
Loss at step 1970: 0.1477
Loss at step 1980: 0.1498
Loss at step 1990: 0.1201
Loss at step 2000: 0.1025
Loss at step 2010: 0.1473
Loss at step 2020: 0.1483
Loss at step 2030: 0.1949
Loss at step 2040: 0.2670
Loss at step 2050: 0.2770
Loss at step 2060: 0.2588
Loss at step 2070: 0.2377
Loss at step 2080: 0.1684
Loss at step 2090: 0.3641
Loss at step 2100: 0.2151
Loss at step 2110: 0.1357
Loss at step 2120: 0.2886
Loss at step 2130: 0.1116
Loss at step 2140: 0.1089
Loss at step 2150: 0.1784
Loss at step 2160: 0.1312
Loss at step 2170: 0.2139
Loss at step 2180: 0.2347
Loss at step 2190: 0.3288
Loss at step 2200: 0.2386
Loss at step 2210: 0.0676
Loss at step 2220: 0.1972
Loss at step 2230: 0.1036
Loss at step 2240: 0.0774
Loss at step 2250: 0.2666
Loss at step 2260: 0.0287
Loss at step 2270: 0.1570
Loss at step 2280: 0.1273
Loss at step 2290: 0.2660
Loss at step 2300: 0.0484
Loss at step 2310: 0.1516
Loss at step 2320: 0.2596
Loss at step 2330: 0.1695
Loss at step 2340: 0.1364
Loss at step 2350: 0.1104
Loss at step 2360: 0.3662
Loss at step 2370: 0.3430
Loss at step 2380: 0.1043
Loss at step 2390: 0.2644
Loss at step 2400: 0.1831
Loss at step 2410: 0.1205
Loss at step 2420: 0.1908
Loss at step 2430: 0.0245
Loss at step 2440: 0.2222
Loss at step 2450: 0.0678
Loss at step 2460: 0.0560
Loss at step 2470: 0.1911
Loss at step 2480: 0.1823
Loss at step 2490: 0.0390
Loss at step 2500: 0.2693
Loss at step 2510: 0.0558
Loss at step 2520: 0.0517
Loss at step 2530: 0.2532
Loss at step 2540: 0.0922
Loss at step 2550: 0.2112
Loss at step 2560: 0.1012
Loss at step 2570: 0.2007
Loss at step 2580: 0.1866
Loss at step 2590: 0.0946
Loss at step 2600: 0.0605
Loss at step 2610: 0.0855
Loss at step 2620: 0.3647
Loss at step 2630: 0.1106
Loss at step 2640: 0.0887
Loss at step 2650: 0.0314
Loss at step 2660: 0.2518
Loss at step 2670: 0.1581
Loss at step 2680: 0.1755
Loss at step 2690: 0.0903
Loss at step 2700: 0.4588
Loss at step 2710: 0.2394
Loss at step 2720: 0.1574
Loss at step 2730: 0.1427
Loss at step 2740: 0.4324
Loss at step 2750: 0.0714
Loss at step 2760: 0.0957
Loss at step 2770: 0.2676
Loss at step 2780: 0.2173
Loss at step 2790: 0.0314
Loss at step 2800: 0.1898
Loss at step 2810: 0.4292
Loss at step 2820: 0.0549
Loss at step 2830: 0.0997
Loss at step 2840: 0.0445
Loss at step 2850: 0.2681
Loss at step 2860: 0.0734
Loss at step 2870: 0.1469
Loss at step 2880: 0.0212
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.547215, 'precision': [0.679834, 0.507346, 0.467002], 'recall': [0.524038, 0.653382, 0.428489], 'f1': [0.591855, 0.571177, 0.446917]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x728e49faca60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 44118 of the training set: {'input_ids': [101, 1030, 12904, 18827, 2629, 6289, 1045, 2031, 2242, 9235, 1998, 2026, 2924, 4003, 2003, 5941, 2061, 1045, 2069, 2031, 6207, 3477, 102, 8013, 2003, 17949, 2055, 1996, 5409, 2326, 2073, 2027, 2031, 2699, 2000, 2005, 2582, 5375, 1012, 4005, 2163, 2008, 1996, 2592, 1999, 3543, 2007, 1996, 2592, 1997, 1996, 2592, 2038, 2042, 3024, 1998, 2163, 2008, 2027, 2097, 2022, 4207, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 17310
Loss at step 10: 1.1450
Loss at step 20: 0.8914
Loss at step 30: 0.8360
Loss at step 40: 0.9891
Loss at step 50: 1.0611
Loss at step 60: 0.9402
Loss at step 70: 0.9513
Loss at step 80: 0.9718
Loss at step 90: 0.7184
Loss at step 100: 0.8321
Loss at step 110: 0.9740
Loss at step 120: 0.9823
Loss at step 130: 0.9279
Loss at step 140: 0.8443
Loss at step 150: 0.9973
Loss at step 160: 0.8614
Loss at step 170: 0.8971
Loss at step 180: 0.8894
Loss at step 190: 1.0090
Loss at step 200: 1.0735
Loss at step 210: 1.0806
Loss at step 220: 0.7267
Loss at step 230: 0.6360
Loss at step 240: 0.9733
Loss at step 250: 0.9426
Loss at step 260: 0.9419
Loss at step 270: 1.1093
Loss at step 280: 1.0898
Loss at step 290: 1.0819
Loss at step 300: 0.8950
Loss at step 310: 0.9711
Loss at step 320: 0.9240
Loss at step 330: 0.6208
Loss at step 340: 1.0582
Loss at step 350: 1.0361
Loss at step 360: 0.8424
Loss at step 370: 0.6404
Loss at step 380: 0.8214
Loss at step 390: 1.0521
Loss at step 400: 1.1069
Loss at step 410: 1.0339
Loss at step 420: 0.8839
Loss at step 430: 1.1653
Loss at step 440: 0.9642
Loss at step 450: 0.9051
Loss at step 460: 0.7824
Loss at step 470: 0.7396
Loss at step 480: 1.1436
Loss at step 490: 0.6533
Loss at step 500: 0.9582
Loss at step 510: 1.0899
Loss at step 520: 0.8506
Loss at step 530: 0.9696
Loss at step 540: 0.9601
Loss at step 550: 0.7345
Loss at step 560: 0.7699
Loss at step 570: 0.7868
Loss at step 580: 0.7241
Loss at step 590: 0.5877
Loss at step 600: 0.6398
Loss at step 610: 1.0167
Loss at step 620: 0.5719
Loss at step 630: 1.0247
Loss at step 640: 0.8183
Loss at step 650: 0.9173
Loss at step 660: 1.0710
Loss at step 670: 0.8122
Loss at step 680: 0.6676
Loss at step 690: 0.6973
Loss at step 700: 0.6131
Loss at step 710: 0.8998
Loss at step 720: 0.9126
Loss at step 730: 1.1042
Loss at step 740: 0.5319
Loss at step 750: 0.6898
Loss at step 760: 1.0427
Loss at step 770: 0.7776
Loss at step 780: 1.3337
Loss at step 790: 0.6819
Loss at step 800: 0.9078
Loss at step 810: 0.7514
Loss at step 820: 0.5115
Loss at step 830: 0.9866
Loss at step 840: 0.9181
Loss at step 850: 0.9410
Loss at step 860: 0.5435
Loss at step 870: 0.9623
Loss at step 880: 0.7218
Loss at step 890: 0.7212
Loss at step 900: 0.6232
Loss at step 910: 0.7372
Loss at step 920: 0.7533
Loss at step 930: 1.0192
Loss at step 940: 0.9705
Loss at step 950: 0.9168
Loss at step 960: 0.6765
Loss at step 970: 0.7675
Loss at step 980: 0.6353
Loss at step 990: 0.8271
Loss at step 1000: 0.5234
Loss at step 1010: 0.6837
Loss at step 1020: 0.3644
Loss at step 1030: 0.5903
Loss at step 1040: 0.5331
Loss at step 1050: 0.8401
Loss at step 1060: 0.8333
Loss at step 1070: 0.8741
Loss at step 1080: 0.4707
Loss at step 1090: 0.9081
Loss at step 1100: 0.7727
Loss at step 1110: 0.6245
Loss at step 1120: 0.5817
Loss at step 1130: 0.3874
Loss at step 1140: 0.6400
Loss at step 1150: 0.7135
Loss at step 1160: 1.1416
Loss at step 1170: 0.6606
Loss at step 1180: 0.6064
Loss at step 1190: 0.6328
Loss at step 1200: 0.2260
Loss at step 1210: 0.7926
Loss at step 1220: 0.6928
Loss at step 1230: 1.1550
Loss at step 1240: 0.7327
Loss at step 1250: 0.6051
Loss at step 1260: 0.7603
Loss at step 1270: 0.6312
Loss at step 1280: 0.5430
Loss at step 1290: 0.7531
Loss at step 1300: 0.9621
Loss at step 1310: 0.5435
Loss at step 1320: 0.5369
Loss at step 1330: 0.6725
Loss at step 1340: 0.5497
Loss at step 1350: 0.5809
Loss at step 1360: 0.8874
Loss at step 1370: 0.3644
Loss at step 1380: 0.7701
Loss at step 1390: 0.7830
Loss at step 1400: 0.6994
Loss at step 1410: 0.8245
Loss at step 1420: 0.4253
Loss at step 1430: 0.6885
Loss at step 1440: 0.8826
Loss at step 1450: 0.8468
Loss at step 1460: 0.3539
Loss at step 1470: 0.9537
Loss at step 1480: 0.3749
Loss at step 1490: 0.5904
Loss at step 1500: 0.7583
Loss at step 1510: 0.7631
Loss at step 1520: 0.4974
Loss at step 1530: 0.6367
Loss at step 1540: 0.6322
Loss at step 1550: 0.8462
Loss at step 1560: 0.4262
Loss at step 1570: 0.4323
Loss at step 1580: 0.6801
Loss at step 1590: 0.4727
Loss at step 1600: 0.5904
Loss at step 1610: 0.5512
Loss at step 1620: 0.9305
Loss at step 1630: 0.9404
Loss at step 1640: 0.4697
Loss at step 1650: 1.0087
Loss at step 1660: 0.3943
Loss at step 1670: 0.3798
Loss at step 1680: 0.5197
Loss at step 1690: 0.5279
Loss at step 1700: 0.4185
Loss at step 1710: 0.3759
Loss at step 1720: 0.5625
Loss at step 1730: 0.3565
Loss at step 1740: 0.6087
Loss at step 1750: 0.9530
Loss at step 1760: 0.2299
Loss at step 1770: 0.3004
Loss at step 1780: 0.3103
Loss at step 1790: 0.5676
Loss at step 1800: 0.8245
Loss at step 1810: 0.4080
Loss at step 1820: 0.3780
Loss at step 1830: 1.0250
Loss at step 1840: 0.7023
Loss at step 1850: 0.8100
Loss at step 1860: 0.5112
Loss at step 1870: 0.2140
Loss at step 1880: 0.7793
Loss at step 1890: 0.6369
Loss at step 1900: 0.5598
Loss at step 1910: 0.4375
Loss at step 1920: 0.6629
Loss at step 1930: 0.3038
Loss at step 1940: 0.4796
Loss at step 1950: 0.5743
Loss at step 1960: 0.5199
Loss at step 1970: 0.5799
Loss at step 1980: 0.6432
Loss at step 1990: 0.8120
Loss at step 2000: 0.2582
Loss at step 2010: 0.4662
Loss at step 2020: 0.4449
Loss at step 2030: 0.5985
Loss at step 2040: 0.8527
Loss at step 2050: 0.4005
Loss at step 2060: 0.4181
Loss at step 2070: 0.4949
Loss at step 2080: 0.5932
Loss at step 2090: 0.3278
Loss at step 2100: 0.2096
Loss at step 2110: 0.5503
Loss at step 2120: 0.3484
Loss at step 2130: 0.5255
Loss at step 2140: 0.2583
Loss at step 2150: 0.3383
Loss at step 2160: 0.6261
Loss at step 2170: 0.3780
Loss at step 2180: 0.5913
Loss at step 2190: 0.3787
Loss at step 2200: 0.2681
Loss at step 2210: 0.7416
Loss at step 2220: 0.5440
Loss at step 2230: 0.5970
Loss at step 2240: 0.5789
Loss at step 2250: 0.4459
Loss at step 2260: 0.1665
Loss at step 2270: 0.7466
Loss at step 2280: 0.2598
Loss at step 2290: 0.5224
Loss at step 2300: 0.1581
Loss at step 2310: 0.2526
Loss at step 2320: 0.5707
Loss at step 2330: 0.4154
Loss at step 2340: 0.4856
Loss at step 2350: 1.0268
Loss at step 2360: 0.3910
Loss at step 2370: 0.2868
Loss at step 2380: 0.7600
Loss at step 2390: 0.3594
Loss at step 2400: 0.2354
Loss at step 2410: 0.4325
Loss at step 2420: 0.4455
Loss at step 2430: 0.5483
Loss at step 2440: 0.3983
Loss at step 2450: 0.3246
Loss at step 2460: 0.2724
Loss at step 2470: 0.6693
Loss at step 2480: 0.6861
Loss at step 2490: 0.7597
Loss at step 2500: 0.1014
Loss at step 2510: 0.7890
Loss at step 2520: 0.1866
Loss at step 2530: 0.1848
Loss at step 2540: 0.3958
Loss at step 2550: 0.4676
Loss at step 2560: 0.2672
Loss at step 2570: 0.3245
Loss at step 2580: 0.4043
Loss at step 2590: 0.4832
Loss at step 5350: 0.5338
Loss at step 5360: 0.0478
Loss at step 5370: 0.2366
Loss at step 5380: 0.0330
Loss at step 5390: 0.3731
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7b9e4938a9d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 83320 of the training set: {'input_ids': [101, 1030, 2310, 21885, 5644, 6279, 6442, 9951, 2326, 2525, 25141, 3064, 1996, 2799, 2099, 1998, 2117, 2051, 1996, 4274, 3632, 2041, 102, 8013, 2003, 17949, 2055, 1996, 4274, 2326, 2029, 2003, 2025, 2551, 1012, 4005, 2163, 2008, 2023, 3277, 2003, 2949, 1998, 11186, 2005, 1996, 3861, 1997, 1996, 4070, 1998, 2036, 2163, 2008, 2027, 2031, 1037, 3553, 2298, 2046, 2023, 3277, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 4329
Loss at step 10: 0.9926
Loss at step 20: 0.9010
Loss at step 30: 0.9418
Loss at step 40: 0.9442
Loss at step 50: 0.9314
Loss at step 60: 0.9649
Loss at step 70: 0.9901
Loss at step 80: 0.7664
Loss at step 90: 0.8879
Loss at step 100: 0.8399
Loss at step 110: 0.8580
Loss at step 120: 0.8728
Loss at step 130: 0.7065
Loss at step 140: 0.7669
Loss at step 150: 0.9656
Loss at step 160: 0.7847
Loss at step 170: 0.8659
Loss at step 180: 0.8233
Loss at step 190: 0.8518
Loss at step 200: 0.7648
Loss at step 210: 0.6967
Loss at step 220: 0.7510
Loss at step 230: 0.8184
Loss at step 240: 0.8398
Loss at step 250: 0.6521
Loss at step 260: 0.8123
Loss at step 270: 0.8100
Loss at step 280: 0.5958
Loss at step 290: 0.8458
Loss at step 300: 0.8630
Loss at step 310: 0.8057
Loss at step 320: 0.6780
Loss at step 330: 0.6401
Loss at step 340: 0.7581
Loss at step 350: 0.6867
Loss at step 360: 0.5835
Loss at step 370: 0.5927
Loss at step 380: 0.5861
Loss at step 390: 0.6317
Loss at step 400: 0.6787
Loss at step 410: 0.6799
Loss at step 420: 0.5334
Loss at step 430: 0.8389
Loss at step 440: 0.7983
Loss at step 450: 0.6400
Loss at step 460: 0.7505
Loss at step 470: 0.5508
Loss at step 480: 0.4624
Loss at step 490: 0.5321
Loss at step 500: 0.7065
Loss at step 510: 0.5171
Loss at step 520: 0.5051
Loss at step 530: 0.6023
Loss at step 540: 0.4249
Loss at step 550: 0.4540
Loss at step 560: 0.6095
Loss at step 570: 0.5288
Loss at step 580: 0.5407
Loss at step 590: 0.4740
Loss at step 600: 0.4191
Loss at step 610: 0.4266
Loss at step 620: 0.6789
Loss at step 630: 0.4493
Loss at step 640: 0.4144
Loss at step 650: 0.2753
Loss at step 660: 0.2556
Loss at step 670: 0.5392
Loss at step 680: 0.5323
Loss at step 690: 0.5089
Loss at step 700: 0.3365
Loss at step 710: 0.5060
Loss at step 720: 0.4522
Loss at step 730: 0.5699
Loss at step 740: 0.1910
Loss at step 750: 0.5182
Loss at step 760: 0.5524
Loss at step 770: 0.2375
Loss at step 780: 0.3572
Loss at step 790: 0.2684
Loss at step 800: 0.4337
Loss at step 810: 0.4016
Loss at step 820: 0.4048
Loss at step 830: 0.3683
Loss at step 840: 0.3166
Loss at step 850: 0.4732
Loss at step 860: 0.3120
Loss at step 870: 0.2090
Loss at step 880: 0.2352
Loss at step 890: 0.1942
Loss at step 900: 0.2206
Loss at step 910: 0.2922
Loss at step 920: 0.1700
Loss at step 930: 0.4116
Loss at step 940: 0.1761
Loss at step 950: 0.2083
Loss at step 960: 0.3339
Loss at step 970: 0.2621
Loss at step 980: 0.3651
Loss at step 990: 0.2595
Loss at step 1000: 0.1362
Loss at step 1010: 0.2218
Loss at step 1020: 0.1979
Loss at step 1030: 0.1392
Loss at step 1040: 0.2958
Loss at step 1050: 0.2673
Loss at step 1060: 0.1737
Loss at step 1070: 0.1927
Loss at step 1080: 0.3628
Loss at step 1090: 0.3404
Loss at step 1100: 0.0707
Loss at step 1110: 0.2364
Loss at step 1120: 0.1570
Loss at step 1130: 0.2378
Loss at step 1140: 0.1411
Loss at step 1150: 0.1454
Loss at step 1160: 0.1924
Loss at step 1170: 0.2845
Loss at step 1180: 0.3497
Loss at step 1190: 0.1292
Loss at step 1200: 0.2120
Loss at step 1210: 0.2402
Loss at step 1220: 0.1314
Loss at step 1230: 0.1772
Loss at step 1240: 0.3782
Loss at step 1250: 0.2521
Loss at step 1260: 0.2351
Loss at step 1270: 0.0963
Loss at step 1280: 0.1712
Loss at step 1290: 0.0942
Loss at step 1300: 0.1777
Loss at step 1310: 0.1165
Loss at step 1320: 0.2642
Loss at step 1330: 0.1246
Loss at step 1340: 0.2106
Loss at step 1350: 0.1844
Loss at step 1360: 0.3070
Loss at step 1370: 0.2131
Loss at step 1380: 0.1661
Loss at step 1390: 0.1002
Loss at step 1400: 0.3003
Loss at step 1410: 0.1191
Loss at step 1420: 0.1005
Loss at step 1430: 0.1261
Loss at step 1440: 0.1790
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.553321, 'precision': [0.618257, 0.544852, 0.475862], 'recall': [0.596955, 0.562399, 0.477509], 'f1': [0.607419, 0.553487, 0.476684]}
{'accuracy': 0.729223, 'precision': 0.475862, 'recall': 0.477509, 'f1': 0.476684, 'WordR': 0.089072}
Loss at step 1450: 0.3132
Loss at step 1460: 0.1934
Loss at step 1470: 0.1376
Loss at step 1480: 0.0737
Loss at step 1490: 0.1470
Loss at step 1500: 0.0600
Loss at step 1510: 0.0310
Loss at step 1520: 0.1352
Loss at step 1530: 0.3405
Loss at step 1540: 0.1828
Loss at step 1550: 0.1531
Loss at step 1560: 0.0313
Loss at step 1570: 0.0516
Loss at step 1580: 0.0846
Loss at step 1590: 0.0876
Loss at step 1600: 0.1535
Loss at step 1610: 0.1747
Loss at step 1620: 0.0620
Loss at step 1630: 0.3196
Loss at step 1640: 0.2162
Loss at step 1650: 0.0694
Loss at step 1660: 0.0795
Loss at step 1670: 0.0722
Loss at step 1680: 0.0742
Loss at step 1690: 0.1347
Loss at step 1700: 0.0835
Loss at step 1710: 0.1330
Loss at step 1720: 0.1805
Loss at step 1730: 0.1835
Loss at step 1740: 0.2301
Loss at step 1750: 0.1132
Loss at step 1760: 0.0708
Loss at step 1770: 0.1088
Loss at step 1780: 0.0497
Loss at step 1790: 0.1462
Loss at step 1800: 0.1430
Loss at step 1810: 0.0510
Loss at step 1820: 0.1686
Loss at step 1830: 0.2040
Loss at step 1840: 0.0822
Loss at step 1850: 0.2075
Loss at step 1860: 0.0516
Loss at step 1870: 0.0821
Loss at step 1880: 0.0742
Loss at step 1890: 0.2277
Loss at step 1900: 0.2672
Loss at step 1910: 0.1221
Loss at step 1920: 0.1305
Loss at step 1930: 0.1468
Loss at step 1940: 0.1820
Loss at step 1950: 0.1637
Loss at step 1960: 0.1345
Loss at step 1970: 0.2270
Loss at step 1980: 0.0300
Loss at step 1990: 0.1797
Loss at step 2000: 0.1051
Loss at step 2010: 0.1325
Loss at step 2020: 0.1895
Loss at step 2030: 0.0917
Loss at step 2040: 0.2638
Loss at step 2050: 0.0995
Loss at step 2060: 0.1854
Loss at step 2070: 0.2176
Loss at step 2080: 0.0882
Loss at step 2090: 0.1361
Loss at step 2100: 0.0490
Loss at step 2110: 0.0801
Loss at step 2120: 0.0793
Loss at step 2130: 0.1974
Loss at step 2140: 0.1159
Loss at step 2150: 0.0992
Loss at step 2160: 0.2475
Loss at step 2170: 0.0798
Loss at step 2180: 0.0741
Loss at step 2190: 0.0098
Loss at step 2200: 0.0856
Loss at step 2210: 0.1112
Loss at step 2220: 0.0953
Loss at step 2230: 0.1457
Loss at step 2240: 0.1516
Loss at step 2250: 0.0996
Loss at step 2260: 0.0970
Loss at step 2270: 0.0456
Loss at step 2280: 0.1533
Loss at step 2290: 0.0424
Loss at step 2300: 0.1061
Loss at step 2310: 0.0462
Loss at step 2320: 0.1929
Loss at step 2330: 0.0516
Loss at step 2340: 0.0322
Loss at step 2350: 0.0700
Loss at step 2360: 0.2294
Loss at step 2370: 0.1027
Loss at step 2380: 0.0179
Loss at step 2390: 0.1272
Loss at step 2400: 0.2266
Loss at step 2410: 0.1285
Loss at step 2420: 0.1436
Loss at step 2430: 0.0862
Loss at step 2440: 0.0416
Loss at step 2450: 0.0236
Loss at step 2460: 0.1967
Loss at step 2470: 0.0721
Loss at step 2480: 0.0532
Loss at step 2490: 0.0985
Loss at step 2500: 0.0740
Loss at step 2510: 0.1903
Loss at step 2520: 0.0419
Loss at step 2530: 0.1481
Loss at step 2540: 0.0747
Loss at step 2550: 0.0074
Loss at step 2560: 0.0122
Loss at step 2570: 0.1395
Loss at step 2580: 0.1339
Loss at step 2590: 0.0936
Loss at step 2600: 0.0690
Loss at step 2610: 0.0272
Loss at step 2620: 0.0745
Loss at step 2630: 0.0254
Loss at step 2640: 0.1332
Loss at step 2650: 0.1302
Loss at step 2660: 0.0175
Loss at step 2670: 0.1650
Loss at step 2680: 0.0111
Loss at step 2690: 0.0102
Loss at step 2700: 0.0616
Loss at step 2710: 0.0368
Loss at step 2720: 0.0436
Loss at step 2730: 0.0639
Loss at step 2740: 0.1051
Loss at step 2750: 0.0705
Loss at step 2760: 0.1231
Loss at step 2770: 0.1700
Loss at step 2780: 0.0169
Loss at step 2790: 0.0837
Loss at step 2800: 0.0213
Loss at step 2810: 0.0960
Loss at step 2820: 0.0085
Loss at step 2830: 0.0563
Loss at step 2840: 0.1226
Loss at step 2850: 0.0842
Loss at step 2860: 0.0102
Loss at step 2870: 0.1723
Loss at step 2880: 0.0244
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.550343, 'precision': [0.690156, 0.486159, 0.53484], 'recall': [0.514022, 0.742351, 0.327566], 'f1': [0.589208, 0.587542, 0.406295]}
{'accuracy': 0.752755, 'precision': 0.53484, 'recall': 0.327566, 'f1': 0.406295, 'WordR': 0.018182}
Loss at step 2890: 0.1958
Loss at step 2900: 0.1480
Loss at step 2910: 0.0901
Loss at step 2920: 0.0122
Loss at step 2930: 0.2369
Loss at step 2940: 0.0102
Loss at step 2950: 0.2003
Loss at step 2960: 0.1384
Loss at step 2970: 0.1288
Loss at step 2980: 0.0808
Loss at step 2990: 0.0325
Loss at step 3000: 0.0397
Loss at step 3010: 0.0164
Loss at step 3020: 0.0506
Loss at step 3030: 0.0458
Loss at step 3040: 0.0892
Loss at step 3050: 0.0489
Loss at step 3060: 0.0698
Loss at step 3070: 0.0314
Loss at step 3080: 0.0269
Loss at step 3090: 0.0263
Loss at step 3100: 0.0079
Loss at step 3110: 0.0656
Loss at step 3120: 0.0083
Loss at step 3130: 0.0250
Loss at step 3140: 0.0195
Loss at step 3150: 0.0254
Loss at step 3160: 0.0198
Loss at step 3170: 0.0992
Loss at step 3180: 0.0547
Loss at step 3190: 0.0050
Loss at step 3200: 0.0521
Loss at step 3210: 0.1102
Loss at step 3220: 0.0570
Loss at step 3230: 0.0050
Loss at step 3240: 0.0185
Loss at step 3250: 0.0112
Loss at step 3260: 0.1732
Loss at step 3270: 0.0345
Loss at step 3280: 0.0138
Loss at step 3290: 0.1804
Loss at step 3300: 0.0047
Loss at step 3310: 0.1145
Loss at step 3320: 0.1447
Loss at step 3330: 0.0438
Loss at step 3340: 0.0049
Loss at step 3350: 0.1765
Loss at step 3360: 0.0827
Loss at step 3370: 0.0390
Loss at step 3380: 0.0622
Loss at step 3390: 0.0221
Loss at step 3400: 0.0060
Loss at step 3410: 0.0872
Loss at step 3420: 0.0159
Loss at step 3430: 0.0320
Loss at step 3440: 0.0042
Loss at step 3450: 0.0923
Loss at step 3460: 0.0247
Loss at step 3470: 0.1319
Loss at step 3480: 0.0263
Loss at step 3490: 0.0174
Loss at step 3500: 0.0092
Loss at step 3510: 0.0733
Loss at step 3520: 0.0039
Loss at step 3530: 0.0385
Loss at step 3540: 0.0663
Loss at step 3550: 0.1608
Loss at step 3560: 0.0532
Loss at step 3570: 0.0021
Loss at step 3580: 0.0107
Loss at step 3590: 0.0037
Loss at step 3600: 0.0581
Loss at step 3610: 0.0360
Loss at step 3620: 0.1126
Loss at step 3630: 0.0925
Loss at step 3640: 0.0099
Loss at step 3650: 0.0358
Loss at step 3660: 0.0019
Loss at step 3670: 0.0498
Loss at step 3680: 0.1015
Loss at step 3690: 0.0046
Loss at step 3700: 0.0613
Loss at step 3710: 0.0057
Loss at step 3720: 0.0796
Loss at step 3730: 0.0173
Loss at step 3740: 0.1027
Loss at step 3750: 0.0038
Loss at step 3760: 0.1403
Loss at step 3770: 0.0319
Loss at step 3780: 0.0256
Loss at step 3790: 0.1766
Loss at step 3800: 0.0114
Loss at step 3810: 0.0204
Loss at step 3820: 0.0049
Loss at step 3830: 0.0056
Loss at step 3840: 0.0461
Loss at step 3850: 0.0816
Loss at step 3860: 0.0079
Loss at step 3870: 0.0496
Loss at step 3880: 0.0199
Loss at step 3890: 0.0564
Loss at step 3900: 0.0308
Loss at step 3910: 0.0022
Loss at step 3920: 0.1060
Loss at step 3930: 0.0432
Loss at step 3940: 0.0996
Loss at step 3950: 0.0448
Loss at step 3960: 0.0068
Loss at step 3970: 0.1362
Loss at step 3980: 0.0842
Loss at step 3990: 0.0044
Loss at step 4000: 0.0107
Loss at step 4010: 0.0031
Loss at step 4020: 0.0415
Loss at step 4030: 0.0029
Loss at step 4040: 0.0974
Loss at step 4050: 0.0895
Loss at step 4060: 0.1132
Loss at step 4070: 0.0323
Loss at step 4080: 0.0038
Loss at step 4090: 0.0356
Loss at step 4100: 0.0065
Loss at step 4110: 0.0049
Loss at step 4120: 0.0997
Loss at step 4130: 0.0594
Loss at step 4140: 0.0674
Loss at step 4150: 0.0092
Loss at step 4160: 0.0185
Loss at step 4170: 0.0302
Loss at step 4180: 0.0026
Loss at step 4190: 0.0056
Loss at step 4200: 0.0589
Loss at step 4210: 0.0975
Loss at step 4220: 0.0090
Loss at step 4230: 0.0044
Loss at step 4240: 0.1037
Loss at step 4250: 0.0105
Loss at step 4260: 0.0204
Loss at step 4270: 0.1156
Loss at step 4280: 0.0746
Loss at step 4290: 0.1232
Loss at step 4300: 0.0708
Loss at step 4310: 0.1275
Loss at step 4320: 0.0544
***** Running testing *****
  Num examples = 6714
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.553024, 'precision': [0.652235, 0.529391, 0.467179], 'recall': [0.561298, 0.612721, 0.455594], 'f1': [0.603359, 0.568016, 0.461314]}
{'accuracy': 0.725201, 'precision': 0.467179, 'recall': 0.455594, 'f1': 0.461314, 'WordR': 0.065069}
