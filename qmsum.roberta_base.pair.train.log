Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7b47391ad280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 382554 of the training set: {'input_ids': [0, 44518, 39322, 35, 166, 33, 5, 27362, 98, 24, 18, 117, 936, 479, 2, 2, 36926, 13700, 31275, 802, 14, 5, 1901, 4972, 197, 28, 205, 13, 19888, 472, 7, 5, 6063, 797, 4, 50118, 44518, 39322, 2528, 14, 24, 115, 45, 28, 1365, 7, 304, 5, 20808, 2441, 8, 6741, 31275, 18, 2979, 4, 50118, 33347, 4827, 1507, 6, 5, 165, 1276, 7, 304, 1901, 4972, 5043, 8, 27913, 39322, 1507, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 54000
Loss at step 10: 1.0397
Loss at step 20: 0.6566
Loss at step 30: 0.5077
Loss at step 40: 0.4883
Loss at step 50: 0.5959
Loss at step 60: 0.5106
Loss at step 70: 0.6342
Loss at step 80: 0.5066
Loss at step 90: 0.4501
Loss at step 100: 0.7015
Loss at step 110: 0.8512
Loss at step 120: 0.6671
Loss at step 130: 0.5972
Loss at step 140: 0.4041
Loss at step 150: 0.6221
Loss at step 160: 0.5197
Loss at step 170: 0.5007
Loss at step 180: 0.4168
Loss at step 190: 0.6697
Loss at step 200: 0.2395
Loss at step 210: 0.6004
Loss at step 220: 0.3221
Loss at step 230: 0.6522
Loss at step 240: 0.2867
Loss at step 250: 0.5162
Loss at step 260: 0.6863
Loss at step 270: 0.4917
Loss at step 280: 0.5553
Loss at step 290: 0.5547
Loss at step 300: 0.4235
Loss at step 310: 0.5972
Loss at step 320: 0.5078
Loss at step 330: 0.3357
Loss at step 340: 0.4163
Loss at step 350: 0.5433
Loss at step 360: 0.4585
Loss at step 370: 0.6933
Loss at step 380: 0.7167
Loss at step 390: 0.5300
Loss at step 400: 0.5580
Loss at step 410: 0.7617
Loss at step 420: 0.5431
Loss at step 430: 0.5039
Loss at step 440: 0.5872
Loss at step 450: 1.1994
Loss at step 460: 0.8387
Loss at step 470: 0.5551
Loss at step 480: 0.8022
Loss at step 490: 0.9810
Loss at step 500: 0.4851
Loss at step 510: 0.5716
Loss at step 520: 0.5588
Loss at step 530: 0.3105
Loss at step 540: 0.6374
Loss at step 550: 0.5195
Loss at step 560: 0.3457
Loss at step 570: 0.4533
Loss at step 580: 0.5042
Loss at step 590: 0.4581
Loss at step 600: 0.4442
Loss at step 610: 0.5986
Loss at step 620: 0.5100
Loss at step 630: 0.5011
Loss at step 640: 0.5410
Loss at step 650: 0.6233
Loss at step 660: 0.5608
Loss at step 670: 0.4708
Loss at step 680: 0.7006
Loss at step 690: 0.4887
Loss at step 700: 0.4918
Loss at step 710: 0.4006
Loss at step 720: 0.3881
Loss at step 730: 0.3794
Loss at step 740: 0.5127
Loss at step 750: 0.3701
Loss at step 760: 0.3006
Loss at step 770: 0.4665
Loss at step 780: 0.5147
Loss at step 790: 0.4553
Loss at step 800: 0.5454
Loss at step 810: 0.1877
Loss at step 820: 0.6059
Loss at step 830: 0.4591
Loss at step 840: 0.4859
Loss at step 850: 0.8260
Loss at step 860: 0.4371
Loss at step 870: 0.4659
Loss at step 880: 0.6263
Loss at step 890: 0.4701
Loss at step 900: 0.5207
Loss at step 910: 0.2636
Loss at step 920: 0.7846
Loss at step 930: 0.4845
Loss at step 940: 0.4752
Loss at step 950: 0.9306
Loss at step 960: 0.6066
Loss at step 970: 0.3469
Loss at step 980: 0.7134
Loss at step 990: 0.5397
Loss at step 1000: 0.3064
Loss at step 1010: 0.6604
Loss at step 1020: 0.7020
Loss at step 1030: 0.6383
Loss at step 1040: 0.5035
Loss at step 1050: 0.2720
Loss at step 1060: 0.5009
Loss at step 1070: 0.5820
Loss at step 1080: 0.6330
Loss at step 1090: 0.4174
Loss at step 1100: 0.5121
Loss at step 1110: 0.3215
Loss at step 1120: 0.5073
Loss at step 1130: 0.5139
Loss at step 1140: 0.3363
Loss at step 1150: 0.3822
Loss at step 1160: 0.4860
Loss at step 1170: 0.4069
Loss at step 1180: 0.6822
Loss at step 1190: 0.4550
Loss at step 1200: 0.5380
Loss at step 1210: 0.4192
Loss at step 1220: 0.2923
Loss at step 1230: 0.5430
Loss at step 1240: 0.3072
Loss at step 1250: 0.2248
Loss at step 1260: 0.5474
Loss at step 1270: 0.5491
Loss at step 1280: 0.3900
Loss at step 1290: 1.0186
Loss at step 1300: 0.3832
Loss at step 1310: 0.3782
Loss at step 1320: 0.4136
Loss at step 1330: 0.4257
Loss at step 1340: 0.7235
Loss at step 1350: 0.7343
Loss at step 1360: 0.2608
Loss at step 1370: 0.5075
Loss at step 1380: 0.3528
Loss at step 1390: 0.8822
Loss at step 1400: 0.6070
Loss at step 1410: 0.5591
Loss at step 1420: 1.0083
Loss at step 1430: 0.4042
Loss at step 1440: 0.6942
Loss at step 1450: 0.4784
Loss at step 1460: 0.3766
Loss at step 1470: 0.7868
Loss at step 1480: 0.5011
Loss at step 1490: 0.6584
Loss at step 1500: 0.2787
Loss at step 1510: 0.7889
Loss at step 1520: 0.4593
Loss at step 1530: 0.5903
Loss at step 1540: 0.7257
Loss at step 1550: 0.5052
Loss at step 1560: 0.7384
Loss at step 1570: 0.5671
Loss at step 1580: 0.5563
Loss at step 1590: 0.2322
Loss at step 1600: 0.5252
Loss at step 1610: 0.4753
Loss at step 1620: 0.7231
Loss at step 1630: 0.4011
Loss at step 1640: 0.6113
Loss at step 1650: 0.6366
Loss at step 1660: 0.5582
Loss at step 1670: 0.7830
Loss at step 1680: 0.3937
Loss at step 1690: 0.5611
Loss at step 1700: 0.5723
Loss at step 1710: 0.5209
Loss at step 1720: 0.5309
Loss at step 1730: 0.6176
Loss at step 1740: 0.3259
Loss at step 1750: 0.2878
Loss at step 1760: 0.6964
Loss at step 1770: 0.6118
Loss at step 1780: 0.8319
Loss at step 1790: 0.4884
Loss at step 1800: 0.5555
Loss at step 1810: 0.8322
Loss at step 1820: 0.5710
Loss at step 1830: 0.5802
Loss at step 1840: 0.5985
Loss at step 1850: 0.5142
Loss at step 1860: 0.5567
Loss at step 1870: 0.4003
Loss at step 1880: 0.5916
Loss at step 1890: 0.6777
Loss at step 1900: 0.7481
Loss at step 1910: 0.4474
Loss at step 1920: 0.5748
Loss at step 1930: 0.3513
Loss at step 1940: 0.3351
Loss at step 1950: 0.9828
Loss at step 1960: 0.4044
Loss at step 1970: 0.6373
Loss at step 1980: 0.4198
Loss at step 1990: 0.5269
Loss at step 2000: 0.3838
Loss at step 2010: 0.6686
Loss at step 2020: 0.8040
Loss at step 2030: 0.9665
Loss at step 2040: 0.4272
Loss at step 2050: 0.7164
Loss at step 2060: 0.5422
Loss at step 2070: 0.6313
Loss at step 2080: 0.5658
Loss at step 2090: 0.6627
Loss at step 2100: 0.8631
Loss at step 2110: 0.9057
Loss at step 2120: 0.7385
Loss at step 2130: 0.6426
Loss at step 2140: 0.6712
Loss at step 2150: 0.5640
Loss at step 2160: 0.3286
Loss at step 2170: 0.4960
Loss at step 2180: 0.7749
Loss at step 2190: 0.7019
Loss at step 2200: 0.4684
Loss at step 2210: 0.3036
Loss at step 2220: 0.7028
Loss at step 2230: 0.4213
Loss at step 2240: 0.6413
Loss at step 2250: 0.9114
Loss at step 2260: 0.3059
Loss at step 2270: 0.6756
Loss at step 2280: 0.5379
Loss at step 2290: 0.5902
Loss at step 2300: 0.5623
Loss at step 2310: 0.5865
Loss at step 2320: 0.6605
Loss at step 2330: 0.8511
Loss at step 2340: 0.7095
Loss at step 2350: 0.6789
Loss at step 2360: 0.6958
Loss at step 2370: 0.7742
Loss at step 2380: 0.6767
Loss at step 2390: 0.4024
Loss at step 2400: 0.5110
Loss at step 2410: 0.8181
Loss at step 2420: 0.6802
Loss at step 2430: 0.4772
Loss at step 2440: 0.7275
Loss at step 2450: 0.5373
Loss at step 2460: 0.6920
Loss at step 2470: 0.7541
Loss at step 2480: 1.0677
Loss at step 2490: 0.3207
Loss at step 2500: 0.6268
Loss at step 2510: 0.4823
Loss at step 2520: 0.5419
Loss at step 2530: 0.3650
Loss at step 2540: 0.4274
Loss at step 2550: 0.6411
Loss at step 2560: 0.3921
Loss at step 2570: 0.7452
Loss at step 2580: 0.5621
Loss at step 2590: 0.4873
Loss at step 2600: 0.3985
Loss at step 2610: 0.6949
Loss at step 2620: 0.6922
Loss at step 2630: 0.4767
Loss at step 2640: 0.3878
Loss at step 2650: 0.5510
Loss at step 2660: 0.6087
Loss at step 2670: 0.5168
Loss at step 2680: 0.6256
Loss at step 2690: 0.5031
Loss at step 2700: 0.3647
Loss at step 2710: 0.3348
Loss at step 2720: 0.5346
Loss at step 2730: 0.5117
Loss at step 2740: 0.6313
Loss at step 2750: 0.7454
Loss at step 2760: 0.5198
Loss at step 2770: 0.7852
Loss at step 2780: 0.7008
Loss at step 2790: 0.3983
Loss at step 2800: 0.7030
Loss at step 2810: 0.7098
Loss at step 2820: 0.3803
Loss at step 2830: 0.6044
Loss at step 2840: 0.8946
Loss at step 2850: 0.5898
Loss at step 2860: 0.5614
Loss at step 2870: 0.8444
Loss at step 2880: 0.6667
Loss at step 2890: 0.6289
Loss at step 2900: 0.6005
Loss at step 2910: 0.7343
Loss at step 2920: 0.3669
Loss at step 2930: 0.5123
Loss at step 2940: 0.3602
Loss at step 2950: 0.4598
Loss at step 2960: 0.4956
Loss at step 2970: 0.3399
Loss at step 2980: 0.6375
Loss at step 2990: 0.4466
Loss at step 3000: 0.4527
Loss at step 3010: 0.4724
Loss at step 3020: 0.7332
Loss at step 3030: 0.7475
Loss at step 3040: 0.8570
Loss at step 3050: 0.7626
Loss at step 3060: 0.5439
Loss at step 3070: 0.6887
Loss at step 3080: 0.6618
Loss at step 3090: 0.6609
Loss at step 3100: 0.4683
Loss at step 3110: 0.4091
Loss at step 3120: 0.6215
Loss at step 3130: 0.6313
Loss at step 3140: 0.6399
Loss at step 3150: 0.4611
Loss at step 3160: 0.7596
Loss at step 3170: 0.6543
Loss at step 3180: 0.5481
Loss at step 3190: 0.6190
Loss at step 3200: 0.5507
Loss at step 3210: 0.4773
Loss at step 3220: 0.4988
Loss at step 3230: 0.7332
Loss at step 3240: 0.5455
Loss at step 3250: 0.7200
Loss at step 3260: 0.5438
Loss at step 3270: 0.6479
Loss at step 3280: 0.8669
Loss at step 3290: 0.6947
Loss at step 3300: 0.6959
Loss at step 3310: 0.7666
Loss at step 3320: 0.6714
Loss at step 3330: 0.6599
Loss at step 3340: 0.4318
Loss at step 3350: 0.6254
Loss at step 3360: 0.7656
Loss at step 3370: 0.8789
Loss at step 3380: 0.5685
Loss at step 3390: 0.6414
Loss at step 3400: 0.5717
Loss at step 3410: 0.6049
Loss at step 3420: 0.6931
Loss at step 3430: 0.6706
Loss at step 3440: 0.6282
Loss at step 3450: 0.7472
Loss at step 3460: 0.6337
Loss at step 3470: 0.8963
Loss at step 3480: 0.4631
Loss at step 3490: 0.9422
Loss at step 3500: 0.5850
Loss at step 3510: 0.7481
Loss at step 3520: 0.4708
Loss at step 3530: 0.6929
Loss at step 3540: 0.6245
Loss at step 3550: 0.5147
Loss at step 3560: 0.9434
Loss at step 3570: 0.8029
Loss at step 3580: 0.4585
Loss at step 3590: 0.6854
Loss at step 3600: 0.8822
Loss at step 3610: 0.4522
Loss at step 3620: 0.8988
Loss at step 3630: 0.6943
Loss at step 3640: 0.6430
Loss at step 3650: 0.9580
Loss at step 3660: 0.6221
Loss at step 3670: 0.6458
Loss at step 3680: 0.7928
Loss at step 3690: 0.6565
Loss at step 3700: 0.5711
Loss at step 3710: 0.8724
Loss at step 3720: 0.9105
Loss at step 3730: 0.8333
Loss at step 3740: 0.7879
Loss at step 3750: 0.4872
Loss at step 3760: 0.5959
Loss at step 3770: 0.5712
Loss at step 3780: 0.7482
Loss at step 3790: 0.8197
Loss at step 3800: 0.7834
Loss at step 3810: 0.5565
Loss at step 3820: 0.6626
Loss at step 3830: 0.7955
Loss at step 3840: 0.6212
Loss at step 3850: 0.6036
Loss at step 3860: 0.4222
Loss at step 3870: 0.7332
Loss at step 3880: 0.5165
Loss at step 3890: 0.7501
Loss at step 3900: 0.8143
Loss at step 3910: 0.8868
Loss at step 3920: 0.7825
Loss at step 3930: 0.4226
Loss at step 3940: 0.6052
Loss at step 3950: 0.6448
Loss at step 3960: 0.8196
Loss at step 3970: 0.5327
Loss at step 3980: 0.8814
Loss at step 3990: 0.5630
Loss at step 4000: 0.7661
Loss at step 4010: 0.5457
Loss at step 4020: 0.3148
Loss at step 4030: 0.5587
Loss at step 4040: 0.5514
Loss at step 4050: 0.5453
Loss at step 4060: 0.6330
Loss at step 4070: 0.5294
Loss at step 4080: 0.4214
Loss at step 4090: 0.3996
Loss at step 4100: 0.6337
Loss at step 4110: 0.3669
Loss at step 4120: 0.6248
Loss at step 4130: 0.8586
Loss at step 4140: 0.8345
Loss at step 4150: 0.8991
Loss at step 4160: 0.4558
Loss at step 4170: 0.6432
Loss at step 4180: 0.2739
Loss at step 4190: 0.5482
Loss at step 4200: 0.6741
Loss at step 4210: 0.7780
Loss at step 4220: 0.3260
Loss at step 4230: 0.4251
Loss at step 4240: 0.2007
Loss at step 4250: 0.6967
Loss at step 4260: 0.6944
Loss at step 4270: 0.3929
Loss at step 4280: 0.5249
Loss at step 4290: 0.5292
Loss at step 4300: 0.5744
Loss at step 4310: 0.6831
Loss at step 4320: 0.5955
Loss at step 4330: 0.6349
Loss at step 4340: 0.6890
Loss at step 4350: 0.5861
Loss at step 4360: 0.5087
Loss at step 4370: 0.5660
Loss at step 4380: 0.5155
Loss at step 4390: 0.8893
Loss at step 4400: 0.3146
Loss at step 4410: 0.6499
Loss at step 4420: 0.6199
Loss at step 4430: 0.6335
Loss at step 4440: 0.5769
Loss at step 4450: 0.6144
Loss at step 4460: 0.3611
Loss at step 4470: 0.4141
Loss at step 4480: 0.3867
Loss at step 4490: 0.6244
Loss at step 4500: 0.4457
Loss at step 4510: 0.4191
Loss at step 4520: 0.3771
Loss at step 4530: 0.7919
Loss at step 4540: 0.5020
Loss at step 4550: 0.6496
Loss at step 4560: 0.4251
Loss at step 4570: 0.7987
Loss at step 4580: 0.8743
Loss at step 4590: 0.6749
Loss at step 4600: 0.5705
Loss at step 4610: 0.6045
Loss at step 4620: 0.9904
Loss at step 4630: 0.7762
Loss at step 4640: 0.5156
Loss at step 4650: 0.4837
Loss at step 4660: 0.3898
Loss at step 4670: 0.7033
Loss at step 4680: 0.5011
Loss at step 4690: 0.3267
Loss at step 4700: 0.6787
Loss at step 4710: 0.4875
Loss at step 4720: 0.5360
Loss at step 4730: 0.5237
Loss at step 4740: 0.7275
Loss at step 4750: 0.4802
Loss at step 4760: 0.6045
Loss at step 4770: 0.5657
Loss at step 4780: 0.4223
Loss at step 4790: 0.7328
Loss at step 4800: 0.4384
Loss at step 4810: 0.7773
Loss at step 4820: 0.6657
Loss at step 4830: 0.6198
Loss at step 4840: 0.7586
Loss at step 4850: 0.6054
Loss at step 4860: 0.6028
Loss at step 4870: 0.5973
Loss at step 4880: 0.3511
Loss at step 4890: 0.5249
Loss at step 4900: 0.6291
Loss at step 4910: 0.3921
Loss at step 4920: 0.4729
Loss at step 4930: 0.7298
Loss at step 4940: 0.6120
Loss at step 4950: 0.4853
Loss at step 4960: 0.5033
Loss at step 4970: 0.4976
Loss at step 4980: 0.2213
Loss at step 4990: 0.6005
Loss at step 5000: 0.4154
Loss at step 5010: 0.6720
Loss at step 5020: 0.4933
Loss at step 5030: 0.5620
Loss at step 5040: 0.4256
Loss at step 5050: 0.6026
Loss at step 5060: 0.7729
Loss at step 5070: 0.4059
Loss at step 5080: 0.5421
Loss at step 5090: 0.7131
Loss at step 5100: 0.7474
Loss at step 5110: 0.6912
Loss at step 5120: 0.4958
Loss at step 5130: 0.7118
Loss at step 5140: 0.5459
Loss at step 5150: 0.4499
Loss at step 5160: 0.4305
Loss at step 5170: 0.8361
Loss at step 5180: 0.7132
Loss at step 5190: 0.6452
Loss at step 5200: 0.5802
Loss at step 5210: 0.4118
Loss at step 5220: 0.7062
Loss at step 5230: 0.4185
Loss at step 5240: 0.6873
Loss at step 5250: 0.4357
Loss at step 5260: 0.6533
Loss at step 5270: 0.6009
Loss at step 5280: 0.5054
Loss at step 5290: 0.5549
Loss at step 5300: 0.5969
Loss at step 5310: 0.4486
Loss at step 5320: 0.6254
Loss at step 5330: 0.5225
Loss at step 5340: 0.4892
Loss at step 5350: 0.5137
Loss at step 5360: 0.5821
Loss at step 5370: 0.7075
Loss at step 5380: 0.3844
Loss at step 5390: 0.7180
Loss at step 5400: 0.4897
Loss at step 5410: 0.4608
Loss at step 5420: 0.5614
Loss at step 5430: 0.7778
Loss at step 5440: 0.5095
Loss at step 5450: 0.4696
Loss at step 5460: 0.3097
Loss at step 5470: 0.5088
Loss at step 5480: 0.7378
Loss at step 5490: 0.5738
Loss at step 5500: 0.8647
Loss at step 5510: 0.5037
Loss at step 5520: 0.4894
Loss at step 5530: 0.3853
Loss at step 5540: 0.5763
Loss at step 5550: 0.3967
Loss at step 5560: 0.5949
Loss at step 5570: 0.4230
Loss at step 5580: 0.5594
Loss at step 5590: 0.6876
Loss at step 5600: 0.4288
Loss at step 5610: 0.7094
Loss at step 5620: 0.8068
Loss at step 5630: 0.4338
Loss at step 5640: 0.5639
Loss at step 5650: 0.4422
Loss at step 5660: 0.3145
Loss at step 5670: 0.5401
Loss at step 5680: 0.4768
Loss at step 5690: 0.5344
Loss at step 5700: 0.6701
Loss at step 5710: 0.7860
Loss at step 5720: 0.8841
Loss at step 5730: 0.5556
Loss at step 5740: 0.7015
Loss at step 5750: 0.5864
Loss at step 5760: 0.8166
Loss at step 5770: 0.6643
Loss at step 5780: 0.4608
Loss at step 5790: 0.6813
Loss at step 5800: 0.5634
Loss at step 5810: 0.4487
Loss at step 5820: 0.8278
Loss at step 5830: 0.4904
Loss at step 5840: 0.6145
Loss at step 5850: 0.7661
Loss at step 5860: 0.6091
Loss at step 5870: 0.5661
Loss at step 5880: 0.6644
Loss at step 5890: 0.7334
Loss at step 5900: 0.6184
Loss at step 5910: 0.5601
Loss at step 5920: 0.4950
Loss at step 5930: 0.7032
Loss at step 5940: 0.7615
Loss at step 5950: 0.7783
Loss at step 5960: 0.7319
Loss at step 5970: 0.6298
Loss at step 5980: 0.6252
Loss at step 5990: 0.4562
Loss at step 6000: 0.7688
Loss at step 6010: 0.3427
Loss at step 6020: 0.5421
Loss at step 6030: 0.7029
Loss at step 6040: 0.4208
Loss at step 6050: 0.5497
Loss at step 6060: 0.6264
Loss at step 6070: 0.5367
Loss at step 6080: 0.7492
Loss at step 6090: 0.4797
Loss at step 6100: 0.6004
Loss at step 6110: 0.4733
Loss at step 6120: 0.5798
Loss at step 6130: 0.4790
Loss at step 6140: 0.6577
Loss at step 6150: 0.9060
Loss at step 6160: 0.6782
Loss at step 6170: 0.6115
Loss at step 6180: 0.7445
Loss at step 6190: 0.7468
Loss at step 6200: 0.4178
Loss at step 6210: 0.7696
Loss at step 6220: 0.8667
Loss at step 6230: 0.7370
Loss at step 6240: 0.8744
Loss at step 6250: 0.6136
Loss at step 6260: 0.6049
Loss at step 6270: 1.0777
Loss at step 6280: 0.9884
Loss at step 6290: 0.8091
Loss at step 6300: 0.7534
Loss at step 6310: 0.8171
Loss at step 6320: 0.9682
Loss at step 6330: 0.7747
Loss at step 6340: 0.6722
Loss at step 6350: 0.6185
Loss at step 6360: 0.5890
Loss at step 6370: 0.8111
Loss at step 6380: 0.3965
Loss at step 6390: 0.6729
Loss at step 6400: 0.3376
Loss at step 6410: 0.6798
Loss at step 6420: 0.6762
Loss at step 6430: 0.5831
Loss at step 6440: 0.6578
Loss at step 6450: 0.6246
Loss at step 6460: 0.8898
Loss at step 6470: 0.5420
Loss at step 6480: 0.7408
Loss at step 6490: 0.5374
Loss at step 6500: 0.6305
Loss at step 6510: 0.6167
Loss at step 6520: 0.9181
Loss at step 6530: 0.3445
Loss at step 6540: 0.4562
Loss at step 6550: 0.4838
Loss at step 6560: 0.6330
Loss at step 6570: 0.5421
Loss at step 6580: 0.6277
Loss at step 6590: 0.6982
Loss at step 6600: 0.6383
Loss at step 6610: 0.8172
Loss at step 6620: 0.4036
Loss at step 6630: 0.6742
Loss at step 6640: 0.5987
Loss at step 6650: 0.7249
Loss at step 6660: 0.7540
Loss at step 6670: 0.6661
Loss at step 6680: 0.6581
Loss at step 6690: 0.7143
Loss at step 6700: 0.5624
Loss at step 6710: 0.8399
Loss at step 6720: 0.7936
Loss at step 6730: 0.6715
Loss at step 6740: 0.5432
Loss at step 6750: 0.4791
Loss at step 6760: 0.4211
Loss at step 6770: 0.6791
Loss at step 6780: 0.4888
Loss at step 6790: 0.4637
Loss at step 6800: 0.5398
Loss at step 6810: 0.6839
Loss at step 6820: 0.4951
Loss at step 6830: 0.4594
Loss at step 6840: 0.6126
Loss at step 6850: 0.4854
Loss at step 6860: 0.6016
Loss at step 6870: 0.5565
Loss at step 6880: 0.4853
Loss at step 6890: 0.6786
Loss at step 6900: 0.7912
Loss at step 6910: 0.7147
Loss at step 6920: 0.7084
Loss at step 6930: 0.7405
Loss at step 6940: 0.8051
Loss at step 6950: 0.8190
Loss at step 6960: 0.8110
Loss at step 6970: 0.5752
Loss at step 6980: 0.8229
Loss at step 6990: 0.8248
Loss at step 7000: 0.4954
Loss at step 7010: 0.4284
Loss at step 7020: 0.7876
Loss at step 7030: 0.6058
Loss at step 7040: 0.5954
Loss at step 7050: 0.6263
Loss at step 7060: 0.8657
Loss at step 7070: 0.6261
Loss at step 7080: 0.4320
Loss at step 7090: 0.6450
Loss at step 7100: 0.8981
Loss at step 7110: 0.5145
Loss at step 7120: 0.7005
Loss at step 7130: 0.6016
Loss at step 7140: 0.5531
Loss at step 7150: 0.4803
Loss at step 7160: 0.7256
Loss at step 7170: 0.4228
Loss at step 7180: 0.6673
Loss at step 7190: 0.5352
Loss at step 7200: 0.4381
Loss at step 7210: 0.4141
Loss at step 7220: 0.4658
Loss at step 7230: 0.9190
Loss at step 7240: 0.8037
Loss at step 7250: 0.7069
Loss at step 7260: 0.3529
Loss at step 7270: 0.5546
Loss at step 7280: 0.8412
Loss at step 7290: 0.6473
Loss at step 7300: 0.9218
Loss at step 7310: 0.6360
Loss at step 7320: 0.7155
Loss at step 7330: 0.5972
Loss at step 7340: 0.6441
Loss at step 7350: 0.8963
Loss at step 7360: 0.5663
Loss at step 7370: 0.5317
Loss at step 7380: 0.7079
Loss at step 7390: 0.4842
Loss at step 7400: 1.1074
Loss at step 7410: 0.9075
Loss at step 7420: 0.8121
Loss at step 7430: 0.5622
Loss at step 7440: 0.6324
Loss at step 7450: 0.7401
Loss at step 7460: 0.7465
Loss at step 7470: 0.4655
Loss at step 7480: 0.6346
Loss at step 7490: 0.6127
Loss at step 7500: 0.8023
Loss at step 7510: 0.6221
Loss at step 7520: 0.5712
Loss at step 7530: 0.4502
Loss at step 7540: 0.7200
Loss at step 7550: 0.7179
Loss at step 7560: 0.6109
Loss at step 7570: 0.7400
Loss at step 7580: 0.6277
Loss at step 7590: 0.5537
Loss at step 7600: 0.7298
Loss at step 7610: 0.4951
Loss at step 7620: 0.6161
Loss at step 7630: 0.6086
Loss at step 7640: 0.5525
Loss at step 7650: 0.4865
Loss at step 7660: 0.4795
Loss at step 7670: 0.5468
Loss at step 7680: 0.6894
Loss at step 7690: 0.3717
Loss at step 7700: 0.4125
Loss at step 7710: 0.5427
Loss at step 7720: 0.5360
Loss at step 7730: 0.4137
Loss at step 7740: 0.6827
Loss at step 7750: 0.6557
Loss at step 7760: 0.5958
Loss at step 7770: 0.5642
Loss at step 7780: 0.6925
Loss at step 7790: 0.5320
Loss at step 7800: 0.7345
Loss at step 7810: 0.6616
Loss at step 7820: 0.3866
Loss at step 7830: 0.7921
Loss at step 7840: 0.8169
Loss at step 7850: 0.8498
Loss at step 7860: 0.8605
Loss at step 7870: 0.6177
Loss at step 7880: 0.5773
Loss at step 7890: 0.6041
Loss at step 7900: 0.7254
Loss at step 7910: 0.8216
Loss at step 7920: 0.6838
Loss at step 7930: 0.6334
Loss at step 7940: 0.6657
Loss at step 7950: 0.7936
Loss at step 7960: 0.4716
Loss at step 7970: 0.6186
Loss at step 7980: 0.6852
Loss at step 7990: 0.6161
Loss at step 8000: 0.6782
Loss at step 8010: 0.7883
Loss at step 8020: 0.3046
Loss at step 8030: 0.8005
Loss at step 8040: 0.7400
Loss at step 8050: 0.6133
Loss at step 8060: 0.4864
Loss at step 8070: 0.5354
Loss at step 8080: 0.4640
Loss at step 8090: 0.7472
Loss at step 8100: 0.5555
Loss at step 8110: 0.6174
Loss at step 8120: 0.7436
Loss at step 8130: 0.4907
Loss at step 8140: 0.5526
Loss at step 8150: 0.7314
Loss at step 8160: 0.6253
Loss at step 8170: 0.6280
Loss at step 8180: 0.6264
Loss at step 8190: 0.7482
Loss at step 8200: 0.7069
Loss at step 8210: 0.5993
Loss at step 8220: 0.5996
Loss at step 8230: 0.8554
Loss at step 8240: 0.5908
Loss at step 8250: 0.7132
Loss at step 8260: 0.7058
Loss at step 8270: 0.5207
Loss at step 8280: 0.4684
Loss at step 8290: 0.6104
Loss at step 8300: 0.4562
Loss at step 8310: 0.5727
Loss at step 8320: 0.8757
Loss at step 8330: 0.8285
Loss at step 8340: 0.9211
Loss at step 8350: 0.6753
Loss at step 8360: 0.5453
Loss at step 8370: 0.5372
Loss at step 8380: 0.4890
Loss at step 8390: 0.4716
Loss at step 8400: 0.7409
Loss at step 8410: 0.6320
Loss at step 8420: 0.6159
Loss at step 8430: 0.5569
Loss at step 8440: 0.9218
Loss at step 8450: 0.8223
Loss at step 8460: 0.6774
Loss at step 8470: 0.7583
Loss at step 8480: 0.5870
Loss at step 8490: 0.5453
Loss at step 8500: 0.6102
Loss at step 8510: 0.5437
Loss at step 8520: 0.5786
Loss at step 8530: 0.5577
Loss at step 8540: 0.8600
Loss at step 8550: 0.3497
Loss at step 8560: 0.5294
Loss at step 8570: 0.9158
Loss at step 8580: 0.3728
Loss at step 8590: 0.3718
Loss at step 8600: 0.4492
Loss at step 8610: 0.7815
Loss at step 8620: 0.8454
Loss at step 8630: 0.6856
Loss at step 8640: 0.6245
Loss at step 8650: 1.1196
Loss at step 8660: 0.8595
Loss at step 8670: 0.8051
Loss at step 8680: 0.5448
Loss at step 8690: 0.5174
Loss at step 8700: 0.8855
Loss at step 8710: 0.8692
Loss at step 8720: 0.8620
Loss at step 8730: 0.7438
Loss at step 8740: 0.8507
Loss at step 8750: 0.6419
Loss at step 8760: 0.6267
Loss at step 8770: 0.3421
Loss at step 8780: 0.5382
Loss at step 8790: 0.4811
Loss at step 8800: 0.4591
Loss at step 8810: 0.4678
Loss at step 8820: 0.7607
Loss at step 8830: 0.5601
Loss at step 8840: 0.3670
Loss at step 8850: 0.6014
Loss at step 8860: 0.6272
Loss at step 8870: 0.7548
Loss at step 8880: 0.4901
Loss at step 8890: 0.6770
Loss at step 8900: 0.6918
Loss at step 8910: 0.9161
Loss at step 8920: 0.4967
Loss at step 8930: 0.6743
Loss at step 8940: 0.4075
Loss at step 8950: 0.7281
Loss at step 8960: 0.6634
Loss at step 8970: 0.4533
Loss at step 8980: 0.5571
Loss at step 8990: 0.7235
Loss at step 9000: 0.5105
Loss at step 9010: 0.6538
Loss at step 9020: 0.6831
Loss at step 9030: 0.6814
Loss at step 9040: 0.6861
Loss at step 9050: 0.6784
Loss at step 9060: 1.0155
Loss at step 9070: 0.7287
Loss at step 9080: 0.6318
Loss at step 9090: 0.5029
Loss at step 9100: 0.7413
Loss at step 9110: 0.4885
Loss at step 9120: 0.6351
Loss at step 9130: 1.1477
Loss at step 9140: 0.5329
Loss at step 9150: 0.2860
Loss at step 9160: 0.6289
Loss at step 9170: 0.6420
Loss at step 9180: 0.5432
Loss at step 9190: 0.7158
Loss at step 9200: 0.7711
Loss at step 9210: 0.6053
Loss at step 9220: 0.5489
Loss at step 9230: 0.6913
Loss at step 9240: 0.5920
Loss at step 9250: 0.8367
Loss at step 9260: 0.6183
Loss at step 9270: 0.6643
Loss at step 9280: 0.6001
Loss at step 9290: 0.4286
Loss at step 9300: 0.6241
Loss at step 9310: 0.7333
Loss at step 9320: 0.8199
Loss at step 9330: 0.7407
Loss at step 9340: 0.4224
Loss at step 9350: 0.7365
Loss at step 9360: 0.8171
Loss at step 9370: 0.4273
Loss at step 9380: 0.7936
Loss at step 9390: 0.6126
Loss at step 9400: 0.6770
Loss at step 9410: 0.6058
Loss at step 9420: 0.7638
Loss at step 9430: 0.6499
Loss at step 9440: 0.7418
Loss at step 9450: 0.6722
Loss at step 9460: 0.9218
Loss at step 9470: 0.7857
Loss at step 9480: 0.7061
Loss at step 9490: 0.7402
Loss at step 9500: 0.7207
Loss at step 9510: 0.3581
Loss at step 9520: 0.4143
Loss at step 9530: 0.6267
Loss at step 9540: 0.5366
Loss at step 9550: 0.7773
Loss at step 9560: 0.7974
Loss at step 9570: 0.7639
Loss at step 9580: 0.6587
Loss at step 9590: 0.7284
Loss at step 9600: 0.6388
Loss at step 9610: 0.7680
Loss at step 9620: 0.6340
Loss at step 9630: 0.6214
Loss at step 9640: 0.3608
Loss at step 9650: 0.4414
Loss at step 9660: 0.4689
Loss at step 9670: 0.5057
Loss at step 9680: 0.6186
Loss at step 9690: 0.4639
Loss at step 9700: 0.6164
Loss at step 9710: 0.8028
Loss at step 9720: 0.7496
Loss at step 9730: 0.7845
Loss at step 9740: 0.3612
Loss at step 9750: 0.8909
Loss at step 9760: 0.5582
Loss at step 9770: 0.4779
Loss at step 9780: 0.3922
Loss at step 9790: 0.8237
Loss at step 9800: 0.6113
Loss at step 9810: 0.5758
Loss at step 9820: 0.5572
Loss at step 9830: 0.6258
Loss at step 9840: 0.7108
Loss at step 9850: 0.6820
Loss at step 9860: 1.0408
Loss at step 9870: 0.6273
Loss at step 9880: 0.6133
Loss at step 9890: 0.6078
Loss at step 9900: 0.7440
Loss at step 9910: 0.7063
Loss at step 9920: 0.4076
Loss at step 9930: 0.6861
Loss at step 9940: 0.8293
Loss at step 9950: 0.5254
Loss at step 9960: 0.6913
Loss at step 9970: 0.6726
Loss at step 9980: 0.8275
Loss at step 9990: 0.7960
Loss at step 10000: 0.6042
Loss at step 10010: 0.3294
Loss at step 10020: 0.5286
Loss at step 10030: 0.6769
Loss at step 10040: 0.6845
Loss at step 10050: 0.7967
Loss at step 10060: 0.5589
Loss at step 10070: 0.8612
Loss at step 10080: 0.6033
Loss at step 10090: 0.6531
Loss at step 10100: 0.9064
Loss at step 10110: 0.5522
Loss at step 10120: 0.5785
Loss at step 10130: 0.4199
Loss at step 10140: 0.9447
Loss at step 10150: 0.4261
Loss at step 10160: 0.6135
Loss at step 10170: 0.8418
Loss at step 10180: 0.5094
Loss at step 10190: 0.5475
Loss at step 10200: 0.5410
Loss at step 10210: 0.6191
Loss at step 10220: 0.6403
Loss at step 10230: 0.5618
Loss at step 10240: 0.6206
Loss at step 10250: 0.5414
Loss at step 10260: 0.6707
Loss at step 10270: 0.7455
Loss at step 10280: 0.5558
Loss at step 10290: 0.5489
Loss at step 10300: 0.5316
Loss at step 10310: 0.9729
Loss at step 10320: 0.7316
Loss at step 10330: 0.7341
Loss at step 10340: 0.6609
Loss at step 10350: 0.4821
Loss at step 10360: 0.5708
Loss at step 10370: 0.7203
Loss at step 10380: 0.5653
Loss at step 10390: 0.4837
Loss at step 10400: 0.8513
Loss at step 10410: 0.7552
Loss at step 10420: 0.5704
Loss at step 10430: 0.7759
Loss at step 10440: 0.3929
Loss at step 10450: 0.4888
Loss at step 10460: 0.7421
Loss at step 10470: 0.6200
Loss at step 10480: 0.3971
Loss at step 10490: 0.4763
Loss at step 10500: 0.7411
Loss at step 10510: 0.6820
Loss at step 10520: 0.7872
Loss at step 10530: 0.5597
Loss at step 10540: 0.5379
Loss at step 10550: 0.4841
Loss at step 10560: 0.7271
Loss at step 10570: 0.5225
Loss at step 10580: 0.4333
Loss at step 10590: 0.6325
Loss at step 10600: 0.7908
Loss at step 10610: 0.8098
Loss at step 10620: 0.4893
Loss at step 10630: 0.9300
Loss at step 10640: 0.5421
Loss at step 10650: 0.6703
Loss at step 10660: 0.7080
Loss at step 10670: 0.6294
Loss at step 10680: 0.5267
Loss at step 10690: 0.7328
Loss at step 10700: 0.8462
Loss at step 10710: 0.5416
Loss at step 10720: 0.6710
Loss at step 10730: 0.6171
Loss at step 10740: 0.4753
Loss at step 10750: 0.7383
Loss at step 10760: 0.6351
Loss at step 10770: 0.5404
Loss at step 10780: 0.6896
Loss at step 10790: 0.4864
Loss at step 10800: 0.7706
Loss at step 10810: 0.5693
Loss at step 10820: 0.4605
Loss at step 10830: 1.0272
Loss at step 10840: 0.6754
Loss at step 10850: 0.6671
Loss at step 10860: 0.7002
Loss at step 10870: 0.5917
Loss at step 10880: 0.6066
Loss at step 10890: 0.8771
Loss at step 10900: 0.7602
Loss at step 10910: 0.5958
Loss at step 10920: 0.6085
Loss at step 10930: 0.6293
Loss at step 10940: 0.5478
Loss at step 10950: 0.6056
Loss at step 10960: 0.8614
Loss at step 10970: 0.7656
Loss at step 10980: 0.7574
Loss at step 10990: 0.6079
Loss at step 11000: 0.7362
Loss at step 11010: 0.3639
Loss at step 11020: 0.6868
Loss at step 11030: 0.7255
Loss at step 11040: 0.8726
Loss at step 11050: 0.5466
Loss at step 11060: 0.7254
Loss at step 11070: 0.7542
Loss at step 11080: 0.5782
Loss at step 11090: 0.7408
Loss at step 11100: 0.6562
Loss at step 11110: 0.8088
Loss at step 11120: 0.6710
Loss at step 11130: 0.6320
Loss at step 11140: 0.6228
Loss at step 11150: 0.6211
Loss at step 11160: 0.9495
Loss at step 11170: 0.7281
Loss at step 11180: 0.6680
Loss at step 11190: 0.7763
Loss at step 11200: 0.6803
Loss at step 11210: 0.8233
Loss at step 11220: 0.6222
Loss at step 11230: 0.6794
Loss at step 11240: 0.5557
Loss at step 11250: 0.7358
Loss at step 11260: 0.4357
Loss at step 11270: 0.8165
Loss at step 11280: 0.5494
Loss at step 11290: 0.5373
Loss at step 11300: 0.7328
Loss at step 11310: 0.6149
Loss at step 11320: 0.5429
Loss at step 11330: 0.7421
Loss at step 11340: 0.6769
Loss at step 11350: 0.4127
Loss at step 11360: 0.7596
Loss at step 11370: 0.7686
Loss at step 11380: 0.3627
Loss at step 11390: 0.6098
Loss at step 11400: 0.7326
Loss at step 11410: 0.7369
Loss at step 11420: 0.8629
Loss at step 11430: 0.5902
Loss at step 11440: 0.8027
Loss at step 11450: 0.6084
Loss at step 11460: 0.6458
Loss at step 11470: 0.6716
Loss at step 11480: 0.6844
Loss at step 11490: 0.7280
Loss at step 11500: 0.9475
Loss at step 11510: 0.6772
Loss at step 11520: 0.4064
Loss at step 11530: 0.7507
Loss at step 11540: 0.6772
Loss at step 11550: 0.5637
Loss at step 11560: 0.9715
Loss at step 11570: 0.6727
Loss at step 11580: 0.6810
Loss at step 11590: 0.6271
Loss at step 11600: 0.4875
Loss at step 11610: 0.7013
Loss at step 11620: 0.5412
Loss at step 11630: 0.4137
Loss at step 11640: 0.7362
Loss at step 11650: 0.9953
Loss at step 11660: 0.3600
Loss at step 11670: 0.6780
Loss at step 11680: 0.6539
Loss at step 11690: 0.4623
Loss at step 11700: 0.8356
Loss at step 11710: 0.4092
Loss at step 11720: 0.4738
Loss at step 11730: 0.6983
Loss at step 11740: 0.9386
Loss at step 11750: 0.6755
Loss at step 11760: 0.6022
Loss at step 11770: 0.6488
Loss at step 11780: 0.7845
Loss at step 11790: 0.9194
Loss at step 11800: 0.4813
Loss at step 11810: 0.8677
Loss at step 11820: 0.5013
Loss at step 11830: 0.7977
Loss at step 11840: 0.4489
Loss at step 11850: 0.6717
Loss at step 11860: 0.9460
Loss at step 11870: 0.6326
Loss at step 11880: 0.5277
Loss at step 11890: 0.7562
Loss at step 11900: 0.7546
Loss at step 11910: 0.2994
Loss at step 11920: 0.7288
Loss at step 11930: 0.8571
Loss at step 11940: 0.5666
Loss at step 11950: 0.4918
Loss at step 11960: 0.4850
Loss at step 11970: 0.2926
Loss at step 11980: 0.5985
Loss at step 11990: 0.4534
Loss at step 12000: 0.5010
Loss at step 12010: 0.4915
Loss at step 12020: 0.5438
Loss at step 12030: 0.4686
Loss at step 12040: 0.7358
Loss at step 12050: 0.5988
Loss at step 12060: 0.7329
Loss at step 12070: 0.7446
Loss at step 12080: 0.6125
Loss at step 12090: 0.2849
Loss at step 12100: 1.0152
Loss at step 12110: 0.4798
Loss at step 12120: 0.6729
Loss at step 12130: 0.6780
Loss at step 12140: 0.6472
Loss at step 12150: 0.5987
Loss at step 12160: 0.6210
Loss at step 12170: 0.6282
Loss at step 12180: 0.4161
Loss at step 12190: 0.4718
Loss at step 12200: 0.5333
Loss at step 12210: 0.6050
Loss at step 12220: 0.4327
Loss at step 12230: 0.7428
Loss at step 12240: 0.8071
Loss at step 12250: 0.4817
Loss at step 12260: 0.6351
Loss at step 12270: 0.7798
Loss at step 12280: 0.5472
Loss at step 12290: 1.0120
Loss at step 12300: 0.6176
Loss at step 12310: 0.7469
Loss at step 12320: 0.4788
Loss at step 12330: 0.7140
Loss at step 12340: 0.6658
Loss at step 12350: 0.6761
Loss at step 12360: 0.5379
Loss at step 12370: 0.6386
Loss at step 12380: 0.6663
Loss at step 12390: 0.5988
Loss at step 12400: 0.5403
Loss at step 12410: 0.9521
Loss at step 12420: 0.4725
Loss at step 12430: 0.6869
Loss at step 12440: 0.6144
Loss at step 12450: 0.4740
Loss at step 12460: 0.6318
Loss at step 12470: 0.5275
Loss at step 12480: 0.7607
Loss at step 12490: 0.5347
Loss at step 12500: 0.7534
Loss at step 12510: 0.6779
Loss at step 12520: 0.5338
Loss at step 12530: 0.5343
Loss at step 12540: 0.4100
Loss at step 12550: 0.7591
Loss at step 12560: 0.6450
Loss at step 12570: 0.7032
Loss at step 12580: 0.5076
Loss at step 12590: 0.6676
Loss at step 12600: 0.6846
Loss at step 12610: 0.6162
Loss at step 12620: 0.7552
Loss at step 12630: 0.1979
Loss at step 12640: 0.5459
Loss at step 12650: 0.6167
Loss at step 12660: 0.6857
Loss at step 12670: 0.5402
Loss at step 12680: 0.7448
Loss at step 12690: 0.6294
Loss at step 12700: 0.6137
Loss at step 12710: 0.5490
Loss at step 12720: 0.6791
Loss at step 12730: 0.5576
Loss at step 12740: 0.4920
Loss at step 12750: 0.4820
Loss at step 12760: 0.6785
Loss at step 12770: 0.6025
Loss at step 12780: 0.4742
Loss at step 12790: 0.6700
Loss at step 12800: 0.6144
Loss at step 12810: 0.6144
Loss at step 12820: 0.7805
Loss at step 12830: 0.6035
Loss at step 12840: 0.7424
Loss at step 12850: 0.7372
Loss at step 12860: 0.6714
Loss at step 12870: 0.6600
Loss at step 12880: 0.5727
Loss at step 12890: 0.7716
Loss at step 12900: 0.4968
Loss at step 12910: 0.6777
Loss at step 12920: 0.5841
Loss at step 12930: 0.6120
Loss at step 12940: 0.6295
Loss at step 12950: 0.6047
Loss at step 12960: 0.5893
Loss at step 12970: 0.6679
Loss at step 12980: 0.6947
Loss at step 12990: 0.7564
Loss at step 13000: 1.0622
Loss at step 13010: 0.4812
Loss at step 13020: 0.5459
Loss at step 13030: 0.6323
Loss at step 13040: 1.0200
Loss at step 13050: 0.7546
Loss at step 13060: 0.6742
Loss at step 13070: 0.7275
Loss at step 13080: 0.6633
Loss at step 13090: 0.6179
Loss at step 13100: 0.6242
Loss at step 13110: 0.5268
Loss at step 13120: 0.7321
Loss at step 13130: 0.6251
Loss at step 13140: 0.5353
Loss at step 13150: 0.3997
Loss at step 13160: 0.6078
Loss at step 13170: 0.5342
Loss at step 13180: 0.4636
Loss at step 13190: 0.8653
Loss at step 13200: 0.6078
Loss at step 13210: 0.4230
Loss at step 13220: 0.5491
Loss at step 13230: 0.4038
Loss at step 13240: 0.5844
Loss at step 13250: 0.4707
Loss at step 13260: 0.5582
Loss at step 13270: 0.6814
Loss at step 13280: 0.5733
Loss at step 13290: 0.7667
Loss at step 13300: 0.5192
Loss at step 13310: 0.7605
Loss at step 13320: 1.0874
Loss at step 13330: 0.4362
Loss at step 13340: 0.7361
Loss at step 13350: 0.7899
Loss at step 13360: 0.6808
Loss at step 13370: 0.5400
Loss at step 13380: 0.6566
Loss at step 13390: 0.4109
Loss at step 13400: 0.6031
Loss at step 13410: 0.6349
Loss at step 13420: 0.6024
Loss at step 13430: 0.5814
Loss at step 13440: 0.7653
Loss at step 13450: 0.4829
Loss at step 13460: 0.7107
Loss at step 13470: 0.6781
Loss at step 13480: 0.8140
Loss at step 13490: 0.8739
Loss at step 13500: 0.5594
Loss at step 13510: 0.3191
Loss at step 13520: 0.3388
Loss at step 13530: 0.3890
Loss at step 13540: 0.4163
Loss at step 13550: 1.0150
Loss at step 13560: 0.6102
Loss at step 13570: 0.3929
Loss at step 13580: 0.7209
Loss at step 13590: 0.9337
Loss at step 13600: 0.8311
Loss at step 13610: 0.9608
Loss at step 13620: 0.5791
Loss at step 13630: 0.7059
Loss at step 13640: 0.6828
Loss at step 13650: 0.5542
Loss at step 13660: 0.6577
Loss at step 13670: 0.4115
Loss at step 13680: 0.6017
Loss at step 13690: 0.4149
Loss at step 13700: 0.7791
Loss at step 13710: 0.7788
Loss at step 13720: 0.6529
Loss at step 13730: 0.5904
Loss at step 13740: 0.7064
Loss at step 13750: 0.7807
Loss at step 13760: 0.3788
Loss at step 13770: 0.5482
Loss at step 13780: 0.6664
Loss at step 13790: 0.6928
Loss at step 13800: 0.7455
Loss at step 13810: 0.7612
Loss at step 13820: 0.6712
Loss at step 13830: 0.7387
Loss at step 13840: 0.6230
Loss at step 13850: 0.4823
Loss at step 13860: 0.5499
Loss at step 13870: 0.6177
Loss at step 13880: 0.6383
Loss at step 13890: 0.8127
Loss at step 13900: 0.5373
Loss at step 13910: 0.4940
Loss at step 13920: 0.5605
Loss at step 13930: 0.8678
Loss at step 13940: 0.4821
Loss at step 13950: 0.6572
Loss at step 13960: 0.4243
Loss at step 13970: 0.6083
Loss at step 13980: 0.8190
Loss at step 13990: 0.6139
Loss at step 14000: 0.6682
Loss at step 14010: 0.4732
Loss at step 14020: 0.3548
Loss at step 14030: 0.6425
Loss at step 14040: 0.8997
Loss at step 14050: 0.7542
Loss at step 14060: 0.5521
Loss at step 14070: 0.6990
Loss at step 14080: 0.4905
Loss at step 14090: 0.8761
Loss at step 14100: 0.8057
Loss at step 14110: 0.6134
Loss at step 14120: 0.7819
Loss at step 14130: 0.8581
Loss at step 14140: 0.7456
Loss at step 14150: 0.9110
Loss at step 14160: 0.5038
Loss at step 14170: 0.5865
Loss at step 14180: 0.6691
Loss at step 14190: 0.7846
Loss at step 14200: 0.5795
Loss at step 14210: 0.4732
Loss at step 14220: 0.4842
Loss at step 14230: 0.7087
Loss at step 14240: 0.5483
Loss at step 14250: 0.5399
Loss at step 14260: 0.7370
Loss at step 14270: 0.4985
Loss at step 14280: 0.5939
Loss at step 14290: 0.6031
Loss at step 14300: 0.6657
Loss at step 14310: 0.7945
Loss at step 14320: 0.5331
Loss at step 14330: 0.6757
Loss at step 14340: 0.6650
Loss at step 14350: 0.6132
Loss at step 14360: 0.4861
Loss at step 14370: 0.9179
Loss at step 14380: 0.6247
Loss at step 14390: 0.4923
Loss at step 14400: 0.5915
Loss at step 14410: 0.4073
Loss at step 14420: 0.6728
Loss at step 14430: 0.5499
Loss at step 14440: 0.5406
Loss at step 14450: 0.5445
Loss at step 14460: 0.9231
Loss at step 14470: 0.6604
Loss at step 14480: 0.8573
Loss at step 14490: 0.8917
Loss at step 14500: 0.6604
Loss at step 14510: 0.7505
Loss at step 14520: 0.5395
Loss at step 14530: 1.2251
Loss at step 14540: 0.5543
Loss at step 14550: 0.6927
Loss at step 14560: 0.8145
Loss at step 14570: 0.9398
Loss at step 14580: 0.6802
Loss at step 14590: 0.7292
Loss at step 14600: 0.5994
Loss at step 14610: 0.4231
Loss at step 14620: 0.6758
Loss at step 14630: 0.4712
Loss at step 14640: 0.4164
Loss at step 14650: 0.5487
Loss at step 14660: 0.8016
Loss at step 14670: 0.6846
Loss at step 14680: 0.6650
Loss at step 14690: 0.6253
Loss at step 14700: 0.7335
Loss at step 14710: 0.7291
Loss at step 14720: 0.5580
Loss at step 14730: 0.6457
Loss at step 14740: 0.4137
Loss at step 14750: 0.3484
Loss at step 14760: 0.8754
Loss at step 14770: 1.0187
Loss at step 14780: 0.5188
Loss at step 14790: 0.4688
Loss at step 14800: 0.7234
Loss at step 14810: 0.6183
Loss at step 14820: 0.6399
Loss at step 14830: 0.9672
Loss at step 14840: 0.7515
Loss at step 14850: 0.4985
Loss at step 14860: 0.6584
Loss at step 14870: 0.6809
Loss at step 14880: 0.5650
Loss at step 14890: 0.8648
Loss at step 14900: 0.5998
Loss at step 14910: 0.6527
Loss at step 14920: 0.5037
Loss at step 14930: 0.7958
Loss at step 14940: 0.5280
Loss at step 14950: 1.0713
Loss at step 14960: 0.6650
Loss at step 14970: 0.5993
Loss at step 14980: 0.6607
Loss at step 14990: 0.6130
Loss at step 15000: 0.6954
Loss at step 15010: 0.6703
Loss at step 15020: 0.7153
Loss at step 15030: 0.5747
Loss at step 15040: 0.6014
Loss at step 15050: 0.5303
Loss at step 15060: 0.6682
Loss at step 15070: 0.7822
Loss at step 15080: 0.6579
Loss at step 15090: 0.8011
Loss at step 15100: 0.5567
Loss at step 15110: 0.7320
Loss at step 15120: 0.5517
Loss at step 15130: 0.6991
Loss at step 15140: 0.4643
Loss at step 15150: 0.5417
Loss at step 15160: 0.6190
Loss at step 15170: 0.8674
Loss at step 15180: 0.8092
Loss at step 15190: 0.8751
Loss at step 15200: 0.6628
Loss at step 15210: 0.9034
Loss at step 15220: 0.8036
Loss at step 15230: 0.6667
Loss at step 15240: 0.6242
Loss at step 15250: 0.5187
Loss at step 15260: 0.6132
Loss at step 15270: 0.5979
Loss at step 15280: 0.7122
Loss at step 15290: 0.7478
Loss at step 15300: 0.4764
Loss at step 15310: 0.8172
Loss at step 15320: 0.5184
Loss at step 15330: 0.5525
Loss at step 15340: 0.8205
Loss at step 15350: 0.4570
Loss at step 15360: 0.8888
Loss at step 15370: 0.7136
Loss at step 15380: 0.7968
Loss at step 15390: 0.7303
Loss at step 15400: 0.5888
Loss at step 15410: 0.6650
Loss at step 15420: 0.5386
Loss at step 15430: 0.5204
Loss at step 15440: 0.5999
Loss at step 15450: 0.9241
Loss at step 15460: 0.5628
Loss at step 15470: 0.6798
Loss at step 15480: 0.9201
Loss at step 15490: 0.4970
Loss at step 15500: 0.4833
Loss at step 15510: 1.0264
Loss at step 15520: 0.6080
Loss at step 15530: 0.6005
Loss at step 15540: 0.4956
Loss at step 15550: 0.5920
Loss at step 15560: 0.4654
Loss at step 15570: 0.5871
Loss at step 15580: 0.6708
Loss at step 15590: 0.4287
Loss at step 15600: 0.5375
Loss at step 15610: 0.7922
Loss at step 15620: 0.6041
Loss at step 15630: 0.3574
Loss at step 15640: 0.7166
Loss at step 15650: 0.6211
Loss at step 15660: 0.4912
Loss at step 15670: 0.8078
Loss at step 15680: 0.7472
Loss at step 15690: 0.9405
Loss at step 15700: 0.5157
Loss at step 15710: 0.6063
Loss at step 15720: 0.7591
Loss at step 15730: 0.4188
Loss at step 15740: 0.5022
Loss at step 15750: 0.7325
Loss at step 15760: 0.4248
Loss at step 15770: 0.9003
Loss at step 15780: 0.8746
Loss at step 15790: 0.6109
Loss at step 15800: 0.5823
Loss at step 15810: 0.6507
Loss at step 15820: 0.5533
Loss at step 15830: 0.6542
Loss at step 15840: 0.5430
Loss at step 15850: 0.7356
Loss at step 15860: 0.9087
Loss at step 15870: 0.6795
Loss at step 15880: 0.7599
Loss at step 15890: 0.5923
Loss at step 15900: 0.7220
Loss at step 15910: 0.7920
Loss at step 15920: 0.6814
Loss at step 15930: 0.5958
Loss at step 15940: 0.4249
Loss at step 15950: 0.5442
Loss at step 15960: 0.4891
Loss at step 15970: 1.1226
Loss at step 15980: 0.4898
Loss at step 15990: 0.6008
Loss at step 16000: 0.6414
Loss at step 16010: 0.7823
Loss at step 16020: 0.6254
Loss at step 16030: 0.4211
Loss at step 16040: 0.7248
Loss at step 16050: 0.6155
Loss at step 16060: 0.5526
Loss at step 16070: 0.7539
Loss at step 16080: 0.6085
Loss at step 16090: 0.7265
Loss at step 16100: 0.7211
Loss at step 16110: 0.5996
Loss at step 16120: 0.6661
Loss at step 16130: 0.6211
Loss at step 16140: 0.6677
Loss at step 16150: 0.6580
Loss at step 16160: 0.3610
Loss at step 16170: 0.6151
Loss at step 16180: 0.7954
Loss at step 16190: 0.5341
Loss at step 16200: 0.8130
Loss at step 16210: 0.4289
Loss at step 16220: 0.5512
Loss at step 16230: 0.6282
Loss at step 16240: 0.6782
Loss at step 16250: 0.9197
Loss at step 16260: 0.5501
Loss at step 16270: 0.5093
Loss at step 16280: 0.6317
Loss at step 16290: 0.8324
Loss at step 16300: 0.7842
Loss at step 16310: 0.6375
Loss at step 16320: 0.7428
Loss at step 16330: 0.5957
Loss at step 16340: 0.8544
Loss at step 16350: 0.8899
Loss at step 16360: 0.6250
Loss at step 16370: 0.4998
Loss at step 16380: 0.6536
Loss at step 16390: 0.6448
Loss at step 16400: 1.0843
Loss at step 16410: 0.6816
Loss at step 16420: 0.5312
Loss at step 16430: 0.6371
Loss at step 16440: 0.8020
Loss at step 16450: 0.6134
Loss at step 16460: 0.7250
Loss at step 16470: 0.8073
Loss at step 16480: 0.8180
Loss at step 16490: 0.6146
Loss at step 16500: 0.5881
Loss at step 16510: 0.6094
Loss at step 16520: 0.6763
Loss at step 16530: 0.9182
Loss at step 16540: 0.7454
Loss at step 16550: 0.7482
Loss at step 16560: 0.8193
Loss at step 16570: 0.6651
Loss at step 16580: 0.5690
Loss at step 16590: 0.7144
Loss at step 16600: 0.8550
Loss at step 16610: 0.6261
Loss at step 16620: 0.5835
Loss at step 16630: 0.6982
Loss at step 16640: 0.4952
Loss at step 16650: 0.5760
Loss at step 16660: 0.7680
Loss at step 16670: 0.6914
Loss at step 16680: 0.6778
Loss at step 16690: 0.8765
Loss at step 16700: 0.8017
Loss at step 16710: 0.4944
Loss at step 16720: 0.7212
Loss at step 16730: 0.6663
Loss at step 16740: 0.7853
Loss at step 16750: 0.6508
Loss at step 16760: 0.7977
Loss at step 16770: 0.5429
Loss at step 16780: 0.5470
Loss at step 16790: 0.6218
Loss at step 16800: 0.5481
Loss at step 16810: 0.6062
Loss at step 16820: 0.5871
Loss at step 16830: 0.8333
Loss at step 16840: 0.4688
Loss at step 16850: 0.6599
Loss at step 16860: 0.9319
Loss at step 16870: 0.7865
Loss at step 16880: 0.5659
Loss at step 16890: 0.6928
Loss at step 16900: 0.5035
Loss at step 16910: 0.7316
Loss at step 16920: 0.5970
Loss at step 16930: 0.4177
Loss at step 16940: 0.4175
Loss at step 16950: 0.6818
Loss at step 16960: 0.4727
Loss at step 16970: 0.6353
Loss at step 16980: 0.4902
Loss at step 16990: 0.8175
Loss at step 17000: 0.7186
Loss at step 17010: 0.5595
Loss at step 17020: 0.2967
Loss at step 17030: 0.8132
Loss at step 17040: 0.7469
Loss at step 17050: 0.5249
Loss at step 17060: 0.6783
Loss at step 17070: 0.4228
Loss at step 17080: 0.6973
Loss at step 17090: 0.5743
Loss at step 17100: 0.5436
Loss at step 17110: 0.5001
Loss at step 17120: 0.7404
Loss at step 17130: 0.4363
Loss at step 17140: 0.5892
Loss at step 17150: 0.9012
Loss at step 17160: 0.6214
Loss at step 17170: 0.6209
Loss at step 17180: 0.5732
Loss at step 17190: 0.9129
Loss at step 17200: 0.6807
Loss at step 17210: 0.9585
Loss at step 17220: 0.6822
Loss at step 17230: 0.6419
Loss at step 17240: 0.8500
Loss at step 17250: 0.5586
Loss at step 17260: 0.7603
Loss at step 17270: 0.6582
Loss at step 17280: 0.6762
Loss at step 17290: 0.6579
Loss at step 17300: 0.4142
Loss at step 17310: 0.7334
Loss at step 17320: 0.5839
Loss at step 17330: 0.8137
Loss at step 17340: 0.3709
Loss at step 17350: 0.8115
Loss at step 17360: 0.7857
Loss at step 17370: 0.5622
Loss at step 17380: 0.6054
Loss at step 17390: 0.6714
Loss at step 17400: 0.7376
Loss at step 17410: 0.5315
Loss at step 17420: 0.7540
Loss at step 17430: 0.7450
Loss at step 17440: 0.8684
Loss at step 17450: 0.6695
Loss at step 17460: 0.8210
Loss at step 17470: 0.4864
Loss at step 17480: 0.5975
Loss at step 17490: 0.8132
Loss at step 17500: 0.7611
Loss at step 17510: 0.7206
Loss at step 17520: 0.4271
Loss at step 17530: 0.4097
Loss at step 17540: 0.6065
Loss at step 17550: 0.5633
Loss at step 17560: 0.4153
Loss at step 17570: 0.7203
Loss at step 17580: 0.2685
Loss at step 17590: 0.6065
Loss at step 17600: 0.8928
Loss at step 17610: 0.8051
Loss at step 17620: 0.6061
Loss at step 17630: 0.5331
Loss at step 17640: 0.8183
Loss at step 17650: 0.7243
Loss at step 17660: 0.7190
Loss at step 17670: 0.9676
Loss at step 17680: 0.5419
Loss at step 17690: 0.6145
Loss at step 17700: 0.6487
Loss at step 17710: 0.5926
Loss at step 17720: 0.6946
Loss at step 17730: 0.5980
Loss at step 17740: 0.5229
Loss at step 17750: 0.7956
Loss at step 17760: 0.7671
Loss at step 17770: 0.6349
Loss at step 17780: 0.5056
Loss at step 17790: 0.8492
Loss at step 17800: 0.8490
Loss at step 17810: 0.7391
Loss at step 17820: 0.8271
Loss at step 17830: 0.7473
Loss at step 17840: 1.1091
Loss at step 17850: 0.6747
Loss at step 17860: 0.5522
Loss at step 17870: 0.7927
Loss at step 17880: 0.4093
Loss at step 17890: 0.4846
Loss at step 17900: 0.5423
Loss at step 17910: 0.5598
Loss at step 17920: 0.6791
Loss at step 17930: 0.6436
Loss at step 17940: 0.5595
Loss at step 17950: 0.4037
Loss at step 17960: 0.6534
Loss at step 17970: 0.4475
Loss at step 17980: 0.4213
Loss at step 17990: 0.9112
Loss at step 18000: 0.2412
***** Running testing *****
  Num examples = 82080
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.813085, 'precision': [0.813085, 0.0, 0.0], 'recall': [1.0, 0.0, 0.0], 'f1': [0.896908, 0.0, 0.0]}
{'accuracy': 0.924464, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'WordR': 0.014344}
Loss at step 18010: 0.5435
Loss at step 18020: 0.5451
Loss at step 18030: 0.5171
Loss at step 18040: 0.3537
Loss at step 18050: 0.5616
Loss at step 18060: 0.5498
Loss at step 18070: 0.8037
Loss at step 18080: 0.6854
Loss at step 18090: 0.6658
Loss at step 18100: 0.5039
Loss at step 18110: 0.6877
Loss at step 18120: 0.6325
Loss at step 18130: 0.6822
Loss at step 18140: 0.7673
Loss at step 18150: 0.7737
Loss at step 18160: 0.7166
Loss at step 18170: 0.7680
Loss at step 18180: 0.2869
Loss at step 18190: 0.4836
Loss at step 18200: 0.7034
Loss at step 18210: 0.4971
Loss at step 18220: 0.8150
Loss at step 18230: 0.7486
Loss at step 18240: 0.9317
Loss at step 18250: 0.5441
Loss at step 18260: 0.6185
Loss at step 18270: 0.6283
Loss at step 18280: 0.4199
Loss at step 18290: 0.6252
Loss at step 18300: 0.3361
Loss at step 18310: 0.5969
Loss at step 18320: 0.5555
Loss at step 18330: 0.7833
Loss at step 18340: 0.7131
Loss at step 18350: 0.5638
Loss at step 18360: 0.4155
Loss at step 18370: 0.6063
Loss at step 18380: 0.7679
Loss at step 18390: 0.5622
Loss at step 18400: 0.9443
Loss at step 18410: 0.7390
Loss at step 18420: 0.5845
Loss at step 18430: 0.6446
Loss at step 18440: 0.5363
Loss at step 18450: 0.6644
Loss at step 18460: 0.7314
Loss at step 18470: 0.3320
Loss at step 18480: 0.8200
Loss at step 18490: 0.5481
Loss at step 18500: 0.6121
Loss at step 18510: 0.5316
Loss at step 18520: 0.7845
Loss at step 18530: 0.6066
Loss at step 18540: 0.5162
Loss at step 18550: 0.7461
Loss at step 18560: 0.6202
Loss at step 18570: 0.9168
Loss at step 18580: 0.5769
Loss at step 18590: 0.5449
Loss at step 18600: 0.4147
Loss at step 18610: 0.4572
Loss at step 18620: 0.5138
Loss at step 18630: 0.7371
Loss at step 18640: 0.7710
Loss at step 18650: 0.5543
Loss at step 18660: 0.4650
Loss at step 18670: 0.5278
Loss at step 18680: 0.7177
Loss at step 18690: 0.8989
Loss at step 18700: 0.6671
Loss at step 18710: 0.6917
Loss at step 18720: 0.5040
Loss at step 18730: 0.4578
Loss at step 18740: 0.4736
Loss at step 18750: 0.5070
Loss at step 18760: 0.4260
Loss at step 18770: 0.3496
Loss at step 18780: 0.4264
Loss at step 18790: 0.7567
Loss at step 18800: 0.7449
Loss at step 18810: 0.6844
Loss at step 18820: 0.4233
Loss at step 18830: 0.4075
Loss at step 18840: 0.6589
Loss at step 18850: 0.3995
Loss at step 18860: 0.6813
Loss at step 18870: 0.7478
Loss at step 18880: 0.4584
Loss at step 18890: 0.8655
Loss at step 18900: 0.5936
Loss at step 18910: 1.0447
Loss at step 18920: 0.7080
Loss at step 18930: 0.7135
Loss at step 18940: 0.6060
Loss at step 18950: 0.5578
Loss at step 18960: 0.4792
Loss at step 18970: 0.6567
Loss at step 18980: 0.4095
Loss at step 18990: 0.9482
Loss at step 19000: 0.6261
Loss at step 19010: 0.6743
Loss at step 19020: 0.7620
Loss at step 19030: 0.8263
Loss at step 19040: 0.6491
Loss at step 19050: 0.5866
Loss at step 19060: 0.7915
Loss at step 19070: 0.4182
Loss at step 19080: 0.7780
Loss at step 19090: 0.5637
Loss at step 19100: 0.5367
Loss at step 19110: 0.5364
Loss at step 19120: 0.5260
Loss at step 19130: 0.6863
Loss at step 19140: 0.5121
Loss at step 19150: 0.6089
Loss at step 19160: 0.6024
Loss at step 19170: 0.4445
Loss at step 19180: 0.6140
Loss at step 19190: 0.8804
Loss at step 19200: 0.8052
Loss at step 19210: 0.4857
Loss at step 19220: 0.7399
Loss at step 19230: 0.3042
Loss at step 19240: 0.6513
Loss at step 19250: 0.6499
Loss at step 19260: 0.9895
Loss at step 19270: 0.5470
Loss at step 19280: 0.7945
Loss at step 19290: 0.7048
Loss at step 19300: 0.5383
Loss at step 19310: 0.7199
Loss at step 19320: 0.6712
Loss at step 19330: 0.7414
Loss at step 19340: 0.5897
Loss at step 19350: 0.6257
Loss at step 19360: 0.4974
Loss at step 19370: 0.5488
Loss at step 19380: 0.5219
Loss at step 19390: 0.5448
Loss at step 19400: 0.5177
Loss at step 19410: 0.5473
Loss at step 19420: 1.0459
Loss at step 19430: 0.6051
Loss at step 19440: 0.7084
Loss at step 19450: 0.7403
Loss at step 19460: 0.5641
Loss at step 19470: 0.8728
Loss at step 19480: 0.7414
Loss at step 19490: 0.6195
Loss at step 19500: 0.6783
Loss at step 19510: 0.6461
Loss at step 19520: 0.9690
Loss at step 19530: 0.7019
Loss at step 19540: 0.5146
Loss at step 19550: 0.8896
Loss at step 19560: 0.5572
Loss at step 19570: 0.4961
Loss at step 19580: 0.5993
Loss at step 19590: 0.5925
Loss at step 19600: 0.7479
Loss at step 19610: 0.5462
Loss at step 19620: 0.4184
Loss at step 19630: 0.7855
Loss at step 19640: 0.7373
Loss at step 19650: 0.6658
Loss at step 19660: 0.3639
Loss at step 19670: 0.5976
Loss at step 19680: 0.6237
Loss at step 19690: 0.5258
Loss at step 19700: 0.3628
Loss at step 19710: 0.7852
Loss at step 19720: 0.4276
Loss at step 19730: 0.6006
Loss at step 19740: 0.4830
Loss at step 19750: 0.6011
Loss at step 19760: 0.6140
Loss at step 19770: 0.6049
Loss at step 19780: 0.5877
Loss at step 19790: 0.5173
Loss at step 19800: 0.5509
Loss at step 19810: 0.5627
Loss at step 19820: 0.7015
Loss at step 19830: 0.6628
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7fc5d6daf160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 61348 of the training set: {'input_ids': [0, 35227, 2606, 510, 2, 2, 44799, 16, 1996, 549, 89, 16, 402, 164, 15, 19, 6979, 2816, 4135, 11, 49, 443, 8, 3742, 473, 45, 173, 13, 59, 1946, 122, 4, 50118, 45443, 3496, 14, 51, 64, 476, 4943, 49, 3822, 124, 62, 8, 542, 33142, 24, 31, 5, 476, 13051, 13, 132, 5251, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 92320
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 4329
Loss at step 10: 1.1077
Loss at step 20: 1.0906
Loss at step 30: 1.0644
Sample 116739 of the training set: {'input_ids': [0, 10006, 16382, 35, 166, 33, 13, 5, 1707, 5043, 479, 2, 2, 133, 333, 1276, 15, 5, 17715, 9, 5, 6063, 797, 11, 10, 10547, 6063, 797, 61, 21, 1365, 7, 304, 24, 8, 24, 24, 1365, 7, 28, 1887, 4, 50118, 22412, 6, 5, 27913, 39322, 802, 51, 197, 129, 33, 10, 319, 9, 20808, 2441, 4, 50118, 1213, 67, 2047, 14, 51, 115, 304, 41, 1823, 5043, 61, 115, 28, 1887, 13, 2384, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Loss at step 10: 0.7803
Loss at step 20: 0.7464
Loss at step 30: 0.5888
Loss at step 40: 0.6451
Loss at step 50: 0.6431
Sample 116739 of the training set: {'input_ids': [0, 10006, 16382, 35, 166, 33, 13, 5, 1707, 5043, 479, 2, 2, 133, 333, 1276, 15, 5, 17715, 9, 5, 6063, 797, 11, 10, 10547, 6063, 797, 61, 21, 1365, 7, 304, 24, 8, 24, 24, 1365, 7, 28, 1887, 4, 50118, 22412, 6, 5, 27913, 39322, 802, 51, 197, 129, 33, 10, 319, 9, 20808, 2441, 4, 50118, 1213, 67, 2047, 14, 51, 115, 304, 41, 1823, 5043, 61, 115, 28, 1887, 13, 2384, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 9000
Loss at step 10: 0.7801
Loss at step 20: 0.7303
Loss at step 30: 0.5307
Loss at step 40: 0.5323
Loss at step 50: 0.5768
Loss at step 60: 0.4651
Loss at step 70: 0.4381
Loss at step 80: 0.5924
Loss at step 90: 0.4504
Loss at step 100: 0.2663
Loss at step 110: 0.3641
Loss at step 120: 0.3775
Loss at step 130: 0.5991
Loss at step 140: 0.5020
Loss at step 150: 0.3815
Loss at step 160: 0.4853
Loss at step 170: 0.2994
Loss at step 180: 0.5221
Loss at step 190: 0.5368
Loss at step 200: 0.5551
Loss at step 210: 0.4374
Loss at step 220: 0.6071
Loss at step 230: 0.6060
Loss at step 240: 0.6596
Loss at step 250: 0.5831
Loss at step 260: 0.4406
Loss at step 270: 0.5791
Loss at step 280: 0.3067
Loss at step 290: 0.5193
Loss at step 300: 0.5481
Loss at step 310: 0.3805
Loss at step 320: 0.5724
Loss at step 330: 0.4182
Loss at step 340: 0.6222
Loss at step 350: 0.4706
Loss at step 360: 0.2678
Loss at step 370: 0.5288
Loss at step 380: 0.2960
Loss at step 390: 0.4182
Loss at step 400: 0.4365
Loss at step 410: 0.3891
Loss at step 420: 0.4555
Loss at step 430: 0.5245
Loss at step 440: 0.3903
Loss at step 450: 0.7051
Loss at step 460: 0.4021
Loss at step 470: 0.6071
Loss at step 480: 0.4179
Loss at step 490: 0.5629
Loss at step 500: 0.3238
Loss at step 510: 0.5707
Loss at step 520: 0.4280
Loss at step 530: 0.3869
Loss at step 540: 0.5052
Loss at step 550: 0.4337
Loss at step 560: 0.4804
Loss at step 570: 0.4065
Loss at step 580: 0.5134
Loss at step 590: 0.4597
Loss at step 600: 0.4023
Loss at step 610: 0.3597
Loss at step 620: 0.2460
Loss at step 630: 0.4987
Loss at step 640: 0.3476
Loss at step 650: 0.3664
Loss at step 660: 0.3810
Loss at step 670: 0.7542
Loss at step 680: 0.3598
Loss at step 690: 0.5465
Loss at step 700: 0.5296
Loss at step 710: 0.5228
Loss at step 720: 0.5506
Loss at step 730: 0.2954
Loss at step 740: 0.3493
Loss at step 750: 0.3403
Loss at step 760: 0.4805
Loss at step 770: 0.5848
Loss at step 780: 0.3192
Loss at step 790: 0.4385
Loss at step 800: 0.4403
Loss at step 810: 0.4025
Loss at step 820: 0.3908
Loss at step 830: 0.4341
Loss at step 840: 0.2709
Loss at step 850: 0.4166
Loss at step 860: 0.2992
Loss at step 870: 0.4264
Loss at step 880: 0.5409
Loss at step 890: 0.4858
Loss at step 900: 0.4804
Loss at step 910: 0.4282
Loss at step 920: 0.4868
Loss at step 930: 0.4221
Loss at step 940: 0.3442
Loss at step 950: 0.3841
Loss at step 960: 0.5962
Loss at step 970: 0.3010
Loss at step 980: 0.3830
Loss at step 990: 0.4150
Loss at step 1000: 0.2998
Loss at step 1010: 0.5005
Loss at step 1020: 0.3589
Loss at step 1030: 0.4450
Loss at step 1040: 0.3939
Loss at step 1050: 0.5204
Loss at step 1060: 0.4601
Loss at step 1070: 0.4185
Loss at step 1080: 0.2089
Loss at step 1090: 0.5336
Loss at step 1100: 0.4713
Loss at step 1110: 0.3845
Loss at step 1120: 0.5541
Loss at step 1130: 0.2579
Loss at step 1140: 0.3443
Loss at step 1150: 0.3420
Loss at step 1160: 0.4967
Loss at step 1170: 0.3298
Loss at step 1180: 0.3684
Loss at step 1190: 0.4259
Loss at step 1200: 0.4025
Loss at step 1210: 0.4056
Loss at step 1220: 0.4137
Loss at step 1230: 0.4300
Loss at step 1240: 0.9736
Loss at step 1250: 0.4907
Loss at step 1260: 0.2922
Loss at step 1270: 0.4319
Loss at step 1280: 0.3735
Loss at step 1290: 0.6322
Loss at step 1300: 0.2811
Loss at step 1310: 0.4623
Loss at step 1320: 0.5214
Loss at step 1330: 0.3817
Loss at step 1340: 0.5416
Loss at step 1350: 0.3663
Loss at step 1360: 0.4027
Loss at step 1370: 0.3551
Loss at step 1380: 0.1907
Loss at step 1390: 0.5567
Loss at step 1400: 0.4092
Loss at step 1410: 0.3874
Loss at step 1420: 0.5841
Loss at step 1430: 0.4034
Loss at step 1440: 0.6320
Loss at step 1450: 0.3186
Loss at step 1460: 0.3218
Loss at step 1470: 0.3813
Loss at step 1480: 0.3387
Loss at step 1490: 0.3563
Loss at step 1500: 0.3256
Loss at step 1510: 0.4623
Loss at step 1520: 0.4220
Loss at step 1530: 0.4120
Loss at step 1540: 0.4312
Loss at step 1550: 0.4086
Loss at step 1560: 0.3700
Loss at step 1570: 0.5217
Loss at step 1580: 0.4601
Loss at step 1590: 0.4603
Loss at step 1600: 0.4672
Loss at step 1610: 0.3389
Loss at step 1620: 0.3708
Loss at step 1630: 0.2114
Loss at step 1640: 0.4833
Loss at step 1650: 0.3986
Loss at step 1660: 0.5070
Loss at step 1670: 0.3639
Loss at step 1680: 0.5432
Loss at step 1690: 0.3585
Loss at step 1700: 0.3936
Loss at step 1710: 0.4661
Loss at step 1720: 0.3225
Loss at step 1730: 0.5232
Loss at step 1740: 0.3807
Loss at step 1750: 0.4345
Loss at step 1760: 0.3345
Loss at step 1770: 0.5516
Loss at step 1780: 0.5708
Loss at step 1790: 0.3228
Loss at step 1800: 0.6373
Loss at step 1810: 0.5284
Loss at step 1820: 0.4210
Loss at step 1830: 0.3828
Loss at step 1840: 0.5754
Loss at step 1850: 0.3299
Loss at step 1860: 0.4884
Loss at step 1870: 0.3959
Loss at step 1880: 0.5415
Loss at step 1890: 0.4137
Loss at step 1900: 0.4442
Loss at step 1910: 0.4749
Loss at step 1920: 0.4468
Loss at step 1930: 0.5192
Loss at step 1940: 0.3931
Loss at step 1950: 0.4999
Loss at step 1960: 0.3930
Loss at step 1970: 0.4267
Loss at step 1980: 0.4440
Loss at step 1990: 0.3807
Loss at step 2000: 0.5012
Loss at step 2010: 0.4121
Loss at step 2020: 0.5287
Loss at step 2030: 0.4068
Loss at step 2040: 0.2152
Loss at step 2050: 0.2800
Loss at step 2060: 0.4700
Loss at step 2070: 0.3684
Loss at step 2080: 0.3248
Loss at step 2090: 0.2881
Loss at step 2100: 0.5180
Loss at step 2110: 0.2721
Loss at step 2120: 0.2792
Loss at step 2130: 0.4022
Loss at step 2140: 0.4427
Loss at step 2150: 0.2502
Loss at step 2160: 0.4630
Loss at step 2170: 0.4441
Loss at step 2180: 0.5250
Loss at step 2190: 0.3451
Loss at step 2200: 0.4042
Loss at step 2210: 0.3811
Loss at step 2220: 0.2614
Loss at step 2230: 0.2605
Loss at step 2240: 0.3679
Loss at step 2250: 0.4080
Loss at step 2260: 0.3795
Loss at step 2270: 0.3668
Loss at step 2280: 0.3049
Loss at step 2290: 0.4477
Loss at step 2300: 0.3120
Loss at step 2310: 0.5993
Loss at step 2320: 0.4008
Loss at step 2330: 0.3355
Loss at step 2340: 0.3224
Loss at step 2350: 0.4852
Loss at step 2360: 0.4004
Loss at step 2370: 0.4658
Loss at step 2380: 0.4684
Loss at step 2390: 0.3636
Loss at step 2400: 0.5712
Loss at step 2410: 0.5491
Loss at step 2420: 0.3076
Loss at step 2430: 0.5238
Loss at step 2440: 0.3122
Loss at step 2450: 0.5050
Loss at step 2460: 0.3074
Loss at step 2470: 0.3986
Loss at step 2480: 0.3009
Loss at step 2490: 0.2616
Loss at step 2500: 0.3042
Loss at step 2510: 0.5308
Loss at step 2520: 0.3949
Loss at step 2530: 0.4636
Loss at step 2540: 0.5307
Loss at step 2550: 0.2667
Loss at step 2560: 0.3230
Loss at step 2570: 0.4304
Loss at step 2580: 0.2650
Loss at step 2590: 0.4954
Loss at step 2600: 0.4685
Loss at step 2610: 0.2601
Loss at step 2620: 0.4291
Loss at step 2630: 0.3999
Loss at step 2640: 0.2926
Loss at step 2650: 0.4208
Loss at step 2660: 0.3838
Loss at step 2670: 0.3538
Loss at step 2680: 0.3798
Loss at step 2690: 0.2282
Loss at step 2700: 0.2368
Loss at step 2710: 0.2898
Loss at step 2720: 0.3763
Loss at step 2730: 0.2937
Loss at step 2740: 0.3311
Loss at step 2750: 0.5351
Loss at step 2760: 0.3217
Loss at step 2770: 0.3372
Loss at step 2780: 0.3365
Loss at step 2790: 0.4057
Loss at step 2800: 0.3078
Loss at step 2810: 0.5189
Loss at step 2820: 0.3496
Loss at step 2830: 0.3016
Loss at step 2840: 0.3375
Loss at step 2850: 0.4360
Loss at step 2860: 0.4807
Loss at step 2870: 0.3288
Loss at step 2880: 0.5517
Loss at step 2890: 0.3112
Loss at step 2900: 0.6729
Loss at step 2910: 0.3870
Loss at step 2920: 0.4828
Loss at step 2930: 0.3826
Loss at step 2940: 0.3174
Loss at step 2950: 0.4501
Loss at step 2960: 0.4227
Loss at step 2970: 0.4802
Loss at step 2980: 0.3927
Loss at step 2990: 0.2314
Loss at step 3000: 0.2126
Loss at step 3010: 0.3718
Loss at step 3020: 0.2598
Loss at step 3030: 0.2608
Loss at step 3040: 0.2360
Loss at step 3050: 0.2765
Loss at step 3060: 0.3889
Loss at step 3070: 0.2752
Loss at step 3080: 0.2582
Loss at step 3090: 0.4273
Loss at step 3100: 0.1868
Loss at step 3110: 0.4008
Loss at step 3120: 0.6534
Loss at step 3130: 0.2482
Loss at step 3140: 0.3761
Loss at step 3150: 0.3374
Loss at step 3160: 0.3802
Loss at step 3170: 0.3221
Loss at step 3180: 0.3203
Loss at step 3190: 0.3929
Loss at step 3200: 0.3694
Loss at step 3210: 0.3784
Loss at step 3220: 0.4928
Loss at step 3230: 0.5078
Loss at step 3240: 0.3452
Loss at step 3250: 0.3429
Loss at step 3260: 0.4428
Loss at step 3270: 0.3889
Loss at step 3280: 0.3800
Loss at step 3290: 0.3970
Loss at step 3300: 0.5276
Loss at step 3310: 0.3480
Loss at step 3320: 0.3604
Loss at step 3330: 0.4065
Loss at step 3340: 0.3182
Loss at step 3350: 0.2496
Loss at step 3360: 0.4996
Loss at step 3370: 0.3612
Loss at step 3380: 0.4031
Loss at step 3390: 0.2992
Loss at step 3400: 0.3169
Loss at step 3410: 0.3395
Loss at step 3420: 0.3794
Loss at step 3430: 0.4002
Loss at step 3440: 0.2977
Loss at step 3450: 0.5281
Loss at step 3460: 0.3176
Loss at step 3470: 0.2547
Loss at step 3480: 0.4585
Loss at step 3490: 0.2518
Loss at step 3500: 0.2299
Loss at step 3510: 0.5250
Loss at step 3520: 0.3268
Loss at step 3530: 0.4659
Loss at step 3540: 0.3400
Loss at step 3550: 0.6001
Loss at step 3560: 0.3637
Loss at step 3570: 0.2339
Loss at step 3580: 0.2452
Loss at step 3590: 0.4180
Loss at step 3600: 0.2797
Loss at step 3610: 0.4350
Loss at step 3620: 0.4732
Loss at step 3630: 0.3156
Loss at step 3640: 0.3361
Loss at step 3650: 0.5080
Loss at step 3660: 0.2175
Loss at step 3670: 0.3464
Loss at step 3680: 0.4047
Loss at step 3690: 0.4638
Loss at step 3700: 0.4745
Loss at step 3710: 0.5339
Loss at step 3720: 0.1913
Loss at step 3730: 0.4657
Loss at step 3740: 0.4509
Loss at step 3750: 0.4168
Loss at step 3760: 0.3898
Loss at step 3770: 0.4327
Loss at step 3780: 0.3224
Loss at step 3790: 0.2745
Loss at step 3800: 0.4355
Loss at step 3810: 0.1901
Loss at step 3820: 0.4184
Loss at step 3830: 0.3161
Loss at step 3840: 0.4720
Loss at step 3850: 0.2973
Loss at step 3860: 0.3514
Loss at step 3870: 0.3204
Loss at step 3880: 0.4875
Loss at step 3890: 0.4167
Loss at step 3900: 0.4400
Loss at step 3910: 0.2331
Loss at step 3920: 0.3164
Loss at step 3930: 0.3818
Loss at step 3940: 0.2328
Loss at step 3950: 0.3953
Loss at step 3960: 0.4138
Loss at step 3970: 0.2434
Loss at step 3980: 0.2444
Loss at step 3990: 0.4806
Loss at step 4000: 0.2213
Loss at step 4010: 0.2526
Loss at step 4020: 0.3507
Loss at step 4030: 0.1809
Loss at step 4040: 0.2181
Loss at step 4050: 0.2922
Loss at step 4060: 0.3494
Loss at step 4070: 0.1788
Loss at step 4080: 0.4081
Loss at step 4090: 0.2975
Loss at step 4100: 0.2697
Loss at step 4110: 0.2634
Loss at step 4120: 0.2717
Loss at step 4130: 0.3416
Loss at step 4140: 0.3300
Loss at step 4150: 0.2973
Loss at step 4160: 0.4339
Loss at step 4170: 0.3963
Loss at step 4180: 0.3603
Loss at step 4190: 0.2104
Loss at step 4200: 0.2796
Loss at step 4210: 0.3046
Loss at step 4220: 0.3714
Loss at step 4230: 0.4035
Loss at step 4240: 0.3353
Loss at step 4250: 0.2851
Loss at step 4260: 0.2489
Loss at step 4270: 0.4600
Loss at step 4280: 0.3821
Loss at step 4290: 0.1059
Loss at step 4300: 0.2593
Loss at step 4310: 0.2775
Loss at step 4320: 0.2740
Loss at step 4330: 0.2848
Loss at step 4340: 0.2854
Loss at step 4350: 0.5292
Loss at step 4360: 0.3994
Loss at step 4370: 0.3417
Loss at step 4380: 0.4466
Loss at step 4390: 0.2586
Loss at step 4400: 0.2396
Loss at step 4410: 0.2502
Loss at step 4420: 0.1625
Loss at step 4430: 0.3704
Loss at step 4440: 0.2577
Loss at step 4450: 0.3408
Loss at step 4460: 0.2051
Loss at step 4470: 0.2809
Loss at step 4480: 0.3275
Loss at step 4490: 0.1762
Loss at step 4500: 0.3065
Loss at step 4510: 0.3111
Loss at step 4520: 0.2523
Loss at step 4530: 0.3145
Loss at step 4540: 0.3115
Loss at step 4550: 0.2701
Loss at step 4560: 0.3354
Loss at step 4570: 0.3737
Loss at step 4580: 0.3087
Loss at step 4590: 0.4697
Loss at step 4600: 0.4045
Loss at step 4610: 0.4719
Loss at step 4620: 0.2903
Loss at step 4630: 0.3072
Loss at step 4640: 0.4504
Loss at step 4650: 0.2799
Loss at step 4660: 0.1955
Loss at step 4670: 0.3319
Loss at step 4680: 0.3151
Loss at step 4690: 0.2918
Loss at step 4700: 0.4905
Loss at step 4710: 0.2461
Loss at step 4720: 0.2935
Loss at step 4730: 0.4659
Loss at step 4740: 0.3299
Loss at step 4750: 0.4823
Loss at step 4760: 0.1430
Loss at step 4770: 0.2702
Loss at step 4780: 0.3163
Loss at step 4790: 0.4497
Loss at step 4800: 0.4078
Loss at step 4810: 0.3017
Loss at step 4820: 0.2026
Loss at step 4830: 0.1991
Loss at step 4840: 0.2271
Loss at step 4850: 0.2161
Loss at step 4860: 0.3832
Loss at step 4870: 0.4406
Loss at step 4880: 0.2376
Loss at step 4890: 0.3793
Loss at step 4900: 0.3576
Loss at step 4910: 0.2298
Loss at step 4920: 0.2569
Loss at step 4930: 0.3871
Loss at step 4940: 0.3560
Loss at step 4950: 0.4340
Loss at step 4960: 0.2563
Loss at step 4970: 0.3338
Loss at step 4980: 0.2779
Loss at step 4990: 0.4144
Loss at step 5000: 0.2744
Loss at step 5010: 0.2086
Loss at step 5020: 0.2189
Loss at step 5030: 0.3264
Loss at step 5040: 0.3326
Loss at step 5050: 0.3914
Loss at step 5060: 0.1938
Loss at step 5070: 0.3432
Loss at step 5080: 0.2503
Loss at step 5090: 0.2960
Loss at step 5100: 0.2486
Loss at step 5110: 0.2171
Loss at step 5120: 0.1566
Loss at step 5130: 0.4230
Loss at step 5140: 0.1664
Loss at step 5150: 0.2648
Loss at step 5160: 0.5469
Loss at step 5170: 0.3954
Loss at step 5180: 0.2033
Loss at step 5190: 0.2257
Loss at step 5200: 0.4653
Loss at step 5210: 0.3470
Loss at step 5220: 0.2369
Loss at step 5230: 0.2395
Loss at step 5240: 0.1752
Loss at step 5250: 0.4826
Loss at step 5260: 0.3263
Loss at step 5270: 0.3167
Loss at step 5280: 0.2727
Loss at step 5290: 0.2372
Loss at step 5300: 0.4487
Loss at step 5310: 0.3539
Loss at step 5320: 0.1969
Loss at step 5330: 0.3105
Loss at step 5340: 0.3925
Loss at step 5350: 0.3222
Loss at step 5360: 0.2686
Loss at step 5370: 0.2889
Loss at step 5380: 0.2230
Loss at step 5390: 0.2515
Loss at step 5400: 0.2623
Loss at step 5410: 0.3408
Loss at step 5420: 0.3273
Loss at step 5430: 0.2827
Loss at step 5440: 0.4449
Loss at step 5450: 0.5269
Loss at step 5460: 0.3534
Loss at step 5470: 0.1908
Loss at step 5480: 0.4643
Loss at step 5490: 0.3114
Loss at step 5500: 0.4497
Loss at step 5510: 0.2546
Loss at step 5520: 0.5312
Loss at step 5530: 0.2932
Loss at step 5540: 0.2704
Loss at step 5550: 0.3082
Loss at step 5560: 0.2573
Loss at step 5570: 0.6479
Loss at step 5580: 0.2326
Loss at step 5590: 0.4420
Loss at step 5600: 0.2491
Loss at step 5610: 0.2862
Loss at step 5620: 0.2107
Loss at step 5630: 0.3941
Loss at step 5640: 0.2403
Loss at step 5650: 0.2856
Loss at step 5660: 0.2061
Loss at step 5670: 0.2486
Loss at step 5680: 0.3006
Loss at step 5690: 0.2052
Loss at step 5700: 0.4373
Loss at step 5710: 0.3713
Loss at step 5720: 0.2969
Loss at step 5730: 0.2560
Loss at step 5740: 0.3686
Loss at step 5750: 0.3619
Loss at step 5760: 0.1978
Loss at step 5770: 0.4723
Loss at step 5780: 0.3742
Loss at step 5790: 0.2246
Loss at step 5800: 0.1941
Loss at step 5810: 0.3262
Loss at step 5820: 0.2796
Loss at step 5830: 0.1288
Loss at step 5840: 0.1835
Loss at step 5850: 0.2655
Loss at step 5860: 0.1742
Loss at step 5870: 0.5961
Loss at step 5880: 0.3310
Loss at step 5890: 0.3940
Loss at step 5900: 0.2000
Loss at step 5910: 0.2837
Loss at step 5920: 0.1282
Loss at step 5930: 0.4791
Loss at step 5940: 0.2401
Loss at step 5950: 0.1945
Loss at step 5960: 0.3134
Loss at step 5970: 0.2809
Loss at step 5980: 0.3570
Loss at step 5990: 0.1305
Loss at step 6000: 0.2205
Loss at step 6010: 0.1970
Loss at step 6020: 0.3064
Loss at step 6030: 0.1671
Loss at step 6040: 0.3188
Loss at step 6050: 0.3705
Loss at step 6060: 0.2942
Loss at step 6070: 0.1703
Loss at step 6080: 0.2933
Loss at step 6090: 0.2306
Loss at step 6100: 0.2788
Loss at step 6110: 0.3035
Loss at step 6120: 0.2331
Loss at step 6130: 0.1920
Loss at step 6140: 0.2007
Loss at step 6150: 0.1851
Loss at step 6160: 0.3059
Loss at step 6170: 0.2815
Loss at step 6180: 0.3352
Loss at step 6190: 0.3700
Loss at step 6200: 0.2958
Loss at step 6210: 0.2798
Loss at step 6220: 0.1992
Loss at step 6230: 0.3560
Loss at step 6240: 0.2324
Loss at step 6250: 0.2114
Loss at step 6260: 0.2774
Loss at step 6270: 0.1543
Loss at step 6280: 0.1214
Loss at step 6290: 0.4041
Loss at step 6300: 0.2433
Loss at step 6310: 0.2699
Loss at step 6320: 0.2622
Loss at step 6330: 0.2981
Loss at step 6340: 0.2523
Loss at step 6350: 0.1909
Loss at step 6360: 0.1739
Loss at step 6370: 0.1907
Loss at step 6380: 0.2273
Loss at step 6390: 0.1200
Loss at step 6400: 0.1879
Loss at step 6410: 0.2039
Loss at step 6420: 0.1977
Loss at step 6430: 0.2783
Loss at step 6440: 0.2092
Loss at step 6450: 0.3125
Loss at step 6460: 0.1775
Loss at step 6470: 0.3240
Loss at step 6480: 0.2502
Loss at step 6490: 0.3048
Loss at step 6500: 0.4655
Loss at step 6510: 0.1795
Loss at step 6520: 0.2578
Loss at step 6530: 0.2888
Loss at step 6540: 0.1187
Loss at step 6550: 0.1648
Loss at step 6560: 0.3769
Loss at step 6570: 0.2012
Loss at step 6580: 0.1817
Loss at step 6590: 0.1519
Loss at step 6600: 0.3100
Loss at step 6610: 0.1813
Loss at step 6620: 0.1620
Loss at step 6630: 0.3177
Loss at step 6640: 0.1984
Loss at step 6650: 0.1555
Loss at step 6660: 0.4159
Loss at step 6670: 0.2290
Loss at step 6680: 0.1909
Loss at step 6690: 0.2997
Loss at step 6700: 0.2056
Loss at step 6710: 0.1255
Loss at step 6720: 0.2551
Loss at step 6730: 0.2753
Loss at step 6740: 0.3971
Loss at step 6750: 0.1845
Loss at step 6760: 0.2101
Loss at step 6770: 0.1915
Loss at step 6780: 0.3976
Loss at step 6790: 0.4526
Loss at step 6800: 0.2762
Loss at step 6810: 0.2491
Loss at step 6820: 0.3090
Loss at step 6830: 0.4046
Loss at step 6840: 0.1150
Loss at step 6850: 0.1986
Loss at step 6860: 0.3826
Loss at step 6870: 0.3270
Loss at step 6880: 0.1645
Loss at step 6890: 0.1995
Loss at step 6900: 0.3222
Loss at step 6910: 0.2015
Loss at step 6920: 0.2217
Loss at step 6930: 0.1414
Loss at step 6940: 0.1797
Loss at step 6950: 0.1300
Loss at step 6960: 0.2394
Loss at step 6970: 0.2701
Loss at step 6980: 0.2705
Loss at step 6990: 0.2476
Loss at step 7000: 0.3120
Loss at step 7010: 0.1882
Loss at step 7020: 0.2541
Loss at step 7030: 0.1686
Loss at step 7040: 0.1269
Loss at step 7050: 0.2976
Loss at step 7060: 0.2483
Loss at step 7070: 0.2589
Loss at step 7080: 0.3017
Loss at step 7090: 0.1951
Loss at step 7100: 0.1948
Loss at step 7110: 0.2988
Loss at step 7120: 0.2700
Loss at step 7130: 0.2822
Loss at step 7140: 0.2167
Loss at step 7150: 0.3039
Loss at step 7160: 0.1679
Loss at step 7170: 0.3539
Loss at step 7180: 0.2529
Loss at step 7190: 0.2255
Loss at step 7200: 0.3661
Loss at step 7210: 0.1698
Loss at step 7220: 0.2959
Loss at step 7230: 0.3556
Loss at step 7240: 0.1827
Loss at step 7250: 0.3236
Loss at step 7260: 0.3312
Loss at step 7270: 0.1960
Loss at step 7280: 0.1968
Loss at step 7290: 0.1459
Loss at step 7300: 0.1326
Loss at step 7310: 0.2606
Loss at step 7320: 0.2673
Loss at step 7330: 0.2226
Loss at step 7340: 0.3040
Loss at step 7350: 0.4041
Loss at step 7360: 0.1877
Loss at step 7370: 0.2339
Loss at step 7380: 0.2547
Loss at step 7390: 0.2948
Loss at step 7400: 0.2924
Loss at step 7410: 0.1443
Loss at step 7420: 0.2955
Loss at step 7430: 0.2387
Loss at step 7440: 0.1637
Loss at step 7450: 0.1997
Loss at step 7460: 0.1453
Loss at step 7470: 0.1866
Loss at step 7480: 0.2627
Loss at step 7490: 0.1293
Loss at step 7500: 0.1909
Loss at step 7510: 0.1801
Loss at step 7520: 0.3179
Loss at step 7530: 0.3860
Loss at step 7540: 0.2486
Loss at step 7550: 0.3206
Loss at step 7560: 0.3826
Loss at step 7570: 0.1387
Loss at step 7580: 0.4951
Loss at step 7590: 0.3444
Loss at step 7600: 0.2381
Loss at step 7610: 0.2071
Loss at step 7620: 0.2053
Loss at step 7630: 0.1594
Loss at step 7640: 0.3473
Loss at step 7650: 0.3711
Loss at step 7660: 0.1598
Loss at step 7670: 0.2181
Loss at step 7680: 0.2255
Loss at step 7690: 0.2859
Loss at step 7700: 0.5143
Loss at step 7710: 0.2962
Loss at step 7720: 0.2326
Loss at step 7730: 0.1639
Loss at step 7740: 0.2279
Loss at step 7750: 0.1787
Loss at step 7760: 0.3412
Loss at step 7770: 0.1248
Loss at step 7780: 0.3336
Loss at step 7790: 0.2042
Loss at step 7800: 0.1023
Loss at step 7810: 0.2985
Loss at step 7820: 0.3110
Loss at step 7830: 0.1551
Loss at step 7840: 0.3421
Loss at step 7850: 0.1527
Loss at step 7860: 0.2769
Loss at step 7870: 0.2926
Loss at step 7880: 0.1822
Loss at step 7890: 0.3141
Loss at step 7900: 0.3830
Loss at step 7910: 0.1786
Loss at step 7920: 0.1736
Loss at step 7930: 0.3175
Loss at step 7940: 0.2775
Loss at step 7950: 0.3339
Loss at step 7960: 0.2184
Loss at step 7970: 0.2248
Loss at step 7980: 0.2321
Loss at step 7990: 0.0915
Loss at step 8000: 0.1902
Loss at step 8010: 0.2236
Loss at step 8020: 0.2338
Loss at step 8030: 0.1554
Loss at step 8040: 0.1826
Loss at step 8050: 0.2047
Loss at step 8060: 0.1836
Loss at step 8070: 0.2160
Loss at step 8080: 0.2259
Loss at step 8090: 0.2535
Loss at step 8100: 0.1599
Loss at step 8110: 0.2333
Loss at step 8120: 0.1904
Loss at step 8130: 0.1423
Loss at step 8140: 0.2639
Loss at step 8150: 0.2178
Loss at step 8160: 0.2937
Loss at step 8170: 0.1829
Loss at step 8180: 0.1481
Loss at step 8190: 0.2554
Loss at step 8200: 0.3780
Loss at step 8210: 0.2792
Loss at step 8220: 0.2457
Loss at step 8230: 0.2265
Loss at step 8240: 0.2016
Loss at step 8250: 0.1127
Loss at step 8260: 0.1640
Loss at step 8270: 0.1828
Loss at step 8280: 0.2262
Loss at step 8290: 0.3325
Loss at step 8300: 0.1965
Loss at step 8310: 0.1454
Loss at step 8320: 0.1564
Loss at step 8330: 0.3662
Loss at step 8340: 0.1423
Loss at step 8350: 0.3317
Loss at step 8360: 0.1432
Loss at step 8370: 0.2025
Loss at step 8380: 0.1466
Loss at step 8390: 0.3338
Loss at step 8400: 0.2582
Loss at step 8410: 0.3232
Loss at step 8420: 0.1913
Loss at step 8430: 0.2486
Loss at step 8440: 0.2843
Loss at step 8450: 0.2540
Loss at step 8460: 0.2901
Loss at step 8470: 0.1693
Loss at step 8480: 0.1324
Loss at step 8490: 0.1813
Loss at step 8500: 0.2106
Loss at step 8510: 0.1475
Loss at step 8520: 0.2110
Loss at step 8530: 0.3164
Loss at step 8540: 0.2020
Loss at step 8550: 0.3321
Loss at step 8560: 0.4039
Loss at step 8570: 0.2974
Loss at step 8580: 0.2118
Loss at step 8590: 0.2260
Loss at step 8600: 0.2586
Loss at step 8610: 0.1421
Loss at step 8620: 0.2702
Loss at step 8630: 0.3569
Loss at step 8640: 0.2719
Loss at step 8650: 0.1802
Loss at step 8660: 0.0932
Loss at step 8670: 0.1402
Loss at step 8680: 0.2256
Loss at step 8690: 0.2370
Loss at step 8700: 0.3874
Loss at step 8710: 0.2923
Loss at step 8720: 0.4201
Loss at step 8730: 0.2734
Loss at step 8740: 0.2290
Loss at step 8750: 0.2409
Loss at step 8760: 0.2095
Loss at step 8770: 0.0886
Loss at step 8780: 0.1855
Loss at step 8790: 0.2380
Loss at step 8800: 0.1857
Loss at step 8810: 0.2016
Loss at step 8820: 0.2554
Loss at step 8830: 0.3112
Loss at step 8840: 0.1031
Loss at step 8850: 0.2101
Loss at step 8860: 0.2359
Loss at step 8870: 0.3069
Loss at step 8880: 0.2006
Loss at step 8890: 0.2045
Loss at step 8900: 0.1822
Loss at step 8910: 0.3471
Loss at step 8920: 0.2746
Loss at step 8930: 0.1278
Loss at step 8940: 0.2709
Loss at step 8950: 0.2571
Loss at step 8960: 0.1806
Loss at step 8970: 0.2423
Loss at step 8980: 0.1359
Loss at step 8990: 0.2328
Loss at step 9000: 0.3503
***** Running testing *****
  Num examples = 82080
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.811184, 'precision': [0.904097, 0.358092, 0.395682], 'recall': [0.917318, 0.335922, 0.369516], 'f1': [0.91066, 0.346653, 0.382152]}
{'accuracy': 0.909747, 'precision': 0.395682, 'recall': 0.369516, 'f1': 0.382152, 'WordR': 0.404599}
