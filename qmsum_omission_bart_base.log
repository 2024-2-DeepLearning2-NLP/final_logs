Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x7fa9a532ac10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Round 1
Start Training!
Sample 261 of the training set: {'input_ids': [0, 2264, 21, 5, 6427, 59, 33664, 15, 5, 92, 6398, 17467, 77, 51, 58, 7345, 5, 20273, 9, 5, 2210, 6398, 17467, 116, 47385, 50121, 50118, 44518, 39322, 35, 152, 65, 16, 25522, 417, 32062, 6920, 254, 24303, 300, 10, 761, 9, 25715, 101, 10, 18292, 2156, 50121, 50118, 33347, 4827, 35, 256, 119, 12, 298, 5471, 2156, 101, 5, 1692, 6148, 479, 50121, 50118, 44518, 39322, 35, 61, 25522, 417, 32062, 6920, 254, 24303, 8976, 479, 50121, 50118, 33347, 4827, 35, 8487, 479, 50121, 50118, 44518, 39322, 35, 12698, 8, 25522, 417, 32062, 6920, 254, 24303, 125, 38, 437, 45, 2230, 686, 141, 47, 1017, 304, 14, 2156, 50121, 50118, 36926, 13700, 31275, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 33347, 4827, 35, 7746, 24, 18, 24282, 101, 33664, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 101, 74, 5, 3034, 283, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 37463, 235, 2156, 157, 2156, 114, 38, 579, 114, 38, 437, 2053, 9, 5, 235, 65, 2156, 38, 348, 300, 5, 276, 631, 11, 760, 9, 127, 5271, 2156, 47, 25715, 24, 8, 5, 77, 47, 1338, 5, 2345, 9, 7252, 25522, 31375, 1536, 9834, 24303, 5765, 6880, 14, 47, 2703, 2156, 47, 1228, 5, 1692, 9, 5, 25715, 479, 50121, 50118, 44518, 39322, 35, 23129, 12, 298, 2957, 2156, 14, 18, 101, 5, 226, 1215, 347, 1215, 495, 1215, 65, 2156, 50121, 50118, 33347, 4827, 35, 5143, 2156, 8578, 479, 50121, 50118, 44518, 39322, 35, 16, 24, 17487, 125, 5, 65, 874, 14, 34, 300, 101, 25522, 31375, 1536, 9834, 24303, 10, 410, 25715, 5043, 15, 5, 526, 479, 125, 38, 38337, 14, 5, 8047, 531, 283, 62, 15, 5, 255, 1215, 846, 1215, 2441, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 8976, 479, 50121, 50118, 33347, 4827, 35, 8976, 2156, 15546, 479, 50121, 50118, 44518, 39322, 35, 38, 206, 14, 18, 99, 14, 16, 479, 407, 209, 32, 95, 10, 367, 2956, 479, 7574, 14, 18, 95, 1341, 15305, 3989, 2156, 10521, 2156, 1326, 1341, 980, 12, 1580, 219, 2156, 53, 350, 171, 14893, 2156, 38, 206, 15, 14, 65, 479, 50121, 50118, 36926, 13700, 31275, 35, 23129, 24, 1326, 5608, 479, 50121, 50118, 33347, 4827, 35, 8976, 2156, 1326, 101, 37463, 1326, 101, 402, 66, 9, 10, 4900, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 8976, 2156, 24, 473, 356, 761, 9, 2702, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 31375, 1536, 9834, 24303, 85, 1326, 101, 11380, 25522, 29183, 24303, 479, 50121, 50118, 10006, 16382, 35, 44258, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 27913, 39322, 969, 5, 3493, 9, 65, 761, 9, 6398, 17467, 19, 45726, 15, 24, 6, 447, 25, 5, 1692, 14893, 4, 318, 5, 2360, 770, 7, 465, 10, 1402, 6880, 6, 51, 95, 956, 7, 25715, 24, 8, 77, 51, 1348, 5, 1552, 1980, 6, 51, 11224, 5, 1692, 9, 5, 25715, 4, 2223, 5, 165, 21, 45, 10028, 19, 5, 7705, 6, 3989, 8, 14893, 9, 5, 2210, 6, 51, 70, 1507, 14, 5, 5043, 9, 10, 25715, 21, 10, 205, 1114, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 32.9068, 'rouge2': 12.2203, 'rougeL': 23.4724, 'rougeLsum': 28.9238}, 'ppl': {'perplexity': 5.0726, 'ref_perplexity': 15.5195}, 'bertscore': {'precision': 88.1123, 'recall': 86.1223, 'f1': 87.0957}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.6203, 'rouge2': 13.7668, 'rougeL': 25.1214, 'rougeLsum': 32.4666}, 'ppl': {'perplexity': 169.9316, 'ref_perplexity': 14.3379}, 'bertscore': {'precision': 87.874, 'recall': 86.8267, 'f1': 87.3364}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.3847, 'rouge2': 12.7626, 'rougeL': 23.9591, 'rougeLsum': 31.0731}, 'ppl': {'perplexity': 4.5662, 'ref_perplexity': 13.8641}, 'bertscore': {'precision': 87.9642, 'recall': 86.5479, 'f1': 87.2405}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.5442, 'rouge2': 12.1712, 'rougeL': 23.2991, 'rougeLsum': 30.4476}, 'ppl': {'perplexity': 5.4383, 'ref_perplexity': 13.8744}, 'bertscore': {'precision': 87.7389, 'recall': 86.5183, 'f1': 87.1159}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.803, 'rouge2': 13.0839, 'rougeL': 24.306, 'rougeLsum': 31.5898}, 'ppl': {'perplexity': 6.5663, 'ref_perplexity': 13.919}, 'bertscore': {'precision': 87.9676, 'recall': 86.7127, 'f1': 87.3255}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 2
Start Training!
Sample 771 of the training set: {'input_ids': [0, 2264, 21, 5, 3184, 9, 5, 1501, 293, 12, 4135, 3373, 4, 47385, 50121, 50118, 44908, 163, 35, 653, 109, 47, 1266, 30, 22, 8225, 19099, 22, 17487, 252, 218, 75, 3993, 88, 932, 269, 5988, 479, 50121, 50118, 44908, 83, 35, 125, 38, 1266, 2156, 596, 109, 52, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 1793, 38, 1236, 687, 50121, 50118, 44908, 83, 35, 318, 52, 10128, 5, 2381, 111, 89, 37908, 55, 3553, 203, 55, 87, 52, 10128, 5, 97, 1980, 2156, 172, 52, 74, 10992, 2156, 190, 11, 42, 1068, 2156, 14, 37, 770, 7, 213, 89, 479, 50121, 50118, 44908, 230, 35, 1063, 50121, 50118, 44908, 83, 35, 407, 2156, 11, 14, 1472, 2156, 52, 2408, 106, 6681, 235, 122, 479, 50121, 50118, 44908, 163, 35, 4954, 479, 25379, 1472, 479, 8976, 479, 125, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 407, 5, 125, 38, 4443, 5, 449, 5, 864, 25522, 417, 32062, 6920, 254, 24303, 14, 38, 21, 25, 3335, 8020, 50, 2085, 1738, 21, 15670, 7, 162, 16, 25522, 417, 32062, 6920, 254, 24303, 1336, 109, 52, 385, 146, 5, 568, 15, 25522, 417, 32062, 6920, 254, 24303, 25, 7, 25522, 417, 32062, 6920, 254, 24303, 61, 65, 7, 4161, 7, 17487, 50121, 50118, 44908, 83, 35, 8976, 2156, 98, 2156, 5, 507, 385, 568, 16, 5, 4069, 9, 209, 130, 479, 407, 456, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 24, 128, 29, 103, 761, 9, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 1501, 293, 111, 1161, 479, 50121, 50118, 44908, 83, 35, 8976, 2156, 686, 479, 50121, 50118, 44908, 230, 35, 4954, 98, 2156, 172, 2156, 5, 864, 939, 407, 172, 127, 864, 16, 326, 7, 47, 172, 2156, 74, 28, 25522, 417, 32062, 6920, 254, 24303, 407, 16, 5, 129, 910, 1219, 52, 64, 146, 70, 209, 2735, 1501, 293, 111, 23097, 2156, 142, 52, 216, 52, 64, 129, 432, 19, 10, 40001, 278, 9, 12558, 2485, 17487, 230, 4987, 1021, 212, 318, 52, 128, 241, 95, 602, 23501, 2777, 11, 2156, 52, 1705, 75, 33, 10, 37908, 13, 358, 678, 864, 2156, 47, 216, 17487, 50121, 50118, 44908, 83, 35, 83, 568, 37908, 13, 358, 678, 864, 2156, 47, 1266, 17487, 50121, 50118, 44908, 230, 35, 2647, 2156, 38, 25522, 417, 32062, 6920, 254, 24303, 101, 2156, 11, 5, 403, 9, 25522, 417, 32062, 6920, 254, 24303, 8976, 479, 96, 5, 6056, 5053, 2125, 9, 2777, 2156, 52, 1979, 75, 28, 441, 7, 1948, 24, 19, 42, 467, 2156, 741, 114, 52, 95, 1368, 230, 4987, 52, 1979, 75, 33, 5, 4577, 37908, 479, 23381, 2156, 885, 99, 47, 128, 241, 579, 15670, 16, 10, 295, 4820, 111, 1534, 37908, 2156, 235, 17487, 50121, 50118, 44908, 83, 35, 8976, 479, 50121, 50118, 44908, 230, 35, 178, 25522, 417, 32062, 6920, 254, 24303, 8, 114, 52, 25522, 417, 32062, 6920, 254, 24303, 178, 114, 951, 25522, 417, 32062, 6920, 254, 24303, 161, 2156, 47, 216, 2156, 37463, 2156, 402, 11, 33830, 7, 5, 467, 2156, 52, 128, 417, 111, 1979, 75, 216, 61, 37908, 7, 356, 23, 7, 1948, 14, 864, 2156, 50121, 50118, 44908, 83, 35, 407, 16, 25522, 417, 32062, 6920, 254, 24303, 8976, 479, 8976, 479, 50121, 50118, 44908, 230, 35, 235, 17487, 50121, 50118, 44908, 163, 35, 256, 5471, 17487, 50121, 50118, 44908, 230, 35, 407, 2156, 53, 25522, 417, 32062, 6920, 254, 24303, 53, 114, 52, 33, 10, 40001, 25522, 417, 32062, 6920, 254, 24303, 653, 17487, 50121, 50118, 44908, 163, 35, 38, 218, 75, 192, 110, 477, 479, 653, 25522, 417, 32062, 6920, 254, 24303, 99, 25522, 417, 32062, 6920, 254, 24303, 99, 38, 524, 2053, 2156, 50, 99, 52, 128, 241, 59, 7, 15393, 259, 16, 52, 128, 241, 460, 6908, 120, 5, 1086, 889, 9, 3266, 8, 49, 41834, 43471, 479, 178, 122, 52, 240, 41, 3827, 467, 50, 6563, 111, 1161, 50, 402, 14, 36705, 1872, 14, 2156, 14, 1326, 23, 70, 5, 3266, 8, 161, 2156, 22, 20, 1924, 16, 2668, 154, 479, 978, 2156, 213, 89, 479, 22, 22, 23129, 2156, 213, 89, 2156, 2668, 154, 2156, 122, 479, 22, 1793, 2156, 22, 20, 1924, 16, 17883, 111, 15, 2156, 42419, 111, 4995, 479, 22, 407, 2156, 37, 1072, 7, 216, 25522, 44970, 24303, 402, 59, 24, 2156, 8, 99, 24, 473, 479, 234, 2957, 17487, 23129, 2156, 6069, 9, 25522, 417, 32062, 6920, 254, 24303, 9, 25522, 417, 32062, 6920, 254, 24303, 9, 5, 8135, 479, 3990, 111, 6304, 1120, 459, 50121, 50118, 44908, 230, 35, 8976, 2156, 53, 125, 141, 473, 5, 3827, 25522, 417, 32062, 6920, 254, 24303, 53, 141, 473, 5, 3827, 467, 216, 25522, 417, 32062, 6920, 254, 24303, 141, 54, 61, 65, 7, 10152, 5, 1924, 2156, 114, 24, 630, 75, 216, 5, 864, 24, 16, 2156, 8, 141, 14, 864, 197, 28, 7173, 17487, 50121, 50118, 44908, 163, 35, 7253, 15, 5, 449, 99, 5, 864, 21, 2156, 98, 99, 5, 19771, 2156, 5, 25099, 4383, 2156, 5, 1068, 8, 5, 3018, 1421, 851, 201, 2156, 52, 376, 62, 19, 209, 3266, 13, 209, 2390, 479, 50121, 50118, 44908, 230, 35, 8976, 38, 216, 479, 125, 141, 109, 52, 2408, 99, 52, 120, 66, 17487, 287, 2156, 61, 65, 939, 6834, 1980, 32, 505, 17487, 407, 127, 939, 407, 2156, 114, 52, 58, 7, 24, 19, 10, 1501, 293, 111, 1161, 2156, 52, 128, 417, 33, 7, 33, 10, 37908, 25522, 417, 32062, 6920, 254, 24303, 13, 358, 864, 14, 52, 1467, 141, 7, 432, 19, 2156, 14, 74, 185, 70, 9, 5, 16584, 8, 2408, 106, 16574, 13, 14, 864, 479, 50121, 50118, 44908, 163, 35, 256, 119, 111, 1368, 5471, 479, 50121, 50118, 44908, 230, 35, 8901, 14, 146, 1472, 17487, 854, 857, 2156, 295, 857, 17487, 50121, 50118, 44908, 83, 35, 12698, 2156, 38, 1266, 2156, 32, 47, 584, 14, 2156, 99, 2594, 114, 47, 860, 7, 3189, 42, 62, 7, 5, 1068, 2156, 50, 32, 52, 95, 4098, 19, 23501, 2777, 17487, 50121, 50118, 44908, 230, 35, 166, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 83, 35, 1534, 14, 110, 477, 17487, 50121, 50118, 44908, 230, 35, 2647, 2156, 117, 479, 38, 25522, 417, 32062, 6920, 254, 24303, 38, 4443, 127, 864, 16, 2156, 1534, 5, 1219, 14, 52, 64, 146, 10, 37908, 856, 50, 25522, 417, 32062, 6920, 254, 24303, 4954, 479, 407, 2156, 2084, 119, 1794, 192, 114, 38, 128, 119, 10985, 479, 3945, 52, 164, 7, 146, 10, 37908, 13, 358, 864, 17487, 8901, 14, 146, 1472, 17487, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 83, 35, 286, 358, 864, 17487, 50121, 50118, 44908, 230, 35, 1793, 45, 479, 50121, 50118, 44908, 83, 35, 2011, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 4337, 1663, 479, 50121, 50118, 44908, 83, 35, 44258, 479, 38, 218, 75, 25522, 417, 32062, 6920, 254, 24303, 1491, 4784, 2156, 38, 74, 206, 479, 38, 1266, 2156, 24, 128, 29, 45, 716, 15, 12558, 2485, 2156, 24, 128, 29, 716, 15, 383, 101, 2156, 37463, 2156, 89, 128, 29, 6908, 28, 10, 37908, 13, 2381, 111, 89, 50, 45, 2156, 8, 89, 128, 29, 6908, 28, 10, 37908, 13, 4789, 2156, 3756, 2156, 38370, 479, 50121, 50118, 44908, 230, 35, 15565, 305, 4954, 479, 407, 2156, 951, 553, 10, 864, 479, 50121, 50118, 44908, 83, 35, 8976, 479, 50121, 50118, 44908, 230, 35, 1336, 109, 52, 2845, 141, 7, 1948, 24, 17487, 50121, 50118, 44908, 163, 35, 2647, 2156, 356, 23, 25522, 417, 32062, 6920, 254, 24303, 356, 25522, 417, 32062, 6920, 254, 24303, 12346, 2512, 19, 42, 3349, 864, 479, 370, 120, 42, 25522, 417, 32062, 6920, 254, 24303, 370, 128, 890, 33, 25522, 417, 32062, 6920, 254, 24303, 1423, 152, 16, 99, 47, 120, 479, 178, 122, 47, 33, 7, 146, 10, 568, 479, 653, 109, 52, 206, 17487, 653, 473, 42, 1137, 201, 17487, 178, 45, 4730, 99, 21, 553, 2156, 8, 99, 1102, 2156, 8, 549, 5, 621, 21, 10, 8376, 50, 10, 400, 2156, 142, 70, 9, 209, 2433, 33, 15546, 416, 1613, 88, 442, 209, 41834, 43471, 479, 653, 25522, 417, 32062, 6920, 254, 24303, 99, 52, 240, 16, 10, 25522, 417, 32062, 6920, 254, 24303, 95, 10, 9562, 14, 161, 2156, 22, 83, 1999, 27785, 345, 16, 25522, 417, 32062, 6920, 254, 24303, 22, 50121, 50118, 44908, 230, 35, 8976, 479, 38, 95, 218, 75, 206, 10, 22, 1924, 111, 185, 111, 70, 22, 1907, 9, 631, 16, 5, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 83, 35, 38, 1266, 2156, 11, 937, 2156, 101, 2156, 52, 351, 75, 95, 33, 167, 130, 2156, 235, 17487, 166, 128, 890, 33, 2156, 37463, 2156, 101, 2156, 171, 2156, 171, 32833, 479, 407, 52, 33, 7, 2156, 101, 25522, 417, 32062, 6920, 254, 24303, 407, 14, 24, 128, 29, 117, 1181, 678, 7, 95, 356, 23, 5, 32833, 1235, 8, 1955, 66, 99, 5, 621, 16, 667, 7, 224, 479, 50121, 50118, 44908, 163, 35, 33597, 479, 3047, 89, 32, 3222, 45950, 14768, 2156, 235, 17487, 20, 37463, 25522, 417, 32062, 6920, 254, 24303, 23129, 2156, 117, 479, 407, 114, 25522, 417, 32062, 6920, 254, 24303, 114, 13, 1246, 2156, 5, 2381, 111, 89, 41834, 3302, 16, 98, 239, 2156, 7252, 2156, 37463, 2156, 885, 114, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 114, 24, 34, 1348, 25522, 417, 32062, 6920, 254, 24303, 1348, 10, 1402, 6958, 2156, 172, 70, 9, 42, 3374, 21821, 479, 407, 479, 318, 25522, 417, 32062, 6920, 254, 24303, 190, 114, 25522, 417, 32062, 6920, 254, 24303, 114, 5, 5043, 50, 5, 750, 50, 402, 16, 2314, 1256, 205, 15, 5, 1528, 37908, 2156, 1528, 923, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 15565, 38, 218, 75, 216, 59, 14, 2156, 740, 4987, 14, 74, 3608, 14, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 163, 35, 91, 1072, 7, 213, 89, 8, 216, 402, 59, 24, 17487, 50121, 50118, 44908, 230, 35, 1832, 51, 33, 7, 28, 7628, 8976, 479, 1832, 51, 33, 7, 28, 23196, 5451, 17487, 50121, 50118, 44908, 163, 35, 38, 206, 7, 103, 5239, 51, 32, 479, 1793, 2085, 51, 128, 241, 45, 479, 50121, 50118, 44908, 230, 35, 230, 4987, 38, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 20, 169, 47, 6190, 99, 51, 2425, 2156, 51, 3559, 75, 16119, 257, 37463, 2156, 51, 399, 75, 2045, 23196, 5451, 7, 162, 479, 50121, 50118, 44908, 163, 35, 2647, 2156, 114, 37, 630, 75, 236, 7, 213, 89, 2156, 190, 114, 5, 4789, 41834, 16245, 102, 407, 479, 50121, 50118, 44908, 230, 35, 15565, 50121, 50118, 44908, 163, 35, 2381, 111, 89, 16, 440, 479, 4789, 16, 755, 2156, 8, 17883, 111, 15, 16, 755, 479, 50121, 50118, 44908, 230, 35, 2647, 2156, 11380, 2156, 95, 66, 9, 5, 97, 130, 2156, 600, 2156, 14, 47, 56, 11, 5, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 163, 35, 44258, 17487, 50121, 50118, 44908, 230, 35, 167, 130, 32833, 479, 20, 111, 385, 252, 399, 75, 2045, 101, 51, 58, 23196, 5451, 479, 50121, 50118, 44908, 163, 35, 440, 2156, 89, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 440, 479, 125, 25522, 417, 32062, 6920, 254, 24303, 85, 128, 29, 149, 5, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44908, 230, 35, 407, 3553, 579, 98, 2156, 11380, 2156, 53, 103, 25522, 417, 32062, 6920, 254, 24303, 407, 2156, 103, 383, 74, 1874, 66, 2156, 8, 103, 383, 74, 202, 28, 505, 479, 50121, 50118, 44908, 163, 35, 256, 119, 111, 1368, 5471, 479, 50121, 50118, 44908, 230, 35, 125, 38, 4443, 99, 128, 29, 15868, 162, 16, 2156, 114, 52, 33, 10, 1501, 293, 111, 1161, 7, 432, 885, 277, 1501, 293, 111, 1161, 7, 432, 19, 42, 2682, 2156, 50121, 50118, 44908, 83, 35, 256, 119, 111, 1368, 5471, 479, 50121, 50118, 44908, 230, 35, 47, 216, 2156, 37463, 2156, 16, 5, 129, 1219, 25522, 417, 32062, 6920, 254, 24303, 4954, 2156, 98, 2156, 38, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 8135, 10490, 1935, 9404, 335, 31, 383, 101, 5, 3018, 8, 1068, 3092, 6, 17456, 88, 10, 278, 9, 568, 32833, 6, 215, 25, 5, 4789, 73, 22130, 73, 19186, 32388, 36, 33207, 43, 34533, 4, 96, 143, 1989, 1068, 6, 144, 9, 5, 39512, 40, 45, 28, 4249, 7, 5, 576, 5377, 4, 9068, 6, 51, 40, 1169, 33, 7, 28, 3349, 37236, 10, 41834, 118, 6, 50, 129, 10, 37105, 9, 5, 678, 568, 32833, 40, 28, 43547, 11, 349, 5852, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.2878, 'rouge2': 10.1313, 'rougeL': 21.6782, 'rougeLsum': 28.5132}, 'ppl': {'perplexity': 8.6602, 'ref_perplexity': 19.2652}, 'bertscore': {'precision': 87.1732, 'recall': 86.4561, 'f1': 86.8062}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.3382, 'rouge2': 11.0413, 'rougeL': 21.9782, 'rougeLsum': 29.7152}, 'ppl': {'perplexity': 6.8043, 'ref_perplexity': 17.585}, 'bertscore': {'precision': 87.6802, 'recall': 86.6973, 'f1': 87.1784}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.9675, 'rouge2': 10.7127, 'rougeL': 22.5205, 'rougeLsum': 29.5472}, 'ppl': {'perplexity': 7.6057, 'ref_perplexity': 17.3319}, 'bertscore': {'precision': 87.7224, 'recall': 86.7356, 'f1': 87.2177}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.7932, 'rouge2': 10.5977, 'rougeL': 22.0982, 'rougeLsum': 29.4816}, 'ppl': {'perplexity': 5.8733, 'ref_perplexity': 17.3915}, 'bertscore': {'precision': 87.5221, 'recall': 86.7508, 'f1': 87.1274}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.0706, 'rouge2': 10.9562, 'rougeL': 22.4486, 'rougeLsum': 29.791}, 'ppl': {'perplexity': 7.6035, 'ref_perplexity': 17.718}, 'bertscore': {'precision': 87.541, 'recall': 86.8186, 'f1': 87.1704}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 3
Start Training!
Sample 673 of the training set: {'input_ids': [0, 38182, 3916, 2072, 5, 3221, 59, 304, 1200, 4, 47385, 50121, 50118, 33347, 4827, 35, 38, 697, 7252, 38, 697, 235, 420, 5, 2014, 31, 41, 490, 980, 11, 886, 479, 166, 33, 20176, 17467, 141, 462, 70, 5, 86, 479, 407, 38, 269, 2254, 49, 49, 6970, 2156, 47, 51, 214, 269, 2721, 3122, 479, 256, 119, 479, 25522, 31375, 1536, 9834, 24303, 8487, 7252, 2156, 1375, 15, 7, 2829, 55, 1473, 2682, 479, 166, 214, 6908, 1067, 59, 695, 9083, 479, 12698, 52, 33, 10, 891, 25522, 31375, 1536, 9834, 24303, 52, 1017, 101, 7, 1331, 24, 13, 59, 10328, 292, 5122, 19, 5, 1963, 4374, 9, 7252, 23843, 153, 5122, 7252, 31, 84, 647, 8, 142, 42, 16, 215, 25522, 417, 32062, 6920, 254, 24303, 42, 16, 13, 2384, 24, 18, 10, 25522, 417, 32062, 6920, 254, 24303, 52, 33, 10, 210, 1186, 9, 3742, 2156, 101, 24, 18, 41, 758, 210, 1186, 2156, 52, 218, 75, 33, 7, 4022, 59, 16047, 479, 12698, 11, 645, 7, 146, 10, 1963, 9, 42, 11259, 2156, 52, 240, 7, 7252, 28, 441, 7, 2592, 349, 65, 23, 10, 4532, 9, 11971, 14865, 5122, 479, 407, 52, 214, 2183, 24, 13, 2330, 99, 52, 1017, 101, 7, 2592, 24, 13, 479, 8487, 7252, 2156, 95, 7, 5368, 10, 410, 828, 9, 3221, 59, 5, 695, 7252, 2156, 38, 115, 25522, 417, 32062, 6920, 254, 24303, 38, 1017, 101, 7, 1798, 59, 110, 3734, 634, 4533, 6063, 5656, 2156, 7252, 110, 78, 2956, 59, 7252, 2351, 10, 92, 910, 6063, 797, 2156, 99, 74, 28, 5, 275, 7252, 101, 47, 25522, 417, 32062, 6920, 254, 24303, 99, 32, 5, 1575, 14, 47, 269, 101, 99, 32, 5, 1575, 14, 47, 218, 75, 101, 2156, 4753, 5906, 102, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 12698, 38, 4157, 77, 89, 18, 101, 237, 430, 14893, 8, 47, 33, 7, 1228, 7, 888, 1004, 15, 5, 255, 1215, 846, 1215, 101, 47, 33, 7, 109, 65, 13, 5, 476, 9, 5, 255, 1215, 846, 1215, 8, 172, 101, 277, 65, 7, 120, 5, 3031, 2441, 15, 8, 402, 1493, 7, 120, 24, 70, 164, 2156, 38, 218, 75, 216, 479, 978, 51, 489, 15224, 70, 430, 6398, 17467, 561, 2156, 8, 38, 218, 75, 216, 114, 38, 4784, 101, 14, 128, 27037, 38, 619, 101, 47, 253, 62, 19, 15778, 37636, 479, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 38, 95, 23126, 1183, 5, 255, 1215, 846, 1215, 25522, 31375, 1536, 9834, 24303, 7252, 479, 50121, 50118, 33347, 4827, 35, 44258, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 14947, 1516, 685, 479, 993, 2345, 9, 101, 2187, 7, 244, 47, 465, 24, 479, 50121, 50118, 44518, 39322, 35, 38, 348, 341, 2156, 38, 348, 341, 6063, 5656, 2156, 13, 383, 101, 255, 1215, 846, 1215, 8, 5, 230, 1215, 495, 1215, 869, 8, 569, 32017, 8, 38, 38, 4443, 51, 214, 51, 214, 1256, 19427, 19427, 410, 3270, 37463, 479, 370, 218, 75, 33, 7, 120, 62, 8, 1656, 420, 5, 929, 7, 464, 10, 4238, 479, 50121, 50118, 33347, 4827, 35, 256, 119, 479, 50121, 50118, 44518, 39322, 35, 407, 941, 114, 47, 214, 951, 269, 22414, 101, 162, 51, 51, 214, 1256, 2579, 479, 12698, 479, 38, 465, 106, 25522, 417, 32062, 6920, 254, 24303, 51, 64, 28, 10, 828, 19887, 2156, 941, 2156, 101, 47, 216, 114, 38, 437, 2494, 255, 1215, 846, 1215, 38, 33, 33, 7, 33, 130, 2559, 6063, 5656, 9, 25522, 417, 32062, 6920, 254, 24303, 11, 760, 9, 162, 2156, 47, 216, 2156, 65, 13, 5, 255, 1215, 846, 1215, 2156, 65, 13, 5, 1778, 2233, 2156, 65, 13, 475, 5, 569, 32017, 25, 157, 479, 12698, 479, 178, 67, 51, 3805, 7, 51, 3805, 7, 28, 10, 828, 15868, 2156, 51, 348, 300, 350, 171, 14893, 15, 106, 37463, 350, 350, 2345, 9, 350, 2345, 9, 6336, 77, 70, 38, 269, 23126, 109, 16, 5405, 15, 8, 160, 2156, 464, 5, 4238, 2156, 464, 5, 3149, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 7252, 479, 38, 2854, 19, 519, 350, 171, 6398, 17467, 198, 479, 1308, 4252, 34, 10, 1086, 31807, 23, 184, 9, 6398, 17467, 13, 1337, 383, 2156, 8, 38, 218, 75, 216, 141, 7, 173, 457, 9, 106, 7252, 479, 653, 18, 505, 13, 162, 2156, 38, 4443, 2156, 16, 14, 24, 18, 1365, 7, 304, 8, 14, 89, 18, 45, 350, 171, 14893, 2156, 51, 32, 45, 350, 650, 2156, 47, 216, 47, 216, 47, 240, 7, 295, 7, 216, 99, 47, 214, 608, 479, 178, 65, 631, 38, 1605, 101, 16, 114, 47, 32, 45, 7252, 2345, 9, 1375, 24, 198, 7, 120, 24, 7, 173, 19, 5, 4047, 763, 12, 2050, 479, 50121, 50118, 10006, 16382, 35, 8976, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 10006, 16382, 21, 7428, 9, 350, 171, 14893, 19, 430, 8047, 12712, 6, 53, 648, 37, 30329, 549, 15224, 70, 430, 6398, 17467, 561, 21, 2139, 142, 37, 1299, 24, 1249, 62, 19, 15778, 37636, 4, 27913, 39322, 802, 6063, 5656, 58, 1256, 2579, 53, 115, 28, 10, 828, 19887, 142, 1434, 399, 75, 33, 7, 120, 62, 8, 1656, 420, 5, 929, 7, 464, 10, 4238, 4, 635, 6, 130, 2559, 6063, 5656, 19, 350, 171, 14893, 13, 1012, 115, 28, 15868, 8, 6336, 4, 6741, 31275, 1507, 19, 519, 350, 171, 6398, 17467, 198, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.1342, 'rouge2': 11.8418, 'rougeL': 22.6919, 'rougeLsum': 28.5476}, 'ppl': {'perplexity': 2.7933, 'ref_perplexity': 11.8231}, 'bertscore': {'precision': 88.1995, 'recall': 86.6576, 'f1': 87.4106}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.2388, 'rouge2': 13.9579, 'rougeL': 24.2744, 'rougeLsum': 30.701}, 'ppl': {'perplexity': 9.1058, 'ref_perplexity': 11.1325}, 'bertscore': {'precision': 88.5636, 'recall': 86.8288, 'f1': 87.6784}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.0374, 'rouge2': 13.783, 'rougeL': 24.4992, 'rougeLsum': 31.3318}, 'ppl': {'perplexity': 10.295, 'ref_perplexity': 10.9813}, 'bertscore': {'precision': 88.361, 'recall': 87.0428, 'f1': 87.686}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.726, 'rouge2': 14.3065, 'rougeL': 25.0243, 'rougeLsum': 31.6712}, 'ppl': {'perplexity': 4.0794, 'ref_perplexity': 11.2097}, 'bertscore': {'precision': 88.0351, 'recall': 87.1401, 'f1': 87.5727}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.7916, 'rouge2': 13.8884, 'rougeL': 24.4119, 'rougeLsum': 31.7666}, 'ppl': {'perplexity': 3.5339, 'ref_perplexity': 11.0104}, 'bertscore': {'precision': 88.238, 'recall': 87.2163, 'f1': 87.7123}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 4
Start Training!
Sample 318 of the training set: {'input_ids': [0, 2264, 222, 925, 3848, 83, 12968, 1054, 224, 59, 915, 116, 47385, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 8487, 4, 178, 5, 97, 477, 6, 9, 768, 6, 16, 5, 915, 4, 318, 5, 1621, 16, 164, 7, 28, 1800, 11, 1110, 9, 5, 4374, 9, 4881, 6585, 14057, 6, 172, 24, 782, 7, 1391, 8, 323, 5, 2163, 4, 6319, 47, 156, 41, 4990, 9, 5, 672, 9, 915, 956, 7, 5731, 42, 563, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 4787, 27824, 40, 28, 505, 4, 166, 855, 109, 146, 3227, 11, 10, 346, 9, 911, 14, 12155, 7, 920, 474, 3489, 6, 8, 6, 9, 768, 6, 14057, 8, 19755, 11, 1989, 4, 407, 6, 5, 864, 9, 5032, 27824, 16, 505, 4, 978, 6, 52, 64, 75, 30127, 41, 7833, 1280, 9, 5799, 14, 40, 28, 956, 7, 2438, 454, 52, 216, 2230, 99, 18, 164, 7, 283, 66, 9, 5, 9434, 8, 99, 2163, 52, 429, 236, 7, 2438, 7, 10, 2388, 3093, 11, 5295, 4, 83, 1955, 9, 984, 398, 153, 7, 984, 698, 153, 10, 76, 34, 57, 1971, 196, 198, 25, 10, 4007, 761, 9, 443, 9, 99, 52, 429, 240, 7, 3754, 6, 53, 14, 74, 240, 7, 28, 4777, 31, 2210, 8864, 4, 166, 240, 7, 356, 23, 2210, 8864, 6, 141, 2375, 51, 32, 4, 2615, 52, 146, 106, 55, 2375, 116, 2615, 52, 120, 357, 923, 31, 106, 116, 178, 89, 189, 157, 28, 10, 403, 13, 92, 915, 6, 8, 14, 18, 10, 864, 6, 9, 768, 6, 14, 74, 240, 7, 28, 3373, 19, 18752, 77, 52, 214, 5591, 5, 507, 1860, 4, 50121, 50118, 487, 15322, 4350, 35, 125, 38, 206, 10, 762, 6077, 25, 157, 16, 52, 416, 216, 89, 16, 915, 420, 474, 6904, 11, 103, 761, 9, 14057, 12, 3368, 518, 4, 407, 6, 38, 206, 99, 52, 269, 240, 7, 206, 59, 420, 5295, 16, 141, 52, 64, 1305, 2388, 3189, 6, 141, 52, 64, 356, 23, 595, 8864, 11, 1110, 9, 442, 686, 14, 51, 214, 357, 15423, 6, 8, 141, 52, 64, 146, 686, 14, 52, 214, 67, 5523, 62, 15, 5, 2210, 1915, 8, 2148, 66, 89, 25, 157, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 178, 473, 5, 672, 9, 915, 6723, 15, 99, 5, 1002, 16, 578, 12196, 5, 724, 16, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 38, 218, 75, 206, 47, 64, 4784, 95, 3104, 5, 80, 4, 20, 696, 9, 5032, 27824, 16, 65, 14, 18, 89, 26670, 9, 549, 52, 2807, 7, 342, 10, 1002, 11, 317, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 125, 141, 74, 52, 216, 14, 24, 18, 145, 341, 4296, 114, 89, 965, 75, 402, 7, 4374, 13, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 6834, 3291, 47, 124, 7, 5, 864, 59, 10437, 4, 166, 240, 4692, 10437, 9, 5, 1337, 8864, 14, 52, 33, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 3216, 6, 53, 396, 10, 1002, 6, 141, 64, 47, 5083, 10516, 116, 318, 47, 218, 75, 216, 99, 47, 214, 667, 7, 109, 6, 141, 64, 47, 5083, 10516, 116, 24820, 6, 47, 214, 490, 7, 9622, 59, 519, 10, 1002, 6, 61, 16, 372, 4, 11258, 47, 2854, 14, 1621, 115, 304, 5, 903, 14, 18, 145, 2622, 149, 5, 15657, 15, 3793, 6696, 1567, 103, 9, 209, 1170, 7, 578, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 2647, 6, 9, 768, 6, 89, 32, 103, 41593, 40288, 14, 32, 567, 7, 5, 12093, 1621, 25, 233, 9, 5, 15657, 15, 2628, 25557, 3793, 6696, 4, 280, 1435, 6, 9, 768, 6, 16, 540, 87, 52, 56, 5291, 6, 8, 14, 6771, 6, 888, 6, 10, 1282, 527, 142, 539, 16, 3114, 11539, 6, 8, 98, 5, 1280, 9, 4696, 11, 3793, 6696, 16, 416, 1158, 7, 7280, 6, 61, 16, 10, 205, 631, 4, 125, 7, 110, 864, 35, 50141, 17276, 52, 304, 5, 1435, 116, 2647, 6, 9, 768, 52, 197, 304, 1435, 4, 38, 437, 45, 5636, 11, 5976, 9, 43851, 700, 41684, 6, 38, 206, 38, 437, 55, 2509, 11, 5, 32367, 9, 5799, 14, 1411, 88, 285, 474, 8864, 87, 88, 14612, 5799, 4, 345, 32, 6, 9, 768, 6, 10, 346, 9, 5287, 14, 52, 855, 1391, 149, 5, 937, 903, 4, 178, 77, 38, 206, 59, 14057, 6, 38, 218, 75, 95, 206, 59, 5, 3487, 650, 14612, 5353, 9, 418, 14, 283, 11, 149, 3046, 1300, 6, 53, 38, 206, 59, 5, 32367, 9, 5, 984, 406, 325, 52, 1930, 11, 474, 8, 592, 575, 8, 141, 52, 64, 24222, 8, 4238, 103, 9, 14, 1567, 4007, 8555, 5287, 11, 937, 6, 8, 1567, 12985, 145, 19755, 8, 14057, 11, 1989, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 370, 214, 584, 14, 24, 18, 540, 87, 421, 4, 9918, 47, 492, 201, 143, 761, 9, 1955, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 38, 437, 6661, 6, 115, 47, 7230, 5, 864, 116, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 370, 224, 14, 89, 16, 540, 418, 14, 18, 283, 11, 149, 209, 41593, 40288, 31, 5, 15657, 6, 64, 47, 4521, 103, 2345, 9, 1955, 116, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 20, 1955, 14, 38, 33, 11, 1508, 16, 59, 984, 4419, 153, 14, 18, 567, 11, 11, 1110, 9, 903, 81, 10, 80, 12, 180, 675, 4, 125, 38, 1017, 33, 7, 4559, 14, 19, 5, 1540, 4, 653, 5, 5291, 578, 4, 520, 5, 4696, 15657, 21, 78, 1146, 11, 6, 89, 21, 103, 26471, 23, 987, 672, 59, 99, 672, 9, 903, 14, 74, 836, 6, 53, 24, 21, 716, 15, 5, 1280, 9, 4696, 14, 21, 855, 172, 11, 6696, 8, 5, 754, 14, 5, 4696, 34, 2906, 11, 6696, 6, 38, 1266, 6, 5, 746, 1280, 577, 7, 5, 987, 16, 540, 8, 12113, 84, 41593, 40288, 32, 540, 4, 7738, 189, 33, 103, 12548, 2415, 4, 50121, 50118, 487, 15322, 4350, 35, 3216, 6, 38, 21, 164, 7, 224, 6, 89, 21, 10, 1084, 12, 180, 266, 626, 147, 5, 15657, 34, 1179, 984, 6115, 153, 7, 1248, 187, 567, 88, 1370, 11, 587, 6, 8, 5, 1461, 1914, 21, 984, 31271, 153, 10, 76, 4, 407, 6, 38, 206, 14, 924, 5, 1280, 9, 173, 14, 18, 57, 626, 30, 539, 198, 3114, 11264, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 178, 5, 41593, 40288, 9, 14, 116, 280, 16, 5, 35990, 578, 29254, 6115, 153, 4, 50121, 50118, 487, 15322, 4350, 35, 374, 10, 987, 672, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 3216, 6, 98, 99, 18, 5, 12093, 35990, 116, 50121, 50118, 34052, 858, 3864, 21851, 3326, 35, 22525, 12, 13664, 4, 50121, 50118, 104, 811, 272, 11760, 890, 811, 3326, 35, 22525, 12, 13664, 116, 272, 5212, 6, 14, 4428, 10, 319, 4, 24820, 6, 24, 18, 10, 205, 6797, 9, 418, 8, 47, 214, 1686, 59, 4848, 984, 398, 153, 7, 984, 698, 153, 4, 407, 6, 3334, 6, 47, 216, 6, 52, 64, 28, 55, 8263, 142, 89, 16, 418, 11, 14, 4728, 114, 14, 418, 21, 3758, 12, 506, 19867, 13, 42, 1989, 3552, 4, 50121, 50118, 14043, 3848, 83, 12968, 1054, 35, 2647, 6, 5, 5799, 16, 164, 7, 28, 10, 588, 696, 14, 52, 240, 7, 1100, 6, 8, 38, 206, 25, 7738, 34, 26, 6, 89, 16, 1435, 9, 1337, 5287, 855, 11, 5, 467, 6, 8, 52, 240, 7, 356, 23, 14, 8, 146, 14, 25, 2375, 25, 678, 4, 2290, 89, 28, 10, 240, 13, 103, 943, 5799, 116, 345, 189, 157, 28, 6, 8, 14, 18, 10, 864, 14, 52, 581, 33, 7, 356, 23, 11, 1110, 9, 5, 1860, 77, 52, 2179, 24, 8, 33, 10, 3221, 19, 18752, 59, 5, 672, 9, 5032, 27824, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 19923, 1757, 956, 7, 5731, 5, 563, 21, 55, 59, 5032, 27824, 4, 925, 3848, 83, 12968, 1054, 26, 51, 855, 222, 146, 3227, 11, 10, 346, 9, 911, 14, 1330, 7, 920, 474, 3489, 6, 8, 6, 9, 768, 6, 14057, 8, 19755, 11, 1989, 4, 178, 122, 6, 309, 7, 2210, 8864, 6, 10, 1955, 9, 984, 398, 153, 7, 984, 698, 153, 10, 76, 56, 57, 1971, 2550, 198, 25, 10, 4007, 761, 9, 443, 9, 99, 51, 429, 240, 4, 925, 3848, 83, 12968, 1054, 67, 342, 3887, 11, 2053, 59, 5, 32367, 9, 5, 984, 406, 325, 51, 1240, 11, 474, 8, 592, 575, 8, 141, 51, 115, 24222, 8, 4238, 103, 9, 14, 1567, 4007, 8555, 5287, 11, 937, 6, 8, 1567, 12985, 145, 19755, 8, 14057, 11, 1989, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.3988, 'rouge2': 11.7097, 'rougeL': 22.9207, 'rougeLsum': 29.8102}, 'ppl': {'perplexity': 4.2713, 'ref_perplexity': 15.9177}, 'bertscore': {'precision': 87.5239, 'recall': 86.8438, 'f1': 87.1701}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.1067, 'rouge2': 11.9087, 'rougeL': 23.6189, 'rougeLsum': 29.967}, 'ppl': {'perplexity': 6.0806, 'ref_perplexity': 14.7591}, 'bertscore': {'precision': 87.8957, 'recall': 86.8497, 'f1': 87.3581}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.0429, 'rouge2': 12.1992, 'rougeL': 23.5747, 'rougeLsum': 30.641}, 'ppl': {'perplexity': 8.2307, 'ref_perplexity': 14.5442}, 'bertscore': {'precision': 87.9902, 'recall': 87.0368, 'f1': 87.4986}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.0142, 'rouge2': 12.4084, 'rougeL': 24.9915, 'rougeLsum': 31.617}, 'ppl': {'perplexity': 3.8546, 'ref_perplexity': 14.0562}, 'bertscore': {'precision': 87.7911, 'recall': 87.2102, 'f1': 87.4917}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.8258, 'rouge2': 12.9248, 'rougeL': 25.181, 'rougeLsum': 32.0799}, 'ppl': {'perplexity': 4.2143, 'ref_perplexity': 13.9558}, 'bertscore': {'precision': 88.2824, 'recall': 87.3402, 'f1': 87.7984}}
Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x7a57b6b7c430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x78fdd38a1550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x77b4c30c2820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x71d0e08e0550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 4
Start Training!
Sample 348 of the training set: {'input_ids': [0, 2264, 222, 5, 529, 2268, 59, 5, 1797, 551, 13, 5, 896, 1035, 1315, 586, 116, 47385, 50121, 50118, 35846, 4, 5031, 2091, 8650, 36, 19877, 459, 611, 10627, 35057, 8923, 25666, 1344, 574, 9578, 6, 37389, 3256, 3837, 47, 6, 427, 4, 31602, 4, 1437, 993, 11006, 4, 453, 35, 4936, 6, 2489, 328, 1437, 8768, 4, 5031, 2091, 8650, 35, 38, 524, 4343, 7, 3594, 5, 343, 9, 226, 9578, 6, 142, 24, 16, 10, 9951, 6, 427, 4, 31602, 4, 85, 16, 5, 343, 147, 34855, 33098, 4762, 267, 1120, 1344, 6, 54, 23498, 5, 1154, 613, 18777, 2079, 6, 21, 2421, 4, 38, 1034, 47, 40, 45, 185, 14, 409, 31, 127, 86, 4, 1308, 864, 16, 13, 5, 692, 9, 17820, 6, 6011, 9091, 2717, 8, 33599, 96, 27953, 4, 374, 587, 398, 6, 5, 1269, 585, 1797, 13, 5, 896, 1035, 1315, 586, 4, 166, 802, 24, 21, 7, 1477, 5, 586, 6, 53, 64, 5, 1269, 4559, 14, 5, 1229, 2442, 5, 276, 227, 587, 406, 8, 466, 116, 50121, 50118, 35846, 4, 1653, 2560, 13559, 90, 10344, 36, 20086, 4742, 9, 17820, 6, 6011, 9091, 2717, 8, 33599, 96, 27953, 3256, 427, 4, 8381, 6, 38, 64, 4559, 14, 52, 33, 1130, 5, 5007, 10256, 13, 896, 1294, 1315, 4, 166, 33, 355, 18634, 38935, 6, 217, 5, 1460, 7, 33, 233, 12, 958, 173, 8, 5, 1460, 7, 33, 5, 1315, 3112, 454, 902, 9, 220, 76, 4, 280, 26, 6, 5, 1229, 21, 45, 1714, 4, 85, 1189, 23, 1510, 6, 151, 633, 15155, 11217, 13, 1510, 6, 151, 521, 6, 19, 727, 207, 9, 106, 145, 28397, 23, 727, 2153, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 35846, 4, 1653, 2560, 13559, 90, 10344, 36, 20086, 4742, 9, 17820, 6, 6011, 9091, 2717, 8, 33599, 96, 27953, 43, 26, 14, 24, 115, 28, 1474, 14, 5, 168, 74, 712, 5, 5007, 10256, 13, 896, 1294, 1315, 4, 20, 586, 56, 355, 18634, 38935, 6, 217, 5, 1460, 7, 33, 233, 12, 958, 173, 8, 5, 1460, 7, 33, 5, 1315, 3112, 454, 902, 9, 220, 76, 4, 280, 26, 6, 5, 1229, 21, 45, 1714, 4, 85, 1189, 23, 1510, 6, 151, 633, 15155, 11217, 13, 1510, 6, 151, 521, 6, 19, 727, 207, 9, 106, 145, 28397, 23, 727, 2153, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 30.6364, 'rouge2': 11.0184, 'rougeL': 21.6361, 'rougeLsum': 26.626}, 'ppl': {'perplexity': 4.7708, 'ref_perplexity': 15.4504}, 'bertscore': {'precision': 88.0422, 'recall': 86.1722, 'f1': 87.0847}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.5096, 'rouge2': 12.8106, 'rougeL': 24.5077, 'rougeLsum': 31.1885}, 'ppl': {'perplexity': 3.2582, 'ref_perplexity': 14.3815}, 'bertscore': {'precision': 88.0502, 'recall': 86.9647, 'f1': 87.4933}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.6056, 'rouge2': 11.9318, 'rougeL': 23.7746, 'rougeLsum': 30.4181}, 'ppl': {'perplexity': 6.6315, 'ref_perplexity': 14.0303}, 'bertscore': {'precision': 88.077, 'recall': 86.8248, 'f1': 87.4341}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.2592, 'rouge2': 12.633, 'rougeL': 24.5822, 'rougeLsum': 31.7949}, 'ppl': {'perplexity': 4.375, 'ref_perplexity': 13.6691}, 'bertscore': {'precision': 88.2301, 'recall': 87.2278, 'f1': 87.7163}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.7901, 'rouge2': 12.6788, 'rougeL': 24.4279, 'rougeLsum': 31.8329}, 'ppl': {'perplexity': 4.7994, 'ref_perplexity': 13.909}, 'bertscore': {'precision': 88.0997, 'recall': 87.2292, 'f1': 87.652}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 5
Start Training!
Sample 8 of the training set: {'input_ids': [0, 38182, 3916, 2072, 5, 6427, 9, 5, 2259, 5043, 77, 7345, 1229, 18442, 4, 47385, 50121, 50118, 10006, 16382, 35, 5534, 2156, 2067, 10, 2289, 479, 16269, 5385, 17487, 653, 16, 10, 7728, 5385, 17487, 1534, 14, 5568, 1122, 7, 99, 52, 236, 17487, 50121, 50118, 33347, 4827, 35, 85, 115, 157, 28, 2156, 50121, 50118, 44518, 39322, 35, 256, 119, 117, 50121, 50118, 33347, 4827, 35, 53, 23, 10, 701, 9, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 14, 18, 14, 2236, 1263, 631, 14, 52, 300, 5, 1047, 59, 479, 50121, 50118, 36926, 13700, 31275, 35, 32125, 237, 479, 50121, 50118, 10006, 16382, 35, 8976, 479, 50121, 50118, 44518, 39322, 35, 125, 38, 802, 24, 21, 95, 2198, 31321, 479, 50121, 50118, 10006, 16382, 35, 370, 300, 10, 1047, 59, 2236, 1263, 17487, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 38, 222, 45, 2156, 50121, 50118, 44518, 39322, 35, 41881, 479, 50121, 50118, 10006, 16382, 35, 98, 479, 50121, 50118, 44518, 39322, 35, 163, 939, 5072, 24, 21, 25522, 29183, 24303, 584, 14, 84, 22905, 56, 283, 62, 19, 10, 6638, 14, 47, 115, 2156, 47, 216, 2156, 224, 20760, 7, 2156, 8, 24, 74, 224, 20760, 124, 11, 10, 5192, 2182, 2236, 25522, 29183, 24303, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 33347, 4827, 35, 8487, 2156, 11380, 52, 581, 2299, 351, 75, 213, 19, 14, 65, 479, 50121, 50118, 10006, 16382, 35, 166, 351, 75, 213, 19, 14, 65, 2156, 222, 47, 224, 17487, 50121, 50118, 33347, 4827, 35, 8976, 2156, 14, 18, 2236, 4972, 2156, 98, 479, 50121, 50118, 10006, 16382, 35, 38, 1266, 38, 25522, 417, 32062, 6920, 254, 24303, 52, 25522, 417, 32062, 6920, 254, 24303, 8487, 2156, 8578, 479, 50121, 50118, 33347, 4827, 35, 12698, 479, 407, 2156, 8578, 11380, 2156, 3822, 2299, 2156, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 407, 24, 1326, 101, 52, 214, 6908, 120, 7495, 9, 5, 1086, 18327, 102, 25522, 31375, 1536, 9834, 24303, 18327, 2630, 631, 479, 50121, 50118, 33347, 4827, 35, 85, 1326, 101, 24, 3867, 52, 64, 3616, 7, 342, 24, 11, 223, 477, 80, 24630, 2156, 7252, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 8487, 479, 25522, 31375, 1536, 9834, 24303, 8487, 479, 50121, 50118, 44518, 39322, 35, 256, 119, 12, 298, 5471, 479, 50121, 50118, 33347, 4827, 35, 5359, 190, 7019, 157, 14223, 11380, 2156, 1256, 203, 477, 80, 24630, 2156, 38, 1017, 224, 479, 407, 52, 581, 989, 14, 65, 13, 122, 479, 25522, 29183, 24303, 52, 581, 95, 33, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 3945, 52, 164, 13, 10, 780, 7705, 23, 70, 17487, 50121, 50118, 33347, 4827, 35, 85, 18, 37463, 10, 403, 9, 7252, 38, 437, 37463, 2829, 17118, 479, 509, 25522, 417, 32062, 6920, 254, 24303, 477, 292, 9, 10, 5122, 13, 65, 1920, 6148, 630, 75, 2369, 1341, 235, 479, 407, 2085, 24, 18, 10, 403, 9, 10, 1920, 6148, 16, 2085, 65, 50, 55, 479, 12698, 479, 50121, 50118, 44518, 39322, 35, 256, 119, 479, 50121, 50118, 36926, 13700, 31275, 35, 2647, 50121, 50118, 33347, 4827, 35, 497, 61, 477, 114, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 38, 21, 25522, 29183, 24303, 13, 10, 403, 479, 1793, 56, 47, 416, 14948, 14, 17487, 50121, 50118, 10006, 16382, 35, 5534, 2156, 780, 7705, 13, 5, 403, 479, 50121, 50118, 33347, 4827, 35, 2647, 47, 300, 477, 292, 89, 479, 85, 18, 5909, 10, 403, 9, 549, 50, 45, 42, 16, 4577, 479, 38, 437, 45, 1341, 686, 114, 51, 214, 25522, 417, 32062, 6920, 254, 24303, 38, 218, 75, 206, 51, 1266, 477, 292, 24630, 228, 6148, 479, 50121, 50118, 44518, 39322, 35, 8487, 2156, 157, 50121, 50118, 10006, 16382, 35, 256, 119, 12, 298, 5471, 479, 50121, 50118, 44518, 39322, 35, 784, 905, 18, 224, 14, 8, 172, 52, 64, 33, 84, 780, 26854, 403, 50121, 50118, 33347, 4827, 35, 407, 50121, 50118, 44518, 39322, 35, 8, 172, 52, 23, 513, 33, 25522, 417, 32062, 6920, 254, 24303, 146, 24, 10, 410, 4851, 7, 2217, 479, 50121, 50118, 33347, 4827, 35, 345, 52, 213, 479, 50121, 50118, 44518, 39322, 35, 3047, 144, 475, 144, 6398, 17467, 32, 10, 5342, 33676, 219, 7705, 14, 1516, 44502, 4628, 223, 143, 12177, 9, 28342, 11, 10, 1207, 929, 479, 50121, 50118, 10006, 16382, 35, 305, 99, 18, 5, 6814, 7705, 17487, 735, 50, 909, 17487, 50121, 50118, 33347, 4827, 35, 1378, 18, 1153, 5, 2340, 7705, 47, 1017, 224, 2156, 50121, 50118, 44518, 39322, 35, 1793, 10521, 479, 50121, 50118, 33347, 4827, 35, 11380, 479, 50121, 50118, 10006, 16382, 35, 5143, 479, 50121, 50118, 33347, 4827, 35, 38, 1341, 101, 14, 7705, 14, 47, 214, 23366, 154, 89, 2156, 50121, 50118, 44518, 39322, 35, 14708, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 33347, 4827, 35, 24, 18, 37463, 2299, 13, 146, 24, 20059, 11, 5, 2933, 190, 357, 479, 50121, 50118, 10006, 16382, 35, 8976, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 44518, 39322, 174, 5, 165, 14, 5, 2365, 56, 1276, 7, 14518, 10, 2236, 4972, 6638, 88, 5, 6063, 797, 98, 14, 5, 165, 56, 7, 11022, 277, 5448, 13, 1434, 7, 12982, 5, 6063, 797, 683, 24, 300, 685, 11, 10, 929, 4, 20, 165, 1276, 7, 146, 5, 6063, 797, 10, 780, 7705, 4, 2276, 6, 5, 6063, 797, 74, 28, 441, 7, 32124, 11, 5, 1207, 929, 4, 3728, 4827, 617, 1850, 14, 5, 165, 115, 67, 146, 5, 6063, 797, 20059, 11, 5, 2933, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 30.9111, 'rouge2': 12.4024, 'rougeL': 23.1173, 'rougeLsum': 27.4815}, 'ppl': {'perplexity': 4.3138, 'ref_perplexity': 11.6603}, 'bertscore': {'precision': 88.964, 'recall': 85.9287, 'f1': 87.4042}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.9013, 'rouge2': 13.3368, 'rougeL': 24.6185, 'rougeLsum': 31.0164}, 'ppl': {'perplexity': 3.9767, 'ref_perplexity': 11.1068}, 'bertscore': {'precision': 87.8244, 'recall': 86.5714, 'f1': 87.1801}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.225, 'rouge2': 14.8914, 'rougeL': 25.8225, 'rougeLsum': 32.3181}, 'ppl': {'perplexity': 6.1597, 'ref_perplexity': 10.8109}, 'bertscore': {'precision': 88.5324, 'recall': 86.8397, 'f1': 87.6672}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.9495, 'rouge2': 13.7962, 'rougeL': 24.3436, 'rougeLsum': 31.0312}, 'ppl': {'perplexity': 5.3543, 'ref_perplexity': 10.7782}, 'bertscore': {'precision': 88.2014, 'recall': 86.5262, 'f1': 87.3429}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.4808, 'rouge2': 14.1084, 'rougeL': 24.7678, 'rougeLsum': 31.6445}, 'ppl': {'perplexity': 5.1222, 'ref_perplexity': 10.9541}, 'bertscore': {'precision': 88.3562, 'recall': 86.7336, 'f1': 87.526}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 6
Start Training!
Sample 394 of the training set: {'input_ids': [0, 2264, 222, 5, 6020, 206, 59, 5947, 33445, 116, 47385, 50121, 50118, 44385, 163, 35, 4954, 2156, 372, 479, 407, 25522, 44970, 24303, 78, 9, 70, 2156, 38, 2854, 14, 7252, 52, 197, 6198, 33445, 2156, 8, 386, 2746, 69, 479, 20847, 582, 13, 5, 86, 79, 128, 29, 342, 11, 25, 157, 479, 12698, 2156, 109, 47, 216, 2230, 141, 7, 109, 14, 2156, 50, 16, 37463, 226, 4882, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 47, 216, 99, 2230, 109, 52, 109, 7, 25522, 417, 32062, 6920, 254, 24303, 7, 342, 69, 15, 5, 10984, 11, 103, 169, 17487, 50121, 50118, 44908, 211, 35, 38, 128, 119, 2198, 36776, 2156, 53, 38, 128, 119, 2882, 7, 1532, 479, 50121, 50118, 44385, 163, 35, 4954, 479, 2647, 2156, 47, 128, 890, 33, 7, 479, 5143, 479, 407, 6992, 2156, 7252, 50121, 50118, 44908, 211, 35, 234, 50121, 50118, 44385, 163, 35, 407, 596, 218, 75, 47, 37463, 1394, 226, 4882, 8, 192, 99, 79, 161, 59, 47, 216, 2230, 99, 52, 109, 13, 951, 11, 3553, 50121, 50118, 44908, 211, 35, 9067, 111, 1907, 5015, 2156, 50121, 50118, 44385, 163, 35, 2647, 2156, 11380, 79, 128, 29, 542, 79, 128, 29, 45, 10, 25522, 417, 32062, 6920, 254, 24303, 10, 1294, 2156, 50121, 50118, 44908, 211, 35, 50, 25522, 417, 32062, 6920, 254, 24303, 17487, 50121, 50118, 44385, 163, 35, 79, 95, 8505, 53, 6992, 479, 50121, 50118, 44908, 211, 35, 44258, 479, 50121, 50118, 44385, 163, 35, 407, 939, 114, 25522, 417, 32062, 6920, 254, 24303, 8976, 2156, 38, 2854, 2156, 79, 12020, 2051, 2156, 79, 10, 888, 21, 25522, 44970, 24303, 37463, 2156, 55, 37463, 2156, 1455, 8, 2682, 87, 25522, 417, 32062, 6920, 254, 24303, 87, 79, 21, 11, 1607, 2156, 98, 79, 222, 10, 357, 633, 87, 38, 74, 33, 28286, 31, 95, 1686, 7, 69, 479, 50121, 50118, 44908, 211, 35, 8976, 479, 50121, 50118, 44385, 163, 35, 407, 38, 206, 14, 128, 29, 372, 479, 50121, 50118, 44908, 211, 35, 152, 16, 2345, 9, 99, 38, 851, 69, 2156, 98, 42, 16, 13, 1246, 1368, 141, 7, 120, 7, 5, 1294, 1789, 2156, 50121, 50118, 44385, 163, 35, 8976, 479, 50121, 50118, 44908, 211, 35, 8, 38, 399, 75, 190, 8921, 24, 66, 259, 8, 11, 103, 1200, 38, 25522, 417, 32062, 6920, 254, 24303, 38, 29341, 24, 66, 10, 410, 828, 7252, 55, 12826, 2156, 50121, 50118, 44385, 163, 35, 5143, 479, 50121, 50118, 44908, 211, 35, 42, 16, 5, 335, 15, 25522, 417, 32062, 6920, 254, 24303, 15, 5, 614, 21168, 225, 22637, 2156, 8, 5, 38567, 21637, 5109, 14, 393, 376, 62, 2156, 8, 7252, 2156, 98, 939, 114, 52, 492, 69, 190, 55, 7252, 2156, 9571, 7, 173, 19, 38, 206, 5, 775, 32, 6908, 28, 190, 357, 479, 50121, 50118, 44385, 163, 35, 5534, 2156, 11380, 2156, 8, 172, 9, 768, 25, 79, 473, 24, 79, 128, 890, 25522, 417, 32062, 6920, 254, 24303, 79, 128, 890, 1532, 787, 787, 479, 1437, 407, 14, 128, 29, 372, 479, 12698, 25522, 44970, 24303, 178, 67, 114, 79, 128, 29, 2882, 7, 185, 15, 5, 633, 9, 13916, 70, 167, 9352, 8, 2682, 14, 74, 28, 4613, 479, 50121, 50118, 44908, 211, 35, 256, 5471, 479, 50121, 50118, 44385, 163, 35, 178, 2156, 37463, 79, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 888, 79, 128, 29, 164, 7, 5318, 334, 11, 10, 761, 9, 41, 14073, 28323, 2156, 98, 38, 206, 42, 16, 70, 95, 2051, 11, 1110, 9, 1368, 69, 2239, 383, 79, 128, 29, 6908, 240, 7, 216, 37463, 2156, 7, 109, 69, 756, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 6020, 21, 182, 8440, 9, 5, 1114, 9, 5947, 33445, 4, 91, 802, 14, 24, 74, 28, 7163, 187, 79, 21, 67, 2882, 7, 185, 81, 5, 3685, 9, 13916, 9352, 4, 96, 1285, 6, 37, 2047, 5, 695, 74, 694, 69, 19, 10, 5130, 2239, 676, 13, 69, 308, 2568, 5318, 334, 173, 11, 14073, 42388, 1023, 4339, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.6587, 'rouge2': 14.273, 'rougeL': 24.5076, 'rougeLsum': 30.2536}, 'ppl': {'perplexity': 3.5435, 'ref_perplexity': 11.6917}, 'bertscore': {'precision': 88.2277, 'recall': 86.5729, 'f1': 87.3795}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.179, 'rouge2': 15.3558, 'rougeL': 25.4518, 'rougeLsum': 31.9732}, 'ppl': {'perplexity': 5.4529, 'ref_perplexity': 11.1719}, 'bertscore': {'precision': 87.9392, 'recall': 87.0409, 'f1': 87.4765}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 37.6148, 'rouge2': 15.4369, 'rougeL': 26.5983, 'rougeLsum': 32.9362}, 'ppl': {'perplexity': 4.4032, 'ref_perplexity': 10.9979}, 'bertscore': {'precision': 88.0278, 'recall': 87.1872, 'f1': 87.595}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 38.1185, 'rouge2': 16.0902, 'rougeL': 26.5396, 'rougeLsum': 33.7838}, 'ppl': {'perplexity': 3.9663, 'ref_perplexity': 10.8921}, 'bertscore': {'precision': 88.0495, 'recall': 87.3119, 'f1': 87.6664}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.4988, 'rouge2': 14.824, 'rougeL': 25.5273, 'rougeLsum': 31.6361}, 'ppl': {'perplexity': 5.4752, 'ref_perplexity': 10.9521}, 'bertscore': {'precision': 87.9274, 'recall': 87.0301, 'f1': 87.4642}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 7
Start Training!
Sample 847 of the training set: {'input_ids': [0, 2264, 222, 1464, 5846, 338, 9224, 8391, 224, 334, 115, 109, 59, 5, 5263, 4044, 227, 12384, 116, 47385, 50121, 50118, 34052, 858, 3864, 21851, 3326, 35, 166, 348, 300, 103, 1142, 122, 198, 2017, 9, 2167, 1134, 9, 12384, 4, 318, 38, 64, 386, 8, 1394, 47, 141, 8082, 16, 24, 14, 5, 4044, 227, 12384, 4973, 13, 481, 334, 7317, 8, 97, 12384, 34, 45, 20546, 11, 5, 94, 158, 107, 6, 941, 576, 5, 984, 29156, 153, 25209, 31322, 4470, 915, 4, 50121, 50118, 5096, 5846, 338, 9224, 8391, 35, 3216, 4, 38, 206, 5929, 1304, 8, 5, 696, 9, 5263, 58, 5, 80, 383, 38, 348, 1581, 11, 5, 1013, 266, 4, 407, 6, 38, 206, 24, 16, 10, 2212, 7, 201, 4, 38, 206, 24, 16, 966, 67, 21556, 14, 1118, 7, 97, 749, 6, 13, 1246, 6, 11, 221, 30483, 6, 52, 109, 8933, 1341, 157, 11, 1110, 9, 2355, 4, 345, 18, 67, 41, 4795, 14, 2085, 5263, 8, 15498, 33, 1130, 6, 98, 14, 52, 214, 11, 10, 169, 878, 7, 489, 202, 4, 178, 38, 206, 67, 6, 25, 38, 21, 5542, 59, 5, 2600, 6, 5263, 269, 16, 10, 592, 10632, 4, 7101, 64, 75, 6136, 14, 15, 49, 308, 4, 407, 6, 89, 32, 10, 319, 9, 40833, 7, 28, 156, 198, 5, 754, 14, 14, 5263, 4044, 2282, 75, 1367, 6, 53, 14, 18, 45, 7, 224, 14, 1304, 64, 75, 109, 402, 59, 24, 6, 8, 38, 348, 2528, 11, 5, 1013, 266, 10, 2345, 9, 10, 80, 12, 4862, 1657, 196, 1548, 4, 509, 16, 5, 92, 12522, 4, 38, 206, 89, 16, 1283, 11, 5, 758, 557, 14, 5307, 8, 2239, 6, 357, 5307, 8, 2239, 6, 2607, 24341, 2129, 12384, 25846, 4, 407, 6, 51, 2364, 55, 31, 24, 14, 49, 6763, 4, 407, 6, 38, 206, 3927, 5307, 8, 2239, 6, 8, 52, 3373, 14, 656, 42, 662, 59, 141, 5, 92, 12522, 16, 269, 70, 59, 3927, 5, 1318, 9, 5307, 8, 2239, 11, 5, 8171, 4, 407, 6, 14, 18, 65, 1548, 6, 8, 172, 5, 97, 1548, 6, 61, 38, 348, 67, 2801, 656, 6, 16, 519, 10, 435, 12, 12804, 1548, 7, 1304, 4, 20, 1304, 14, 109, 55, 5116, 3692, 5, 5263, 4044, 32, 5, 1304, 14, 185, 14, 761, 9, 1548, 4, 85, 839, 1903, 5, 12384, 4, 85, 839, 1903, 49, 1232, 4, 85, 18, 59, 442, 25209, 157, 12, 9442, 269, 239, 15, 5, 4026, 4, 85, 18, 182, 1202, 13, 408, 7, 109, 157, 11, 334, 114, 51, 348, 300, 70, 6134, 9, 383, 2909, 11, 49, 3618, 4, 407, 6, 24, 18, 505, 14, 1304, 64, 2085, 1203, 7049, 167, 1232, 7, 97, 518, 14, 64, 323, 106, 8, 244, 106, 4, 407, 6, 24, 18, 1341, 10, 2632, 578, 4, 85, 18, 4087, 13, 1304, 7, 213, 159, 42, 3420, 6, 8, 38, 206, 5, 55, 244, 52, 64, 492, 1304, 7, 185, 14, 1548, 6, 5, 357, 4, 125, 5, 1304, 14, 109, 109, 24, 109, 1796, 10, 319, 31, 24, 4, 252, 33, 357, 4921, 31, 1041, 6, 31, 5, 1232, 6, 8, 14, 172, 6771, 124, 15, 5, 173, 9, 5, 408, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 970, 21, 10, 80, 12, 4862, 1657, 196, 1548, 7, 6136, 5, 936, 4, 509, 16, 5, 92, 12522, 6, 61, 21, 70, 59, 3927, 5, 1318, 9, 5307, 8, 2239, 11, 5, 8171, 4, 11253, 12384, 115, 2364, 55, 31, 24, 87, 49, 6763, 4, 20, 97, 65, 21, 7, 33, 10, 435, 12, 12804, 1548, 7, 1304, 149, 1903, 5, 12384, 8, 49, 1232, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.5866, 'rouge2': 13.6688, 'rougeL': 24.8659, 'rougeLsum': 31.2298}, 'ppl': {'perplexity': 3.7218, 'ref_perplexity': 14.4451}, 'bertscore': {'precision': 88.3456, 'recall': 86.9343, 'f1': 87.6256}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.8923, 'rouge2': 13.7839, 'rougeL': 24.9154, 'rougeLsum': 32.1532}, 'ppl': {'perplexity': 9.539, 'ref_perplexity': 13.553}, 'bertscore': {'precision': 87.8344, 'recall': 87.2057, 'f1': 87.5097}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 37.8983, 'rouge2': 14.5024, 'rougeL': 25.9739, 'rougeLsum': 33.2506}, 'ppl': {'perplexity': 4.6687, 'ref_perplexity': 13.4703}, 'bertscore': {'precision': 87.9505, 'recall': 87.2721, 'f1': 87.5992}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.6101, 'rouge2': 13.1769, 'rougeL': 24.8565, 'rougeLsum': 31.4923}, 'ppl': {'perplexity': 5.6787, 'ref_perplexity': 13.3338}, 'bertscore': {'precision': 88.2073, 'recall': 87.152, 'f1': 87.6641}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 37.7541, 'rouge2': 13.7495, 'rougeL': 25.0848, 'rougeLsum': 32.9572}, 'ppl': {'perplexity': 5.6805, 'ref_perplexity': 13.315}, 'bertscore': {'precision': 88.226, 'recall': 87.2813, 'f1': 87.7403}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 8
Start Training!
Sample 222 of the training set: {'input_ids': [0, 38182, 3916, 2072, 16288, 15, 447, 1521, 6, 3165, 8047, 8, 12628, 3471, 4, 47385, 50121, 50118, 33347, 4827, 35, 25522, 31375, 1536, 9834, 24303, 2497, 7, 192, 47, 70, 456, 479, 2780, 18, 192, 114, 14, 606, 62, 479, 152, 16, 84, 12628, 1521, 529, 479, 12698, 479, 1801, 10, 15636, 150, 127, 39814, 606, 62, 479, 8923, 17377, 4882, 479, 8487, 479, 256, 119, 7252, 52, 342, 5, 2734, 11, 8917, 479, 2780, 18, 386, 479, 8487, 2156, 84, 4026, 452, 7252, 25522, 417, 32062, 6920, 254, 24303, 95, 1649, 5, 86, 2156, 24, 18, 11971, 30361, 479, 12698, 479, 38, 437, 6908, 109, 41, 1273, 2156, 1067, 59, 7252, 25522, 417, 32062, 6920, 254, 24303, 222, 47, 70, 120, 5, 728, 17487, 38, 364, 12, 6380, 196, 106, 7, 47, 479, 38, 437, 67, 2057, 128, 991, 25522, 417, 32062, 6920, 254, 24303, 106, 11, 5, 1373, 32038, 479, 50121, 50118, 44518, 39322, 35, 33597, 479, 50121, 50118, 33347, 4827, 35, 407, 7252, 172, 38, 25522, 417, 32062, 6920, 254, 24303, 52, 581, 1067, 59, 84, 937, 10366, 50121, 50118, 36926, 13700, 31275, 35, 5143, 479, 50121, 50118, 33347, 4827, 35, 8, 33, 110, 130, 16288, 479, 12698, 38, 581, 1067, 59, 5, 92, 695, 3471, 38, 348, 95, 829, 2156, 8, 172, 52, 33, 7, 146, 10, 568, 15, 84, 6063, 797, 8047, 479, 3347, 52, 581, 95, 593, 479, 166, 214, 1158, 42, 529, 23, 2219, 11971, 30361, 8, 52, 33, 24503, 728, 479, 407, 25522, 417, 32062, 6920, 254, 24303, 1234, 9, 70, 5, 12628, 1521, 10366, 479, 23129, 52, 240, 7, 489, 11, 1508, 5, 3018, 7404, 30643, 2156, 99, 782, 8, 22215, 32, 7, 28, 20218, 2156, 5, 25522, 29183, 24303, 8047, 1521, 2156, 99, 3038, 5, 26529, 197, 33, 2156, 8, 5, 447, 1521, 2156, 141, 5, 26529, 888, 1364, 7, 19805, 63, 5043, 479, 8487, 2156, 130, 16288, 2156, 7252, 47, 64, 213, 11, 143, 645, 47, 2807, 7252, 479, 50121, 50118, 10006, 16382, 35, 25522, 29183, 24303, 256, 119, 5658, 52, 213, 11, 5, 645, 14, 47, 95, 222, 24, 17487, 50121, 50118, 33347, 4827, 35, 9136, 2156, 2540, 109, 479, 50121, 50118, 10006, 16382, 35, 38, 46910, 479, 1336, 109, 38, 9337, 127, 2441, 62, 17487, 50121, 50118, 36926, 13700, 31275, 35, 38, 206, 2156, 47, 429, 33, 7, 18411, 3199, 479, 50121, 50118, 33347, 4827, 35, 3216, 38, 109, 479, 8976, 479, 50121, 50118, 44518, 39322, 35, 2647, 89, 18, 10, 29295, 10, 29295, 10242, 95, 95, 14, 65, 89, 50121, 50118, 10006, 16382, 35, 4820, 473, 24, 213, 17487, 256, 119, 12, 298, 5471, 479, 44258, 2156, 38, 437, 45, 3518, 7, 517, 42, 2156, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 53, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 7746, 14, 18, 24, 2156, 1423, 2462, 479, 50121, 50118, 10006, 16382, 35, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 178, 172, 47, 33, 7, 1228, 5043, 274, 1215, 799, 50121, 50118, 33347, 4827, 35, 42419, 2156, 274, 1215, 799, 2156, 11380, 479, 50121, 50118, 44518, 39322, 35, 38, 206, 24, 16, 15, 110, 9972, 479, 50121, 50118, 33347, 4827, 35, 20, 2440, 65, 2156, 274, 1215, 487, 1215, 479, 50121, 50118, 10006, 16382, 35, 4820, 18, 5043, 17487, 440, 6029, 479, 50121, 50118, 33347, 4827, 35, 1534, 24, 30251, 11, 70, 5, 169, 8, 47, 34797, 24, 11, 8, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 7746, 2156, 2067, 2156, 128, 29, 21927, 11, 479, 50121, 50118, 36926, 13700, 31275, 35, 38, 38, 206, 47, 95, 33, 7, 1920, 24, 11, 269, 543, 479, 50121, 50118, 10006, 16382, 35, 23726, 5, 21927, 479, 50121, 50118, 44518, 39322, 35, 280, 18, 24, 479, 50121, 50118, 36926, 13700, 31275, 35, 5534, 2156, 300, 24, 479, 50121, 50118, 33347, 4827, 35, 256, 119, 128, 33206, 479, 50121, 50118, 10006, 16382, 35, 256, 119, 30103, 50121, 50118, 33347, 4827, 35, 85, 18, 602, 24, 10, 410, 828, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 38, 348, 393, 7391, 7, 932, 479, 50121, 50118, 36926, 13700, 31275, 35, 256, 119, 2156, 5063, 33, 38, 479, 50121, 50118, 33347, 4827, 35, 128, 28421, 89, 47, 213, 479, 50121, 50118, 10006, 16382, 35, 41881, 2156, 98, 2156, 50121, 50118, 36926, 13700, 31275, 35, 41881, 479, 50121, 50118, 10006, 16382, 35, 38, 218, 75, 216, 114, 47, 1669, 32, 441, 7, 120, 899, 7, 7252, 5, 266, 14, 21, 804, 50, 114, 38, 437, 5, 129, 65, 54, 16, 479, 125, 2156, 38, 218, 75, 190, 216, 141, 7, 310, 42, 479, 440, 479, 50121, 50118, 33347, 4827, 35, 977, 5, 410, 5209, 479, 85, 18, 5, 7252, 25522, 417, 32062, 6920, 254, 24303, 24, 1326, 101, 10, 854, 1215, 761, 9, 25522, 417, 32062, 6920, 254, 24303, 81, 89, 1065, 29109, 479, 345, 2156, 14, 65, 2156, 89, 47, 213, 479, 50121, 50118, 10006, 16382, 35, 41881, 479, 407, 52, 214, 95, 6908, 1067, 10, 410, 828, 59, 5, 12628, 3471, 14, 82, 17966, 77, 51, 58, 553, 479, 12698, 38, 4443, 2822, 39067, 222, 103, 210, 557, 479, 252, 56, 10, 6317, 9352, 8, 49, 41421, 6348, 8, 51, 3996, 106, 1183, 255, 1215, 846, 1215, 8, 2673, 5, 13135, 14, 51, 341, 1989, 14893, 8, 5, 21623, 14, 167, 14893, 56, 479, 653, 51, 303, 21, 51, 24305, 82, 18, 22215, 8, 782, 479, 274, 34722, 15, 49, 22215, 2156, 7252, 82, 4010, 26, 14, 51, 802, 6398, 17467, 58, 11355, 25522, 31375, 1536, 9834, 24303, 2156, 39676, 292, 228, 715, 9, 5, 10, 6317, 9352, 1581, 14, 8, 14, 51, 25522, 417, 32062, 6920, 254, 24303, 55, 7769, 600, 2156, 42991, 228, 715, 26, 14, 51, 74, 28, 2882, 7, 582, 55, 13, 10, 33639, 906, 546, 6063, 479, 38, 218, 75, 216, 932, 1684, 99, 13185, 839, 2156, 50121, 50118, 36926, 13700, 31275, 35, 44258, 479, 50121, 50118, 10006, 16382, 35, 53, 14, 18, 1605, 9, 304, 7, 201, 2156, 38, 206, 479, 12698, 67, 51, 222, 103, 1142, 15, 2236, 4972, 8, 303, 14, 5, 4724, 13, 2236, 4972, 21, 11, 37794, 1330, 7, 1046, 2156, 98, 3240, 82, 58, 55, 19901, 7, 236, 402, 19, 2236, 4972, 2156, 9641, 5, 2530, 82, 11, 5, 101, 33910, 8, 1065, 2835, 50, 98, 222, 45, 269, 206, 14, 51, 74, 582, 55, 418, 13, 2236, 15603, 8237, 479, 50121, 50118, 36926, 13700, 31275, 35, 128, 28421, 479, 50121, 50118, 10006, 16382, 35, 12698, 82, 67, 56, 1402, 25308, 2156, 14, 38, 206, 14, 52, 115, 860, 7, 185, 88, 6077, 19, 84, 1521, 479, 280, 145, 82, 449, 7252, 8164, 19, 2086, 49, 6398, 17467, 479, 38, 206, 2156, 81, 14865, 135, 9, 5, 82, 2801, 14, 14, 21, 49, 934, 8413, 479, 1806, 32, 67, 8164, 19, 5, 9600, 24, 16, 7, 1532, 141, 7, 304, 10, 6063, 8, 38, 206, 14, 3405, 124, 7, 99, 47, 58, 584, 137, 50121, 50118, 36926, 13700, 31275, 35, 44258, 479, 50121, 50118, 10006, 16382, 35, 95, 14, 89, 18, 350, 171, 14893, 2156, 24, 95, 782, 7, 28, 1365, 7, 304, 479, 85, 67, 2801, 402, 373, 248, 1215, 104, 1215, 100, 1215, 8, 38, 21, 2818, 951, 429, 28, 441, 7, 6296, 162, 25, 7, 99, 248, 1215, 104, 1215, 100, 1215, 16, 2156, 50121, 50118, 44518, 39322, 35, 2825, 40763, 8793, 1356, 479, 50121, 50118, 10006, 16382, 35, 142, 38, 218, 75, 216, 479, 50121, 50118, 33347, 4827, 35, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 2825, 40763, 8793, 1356, 479, 50121, 50118, 10006, 16382, 35, 653, 17487, 7746, 479, 345, 52, 213, 479, 50121, 50118, 44518, 39322, 35, 407, 114, 47, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 22545, 479, 1806, 109, 45, 101, 14, 479, 407, 38, 4443, 2345, 9, 5, 512, 18239, 10615, 1907, 631, 2156, 82, 109, 45, 101, 14, 2156, 7252, 5, 30159, 304, 2156, 38, 4443, 2156, 1726, 10, 8793, 479, 12698, 546, 23, 5, 782, 82, 17966, 2156, 5, 936, 235, 122, 16, 14, 82, 18, 6398, 17467, 32, 45, 8150, 49, 1633, 5759, 479, 1806, 32, 129, 634, 2724, 228, 715, 9, 5, 14893, 14, 51, 33, 1661, 7, 106, 15, 49, 6063, 479, 178, 99, 82, 109, 144, 747, 16, 2992, 5, 4238, 8, 2992, 5, 3149, 479, 1806, 67, 992, 1115, 101, 7, 464, 5, 4238, 2156, 59, 7252, 33910, 292, 228, 715, 148, 41, 1946, 9, 304, 479, 407, 52, 269, 95, 240, 7, 1056, 11, 15, 167, 7267, 8, 4238, 24622, 268, 1195, 87, 383, 101, 5, 6086, 9629, 2156, 5, 2441, 9629, 8, 5, 4238, 9629, 2156, 142, 51, 214, 341, 203, 55, 4047, 42615, 8, 1153, 95, 29036, 99, 18, 164, 15, 479, 407, 38, 206, 14, 103, 383, 14, 52, 429, 23126, 206, 59, 2156, 5, 1114, 9, 41, 226, 1215, 347, 1215, 495, 1215, 2441, 21, 1146, 62, 1712, 51, 399, 75, 33, 143, 1254, 15, 99, 82, 18, 16953, 15, 14, 58, 2156, 98, 38, 46910, 216, 114, 14, 18, 567, 7, 162, 423, 2156, 50, 402, 101, 14, 479, 125, 402, 13, 201, 7, 1701, 67, 95, 5, 10632, 14, 540, 16, 55, 77, 24, 606, 7, 5, 14893, 15, 5, 6063, 50, 99, 52, 23126, 146, 18815, 7, 304, 2156, 146, 686, 14, 2156, 47, 216, 2156, 402, 101, 41, 6086, 2749, 965, 75, 576, 25, 203, 3585, 8, 11915, 15, 5, 6063, 25, 402, 101, 4238, 2992, 14, 18, 341, 10, 319, 55, 747, 479, 178, 5072, 11, 645, 13, 201, 7, 339, 81, 7, 5, 2267, 52, 95, 240, 7, 1056, 15, 99, 24, 1326, 101, 2156, 14, 24, 34, 10, 13185, 2868, 8, 14, 24, 18, 45, 11355, 25522, 31375, 1536, 9834, 24303, 8, 14, 24, 2653, 101, 5, 169, 51, 214, 6908, 304, 24, 2156, 98, 24, 630, 75, 492, 106, 143, 865, 1746, 50, 383, 101, 14, 479, 50121, 50118, 36926, 13700, 31275, 35, 44258, 479, 50121, 50118, 33347, 4827, 35, 3837, 47, 182, 203, 479, 280, 21, 14, 21, 372, 479, 50121, 50118, 36926, 13700, 31275, 35, 256, 119, 128, 33206, 479, 50121, 50118, 33347, 4827, 35, 12698, 25522, 31375, 1536, 9834, 24303, 128, 29, 517, 15, 7, 5, 220, 5209, 7252, 15, 3038, 479, 6871, 14, 47, 17487, 50121, 50118, 10006, 16382, 35, 44258, 479, 50121, 50118, 33347, 4827, 35, 2860, 479, 50121, 50118, 10006, 16382, 35, 8976, 2156, 33, 38, 21219, 10461, 196, 24, 17487, 50121, 50118, 33347, 4827, 35, 23726, 479, 27913, 12332, 2156, 235, 479, 39322, 479, 50121, 50118, 10006, 16382, 35, 1398, 52, 213, 479, 50121, 50118, 44518, 39322, 35, 3576, 268, 479, 50121, 50118, 10006, 16382, 35, 256, 119, 12, 298, 5471, 479, 178, 38, 206, 14, 18, 11, 5, 1373, 2156, 114, 38, 222, 24, 235, 2156, 114, 1268, 1072, 7, 356, 23, 24, 479, 50121, 50118, 33347, 4827, 35, 256, 119, 128, 33206, 2156, 3392, 47, 479, 50121, 50118, 36926, 13700, 31275, 35, 8487, 2156, 372, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 33347, 4827, 35, 8487, 479, 50121, 50118, 44518, 39322, 35, 1398, 52, 213, 479, 5143, 98, 38, 437, 6908, 1067, 59, 5, 3165, 3165, 8047, 1521, 9, 5, 6063, 797, 7252, 479, 166, 240, 7, 386, 30, 2811, 99, 10, 6063, 797, 888, 16, 479, 85, 18, 10, 2187, 14, 2386, 201, 7, 2592, 1402, 3038, 15, 84, 2384, 2156, 98, 939, 24, 18, 5072, 10, 4358, 2187, 479, 166, 52, 1137, 5, 6063, 797, 99, 52, 236, 7, 109, 2156, 24, 11210, 10, 1579, 7, 5, 2384, 584, 464, 5, 4238, 2156, 464, 5, 3149, 2156, 37463, 11380, 2156, 9160, 209, 9629, 2156, 9160, 5, 27406, 479, 12698, 141, 109, 52, 888, 213, 59, 15293, 10, 92, 2384, 6063, 797, 17487, 1234, 631, 7, 109, 16, 7, 283, 62, 19, 5, 1521, 17697, 479, 166, 240, 7, 216, 99, 84, 507, 1152, 16, 6908, 28, 101, 2156, 98, 52, 240, 10, 10, 699, 1114, 9, 2230, 99, 42, 1152, 473, 2156, 37463, 141, 24, 1364, 2156, 8, 99, 5, 253, 12, 12105, 16, 6908, 236, 31, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 78, 5209, 21, 59, 1434, 108, 12628, 3471, 4, 9020, 2801, 14, 1812, 207, 9, 82, 101, 13185, 2868, 6063, 8, 3240, 82, 6573, 2236, 4972, 4, 20, 200, 5209, 21, 59, 3165, 8047, 4, 27913, 39322, 2528, 14, 25, 10, 4358, 3944, 6, 5, 6063, 197, 28, 7708, 8, 35389, 20890, 3435, 1887, 4, 20, 371, 5209, 21, 59, 447, 1521, 4, 6741, 31275, 2942, 237, 1049, 6411, 6, 101, 10, 3822, 7, 146, 24, 173, 6, 5, 6638, 7, 10304, 5, 414, 6, 5, 3018, 14, 18, 10568, 5, 6638, 6, 8, 5, 4047, 763, 12, 2050, 32384, 14, 3136, 5, 414, 7, 5, 4797, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.6059, 'rouge2': 13.8002, 'rougeL': 24.4003, 'rougeLsum': 30.4912}, 'ppl': {'perplexity': 3.684, 'ref_perplexity': 9.8725}, 'bertscore': {'precision': 88.0256, 'recall': 86.535, 'f1': 87.2645}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.218, 'rouge2': 14.0688, 'rougeL': 25.0765, 'rougeLsum': 32.0579}, 'ppl': {'perplexity': 6.6708, 'ref_perplexity': 9.6021}, 'bertscore': {'precision': 87.8223, 'recall': 86.7458, 'f1': 87.2692}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 38.1666, 'rouge2': 15.0231, 'rougeL': 25.646, 'rougeLsum': 33.6736}, 'ppl': {'perplexity': 5.3855, 'ref_perplexity': 9.3082}, 'bertscore': {'precision': 87.802, 'recall': 87.0058, 'f1': 87.3928}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 37.4791, 'rouge2': 15.3296, 'rougeL': 25.831, 'rougeLsum': 32.8717}, 'ppl': {'perplexity': 6.2707, 'ref_perplexity': 9.1497}, 'bertscore': {'precision': 88.2511, 'recall': 86.8987, 'f1': 87.5597}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 38.0927, 'rouge2': 15.9724, 'rougeL': 26.8598, 'rougeLsum': 33.8241}, 'ppl': {'perplexity': 5.4867, 'ref_perplexity': 9.1985}, 'bertscore': {'precision': 88.1473, 'recall': 87.0481, 'f1': 87.5842}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 9
Start Training!
Sample 967 of the training set: {'input_ids': [0, 1779, 74, 5, 173, 9, 5574, 28, 2121, 116, 47385, 50121, 50118, 725, 4550, 179, 871, 3326, 35, 178, 77, 40, 5, 173, 28, 2121, 6, 8, 99, 40, 5, 7762, 28, 116, 50121, 50118, 530, 17698, 219, 1604, 3326, 35, 2647, 6, 5, 333, 16, 855, 7580, 19, 2206, 8, 97, 503, 15, 14176, 10, 563, 6, 53, 67, 6, 11806, 24354, 6, 14, 563, 16, 89, 7, 323, 5, 1800, 5574, 9, 5, 12522, 6, 98, 14, 52, 214, 182, 699, 59, 5, 4502, 8, 9582, 11, 5, 1692, 14390, 11, 42, 4096, 4359, 511, 5, 5362, 4, 166, 33, 7, 517, 122, 31, 5, 5362, 88, 10, 16476, 1056, 15, 5574, 4, 20, 750, 9, 8709, 23794, 16, 455, 9, 5500, 2339, 6, 8, 6, 5658, 38, 224, 6, 9202, 219, 5574, 4, 50141, 133, 173, 14, 34, 1613, 88, 14, 12522, 16, 350, 505, 13, 5574, 7, 28, 314, 7, 778, 4, 85, 18, 350, 505, 4, 85, 18, 350, 205, 7, 28, 314, 7, 778, 4, 407, 6, 960, 122, 16, 10, 16476, 1056, 15, 1800, 5574, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 5975, 5, 333, 21, 7580, 19, 2206, 8, 97, 503, 15, 14176, 10, 563, 6, 53, 67, 6, 14, 563, 21, 89, 7, 323, 5, 1800, 5574, 9, 5, 12522, 4, 7253, 15, 5, 1617, 31, 16071, 219, 6, 51, 58, 182, 699, 59, 5, 4502, 8, 9582, 11, 5, 1692, 14390, 11, 42, 11054, 511, 5, 5362, 4, 978, 51, 56, 1410, 88, 10, 16476, 1056, 15, 5574, 4, 23381, 6, 960, 122, 21, 41, 9723, 15, 1800, 5574, 4, 2]}.
***** Running training *****
  Num examples = 986
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 31.9672, 'rouge2': 11.5708, 'rougeL': 22.079, 'rougeLsum': 28.4232}, 'ppl': {'perplexity': 6.7215, 'ref_perplexity': 15.2236}, 'bertscore': {'precision': 87.6518, 'recall': 86.3558, 'f1': 86.9848}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.1074, 'rouge2': 11.6912, 'rougeL': 22.6224, 'rougeLsum': 29.5881}, 'ppl': {'perplexity': 4.1998, 'ref_perplexity': 14.7541}, 'bertscore': {'precision': 87.366, 'recall': 86.5807, 'f1': 86.9569}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 34.2122, 'rouge2': 12.0996, 'rougeL': 23.0097, 'rougeLsum': 30.7206}, 'ppl': {'perplexity': 3.1153, 'ref_perplexity': 14.9396}, 'bertscore': {'precision': 87.1743, 'recall': 86.802, 'f1': 86.9745}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.3329, 'rouge2': 11.3131, 'rougeL': 22.1638, 'rougeLsum': 30.1324}, 'ppl': {'perplexity': 3.5164, 'ref_perplexity': 14.8231}, 'bertscore': {'precision': 87.1363, 'recall': 86.6652, 'f1': 86.8865}}
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.825, 'rouge2': 11.6929, 'rougeL': 22.9434, 'rougeLsum': 30.2244}, 'ppl': {'perplexity': 5.9918, 'ref_perplexity': 14.8148}, 'bertscore': {'precision': 87.3131, 'recall': 86.7885, 'f1': 87.036}}

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round 10
Start Training!
Sample 749 of the training set: {'input_ids': [0, 2264, 222, 5, 333, 2268, 59, 5, 7728, 9626, 116, 47385, 50121, 50118, 10006, 16382, 35, 653, 59, 5, 95, 2226, 37463, 7728, 9626, 17487, 50121, 50118, 44518, 39322, 35, 38, 206, 1920, 12, 4297, 38550, 16, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 653, 59, 99, 17487, 50121, 50118, 10006, 16382, 35, 272, 89, 2156, 5, 7728, 9626, 2156, 7728, 5385, 631, 479, 50121, 50118, 36926, 13700, 31275, 35, 2647, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 2647, 2156, 99, 109, 52, 240, 10, 5385, 13, 11, 10, 6063, 797, 1933, 17487, 50121, 50118, 10006, 16382, 35, 256, 119, 2156, 38, 46910, 479, 1456, 3035, 479, 50121, 50118, 36926, 13700, 31275, 35, 85, 1017, 28, 24, 1017, 28, 3035, 2156, 53, 51, 32, 584, 51, 348, 95, 2226, 24, 2156, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 5331, 80, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 36926, 13700, 31275, 35, 38, 437, 95, 24224, 479, 125, 24, 18, 6908, 28, 5, 144, 3214, 1973, 2156, 1153, 8, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 208, 50121, 50118, 44518, 39322, 35, 2032, 5, 5, 25522, 31375, 1536, 9834, 24303, 1901, 4972, 7252, 1973, 16, 25522, 417, 32062, 6920, 254, 24303, 24, 630, 75, 2045, 269, 182, 6177, 13, 201, 37463, 2156, 50121, 50118, 33347, 4827, 35, 8976, 479, 85, 18, 45, 402, 14, 52, 23126, 326, 213, 88, 19, 42, 1152, 479, 50121, 50118, 44518, 39322, 35, 128, 27037, 37463, 25522, 417, 32062, 6920, 254, 24303, 20, 11380, 5, 1246, 14, 51, 214, 416, 634, 24, 13, 16, 19, 5, 3895, 3563, 2156, 147, 2156, 5072, 2156, 47, 64, 586, 10, 7728, 33755, 7252, 25522, 417, 32062, 6920, 254, 24303, 280, 77, 47, 224, 402, 24, 40, 492, 10, 1263, 2156, 8, 47, 586, 5, 1263, 25, 157, 479, 1801, 37463, 16610, 9, 21152, 14, 47, 638, 2512, 479, 407, 47, 64, 586, 110, 3895, 12, 5406, 14, 77, 47, 224, 2156, 205, 662, 2156, 7, 24, 24, 161, 2156, 20760, 4434, 2156, 50, 3046, 479, 50121, 50118, 33347, 4827, 35, 8976, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 125, 2156, 38, 1266, 2156, 24, 18, 45, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 289, 119, 479, 50121, 50118, 44518, 39322, 35, 24, 74, 28, 65, 631, 114, 24, 21, 1901, 4972, 147, 47, 224, 402, 8, 24, 4072, 5, 255, 1215, 846, 1215, 15, 101, 2156, 1004, 5, 255, 1215, 846, 1215, 15, 2156, 8, 939, 4072, 25522, 417, 32062, 6920, 254, 24303, 606, 15, 2156, 53, 24, 18, 45, 14, 479, 85, 95, 2029, 47, 10, 24, 95, 2029, 47, 10, 14580, 1263, 479, 50121, 50118, 10006, 16382, 35, 5534, 2156, 24, 95, 2029, 41, 1948, 479, 50121, 50118, 44518, 39322, 35, 407, 2156, 11380, 2156, 38, 1266, 2156, 101, 99, 18, 5, 477, 9, 584, 2156, 20920, 6063, 2156, 38, 1266, 2156, 20760, 2156, 141, 141, 32, 47, 17487, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 5534, 2156, 172, 172, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 8976, 479, 8976, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 36926, 13700, 31275, 35, 25522, 31375, 1536, 9834, 24303, 1801, 114, 47, 32, 269, 20100, 2156, 2085, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 38, 802, 38, 802, 24, 21, 25522, 417, 32062, 6920, 254, 24303, 77, 51, 26, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 8976, 2156, 114, 47, 214, 269, 20100, 2156, 24, 16, 24, 18, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 38, 802, 77, 51, 26, 2156, 2236, 4972, 2156, 51, 2425, 7252, 101, 2156, 50121, 50118, 36926, 13700, 31275, 35, 5331, 292, 479, 178, 172, 24, 21737, 15, 479, 50121, 50118, 10006, 16382, 35, 4238, 292, 2156, 8, 24, 40, 464, 479, 50121, 50118, 44518, 39322, 35, 440, 2156, 21152, 14, 885, 14, 885, 14, 74, 28, 55, 6177, 479, 50121, 50118, 10006, 16382, 35, 2011, 47, 1067, 7, 24, 479, 2615, 38, 33, 4238, 292, 17487, 50121, 50118, 33347, 4827, 35, 8976, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 85, 24, 18, 95, 10, 6063, 14, 1431, 7, 47, 479, 50121, 50118, 33347, 4827, 35, 8976, 479, 8976, 479, 50121, 50118, 44518, 39322, 35, 23129, 479, 25522, 31375, 1536, 9834, 24303, 38, 1266, 7, 1402, 25072, 479, 50121, 50118, 10006, 16382, 35, 5534, 2156, 172, 25522, 29183, 24303, 4309, 59, 24, 479, 5534, 235, 8578, 479, 50121, 50118, 33347, 4827, 35, 8976, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 10006, 16382, 4298, 196, 13, 5, 1114, 9, 5, 7728, 9626, 6, 61, 16, 402, 101, 10, 7728, 5385, 6, 61, 34, 5, 5043, 9, 2236, 4972, 6, 8, 64, 244, 5405, 6237, 15, 1576, 5, 2698, 9, 1434, 4, 125, 42, 1114, 21, 6978, 142, 9, 49922, 1571, 8, 4553, 1290, 12367, 4, 2]}.
***** Running training *****
  Num examples = 981
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 310
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 32.9703, 'rouge2': 11.6659, 'rougeL': 22.416, 'rougeLsum': 29.0881}, 'ppl': {'perplexity': 7.883, 'ref_perplexity': 15.1882}, 'bertscore': {'precision': 86.6958, 'recall': 86.417, 'f1': 86.5408}}
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.3767, 'rouge2': 12.9539, 'rougeL': 24.3261, 'rougeLsum': 31.262}, 'ppl': {'perplexity': 4.6047, 'ref_perplexity': 14.2273}, 'bertscore': {'precision': 86.9836, 'recall': 86.6563, 'f1': 86.8057}}
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.3823, 'rouge2': 12.9009, 'rougeL': 24.2417, 'rougeLsum': 31.3561}, 'ppl': {'perplexity': 4.1873, 'ref_perplexity': 13.7014}, 'bertscore': {'precision': 87.1298, 'recall': 86.8846, 'f1': 86.9899}}
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.6345, 'rouge2': 12.7474, 'rougeL': 24.0422, 'rougeLsum': 31.3505}, 'ppl': {'perplexity': 4.4496, 'ref_perplexity': 13.3311}, 'bertscore': {'precision': 86.8902, 'recall': 86.8428, 'f1': 86.85}}
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.4246, 'rouge2': 13.0616, 'rougeL': 24.403, 'rougeLsum': 30.951}, 'ppl': {'perplexity': 4.8966, 'ref_perplexity': 13.3913}, 'bertscore': {'precision': 87.0772, 'recall': 86.8142, 'f1': 86.9306}}

Start Predicting!
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Round All
Start Training!
Sample 694 of the training set: {'input_ids': [0, 38182, 3916, 2072, 5, 3221, 59, 27913, 39322, 1521, 59, 6063, 797, 4, 47385, 50121, 50118, 44518, 39322, 35, 38, 56, 99, 38, 95, 37463, 38, 197, 25522, 29183, 24303, 456, 479, 38, 2649, 10, 367, 14893, 2156, 53, 479, 2647, 479, 497, 513, 37463, 25522, 31375, 1536, 9834, 24303, 25522, 417, 32062, 6920, 254, 24303, 99, 52, 197, 67, 33, 15, 2156, 38, 95, 8715, 2156, 7252, 10, 5765, 7, 213, 124, 149, 5, 78, 37463, 25522, 29183, 24303, 114, 114, 47, 2842, 1735, 2156, 47, 64, 75, 213, 124, 7, 42, 37463, 235, 409, 122, 479, 50121, 50118, 10006, 16382, 35, 256, 119, 12, 298, 5471, 479, 8976, 2156, 47, 33, 7, 213, 124, 479, 8976, 37463, 37463, 479, 50121, 50118, 44518, 39322, 35, 152, 25522, 29183, 24303, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 26421, 75, 24, 357, 7, 33, 5, 2369, 8, 5, 937, 14893, 25490, 17487, 50121, 50118, 44518, 39322, 35, 38, 218, 75, 216, 479, 50121, 50118, 10006, 16382, 35, 590, 5, 10877, 8, 5, 2704, 479, 50121, 50118, 44518, 39322, 35, 38, 206, 24, 18, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 38, 206, 24, 18, 3013, 87, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 38, 218, 75, 216, 479, 38, 38, 802, 24, 21, 37463, 3013, 7, 3679, 42, 169, 2156, 50121, 50118, 10006, 16382, 35, 8976, 479, 50121, 50118, 44518, 39322, 35, 53, 38, 218, 75, 216, 99, 51, 206, 479, 50121, 50118, 36926, 13700, 31275, 35, 19719, 17487, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 33347, 4827, 35, 256, 119, 479, 50121, 50118, 44518, 39322, 35, 11258, 47, 101, 5, 5, 14893, 25490, 50, 748, 12194, 17487, 50121, 50118, 10006, 16382, 35, 286, 2369, 8, 4238, 479, 50121, 50118, 44518, 39322, 35, 2032, 50121, 50118, 36926, 13700, 31275, 35, 6748, 8845, 15, 5, 2441, 479, 318, 47, 146, 5, 2441, 12194, 24, 630, 75, 948, 479, 318, 47, 146, 24, 37463, 11, 10, 45140, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 5143, 157, 114, 52, 146, 24, 101, 42, 2156, 38, 206, 114, 47, 50121, 50118, 10006, 16382, 35, 8976, 479, 50121, 50118, 36926, 13700, 31275, 35, 38, 98, 24, 18, 24, 18, 24, 18, 24, 18, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 342, 24, 101, 42, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 5041, 479, 25522, 29183, 24303, 50121, 50118, 10006, 16382, 35, 38, 206, 24, 18, 24, 18, 3013, 7, 33, 24, 402, 101, 25522, 417, 32062, 6920, 254, 24303, 5534, 10, 6148, 37463, 10877, 259, 2156, 2704, 81, 259, 479, 83, 10877, 259, 2156, 2704, 81, 259, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 5534, 2156, 8578, 479, 50121, 50118, 10006, 16382, 35, 178, 15, 259, 479, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 20, 97, 14893, 8, 15, 259, 25522, 31375, 1536, 9834, 24303, 5, 299, 479, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 20, 1735, 8, 172, 47, 33, 402, 101, 37463, 5, 221, 1215, 81, 259, 2156, 50121, 50118, 44518, 39322, 35, 125, 38, 206, 38, 885, 1438, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 50121, 50118, 10006, 16382, 35, 8, 5, 2369, 479, 50121, 50118, 44518, 39322, 35, 38, 206, 14, 18, 10, 948, 9, 99, 47, 214, 341, 7, 479, 50121, 50118, 10006, 16382, 35, 13676, 37463, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 38, 206, 38, 74, 342, 10, 2704, 8, 10, 5251, 37463, 259, 479, 178, 172, 5, 221, 1215, 11, 5, 1084, 11, 5, 1692, 8, 5, 2369, 37463, 11, 5, 1692, 479, 50121, 50118, 36926, 13700, 31275, 35, 23129, 479, 25522, 29183, 24303, 50121, 50118, 10006, 16382, 35, 13676, 101, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 840, 50121, 50118, 33347, 4827, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 36926, 13700, 31275, 35, 4624, 110, 86, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 4642, 10877, 2704, 37463, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 11206, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 8976, 50121, 50118, 10006, 16382, 35, 10877, 479, 50121, 50118, 44518, 39322, 35, 53, 38, 206, 77, 47, 32, 1826, 24, 2156, 47, 115, 1228, 5, 10877, 8, 5, 2704, 8, 19, 5, 97, 8411, 5, 10877, 8, 5, 2704, 479, 50121, 50118, 10006, 16382, 35, 8976, 38, 206, 47, 214, 164, 7, 579, 47, 214, 164, 7, 304, 24, 19, 65, 15459, 479, 50121, 50118, 44518, 39322, 35, 8976, 53, 38, 206, 25522, 417, 32062, 6920, 254, 24303, 8976, 38, 218, 75, 216, 479, 25522, 31375, 1536, 9834, 24303, 6259, 38, 33, 103, 7721, 479, 50121, 50118, 36926, 13700, 31275, 35, 305, 50121, 50118, 44518, 39322, 35, 38, 218, 75, 216, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 166, 581, 989, 14, 7, 5, 41421, 4675, 172, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 3394, 18, 5, 41421, 4675, 17487, 50121, 50118, 44518, 39322, 35, 280, 18, 162, 479, 50121, 50118, 10006, 16382, 35, 264, 16, 479, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 125, 38, 437, 164, 7, 356, 114, 38, 348, 300, 103, 7721, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 128, 27037, 2532, 47, 197, 2807, 99, 18, 144, 747, 341, 479, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 5534, 479, 50121, 50118, 44518, 39322, 35, 20, 25522, 417, 32062, 6920, 254, 24303, 128, 43326, 51, 64, 304, 14, 357, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 9051, 661, 6761, 479, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 5534, 25522, 29183, 24303, 25522, 417, 32062, 6920, 254, 24303, 38, 33, 14, 167, 579, 1530, 479, 50121, 50118, 44518, 39322, 35, 38, 25522, 417, 32062, 6920, 254, 24303, 1398, 16, 84, 25522, 417, 32062, 6920, 254, 24303, 259, 32, 37463, 50121, 50118, 10006, 16382, 35, 1793, 10, 205, 1183, 479, 50121, 50118, 44518, 39322, 35, 38, 218, 75, 269, 216, 479, 50121, 50118, 10006, 16382, 35, 163, 50121, 50118, 33347, 4827, 35, 16225, 18, 6062, 11, 39, 414, 479, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 8976, 479, 50121, 50118, 10006, 16382, 35, 5331, 4230, 479, 2595, 1946, 65, 6317, 8, 33910, 799, 479, 18071, 4230, 237, 498, 41, 1946, 50121, 50118, 44518, 39322, 35, 8976, 53, 25522, 29183, 24303, 25522, 417, 32062, 6920, 254, 24303, 125, 38, 1266, 114, 24, 18, 2333, 2704, 50, 5251, 1065, 349, 97, 50, 220, 7, 349, 97, 15, 10, 2340, 6063, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 407, 45, 141, 203, 25522, 417, 32062, 6920, 254, 24303, 295, 45, 141, 747, 24, 18, 341, 2156, 53, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 23129, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 44518, 39322, 35, 305, 99, 18, 99, 18, 4505, 50, 2340, 479, 50121, 50118, 10006, 16382, 35, 8976, 2156, 14, 7971, 15, 15, 15, 5, 6063, 479, 50121, 50118, 44518, 39322, 35, 3216, 38, 437, 546, 259, 479, 125, 259, 18, 24, 18, 874, 2156, 50121, 50118, 36926, 13700, 31275, 35, 23129, 479, 50121, 50118, 44518, 39322, 35, 259, 67, 2156, 8, 122, 259, 18, 259, 18, 220, 7, 349, 97, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 38, 206, 24, 18, 25522, 29183, 24303, 24, 18, 10, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 38, 206, 25522, 29183, 24303, 142, 38, 33, 80, 20928, 12545, 23, 184, 479, 509, 16, 25490, 2156, 65, 16, 12194, 2156, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 24, 473, 24, 630, 75, 269, 948, 2156, 50121, 50118, 10006, 16382, 35, 98, 24, 7971, 479, 50121, 50118, 36926, 13700, 31275, 35, 53, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 8976, 479, 6748, 8845, 479, 50121, 50118, 36926, 13700, 31275, 35, 23129, 479, 50121, 50118, 44518, 39322, 35, 38, 206, 5, 5, 5, 3149, 21, 2333, 37463, 1065, 349, 97, 2156, 142, 47, 213, 723, 8, 159, 479, 50121, 50118, 10006, 16382, 35, 8976, 2156, 795, 479, 50121, 50118, 44518, 39322, 35, 178, 5, 5, 582, 5, 5, 5, 586, 16, 220, 7, 349, 97, 2156, 142, 47, 74, 213, 617, 8, 124, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 50121, 50118, 44518, 39322, 35, 280, 18, 141, 24, 18, 16, 2333, 25522, 29183, 24303, 77, 38, 356, 259, 50121, 50118, 10006, 16382, 35, 7447, 479, 50121, 50118, 44518, 39322, 35, 14, 18, 99, 38, 192, 479, 50121, 50118, 36926, 13700, 31275, 35, 8487, 2156, 7252, 479, 2780, 18, 847, 7, 5, 7859, 479, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 44518, 39322, 35, 25522, 31375, 1536, 9834, 24303, 50121, 50118, 10006, 16382, 35, 25522, 31375, 1536, 9834, 24303, 85, 18, 300, 7, 464, 479, 50121, 50118, 33347, 4827, 35, 8976, 157, 38, 206, 52, 33, 52, 218, 75, 33, 7, 2845, 59, 14, 122, 141, 147, 52, 40, 342, 5, 14893, 50121, 50118, 44518, 39322, 35, 38, 218, 75, 216, 479, 50121, 50118, 33347, 4827, 35, 95, 5, 50121, 50118, 10006, 16382, 35, 8976, 16, 24, 16, 3018, 12332, 479, 50121, 50118, 33347, 4827, 35, 14198, 479, 50121, 50118, 44518, 39322, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 43006, 479, 50121, 50118, 44518, 39322, 35, 152, 4286, 16, 11, 5, 3031, 1521, 2156, 53, 47, 197, 216, 147, 47, 74, 317, 10, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 8487, 2156, 157, 2084, 8976, 479, 39322, 2156, 11380, 479, 50121, 50118, 44518, 39322, 35, 178, 5, 1901, 37463, 5658, 52, 5731, 14, 17487, 1793, 37463, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 33347, 4827, 35, 8976, 157, 52, 95, 1317, 59, 5, 92, 37463, 806, 2156, 41437, 17487, 50121, 50118, 36926, 13700, 31275, 35, 5974, 2156, 37463, 479, 50121, 50118, 10006, 16382, 35, 8976, 38, 206, 24, 18, 24, 18, 1365, 13, 14, 2156, 147, 32, 47, 2156, 53, 172, 24, 161, 38, 437, 259, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 50121, 50118, 44518, 39322, 35, 8976, 479, 125, 172, 47, 197, 67, 465, 10, 317, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 36926, 13700, 31275, 35, 25379, 24, 678, 7, 25522, 417, 32062, 6920, 254, 24303, 50121, 50118, 10006, 16382, 35, 8976, 479, 50121, 50118, 44518, 39322, 35, 370, 115, 114, 47, 109, 24, 101, 42, 47, 115, 342, 24, 11, 10, 2797, 50, 402, 479, 370, 64, 1067, 88, 5, 2797, 479, 50121, 50118, 36926, 13700, 31275, 35, 256, 119, 479, 50121, 50118, 10006, 16382, 35, 8976, 2156, 10, 18896, 2156, 11380, 479, 50121, 50118, 36926, 13700, 31275, 35, 25522, 29183, 24303, 1491, 190, 2139, 2156, 50121, 50118, 33347, 4827, 35, 256, 119, 12, 298, 5471, 479, 50121, 50118, 36926, 13700, 31275, 35, 142, 10, 784, 16037, 25522, 417, 32062, 6920, 254, 24303, 47, 64, 146, 10, 182, 5685, 18896, 2156, 98, 24, 817, 24, 678, 7, 37463, 25522, 31375, 1536, 9834, 24303, 95, 342, 24, 10, 8508, 37463, 12213, 24, 50, 15, 5, 15, 5, 2576, 9, 5, 37463, 6063, 479, 50121, 50118, 33347, 4827, 35, 8976, 2085, 23, 5, 2576, 147, 47, 64, 64, 16068, 946, 24, 19, 47, 865, 14, 89, 18, 67, 10, 18896, 37463, 50121, 50118, 44518, 39322, 35, 8976, 479, 125, 25522, 417, 32062, 6920, 254, 24303, 8976, 2156, 14, 18, 67, 25522, 29183, 24303, 479, 50121, 50118, 36926, 13700, 31275, 35, 8976, 479, 50121, 50118, 10006, 16382, 35, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 44518, 39322, 1887, 737, 9, 2340, 8, 4505, 14893, 215, 25, 2704, 6, 10877, 6, 2369, 8, 586, 4, 3728, 4827, 2528, 5, 18896, 21, 10, 780, 477, 98, 24, 197, 28, 2325, 23472, 4, 1892, 27913, 39322, 1276, 7, 342, 5, 18896, 11, 10, 138, 6655, 4, 9020, 802, 25715, 21, 12148, 6, 6741, 31275, 1507, 8, 1887, 25715, 15, 5, 526, 4, 3728, 4827, 802, 6063, 797, 74, 28, 33639, 906, 19, 25715, 4, 2]}.
***** Running training *****
  Num examples = 1095
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 345
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 32.1571, 'rouge2': 11.0672, 'rougeL': 22.5156, 'rougeLsum': 28.1515}, 'ppl': {'perplexity': 4.438, 'ref_perplexity': 15.5365}, 'bertscore': {'precision': 87.9008, 'recall': 86.17, 'f1': 87.0139}}
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 33.1963, 'rouge2': 11.5755, 'rougeL': 22.7934, 'rougeLsum': 29.1452}, 'ppl': {'perplexity': 3.7433, 'ref_perplexity': 14.1359}, 'bertscore': {'precision': 88.0994, 'recall': 86.4803, 'f1': 87.273}}
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 36.0712, 'rouge2': 12.9916, 'rougeL': 23.9498, 'rougeLsum': 31.6304}, 'ppl': {'perplexity': 8.5532, 'ref_perplexity': 14.085}, 'bertscore': {'precision': 87.4679, 'recall': 86.9466, 'f1': 87.1967}}
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.8048, 'rouge2': 12.9996, 'rougeL': 24.5832, 'rougeLsum': 31.7543}, 'ppl': {'perplexity': 7.2478, 'ref_perplexity': 13.9788}, 'bertscore': {'precision': 87.928, 'recall': 86.9134, 'f1': 87.407}}
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
Using default tokenizer.
{'rouge': {'rouge1': 35.9929, 'rouge2': 12.8017, 'rougeL': 24.2486, 'rougeLsum': 31.736}, 'ppl': {'perplexity': 5.6792, 'ref_perplexity': 13.9413}, 'bertscore': {'precision': 87.8941, 'recall': 86.9659, 'f1': 87.4166}}

Start Predicting!
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
***** Running testing *****
  Num examples = 244
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x7d90eb4b7310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 114
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 1
  Total eval batch size = 1
***** Running testing *****
  Num examples = 244
  Instantaneous batch size per device = 1
  Total eval batch size = 1





Start Predicting!

Start Predicting!
***** Running testing *****
  Num examples = 1095
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
***** Running testing *****
  Num examples = 244
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x7ff2ee6395e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x75cc1f23a5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
***** Running testing *****
  Num examples = 244
  Instantaneous batch size per device = 4
  Total eval batch size = 4




Parameter 'function'=<function get_datasets.<locals>.preprocess_function at 0x7562af4395e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 109
  Instantaneous batch size per device = 4
  Total eval batch size = 4





Start Predicting!
***** Running testing *****
  Num examples = 237
  Instantaneous batch size per device = 4
  Total eval batch size = 4
***** Running testing *****
  Num examples = 244
  Instantaneous batch size per device = 4
  Total eval batch size = 4




