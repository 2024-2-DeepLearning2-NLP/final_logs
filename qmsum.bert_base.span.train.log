Parameter 'function'=<function get_omission_datasets.<locals>.span_func at 0x7d337e103b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 10415 of the training set: {'input_ids': [101, 1996, 2704, 1997, 3010, 4348, 4541, 2008, 1996, 2231, 2815, 5462, 2000, 8498, 1996, 2330, 2791, 1998, 16987, 2076, 1996, 5325, 1012, 2174, 1010, 1996, 4559, 2283, 8781, 2055, 1996, 11897, 28768, 2052, 2022, 2583, 2000, 2202, 6428, 8041, 2895, 2006, 4291, 4290, 1012, 9308, 1010, 1996, 2704, 1997, 3097, 3821, 4081, 2008, 1996, 3539, 2704, 1997, 16485, 1010, 1996, 6090, 3207, 7712, 1998, 2027, 2323, 2025, 2022, 5719, 2000, 7078, 5676, 2008, 1996, 11897, 11897, 11897, 28768, 2005, 1996, 2231, 2018, 2025, 2560, 1010, 1996, 2522, 17258, 1011, 2539, 7865, 1012, 102, 101, 2054, 2001, 2056, 2006, 2308, 3314, 1029, 1001, 1001, 102, 101, 10189, 1012, 3021, 22822, 22084, 2226, 1006, 2704, 1997, 5446, 1007, 1024, 2720, 1012, 3242, 1010, 2057, 3613, 2000, 6133, 2256, 10807, 3663, 1999, 1037, 3625, 5450, 1010, 1998, 2057, 1005, 2222, 3613, 2000, 2079, 2008, 1012, 102, 101, 10189, 1012, 5578, 13433, 18622, 6777, 2890, 1024, 2129, 2172, 2515, 1996, 2231, 1997, 2710, 12533, 2000, 1996, 2111, 1005, 1055, 3072, 1997, 2859, 1029, 102, 101, 10189, 1012, 3021, 22822, 22084, 2226, 1024, 2720, 1012, 3242, 1010, 2057, 2052, 2022, 3407, 2000, 3073, 2592, 1012, 2065, 1996, 2266, 2052, 2066, 2000, 4604, 2026, 2436, 3980, 3495, 1010, 1045, 1005, 1040, 2022, 3407, 2000, 3073, 2023, 2592, 1012, 102, 101, 10189, 1012, 5578, 13433, 18622, 6777, 2890, 1024, 2009, 4332, 2041, 1045, 2106, 1037, 2733, 3283, 1012, 2027, 2145, 4033, 1005, 1056, 3024, 6998, 2000, 1996, 3980, 1010, 1999, 3327, 1996, 3160, 4953, 2040, 8617, 2710, 1005, 1055, 3097, 1011, 2218, 7016, 1012, 2057, 2113, 2008, 5560, 1037, 2353, 1997, 2256, 7016, 2003, 3079, 2011, 15040, 1012, 2129, 2172, 1997, 2008, 7016, 2003, 3079, 2011, 18496, 2545, 2013, 1996, 2111, 1005, 1055, 3072, 1997, 2859, 1029, 102, 101, 10189, 1012, 3021, 22822, 22084, 2226, 1024, 2720, 1012, 3242, 1010, 2004, 1045, 2056, 1010, 2057, 1005, 1040, 2022, 3407, 2000, 7514, 2000, 2122, 3980, 3495, 1012, 2057, 1005, 2222, 2079, 2061, 1012, 2057, 1005, 2222, 2131, 2000, 2009, 1999, 2344, 1010, 2004, 2057, 2147, 2083, 2023, 5325, 1010, 2437, 2469, 2057, 3579, 2006, 16485, 2034, 1012, 102, 101, 10189, 1012, 5578, 13433, 18622, 6777, 2890, 1024, 2129, 2172, 2003, 3079, 2011, 18496, 2545, 2013, 8174, 9264, 1029, 102, 101, 10189, 1012, 3021, 22822, 22084, 2226, 1024, 2153, 1010, 2720, 1012, 3242, 1010, 2057, 1005, 1040, 2022, 3407, 2000, 3073, 2592, 1999, 2023, 7634, 2323, 1996, 2266, 4299, 2000, 4604, 1037, 5227, 3495, 2000, 2026, 2436, 1012, 102, 101, 10189, 1012, 5578, 13433, 18622, 6777, 2890, 1024, 2029, 1045, 2031, 1012, 2720, 1012, 3242, 1010, 3048, 2247, 2000, 1996, 14670, 1997, 1996, 7016, 2006, 2256, 2111, 1010, 2129, 2172, 2052, 1037, 1015, 1003, 3623, 1999, 1996, 4621, 3037, 3446, 2006, 2710, 1005, 1055, 2120, 7016, 3465, 3010, 26457, 1029, 102, 101, 10189, 1012, 3021, 22822, 22084, 2226, 1024, 2720, 1012, 3242, 1010, 2057, 3613, 2000, 6133, 2256, 9837, 3853, 24501, 26029, 5332, 6321, 1012, 1045, 1005, 1040, 2022, 3407, 2000, 2131, 3361, 16268, 2000, 1996, 2266, 2065, 2002, 1005, 1040, 2066, 2000, 4604, 2216, 3495, 2000, 2026, 2436, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 4, 5, -1, 1, 7]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 27375
Loss at step 10: 3.8375
Loss at step 20: 3.0492
Loss at step 30: 4.0231
Loss at step 40: 3.2074
Loss at step 50: 4.0923
Loss at step 60: 6.2350
Loss at step 70: 3.0599
Loss at step 80: 4.4523
Loss at step 90: 3.4773
Loss at step 100: 2.7104
Loss at step 110: 4.1792
Loss at step 120: 3.0831
Loss at step 130: 2.1661
Loss at step 140: 1.6809
Loss at step 150: 1.7026
Loss at step 160: 5.5577
Loss at step 170: 5.2099
Loss at step 180: 3.2551
Loss at step 190: 1.8983
Loss at step 200: 1.8393
Loss at step 210: 0.7429
Loss at step 220: 1.8197
Loss at step 230: 2.5901
Loss at step 240: 2.4811
Loss at step 250: 2.8455
Loss at step 260: 1.8823
Loss at step 270: 1.9658
Loss at step 280: 3.5836
Loss at step 290: 0.9109
Loss at step 300: 2.3380
Loss at step 310: 2.2377
Loss at step 320: 2.3813
Loss at step 330: 2.1503
Loss at step 340: 2.8680
Loss at step 350: 2.2745
Loss at step 360: 1.8194
Loss at step 370: 3.2307
Loss at step 380: 2.2233
Loss at step 390: 2.8140
Loss at step 400: 1.4881
Loss at step 410: 1.8142
Loss at step 420: 1.3093
Loss at step 430: 2.0371
Loss at step 440: 6.2659
Loss at step 450: 2.0889
Loss at step 460: 2.5595
Loss at step 470: 4.6216
Loss at step 480: 2.6021
Loss at step 490: 1.7693
Loss at step 500: 2.5270
Loss at step 510: 2.4738
Loss at step 520: 1.9601
Loss at step 530: 2.3026
Loss at step 540: 2.6456
Loss at step 550: 2.0111
Loss at step 560: 2.4555
Loss at step 570: 1.5785
Loss at step 580: 2.4837
Loss at step 590: 2.3682
Loss at step 600: 2.5596
Loss at step 610: 2.5431
Loss at step 620: 3.4405
Loss at step 630: 1.2449
Loss at step 640: 2.8234
Loss at step 650: 3.0704
Loss at step 660: 0.9594
Loss at step 670: 1.8716
Loss at step 680: 1.2249
Loss at step 690: 2.4023
Loss at step 700: 2.6323
Loss at step 710: 2.5028
Loss at step 720: 2.5071
Loss at step 730: 2.5760
Loss at step 740: 2.5010
Loss at step 750: 2.5330
Loss at step 760: 0.8374
Loss at step 770: 2.5212
Loss at step 780: 1.5652
Loss at step 790: 2.2227
Loss at step 800: 2.3401
Loss at step 810: 2.3014
Loss at step 820: 2.3673
Loss at step 830: 1.8294
Loss at step 840: 2.3861
Loss at step 850: 2.8341
Loss at step 860: 2.0229
Loss at step 870: 1.8219
Loss at step 880: 1.3381
Loss at step 890: 1.8965
Loss at step 900: 2.1586
Loss at step 910: 2.1189
Loss at step 920: 0.6726
Loss at step 930: 2.8223
Loss at step 940: 2.3556
Loss at step 950: 1.7933
Loss at step 960: 0.9771
Loss at step 970: 2.7257
Loss at step 980: 2.1990
Loss at step 990: 1.7826
Loss at step 1000: 1.8599
Loss at step 1010: 1.6565
Loss at step 1020: 2.5343
Loss at step 1030: 1.4674
Loss at step 1040: 1.2723
Loss at step 1050: 2.0646
Loss at step 1060: 1.3133
Loss at step 1070: 2.3107
Loss at step 1080: 1.6688
Loss at step 1090: 1.8632
Loss at step 1100: 1.6984
Loss at step 1110: 2.0100
Loss at step 1120: 2.7333
Loss at step 1130: 1.4437
Loss at step 1140: 1.4854
Loss at step 1150: 1.3292
Loss at step 1160: 2.4537
Loss at step 1170: 2.0655
Loss at step 1180: 2.2685
Loss at step 1190: 1.3635
Loss at step 1200: 1.6315
Loss at step 1210: 1.8724
Loss at step 1220: 2.5627
Loss at step 1230: 1.7534
Loss at step 1240: 1.5127
Loss at step 1250: 1.3110
Loss at step 1260: 1.6714
Loss at step 1270: 2.0778
Loss at step 1280: 1.8243
Loss at step 1290: 0.7420
Loss at step 1300: 0.6007
Loss at step 1310: 1.5418
Loss at step 1320: 0.9857
Loss at step 1330: 1.4639
Loss at step 1340: 1.9189
Loss at step 1350: 1.4282
Loss at step 1360: 2.5030
Loss at step 1370: 0.1583
Loss at step 1380: 1.7972
Loss at step 1390: 2.2573
Loss at step 1400: 1.6849
Loss at step 1410: 0.9079
Loss at step 1420: 1.7412
Loss at step 1430: 0.9688
Loss at step 1440: 2.3354
Loss at step 1450: 1.8123
Loss at step 1460: 0.4996
Loss at step 1470: 1.8655
Loss at step 1480: 0.5732
Loss at step 1490: 2.1606
Loss at step 1500: 1.6460
Loss at step 1510: 2.5280
Loss at step 1520: 2.6338
Loss at step 1530: 0.7834
Loss at step 1540: 1.3599
Loss at step 1550: 2.1003
Loss at step 1560: 2.0265
Loss at step 1570: 1.6751
Loss at step 1580: 1.5077
Loss at step 1590: 1.9273
Loss at step 1600: 1.6846
Loss at step 1610: 1.6557
Loss at step 1620: 2.0005
Loss at step 1630: 1.4051
Loss at step 1640: 2.3070
Loss at step 1650: 1.1499
Loss at step 1660: 2.3325
Loss at step 1670: 2.3867
Loss at step 1680: 1.3855
Loss at step 1690: 1.6804
Loss at step 1700: 2.2724
Loss at step 1710: 2.7946
Loss at step 1720: 2.3857
Loss at step 1730: 1.9625
Loss at step 1740: 0.2992
Loss at step 1750: 2.0376
Loss at step 1760: 1.9505
Loss at step 1770: 1.3298
Loss at step 1780: 1.0677
Loss at step 1790: 2.4779
Loss at step 1800: 1.6857
Loss at step 1810: 0.8687
Loss at step 1820: 0.5881
Loss at step 1830: 2.2239
Loss at step 1840: 1.3643
Loss at step 1850: 2.0942
Loss at step 1860: 1.5479
Loss at step 1870: 1.9037
Loss at step 1880: 0.9255
Loss at step 1890: 1.3123
Loss at step 1900: 1.5257
Loss at step 1910: 1.6379
Loss at step 1920: 1.4234
Loss at step 1930: 1.6503
Loss at step 1940: 0.9291
Loss at step 1950: 2.1276
Loss at step 1960: 1.4106
Loss at step 1970: 1.8328
Loss at step 1980: 2.2130
Loss at step 1990: 1.4395
Loss at step 2000: 1.0261
Loss at step 2010: 0.8949
Loss at step 2020: 0.7500
Loss at step 2030: 1.0517
Loss at step 2040: 1.5137
Loss at step 2050: 1.7287
Loss at step 2060: 1.7811
Loss at step 2070: 1.7988
Loss at step 2080: 1.3020
Loss at step 2090: 1.8784
Loss at step 2100: 1.4041
Loss at step 2110: 1.2512
Loss at step 2120: 1.0202
Loss at step 2130: 2.1288
Loss at step 2140: 1.5408
Loss at step 2150: 1.9933
Loss at step 2160: 0.9004
Loss at step 2170: 0.7023
Loss at step 2180: 0.8854
Loss at step 2190: 1.3890
Loss at step 2200: 1.0780
Loss at step 2210: 1.8555
Loss at step 2220: 1.4540
Loss at step 2230: 2.0446
Loss at step 2240: 1.5922
Loss at step 2250: 1.7351
Loss at step 2260: 1.3279
Loss at step 2270: 2.1400
Loss at step 2280: 1.2048
Loss at step 2290: 1.2533
Loss at step 2300: 0.7014
Loss at step 2310: 1.6681
Loss at step 2320: 1.2824
Loss at step 2330: 1.5691
Loss at step 2340: 1.9149
Loss at step 2350: 1.1325
Loss at step 2360: 1.3798
Loss at step 2370: 0.3716
Loss at step 2380: 1.6861
Loss at step 2390: 1.5220
Loss at step 2400: 1.4670
Loss at step 2410: 0.8896
Loss at step 2420: 1.1996
Loss at step 2430: 1.2196
Loss at step 2440: 1.7030
Loss at step 2450: 1.6649
Loss at step 2460: 1.9632
Loss at step 2470: 1.7764
Loss at step 2480: 1.3788
Loss at step 2490: 1.7919
Loss at step 2500: 1.7239
Loss at step 2510: 1.6366
Loss at step 2520: 1.3410
Loss at step 2530: 1.4481
Loss at step 2540: 1.1566
Loss at step 2550: 1.5693
Loss at step 2560: 1.6247
Loss at step 2570: 1.3392
Loss at step 2580: 1.2951
Loss at step 2590: 1.7005
Loss at step 2600: 1.5337
Loss at step 2610: 0.9234
Loss at step 2620: 2.2522
Loss at step 2630: 1.6383
Loss at step 2640: 1.3393
Loss at step 2650: 1.8095
Loss at step 2660: 0.7552
Loss at step 2670: 2.1935
Loss at step 2680: 1.6727
Loss at step 2690: 1.2647
Loss at step 2700: 1.2439
Loss at step 2710: 2.5476
Loss at step 2720: 1.8686
Loss at step 2730: 1.7759
Loss at step 2740: 1.5379
Loss at step 2750: 1.3920
Loss at step 2760: 1.8163
Loss at step 2770: 1.5673
Loss at step 2780: 1.1812
Loss at step 2790: 1.1113
Loss at step 2800: 1.2578
Loss at step 2810: 1.1914
Loss at step 2820: 1.6626
Loss at step 2830: 2.4498
Loss at step 2840: 2.1684
Loss at step 2850: 1.4247
Loss at step 2860: 1.6604
Loss at step 2870: 2.5190
Loss at step 2880: 1.5263
Loss at step 2890: 1.1524
Loss at step 2900: 0.4825
Loss at step 2910: 2.5376
Loss at step 2920: 1.6194
Loss at step 2930: 0.7335
Loss at step 2940: 1.4416
Loss at step 2950: 1.4146
Loss at step 2960: 1.6017
Loss at step 2970: 1.1855
Loss at step 2980: 1.4547
Loss at step 2990: 0.8305
Loss at step 3000: 1.6895
Loss at step 3010: 0.8755
Loss at step 3020: 1.0231
Loss at step 3030: 1.2965
Loss at step 3040: 1.0257
Loss at step 3050: 1.1381
Loss at step 3060: 1.9853
Loss at step 3070: 1.1737
Loss at step 3080: 1.6831
Loss at step 3090: 1.7891
Loss at step 3100: 1.0973
Loss at step 3110: 1.2131
Loss at step 3120: 1.5437
Loss at step 3130: 1.5714
Loss at step 3140: 1.3509
Loss at step 3150: 0.6886
Loss at step 3160: 0.9933
Loss at step 3170: 1.9287
Loss at step 3180: 1.7688
Loss at step 3190: 2.1748
Loss at step 3200: 1.6928
Loss at step 3210: 1.5640
Loss at step 3220: 1.9127
Loss at step 3230: 1.1242
Loss at step 3240: 1.0797
Loss at step 3250: 1.5561
Loss at step 3260: 1.2707
Loss at step 3270: 1.3113
Loss at step 3280: 2.3132
Loss at step 3290: 0.9641
Loss at step 3300: 1.1468
Loss at step 3310: 1.3460
Loss at step 3320: 1.5537
Loss at step 3330: 0.8200
Loss at step 3340: 1.3138
Loss at step 3350: 1.6183
Loss at step 3360: 1.3681
Loss at step 3370: 1.0560
Loss at step 3380: 0.4895
Loss at step 3390: 1.2246
Loss at step 3400: 0.8130
Loss at step 3410: 0.8639
Loss at step 3420: 1.3756
Loss at step 3430: 1.7595
Loss at step 3440: 1.6233
Loss at step 3450: 1.6507
Loss at step 3460: 1.8945
Loss at step 3470: 0.9834
Loss at step 3480: 0.5552
Loss at step 3490: 0.9681
Loss at step 3500: 1.3011
Loss at step 3510: 0.9304
Loss at step 3520: 1.9256
Loss at step 3530: 1.0532
Loss at step 3540: 1.6356
Loss at step 3550: 1.2408
Loss at step 3560: 0.9143
Loss at step 3570: 1.6170
Loss at step 3580: 1.3404
Loss at step 3590: 1.4151
Loss at step 3600: 1.2731
Loss at step 3610: 1.2198
Loss at step 3620: 1.8415
Loss at step 3630: 0.9764
Loss at step 3640: 2.0626
Loss at step 3650: 1.3202
Loss at step 3660: 1.4889
Loss at step 3670: 1.8284
Loss at step 3680: 1.0708
Loss at step 3690: 1.1271
Loss at step 3700: 0.8174
Loss at step 3710: 1.2954
Loss at step 3720: 0.6961
Loss at step 3730: 1.3043
Loss at step 3740: 0.9340
Loss at step 3750: 0.5679
Loss at step 3760: 1.2407
Loss at step 3770: 1.4721
Loss at step 3780: 1.0632
Loss at step 3790: 1.2895
Loss at step 3800: 1.5175
Loss at step 3810: 1.5201
Loss at step 3820: 1.0467
Loss at step 3830: 1.0859
Loss at step 3840: 1.7677
Loss at step 3850: 1.1423
Loss at step 3860: 1.6543
Loss at step 3870: 1.3774
Loss at step 3880: 0.5976
Loss at step 3890: 0.3470
Loss at step 3900: 1.5840
Loss at step 3910: 1.0616
Loss at step 3920: 0.7676
Loss at step 3930: 1.5250
Loss at step 3940: 0.8513
Loss at step 3950: 1.1326
Loss at step 3960: 1.7755
Loss at step 3970: 0.6406
Loss at step 3980: 1.0202
Loss at step 3990: 1.4249
Loss at step 4000: 1.2281
Loss at step 4010: 0.6599
Loss at step 4020: 2.3207
Loss at step 4030: 1.4055
Loss at step 4040: 1.5763
Loss at step 4050: 1.5398
Loss at step 4060: 0.6888
Loss at step 4070: 1.4563
Loss at step 4080: 1.2913
Loss at step 4090: 1.3841
Loss at step 4100: 0.9695
Loss at step 4110: 2.0026
Loss at step 4120: 1.3497
Loss at step 4130: 0.9072
Loss at step 4140: 1.4967
Loss at step 4150: 0.9600
Loss at step 4160: 1.6119
Loss at step 4170: 0.9786
Loss at step 4180: 1.7217
Loss at step 4190: 1.3201
Loss at step 4200: 2.0426
Loss at step 4210: 0.9616
Loss at step 4220: 1.1554
Loss at step 4230: 0.8887
Loss at step 4240: 1.0632
Loss at step 4250: 2.0371
Loss at step 4260: 0.6938
Loss at step 4270: 1.0908
Loss at step 4280: 1.3790
Loss at step 4290: 1.7785
Loss at step 4300: 0.5876
Loss at step 4310: 0.4375
Loss at step 4320: 1.3677
Loss at step 4330: 1.3859
Loss at step 4340: 1.1422
Loss at step 4350: 1.5178
Loss at step 4360: 2.0449
Loss at step 4370: 1.3534
Loss at step 4380: 0.1498
Loss at step 4390: 0.0717
Loss at step 4400: 1.4183
Loss at step 4410: 2.1294
Loss at step 4420: 0.9202
Loss at step 4430: 1.6233
Loss at step 4440: 0.7886
Loss at step 4450: 0.8325
Loss at step 4460: 0.9945
Loss at step 4470: 1.6593
Loss at step 4480: 1.5807
Loss at step 4490: 0.6566
Loss at step 4500: 0.7245
Loss at step 4510: 1.2184
Loss at step 4520: 1.1519
Loss at step 4530: 1.5986
Loss at step 4540: 1.5841
Loss at step 4550: 2.0531
Loss at step 4560: 1.0746
Loss at step 4570: 1.3695
Loss at step 4580: 0.8150
Loss at step 4590: 0.9662
Loss at step 4600: 0.7388
Loss at step 4610: 1.7010
Loss at step 4620: 0.4678
Loss at step 4630: 0.9906
Loss at step 4640: 0.8422
Loss at step 4650: 0.7906
Loss at step 4660: 0.6225
Loss at step 4670: 1.7857
Loss at step 4680: 0.9596
Loss at step 4690: 1.2415
Loss at step 4700: 1.1401
Loss at step 4710: 0.9789
Loss at step 4720: 1.1357
Loss at step 4730: 0.8477
Loss at step 4740: 0.6530
Loss at step 4750: 0.4632
Loss at step 4760: 1.2240
Loss at step 4770: 1.4959
Loss at step 4780: 1.0070
Loss at step 4790: 1.4317
Loss at step 4800: 0.8694
Loss at step 4810: 1.4276
Loss at step 4820: 1.2332
Loss at step 4830: 1.6276
Loss at step 4840: 1.0401
Loss at step 4850: 1.6550
Loss at step 4860: 2.4617
Loss at step 4870: 1.0951
Loss at step 4880: 0.7964
Loss at step 4890: 1.3186
Loss at step 4900: 1.8483
Loss at step 4910: 0.9738
Loss at step 4920: 1.0475
Loss at step 4930: 1.7612
Loss at step 4940: 1.0199
Loss at step 4950: 0.6170
Loss at step 4960: 1.0083
Loss at step 4970: 1.7651
Loss at step 4980: 1.0570
Loss at step 4990: 1.2960
Loss at step 5000: 0.8892
Loss at step 5010: 0.6604
Loss at step 5020: 1.4086
Loss at step 5030: 1.1834
Loss at step 5040: 0.7768
Loss at step 5050: 0.7721
Loss at step 5060: 0.2547
Loss at step 5070: 1.3205
Loss at step 5080: 1.6712
Loss at step 5090: 1.2222
Loss at step 5100: 1.2404
Loss at step 5110: 0.6136
Loss at step 5120: 1.2683
Loss at step 5130: 2.1986
Loss at step 5140: 1.5354
Loss at step 5150: 0.9450
Loss at step 5160: 1.0283
Loss at step 5170: 0.6277
Loss at step 5180: 1.0527
Loss at step 5190: 0.2996
Loss at step 5200: 1.4333
Loss at step 5210: 1.7101
Loss at step 5220: 1.0943
Loss at step 5230: 1.8758
Loss at step 5240: 1.1244
Loss at step 5250: 1.0281
Loss at step 5260: 0.5510
Loss at step 5270: 1.0296
Loss at step 5280: 0.8018
Loss at step 5290: 1.2231
Loss at step 5300: 1.3796
Loss at step 5310: 1.9329
Loss at step 5320: 0.6240
Loss at step 5330: 0.9342
Loss at step 5340: 0.8980
Loss at step 5350: 1.3762
Loss at step 5360: 0.6509
Loss at step 5370: 1.7660
Loss at step 5380: 1.6135
Loss at step 5390: 1.1344
Loss at step 5400: 1.0332
Loss at step 5410: 1.7168
Loss at step 5420: 1.0530
Loss at step 5430: 1.6181
Loss at step 5440: 1.0692
Loss at step 5450: 0.8038
Loss at step 5460: 0.7778
Loss at step 5470: 0.6336
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.696253, 'precision': [0.761591, 0.342974, 0.542045], 'recall': [0.931439, 0.205717, 0.289091], 'f1': [0.837995, 0.257178, 0.377075]}
{'accuracy': 0.854364, 'precision': 0.542045, 'recall': 0.289091, 'f1': 0.377075, 'WordR': 0.131316}
Loss at step 5480: 0.4436
Loss at step 5490: 0.7859
Loss at step 5500: 0.6306
Loss at step 5510: 0.8900
Loss at step 5520: 1.3293
Loss at step 5530: 1.3268
Loss at step 5540: 1.2682
Loss at step 5550: 0.7449
Loss at step 5560: 1.5886
Loss at step 5570: 1.1474
Loss at step 5580: 0.5988
Loss at step 5590: 0.1958
Loss at step 5600: 0.9204
Loss at step 5610: 1.7052
Loss at step 5620: 0.9457
Loss at step 5630: 0.9761
Loss at step 5640: 1.3293
Loss at step 5650: 0.3826
Loss at step 5660: 0.9561
Loss at step 5670: 0.9585
Loss at step 5680: 1.0919
Loss at step 5690: 1.1609
Loss at step 5700: 1.0733
Loss at step 5710: 0.6018
Loss at step 5720: 0.7584
Loss at step 5730: 1.2354
Loss at step 5740: 1.5119
Loss at step 5750: 1.3813
Loss at step 5760: 0.9728
Loss at step 5770: 0.9851
Loss at step 5780: 0.7444
Loss at step 5790: 0.6296
Loss at step 5800: 1.7460
Loss at step 5810: 0.8175
Loss at step 5820: 0.8595
Loss at step 5830: 1.1151
Loss at step 5840: 0.9637
Loss at step 5850: 1.1240
Loss at step 5860: 0.6721
Loss at step 5870: 0.4801
Loss at step 5880: 0.9530
Loss at step 5890: 1.3456
Loss at step 5900: 0.5886
Loss at step 5910: 0.4346
Loss at step 5920: 1.1088
Loss at step 5930: 1.3013
Loss at step 5940: 1.0635
Loss at step 5950: 1.3983
Loss at step 5960: 1.1608
Loss at step 5970: 0.9896
Loss at step 5980: 0.7334
Loss at step 5990: 1.2053
Loss at step 6000: 2.0578
Loss at step 6010: 1.0203
Loss at step 6020: 1.4523
Loss at step 6030: 0.5781
Loss at step 6040: 0.2202
Loss at step 6050: 0.8050
Loss at step 6060: 0.8426
Loss at step 6070: 1.1922
Loss at step 6080: 0.9363
Loss at step 6090: 1.1921
Loss at step 6100: 1.3777
Loss at step 6110: 0.6283
Loss at step 6120: 0.9765
Loss at step 6130: 0.9137
Loss at step 6140: 0.7472
Loss at step 6150: 1.0748
Loss at step 6160: 0.4156
Loss at step 6170: 0.6596
Loss at step 6180: 0.4715
Loss at step 6190: 0.7781
Loss at step 6200: 1.2843
Loss at step 6210: 0.7383
Loss at step 6220: 0.7752
Loss at step 6230: 1.6715
Loss at step 6240: 2.0459
Loss at step 6250: 1.0436
Loss at step 6260: 0.5555
Loss at step 6270: 1.1343
Loss at step 6280: 0.5251
Loss at step 6290: 0.8491
Loss at step 6300: 1.0172
Loss at step 6310: 1.3549
Loss at step 6320: 1.6945
Loss at step 6330: 0.5445
Loss at step 6340: 0.5818
Loss at step 6350: 0.4090
Loss at step 6360: 0.0597
Loss at step 6370: 0.6590
Loss at step 6380: 1.2399
Loss at step 6390: 0.1949
Loss at step 6400: 1.0216
Loss at step 6410: 1.2365
Loss at step 6420: 1.4683
Loss at step 6430: 1.0763
Loss at step 6440: 1.1614
Loss at step 6450: 1.5156
Loss at step 6460: 1.6292
Loss at step 6470: 1.1166
Loss at step 6480: 1.2004
Loss at step 6490: 0.9927
Loss at step 6500: 0.4250
Loss at step 6510: 0.5497
Loss at step 6520: 0.3158
Loss at step 6530: 1.1870
Loss at step 6540: 1.2120
Loss at step 6550: 0.3686
Loss at step 6560: 1.6987
Loss at step 6570: 1.1401
Loss at step 6580: 1.6004
Loss at step 6590: 0.6112
Loss at step 6600: 0.7155
Loss at step 6610: 1.0760
Loss at step 6620: 0.9095
Loss at step 6630: 0.6287
Loss at step 6640: 1.2726
Loss at step 6650: 0.4660
Loss at step 6660: 0.8993
Loss at step 6670: 1.4394
Loss at step 6680: 0.8599
Loss at step 6690: 0.9966
Loss at step 6700: 1.9233
Loss at step 6710: 1.1213
Loss at step 6720: 1.0139
Loss at step 6730: 0.3726
Loss at step 6740: 0.5148
Loss at step 6750: 0.7994
Loss at step 6760: 1.3576
Loss at step 6770: 0.9941
Loss at step 6780: 0.9357
Loss at step 6790: 1.0774
Loss at step 6800: 1.1548
Loss at step 6810: 1.0862
Loss at step 6820: 1.2435
Loss at step 6830: 0.7080
Loss at step 6840: 0.9173
Loss at step 6850: 0.1175
Loss at step 6860: 0.4624
Loss at step 6870: 1.5401
Loss at step 6880: 0.7681
Loss at step 6890: 0.6959
Loss at step 6900: 0.8063
Loss at step 6910: 0.8565
Loss at step 6920: 1.1644
Loss at step 6930: 1.0157
Loss at step 6940: 1.3725
Loss at step 6950: 0.4780
Loss at step 6960: 0.6802
Loss at step 6970: 1.0390
Loss at step 6980: 1.2415
Loss at step 6990: 0.5755
Loss at step 7000: 0.3538
Loss at step 7010: 1.7614
Loss at step 7020: 0.6074
Loss at step 7030: 0.5917
Loss at step 7040: 1.1688
Loss at step 7050: 0.6532
Loss at step 7060: 1.2470
Loss at step 7070: 1.1762
Loss at step 7080: 0.6246
Loss at step 7090: 0.7375
Loss at step 7100: 1.4081
Loss at step 7110: 1.3224
Loss at step 7120: 1.1068
Loss at step 7130: 1.2492
Loss at step 7140: 0.8097
Loss at step 7150: 1.4606
Loss at step 7160: 0.2819
Loss at step 7170: 1.9837
Loss at step 7180: 0.8957
Loss at step 7190: 0.4666
Loss at step 7200: 0.3151
Loss at step 7210: 1.3532
Loss at step 7220: 0.6434
Loss at step 7230: 1.5676
Loss at step 7240: 0.8928
Loss at step 7250: 1.5963
Loss at step 7260: 1.1659
Loss at step 7270: 0.8529
Loss at step 7280: 1.7840
Loss at step 7290: 0.7925
Loss at step 7300: 1.0970
Loss at step 7310: 0.3627
Loss at step 7320: 1.2304
Loss at step 7330: 1.0564
Loss at step 7340: 1.0103
Loss at step 7350: 1.1810
Loss at step 7360: 1.0716
Loss at step 7370: 0.0210
Loss at step 7380: 0.9935
Loss at step 7390: 0.9242
Loss at step 7400: 1.1015
Loss at step 7410: 0.6732
Loss at step 7420: 0.8018
Loss at step 7430: 0.7938
Loss at step 7440: 1.3305
Loss at step 7450: 1.5093
Loss at step 7460: 0.4637
Loss at step 7470: 1.0875
Loss at step 7480: 1.0035
Loss at step 7490: 0.8901
Loss at step 7500: 1.0035
Loss at step 7510: 1.1749
Loss at step 7520: 0.6348
Loss at step 7530: 1.1917
Loss at step 7540: 1.1199
Loss at step 7550: 0.1704
Loss at step 7560: 0.3671
Loss at step 7570: 0.6609
Loss at step 7580: 0.2572
Loss at step 7590: 1.1677
Loss at step 7600: 0.9262
Loss at step 7610: 1.3065
Loss at step 7620: 0.8102
Loss at step 7630: 1.1417
Loss at step 7640: 1.0755
Loss at step 7650: 0.4038
Loss at step 7660: 0.8264
Loss at step 7670: 0.4049
Loss at step 7680: 1.6622
Loss at step 7690: 1.0037
Loss at step 7700: 0.7155
Loss at step 7710: 0.2890
Loss at step 7720: 0.6510
Loss at step 7730: 0.8352
Loss at step 7740: 0.7858
Loss at step 7750: 1.0933
Loss at step 7760: 1.1000
Loss at step 7770: 1.3187
Loss at step 7780: 1.5408
Loss at step 7790: 0.8639
Loss at step 7800: 1.1289
Loss at step 7810: 1.3615
Loss at step 7820: 1.0311
Loss at step 7830: 1.2558
Loss at step 7840: 0.5801
Loss at step 7850: 1.0071
Loss at step 7860: 0.7130
Loss at step 7870: 0.8318
Loss at step 7880: 0.5703
Loss at step 7890: 0.7730
Loss at step 7900: 0.8683
Loss at step 7910: 0.8010
Loss at step 7920: 0.7989
Loss at step 7930: 1.4310
Loss at step 7940: 0.5409
Loss at step 7950: 1.1036
Loss at step 7960: 0.1355
Loss at step 7970: 0.9999
Loss at step 7980: 0.2760
Loss at step 7990: 1.2135
Loss at step 8000: 0.0368
Loss at step 8010: 0.1705
Loss at step 8020: 0.8328
Loss at step 8030: 0.7496
Loss at step 8040: 0.8614
Loss at step 8050: 0.7016
Loss at step 8060: 0.8165
Loss at step 8070: 0.7928
Loss at step 8080: 0.6495
Loss at step 8090: 1.0354
Loss at step 8100: 0.8176
Loss at step 8110: 0.9116
Loss at step 8120: 1.2845
Loss at step 8130: 1.5652
Loss at step 8140: 0.5202
Loss at step 8150: 0.9516
Loss at step 8160: 1.3716
Loss at step 8170: 0.6322
Loss at step 8180: 1.0955
Loss at step 8190: 0.5686
Loss at step 8200: 1.9796
Loss at step 8210: 0.8081
Loss at step 8220: 1.7689
Loss at step 8230: 0.9781
Loss at step 8240: 0.5648
Loss at step 8250: 0.7654
Loss at step 8260: 0.6244
Loss at step 8270: 0.8753
Loss at step 8280: 0.4517
Loss at step 8290: 0.3223
Loss at step 8300: 0.7095
Loss at step 8310: 1.1369
Loss at step 8320: 0.9783
Loss at step 8330: 1.2195
Loss at step 8340: 0.7365
Loss at step 8350: 1.1329
Loss at step 8360: 0.9670
Loss at step 8370: 1.4229
Loss at step 8380: 0.7500
Loss at step 8390: 0.5603
Loss at step 8400: 2.0575
Loss at step 8410: 1.4876
Loss at step 8420: 0.5651
Loss at step 8430: 0.8323
Loss at step 8440: 1.3668
Loss at step 8450: 0.3580
Loss at step 8460: 1.1765
Loss at step 8470: 0.4642
Loss at step 8480: 1.1413
Loss at step 8490: 0.7053
Loss at step 8500: 1.0712
Loss at step 8510: 1.0852
Loss at step 8520: 0.9498
Loss at step 8530: 0.6448
Loss at step 8540: 1.4982
Loss at step 8550: 0.8875
Loss at step 8560: 0.6206
Loss at step 8570: 1.3113
Loss at step 8580: 1.3057
Loss at step 8590: 0.6884
Loss at step 8600: 0.4138
Loss at step 8610: 0.9636
Loss at step 8620: 0.5751
Loss at step 8630: 0.1816
Loss at step 8640: 1.4461
Loss at step 8650: 0.7434
Loss at step 8660: 0.9748
Loss at step 8670: 0.8252
Loss at step 8680: 0.6650
Loss at step 8690: 0.8331
Loss at step 8700: 0.5739
Loss at step 8710: 0.4763
Loss at step 8720: 0.6270
Loss at step 8730: 0.3510
Loss at step 8740: 0.3612
Loss at step 8750: 0.6762
Loss at step 8760: 1.2000
Loss at step 8770: 1.8060
Loss at step 8780: 0.8513
Loss at step 8790: 0.1530
Loss at step 8800: 0.9426
Loss at step 8810: 0.8964
Loss at step 8820: 0.1409
Loss at step 8830: 1.3943
Loss at step 8840: 0.8817
Loss at step 8850: 1.3083
Loss at step 8860: 1.4313
Loss at step 8870: 0.6107
Loss at step 8880: 0.8664
Loss at step 8890: 0.8162
Loss at step 8900: 1.6397
Loss at step 8910: 0.5066
Loss at step 8920: 0.8543
Loss at step 8930: 1.0917
Loss at step 8940: 0.5471
Loss at step 8950: 0.8020
Loss at step 8960: 0.6520
Loss at step 8970: 0.4844
Loss at step 8980: 0.6392
Loss at step 8990: 1.1120
Loss at step 9000: 0.7243
Loss at step 9010: 0.7769
Loss at step 9020: 0.9915
Loss at step 9030: 0.9415
Loss at step 9040: 1.2907
Loss at step 9050: 0.5639
Loss at step 9060: 1.4224
Loss at step 9070: 1.1328
Loss at step 9080: 0.8387
Loss at step 9090: 0.5322
Loss at step 9100: 0.4932
Loss at step 9110: 0.1234
Loss at step 9120: 1.4378
Loss at step 9130: 0.5797
Loss at step 9140: 0.5554
Loss at step 9150: 0.5151
Loss at step 9160: 0.5684
Loss at step 9170: 0.7952
Loss at step 9180: 0.6120
Loss at step 9190: 0.7107
Loss at step 9200: 0.9342
Loss at step 9210: 0.6671
Loss at step 9220: 0.7276
Loss at step 9230: 1.5394
Loss at step 9240: 0.3651
Loss at step 9250: 0.0496
Loss at step 9260: 0.5359
Loss at step 9270: 1.0131
Loss at step 9280: 1.5457
Loss at step 9290: 2.3149
Loss at step 9300: 0.7663
Loss at step 9310: 0.4808
Loss at step 9320: 0.5092
Loss at step 9330: 1.1351
Loss at step 9340: 0.9679
Loss at step 9350: 0.6775
Loss at step 9360: 1.2373
Loss at step 9370: 0.0769
Loss at step 9380: 0.9578
Loss at step 9390: 0.6862
Loss at step 9400: 1.1326
Loss at step 9410: 0.2815
Loss at step 9420: 0.5869
Loss at step 9430: 0.7082
Loss at step 9440: 0.6666
Loss at step 9450: 0.6829
Loss at step 9460: 0.6105
Loss at step 9470: 0.7877
Loss at step 9480: 0.1889
Loss at step 9490: 0.6964
Loss at step 9500: 1.9826
Loss at step 9510: 0.8292
Loss at step 9520: 0.8639
Loss at step 9530: 0.7138
Loss at step 9540: 0.9181
Loss at step 9550: 0.4803
Loss at step 9560: 0.6231
Loss at step 9570: 0.7702
Loss at step 9580: 1.0941
Loss at step 9590: 0.6898
Loss at step 9600: 0.7944
Loss at step 9610: 0.6160
Loss at step 9620: 1.0879
Loss at step 9630: 0.4160
Loss at step 9640: 1.2015
Loss at step 9650: 0.9301
Loss at step 9660: 0.9158
Loss at step 9670: 0.4386
Loss at step 9680: 0.7274
Loss at step 9690: 0.1373
Loss at step 9700: 0.3340
Loss at step 9710: 0.7287
Loss at step 9720: 0.6915
Loss at step 9730: 0.4996
Loss at step 9740: 1.0533
Loss at step 9750: 0.1220
Loss at step 9760: 0.4896
Loss at step 9770: 0.4275
Loss at step 9780: 0.2451
Loss at step 9790: 0.7061
Loss at step 9800: 0.1753
Loss at step 9810: 0.6293
Loss at step 9820: 0.7097
Loss at step 9830: 0.4553
Loss at step 9840: 0.7023
Loss at step 9850: 1.3050
Loss at step 9860: 1.0254
Loss at step 9870: 0.3053
Loss at step 9880: 0.6200
Loss at step 9890: 0.6645
Loss at step 9900: 0.6680
Loss at step 9910: 0.0972
Loss at step 9920: 0.5686
Loss at step 9930: 0.3659
Loss at step 9940: 0.4368
Loss at step 9950: 0.2112
Loss at step 9960: 0.5492
Loss at step 9970: 0.8418
Loss at step 9980: 1.1080
Loss at step 9990: 0.9638
Loss at step 10000: 0.0687
Loss at step 10010: 0.3079
Loss at step 10020: 0.0378
Loss at step 10030: 0.1105
Loss at step 10040: 0.7342
Loss at step 10050: 0.5917
Loss at step 10060: 0.5807
Loss at step 10070: 0.3716
Loss at step 10080: 0.3544
Loss at step 10090: 1.0706
Loss at step 10100: 0.5733
Loss at step 10110: 2.0695
Loss at step 10120: 0.5005
Loss at step 10130: 0.9435
Loss at step 10140: 0.6873
Loss at step 10150: 0.6416
Loss at step 10160: 0.4958
Loss at step 10170: 0.3578
Loss at step 10180: 0.8988
Loss at step 10190: 0.1729
Loss at step 10200: 0.7530
Loss at step 10210: 1.2623
Loss at step 10220: 0.8924
Loss at step 10230: 1.9232
Loss at step 10240: 1.1558
Loss at step 10250: 1.3530
Loss at step 10260: 1.1585
Loss at step 10270: 0.6122
Loss at step 10280: 0.6981
Loss at step 10290: 0.9320
Loss at step 10300: 0.9175
Loss at step 10310: 1.2746
Loss at step 10320: 0.3681
Loss at step 10330: 0.8112
Loss at step 10340: 0.9775
Loss at step 10350: 0.2359
Loss at step 10360: 0.3896
Loss at step 10370: 0.9113
Loss at step 10380: 0.7604
Loss at step 10390: 0.2179
Loss at step 10400: 0.2041
Loss at step 10410: 0.4116
Loss at step 10420: 0.3909
Loss at step 10430: 0.5610
Loss at step 10440: 0.5327
Loss at step 10450: 1.0955
Loss at step 10460: 2.1185
Loss at step 10470: 0.8032
Loss at step 10480: 0.1665
Loss at step 10490: 0.6007
Loss at step 10500: 0.4898
Loss at step 10510: 0.8155
Loss at step 10520: 0.4904
Loss at step 10530: 0.2440
Loss at step 10540: 1.3733
Loss at step 10550: 1.6347
Loss at step 10560: 1.0112
Loss at step 10570: 1.6986
Loss at step 10580: 2.0109
Loss at step 10590: 1.1592
Loss at step 10600: 1.0190
Loss at step 10610: 0.1917
Loss at step 10620: 0.8831
Loss at step 10630: 0.8964
Loss at step 10640: 0.3718
Loss at step 10650: 0.9277
Loss at step 10660: 1.1592
Loss at step 10670: 0.3635
Loss at step 10680: 0.9307
Loss at step 10690: 0.6186
Loss at step 10700: 1.1579
Loss at step 10710: 1.0521
Loss at step 10720: 0.6259
Loss at step 10730: 0.2203
Loss at step 10740: 0.3859
Loss at step 10750: 0.8958
Loss at step 10760: 0.5507
Loss at step 10770: 0.9392
Loss at step 10780: 0.9613
Loss at step 10790: 1.0870
Loss at step 10800: 0.9021
Loss at step 10810: 0.1447
Loss at step 10820: 0.9693
Loss at step 10830: 0.7961
Loss at step 10840: 0.5039
Loss at step 10850: 0.2494
Loss at step 10860: 0.8624
Loss at step 10870: 0.9147
Loss at step 10880: 1.3270
Loss at step 10890: 0.2683
Loss at step 10900: 0.3475
Loss at step 10910: 1.1591
Loss at step 10920: 0.3676
Loss at step 10930: 1.0633
Loss at step 10940: 0.1122
Loss at step 10950: 0.1416
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.70249, 'precision': [0.84442, 0.379551, 0.588571], 'recall': [0.848982, 0.532372, 0.280909], 'f1': [0.846695, 0.443156, 0.380308]}
{'accuracy': 0.860417, 'precision': 0.588571, 'recall': 0.280909, 'f1': 0.380308, 'WordR': 0.014344}
Loss at step 10960: 0.3621
Loss at step 10970: 0.9383
Loss at step 10980: 0.3538
Loss at step 10990: 0.3370
Loss at step 11000: 1.1063
Loss at step 11010: 1.5028
Loss at step 11020: 0.1022
Loss at step 11030: 0.2198
Loss at step 11040: 0.9324
Loss at step 11050: 0.1784
Loss at step 11060: 0.9815
Loss at step 11070: 0.7092
Loss at step 11080: 1.2479
Loss at step 11090: 0.1339
Loss at step 11100: 0.8052
Loss at step 11110: 0.8960
Loss at step 11120: 0.1172
Loss at step 11130: 0.5971
Loss at step 11140: 0.2777
Loss at step 11150: 0.6241
Loss at step 11160: 0.5192
Loss at step 11170: 1.0933
Loss at step 11180: 0.6986
Loss at step 11190: 0.6016
Loss at step 11200: 0.1944
Loss at step 11210: 0.2402
Loss at step 11220: 0.2288
Loss at step 11230: 0.2159
Loss at step 11240: 0.7111
Loss at step 11250: 0.5120
Loss at step 11260: 0.6734
Loss at step 11270: 0.0164
Loss at step 11280: 0.6561
Loss at step 11290: 0.9968
Loss at step 11300: 0.1833
Loss at step 11310: 1.2942
Loss at step 11320: 0.5733
Loss at step 11330: 0.9167
Loss at step 11340: 0.5195
Loss at step 11350: 0.5984
Loss at step 11360: 0.7139
Loss at step 11370: 0.6021
Loss at step 11380: 1.0369
Loss at step 11390: 0.4161
Loss at step 11400: 0.4718
Loss at step 11410: 0.7464
Loss at step 11420: 0.6700
Loss at step 11430: 0.8221
Loss at step 11440: 0.5413
Loss at step 11450: 0.2312
Loss at step 11460: 0.1810
Loss at step 11470: 0.2414
Loss at step 11480: 0.7068
Loss at step 11490: 0.9535
Loss at step 11500: 0.5665
Loss at step 11510: 0.2254
Loss at step 11520: 0.1869
Loss at step 11530: 0.6539
Loss at step 11540: 0.2892
Loss at step 11550: 0.9333
Loss at step 11560: 0.6140
Loss at step 11570: 0.2651
Loss at step 11580: 0.9014
Loss at step 11590: 0.6569
Loss at step 11600: 0.4810
Loss at step 11610: 0.0440
Loss at step 11620: 1.1124
Loss at step 11630: 0.5450
Loss at step 11640: 0.8456
Loss at step 11650: 0.1741
Loss at step 11660: 0.1369
Loss at step 11670: 0.2600
Loss at step 11680: 0.9767
Loss at step 11690: 3.0470
Loss at step 11700: 0.2362
Loss at step 11710: 0.2699
Loss at step 11720: 0.2733
Loss at step 11730: 0.8402
Loss at step 11740: 0.3076
Loss at step 11750: 1.7667
Loss at step 11760: 0.1245
Loss at step 11770: 0.7988
Loss at step 11780: 0.5277
Loss at step 11790: 0.2777
Loss at step 11800: 0.5703
Loss at step 11810: 0.1627
Loss at step 11820: 0.9980
Loss at step 11830: 0.9392
Loss at step 11840: 0.5097
Loss at step 11850: 0.7921
Loss at step 11860: 0.6931
Loss at step 11870: 0.3652
Loss at step 11880: 0.0034
Loss at step 11890: 1.4326
Loss at step 11900: 0.2773
Loss at step 11910: 1.5496
Loss at step 11920: 1.0480
Loss at step 11930: 0.7801
Loss at step 11940: 0.2917
Loss at step 11950: 0.1542
Loss at step 11960: 0.5547
Loss at step 11970: 0.5760
Loss at step 11980: 0.7120
Loss at step 11990: 0.7038
Loss at step 12000: 0.0083
Loss at step 12010: 0.0032
Loss at step 12020: 0.5598
Loss at step 12030: 0.3855
Loss at step 12040: 0.4730
Loss at step 12050: 0.6167
Loss at step 12060: 0.4709
Loss at step 12070: 0.4572
Loss at step 12080: 0.4362
Loss at step 12090: 0.4617
Loss at step 12100: 0.3094
Loss at step 12110: 0.0640
Loss at step 12120: 0.9232
Loss at step 12130: 0.1086
Loss at step 12140: 0.1430
Loss at step 12150: 0.5262
Loss at step 12160: 0.2144
Loss at step 12170: 0.5308
Loss at step 12180: 0.3140
Loss at step 12190: 0.3853
Loss at step 12200: 0.2621
Loss at step 12210: 0.4620
Loss at step 12220: 0.5684
Loss at step 12230: 0.6014
Loss at step 12240: 0.4604
Loss at step 12250: 0.4088
Loss at step 12260: 0.2747
Loss at step 12270: 0.7926
Loss at step 12280: 0.2902
Loss at step 12290: 0.2841
Loss at step 12300: 0.4022
Loss at step 12310: 0.5957
Loss at step 12320: 1.1016
Loss at step 12330: 0.8289
Loss at step 12340: 0.5167
Loss at step 12350: 0.3403
Loss at step 12360: 0.2412
Loss at step 12370: 0.3603
Loss at step 12380: 0.6603
Loss at step 12390: 0.4950
Loss at step 12400: 0.0231
Loss at step 12410: 0.8065
Loss at step 12420: 0.1532
Loss at step 12430: 0.0001
Loss at step 12440: 0.9244
Loss at step 12450: 0.5021
Loss at step 12460: 0.4823
Loss at step 12470: 0.4294
Loss at step 12480: 0.9515
Loss at step 12490: 0.1356
Loss at step 12500: 0.4000
Loss at step 12510: 0.3639
Loss at step 12520: 0.4925
Loss at step 12530: 0.5086
Loss at step 12540: 0.7018
Loss at step 12550: 0.5888
Loss at step 12560: 0.4048
Loss at step 12570: 1.0055
Loss at step 12580: 0.2365
Loss at step 12590: 0.6220
Loss at step 12600: 1.1115
Loss at step 12610: 0.5285
Loss at step 12620: 0.3936
Loss at step 12630: 0.4083
Loss at step 12640: 0.6902
Loss at step 12650: 0.2138
Loss at step 12660: 0.3329
Loss at step 12670: 0.3852
Loss at step 12680: 1.1834
Loss at step 12690: 0.6305
Loss at step 12700: 0.5550
Loss at step 12710: 0.4750
Loss at step 12720: 0.4905
Loss at step 12730: 0.6325
Loss at step 12740: 0.2839
Loss at step 12750: 0.2792
Loss at step 12760: 0.2031
Loss at step 12770: 0.6318
Loss at step 12780: 0.7123
Loss at step 12790: 0.5914
Loss at step 12800: 0.1240
Loss at step 12810: 0.4640
Loss at step 12820: 0.1694
Loss at step 12830: 1.2773
Loss at step 12840: 0.0153
Loss at step 12850: 0.3510
Loss at step 12860: 0.6874
Loss at step 12870: 0.1552
Loss at step 12880: 0.6589
Loss at step 12890: 0.4388
Loss at step 12900: 1.1928
Loss at step 12910: 0.6844
Loss at step 12920: 0.0688
Loss at step 12930: 0.5583
Loss at step 12940: 0.2012
Loss at step 12950: 0.5239
Loss at step 12960: 0.1246
Loss at step 12970: 0.6546
Loss at step 12980: 0.2179
Loss at step 12990: 0.3556
Loss at step 13000: 0.4091
Loss at step 13010: 0.0016
Loss at step 13020: 0.3209
Loss at step 13030: 0.1730
Loss at step 13040: 0.0088
Loss at step 13050: 0.0099
Loss at step 13060: 0.3729
Loss at step 13070: 0.9050
Loss at step 13080: 0.7063
Loss at step 13090: 0.5295
Loss at step 13100: 1.2896
Loss at step 13110: 0.7099
Loss at step 13120: 0.4908
Loss at step 13130: 0.2327
Loss at step 13140: 0.5360
Loss at step 13150: 0.4111
Loss at step 13160: 0.3449
Loss at step 13170: 0.5035
Loss at step 13180: 0.5818
Loss at step 13190: 0.8540
Loss at step 13200: 0.3213
Loss at step 13210: 0.8966
Loss at step 13220: 0.6048
Loss at step 13230: 0.4055
Loss at step 13240: 0.4906
Loss at step 13250: 0.0409
Loss at step 13260: 0.4286
Loss at step 13270: 0.3706
Loss at step 13280: 0.3811
Loss at step 13290: 0.4541
Loss at step 13300: 0.5928
Loss at step 13310: 1.2018
Loss at step 13320: 0.3030
Loss at step 13330: 1.3191
Loss at step 13340: 0.4024
Loss at step 13350: 0.4728
Loss at step 13360: 0.4935
Loss at step 13370: 0.0999
Loss at step 13380: 0.9435
Loss at step 13390: 0.2318
Loss at step 13400: 0.0068
Loss at step 13410: 1.3602
Loss at step 13420: 0.2036
Loss at step 13430: 0.8344
Loss at step 13440: 0.1591
Loss at step 13450: 0.2891
Loss at step 13460: 0.3706
Loss at step 13470: 0.3009
Loss at step 13480: 0.8685
Loss at step 13490: 0.0875
Loss at step 13500: 0.6258
Loss at step 13510: 0.0116
Loss at step 13520: 0.4788
Loss at step 13530: 0.5233
Loss at step 13540: 0.5105
Loss at step 13550: 0.2190
Loss at step 13560: 0.4166
Loss at step 13570: 0.6903
Loss at step 13580: 0.6661
Loss at step 13590: 0.7702
Loss at step 13600: 0.8364
Loss at step 13610: 0.1988
Loss at step 13620: 0.8062
Loss at step 13630: 0.6422
Loss at step 13640: 0.3913
Loss at step 13650: 0.1134
Loss at step 13660: 0.2640
Loss at step 13670: 2.0033
Loss at step 13680: 0.0420
Loss at step 13690: 0.0567
Loss at step 13700: 0.6587
Loss at step 13710: 0.3690
Loss at step 13720: 0.3169
Loss at step 13730: 0.1931
Loss at step 13740: 2.6559
Loss at step 13750: 0.0001
Loss at step 13760: 0.8848
Loss at step 13770: 0.8832
Loss at step 13780: 0.8231
Loss at step 13790: 0.2355
Loss at step 13800: 0.2945
Loss at step 13810: 0.0515
Loss at step 13820: 0.4695
Loss at step 13830: 0.0081
Loss at step 13840: 0.7976
Loss at step 13850: 0.6812
Loss at step 13860: 0.2133
Loss at step 13870: 1.0084
Loss at step 13880: 0.6131
Loss at step 13890: 0.6152
Loss at step 13900: 0.0713
Loss at step 13910: 0.8433
Loss at step 13920: 0.0865
Loss at step 13930: 0.0874
Loss at step 13940: 0.6034
Loss at step 13950: 0.1817
Loss at step 13960: 0.5265
Loss at step 13970: 0.1134
Loss at step 13980: 0.5579
Loss at step 13990: 0.5332
Loss at step 14000: 0.1411
Loss at step 14010: 0.3646
Loss at step 14020: 0.4548
Loss at step 14030: 0.3245
Loss at step 14040: 0.2105
Loss at step 14050: 1.3524
Loss at step 14060: 0.4633
Loss at step 14070: 0.4010
Loss at step 14080: 0.3053
Loss at step 14090: 0.5965
Loss at step 14100: 0.6667
Loss at step 14110: 0.3909
Loss at step 14120: 0.0919
Loss at step 14130: 0.1534
Loss at step 14140: 0.7362
Loss at step 14150: 0.2672
Loss at step 14160: 0.3033
Loss at step 14170: 0.3323
Loss at step 14180: 0.4403
Loss at step 14190: 0.3322
Loss at step 14200: 0.4871
Loss at step 14210: 0.5182
Loss at step 14220: 1.1277
Loss at step 14230: 0.6760
Loss at step 14240: 0.4028
Loss at step 14250: 0.9557
Loss at step 14260: 0.4976
Loss at step 14270: 0.4757
Loss at step 14280: 0.3220
Loss at step 14290: 0.5749
Loss at step 14300: 2.8245
Loss at step 14310: 0.9902
Loss at step 14320: 0.1372
Loss at step 14330: 0.0264
Loss at step 14340: 0.5266
Loss at step 14350: 0.3576
Loss at step 14360: 0.2606
Loss at step 14370: 0.8879
Loss at step 14380: 0.1923
Loss at step 14390: 0.1281
Loss at step 14400: 0.4449
Loss at step 14410: 0.2956
Loss at step 14420: 0.5592
Loss at step 14430: 0.6495
Loss at step 14440: 0.5321
Loss at step 14450: 0.8113
Loss at step 14460: 0.1761
Loss at step 14470: 0.3959
Loss at step 14480: 1.1160
Loss at step 14490: 0.0475
Loss at step 14500: 0.6980
Loss at step 14510: 0.6961
Loss at step 14520: 0.0232
Loss at step 14530: 0.4571
Loss at step 14540: 0.5582
Loss at step 14550: 0.4857
Loss at step 14560: 0.9199
Loss at step 14570: 0.0497
Loss at step 14580: 0.2037
Loss at step 14590: 1.0772
Loss at step 14600: 0.2015
Loss at step 14610: 0.2823
Loss at step 14620: 1.6067
Loss at step 14630: 0.4942
Loss at step 14640: 0.6896
Loss at step 14650: 0.0919
Loss at step 14660: 0.1076
Loss at step 14670: 0.2095
Loss at step 14680: 0.2336
Loss at step 14690: 0.1809
Loss at step 14700: 0.1719
Loss at step 14710: 1.0166
Loss at step 14720: 1.3371
Loss at step 14730: 0.0494
Loss at step 14740: 0.7285
Loss at step 14750: 0.3142
Loss at step 14760: 0.1513
Loss at step 14770: 0.9542
Loss at step 14780: 0.7740
Loss at step 14790: 0.0003
Loss at step 14800: 0.6107
Loss at step 14810: 0.0931
Loss at step 14820: 0.0484
Loss at step 14830: 0.7460
Loss at step 14840: 1.0796
Loss at step 14850: 0.6852
Loss at step 14860: 0.4357
Loss at step 14870: 0.3768
Loss at step 14880: 0.2198
Loss at step 14890: 0.4115
Loss at step 14900: 0.7909
Loss at step 14910: 0.0020
Loss at step 14920: 0.2828
Loss at step 14930: 0.6711
Loss at step 14940: 1.2447
Loss at step 14950: 0.9241
Loss at step 14960: 0.1030
Loss at step 14970: 0.7554
Loss at step 14980: 0.4409
Loss at step 14990: 0.0181
Loss at step 15000: 0.0904
Loss at step 15010: 0.3712
Loss at step 15020: 0.0757
Loss at step 15030: 1.1182
Loss at step 15040: 0.5364
Loss at step 15050: 0.1527
Loss at step 15060: 0.3167
Loss at step 15070: 0.6783
Loss at step 15080: 0.3801
Loss at step 15090: 0.2464
Loss at step 15100: 0.1230
Loss at step 15110: 0.3151
Loss at step 15120: 0.0486
Loss at step 15130: 0.2374
Loss at step 15140: 0.0083
Loss at step 15150: 0.2253
Loss at step 15160: 0.3443
Loss at step 15170: 0.1552
Loss at step 15180: 0.4305
Loss at step 15190: 0.7015
Loss at step 15200: 0.0643
Loss at step 15210: 0.6540
Loss at step 15220: 0.3282
Loss at step 15230: 0.2533
Loss at step 15240: 0.0440
Loss at step 15250: 0.1362
Loss at step 15260: 0.5977
Loss at step 15270: 0.5568
Loss at step 15280: 0.0131
Loss at step 15290: 0.5209
Loss at step 15300: 0.0163
Loss at step 15310: 0.5890
Loss at step 15320: 0.1147
Loss at step 15330: 0.7413
Loss at step 15340: 0.7351
Loss at step 15350: 0.1215
Loss at step 15360: 0.2023
Loss at step 15370: 0.5910
Loss at step 15380: 0.6004
Loss at step 15390: 0.0526
Loss at step 15400: 0.2302
Loss at step 15410: 1.2540
Loss at step 15420: 0.4334
Loss at step 15430: 0.5998
Loss at step 15440: 0.0498
Loss at step 15450: 0.7651
Loss at step 15460: 0.7477
Loss at step 15470: 0.0811
Loss at step 15480: 0.8365
Loss at step 15490: 1.1012
Loss at step 15500: 0.1880
Loss at step 15510: 0.1409
Loss at step 15520: 0.9806
Loss at step 15530: 0.3632
Loss at step 15540: 0.5310
Loss at step 15550: 0.7070
Loss at step 15560: 0.4682
Loss at step 15570: 0.3127
Loss at step 15580: 0.0438
Loss at step 15590: 0.2379
Loss at step 15600: 0.3037
Loss at step 15610: 1.0443
Loss at step 15620: 0.1151
Loss at step 15630: 0.1960
Loss at step 15640: 0.4338
Loss at step 15650: 0.7006
Loss at step 15660: 0.0488
Loss at step 15670: 0.1163
Loss at step 15680: 0.2613
Loss at step 15690: 0.2335
Loss at step 15700: 0.4112
Loss at step 15710: 0.3079
Loss at step 15720: 0.0233
Loss at step 15730: 0.9316
Loss at step 15740: 1.1901
Loss at step 15750: 0.7559
Loss at step 15760: 0.2778
Loss at step 15770: 1.1840
Loss at step 15780: 0.0729
Loss at step 15790: 2.6021
Loss at step 15800: 0.3960
Loss at step 15810: 0.4744
Loss at step 15820: 0.2636
Loss at step 15830: 1.0303
Loss at step 15840: 0.3060
Loss at step 15850: 0.6526
Loss at step 15860: 0.0021
Loss at step 15870: 1.1312
Loss at step 15880: 0.0199
Loss at step 15890: 0.5238
Loss at step 15900: 0.0184
Loss at step 15910: 0.1428
Loss at step 15920: 0.5239
Loss at step 15930: 0.3027
Loss at step 15940: 0.1354
Loss at step 15950: 0.3229
Loss at step 15960: 0.6284
Loss at step 15970: 0.5763
Loss at step 15980: 0.4366
Loss at step 15990: 0.2817
Loss at step 16000: 0.1191
Loss at step 16010: 1.1016
Loss at step 16020: 0.4018
Loss at step 16030: 2.0309
Loss at step 16040: 0.0067
Loss at step 16050: 0.9770
Loss at step 16060: 0.3114
Loss at step 16070: 0.0825
Loss at step 16080: 0.8319
Loss at step 16090: 0.0281
Loss at step 16100: 0.4278
Loss at step 16110: 0.5001
Loss at step 16120: 0.1450
Loss at step 16130: 0.1405
Loss at step 16140: 0.1939
Loss at step 16150: 0.7514
Loss at step 16160: 0.5258
Loss at step 16170: 0.7060
Loss at step 16180: 0.7990
Loss at step 16190: 0.3758
Loss at step 16200: 0.5697
Loss at step 16210: 0.4619
Loss at step 16220: 0.1386
Loss at step 16230: 0.2570
Loss at step 16240: 0.1178
Loss at step 16250: 0.5807
Loss at step 16260: 0.6264
Loss at step 16270: 0.5667
Loss at step 16280: 0.0540
Loss at step 16290: 0.4956
Loss at step 16300: 0.4285
Loss at step 16310: 0.2574
Loss at step 16320: 1.1387
Loss at step 16330: 0.1267
Loss at step 16340: 0.1733
Loss at step 16350: 0.2449
Loss at step 16360: 0.2043
Loss at step 16370: 0.9740
Loss at step 16380: 1.3327
Loss at step 16390: 1.2881
Loss at step 16400: 0.2318
Loss at step 16410: 0.0320
Loss at step 16420: 1.6473
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.721573, 'precision': [0.833093, 0.404867, 0.563394], 'recall': [0.890386, 0.426826, 0.358182], 'f1': [0.860787, 0.415557, 0.43794]}
{'accuracy': 0.859816, 'precision': 0.563394, 'recall': 0.358182, 'f1': 0.43794, 'WordR': 0.131316}
Loss at step 16430: 0.0513
Loss at step 16440: 0.2097
Loss at step 16450: 0.1298
Loss at step 16460: 0.0325
Loss at step 16470: 0.1299
Loss at step 16480: 1.4337
Loss at step 16490: 0.3845
Loss at step 16500: 0.5293
Loss at step 16510: 0.5928
Loss at step 16520: 0.9879
Loss at step 16530: 0.8524
Loss at step 16540: 0.4662
Loss at step 16550: 0.0128
Loss at step 16560: 0.0448
Loss at step 16570: 0.2833
Loss at step 16580: 0.7738
Loss at step 16590: 0.3558
Loss at step 16600: 0.1332
Loss at step 16610: 0.0067
Loss at step 16620: 0.1311
Loss at step 16630: 0.0326
Loss at step 16640: 0.5636
Loss at step 16650: 0.1564
Loss at step 16660: 1.0463
Loss at step 16670: 0.0540
Loss at step 16680: 0.0245
Loss at step 16690: 0.2685
Loss at step 16700: 0.5883
Loss at step 16710: 0.0072
Loss at step 16720: 0.0857
Loss at step 16730: 0.0254
Loss at step 16740: 0.9260
Loss at step 16750: 0.2170
Loss at step 16760: 0.0984
Loss at step 16770: 0.7781
Loss at step 16780: 0.3586
Loss at step 16790: 0.0588
Loss at step 16800: 0.0001
Loss at step 16810: 0.3911
Loss at step 16820: 0.1655
Loss at step 16830: 0.3083
Loss at step 16840: 0.5852
Loss at step 16850: 0.3030
Loss at step 16860: 0.5096
Loss at step 16870: 0.8394
Loss at step 16880: 0.0008
Loss at step 16890: 0.2455
Loss at step 16900: 0.2192
Loss at step 16910: 0.2364
Loss at step 16920: 0.0172
Loss at step 16930: 0.5514
Loss at step 16940: 0.0155
Loss at step 16950: 0.0278
Loss at step 16960: 1.0359
Loss at step 16970: 0.1765
Loss at step 16980: 0.7450
Loss at step 16990: 0.6412
Loss at step 17000: 0.2100
Loss at step 17010: 0.1830
Loss at step 17020: 0.4493
Loss at step 17030: 0.0003
Loss at step 17040: 0.7284
Loss at step 17050: 0.3035
Loss at step 17060: 0.2417
Loss at step 17070: 0.0095
Loss at step 17080: 0.3873
Loss at step 17090: 0.0154
Loss at step 17100: 1.1189
Loss at step 17110: 0.5589
Loss at step 17120: 0.0808
Loss at step 17130: 0.2613
Loss at step 17140: 0.0001
Loss at step 17150: 0.0276
Loss at step 17160: 0.1258
Loss at step 17170: 0.0929
Loss at step 17180: 0.0270
Loss at step 17190: 0.0924
Loss at step 17200: 0.0020
Loss at step 17210: 0.2593
Loss at step 17220: 0.0364
Loss at step 17230: 0.4396
Loss at step 17240: 0.0938
Loss at step 17250: 0.2746
Loss at step 17260: 1.4188
Loss at step 17270: 0.0737
Loss at step 17280: 1.2540
Loss at step 17290: 1.0526
Loss at step 17300: 0.1367
Loss at step 17310: 0.2509
Loss at step 17320: 0.0289
Loss at step 17330: 0.6141
Loss at step 17340: 0.9830
Loss at step 17350: 0.2324
Loss at step 17360: 0.0861
Loss at step 17370: 0.8491
Loss at step 17380: 0.5845
Loss at step 17390: 0.3306
Loss at step 17400: 0.4392
Loss at step 17410: 0.4666
Loss at step 17420: 0.1602
Loss at step 17430: 0.0158
Loss at step 17440: 0.0189
Loss at step 17450: 0.6552
Loss at step 17460: 0.3984
Loss at step 17470: 0.0068
Loss at step 17480: 0.0330
Loss at step 17490: 0.3431
Loss at step 17500: 0.6723
Loss at step 17510: 0.0432
Loss at step 17520: 0.1900
Loss at step 17530: 0.4434
Loss at step 17540: 1.9844
Loss at step 17550: 0.0103
Loss at step 17560: 0.2482
Loss at step 17570: 0.6230
Loss at step 17580: 0.0012
Loss at step 17590: 0.0014
Loss at step 17600: 0.0332
Loss at step 17610: 1.7267
Loss at step 17620: 0.1526
Loss at step 17630: 0.7201
Loss at step 17640: 0.1736
Loss at step 17650: 0.6508
Loss at step 17660: 0.3124
Loss at step 17670: 0.8937
Loss at step 17680: 0.4777
Loss at step 17690: 0.8177
Loss at step 17700: 0.8922
Loss at step 17710: 0.2787
Loss at step 17720: 0.3092
Loss at step 17730: 0.2110
Loss at step 17740: 0.7884
Loss at step 17750: 1.0119
Loss at step 17760: 0.7095
Loss at step 17770: 0.7789
Loss at step 17780: 0.2340
Loss at step 17790: 0.0000
Loss at step 17800: 0.8669
Loss at step 17810: 0.1413
Loss at step 17820: 0.9858
Loss at step 17830: 0.1752
Loss at step 17840: 0.2073
Loss at step 17850: 0.0036
Loss at step 17860: 0.3096
Loss at step 17870: 0.1952
Loss at step 17880: 0.3093
Loss at step 17890: 0.0995
Loss at step 17900: 0.6144
Loss at step 17910: 0.2289
Loss at step 17920: 0.0174
Loss at step 17930: 1.2480
Loss at step 17940: 0.6372
Loss at step 17950: 0.0000
Loss at step 17960: 0.3071
Loss at step 17970: 0.0007
Loss at step 17980: 0.0100
Loss at step 17990: 0.6049
Loss at step 18000: 0.0007
Loss at step 18010: 0.0034
Loss at step 18020: 0.5456
Loss at step 18030: 0.1372
Loss at step 18040: 0.2053
Loss at step 18050: 0.5946
Loss at step 18060: 1.1074
Loss at step 18070: 0.1016
Loss at step 18080: 0.0308
Loss at step 18090: 0.1202
Loss at step 18100: 0.1465
Loss at step 18110: 0.3800
Loss at step 18120: 0.0572
Loss at step 18130: 0.0230
Loss at step 18140: 0.0426
Loss at step 18150: 0.2273
Loss at step 18160: 0.1910
Loss at step 18170: 0.3895
Loss at step 18180: 0.2520
Loss at step 18190: 0.1384
Loss at step 18200: 0.0007
Loss at step 18210: 0.0349
Loss at step 18220: 0.1777
Loss at step 18230: 0.0007
Loss at step 18240: 1.2772
Loss at step 18250: 0.5262
Loss at step 18260: 0.1427
Loss at step 18270: 0.3310
Loss at step 18280: 0.0352
Loss at step 18290: 0.3923
Loss at step 18300: 0.0780
Loss at step 18310: 1.0097
Loss at step 18320: 0.0718
Loss at step 18330: 0.2157
Loss at step 18340: 0.0097
Loss at step 18350: 1.4534
Loss at step 18360: 0.3413
Loss at step 18370: 0.6346
Loss at step 18380: 0.6500
Loss at step 18390: 0.1326
Loss at step 18400: 0.6463
Loss at step 18410: 0.0157
Loss at step 18420: 0.0206
Loss at step 18430: 0.7126
Loss at step 18440: 0.0000
Loss at step 18450: 0.3426
Loss at step 18460: 0.0318
Loss at step 18470: 0.5883
Loss at step 18480: 0.0666
Loss at step 18490: 0.2909
Loss at step 18500: 0.4736
Loss at step 18510: 0.1043
Loss at step 18520: 0.0903
Loss at step 18530: 0.2166
Loss at step 18540: 0.4773
Loss at step 18550: 0.0054
Loss at step 18560: 0.5181
Loss at step 18570: 0.4162
Loss at step 18580: 0.0105
Loss at step 18590: 0.7822
Loss at step 18600: 0.1240
Loss at step 18610: 0.0002
Loss at step 18620: 0.0375
Loss at step 18630: 0.7394
Loss at step 18640: 0.0369
Loss at step 18650: 0.3715
Loss at step 18660: 0.3618
Loss at step 18670: 0.0425
Loss at step 18680: 0.6419
Loss at step 18690: 0.2022
Loss at step 18700: 0.0002
Loss at step 18710: 0.0928
Loss at step 18720: 0.0658
Loss at step 18730: 0.8084
Loss at step 18740: 0.0593
Loss at step 18750: 0.0794
Loss at step 18760: 0.2679
Loss at step 18770: 0.0647
Loss at step 18780: 0.3969
Loss at step 18790: 0.0036
Loss at step 18800: 0.1550
Loss at step 18810: 0.0687
Loss at step 18820: 0.0024
Loss at step 18830: 0.0555
Loss at step 18840: 0.6517
Loss at step 18850: 0.0111
Loss at step 18860: 1.8804
Loss at step 18870: 0.6629
Loss at step 18880: 0.5561
Loss at step 18890: 0.0797
Loss at step 18900: 0.4150
Loss at step 18910: 0.4462
Loss at step 18920: 0.0040
Loss at step 18930: 0.0017
Loss at step 18940: 0.0622
Loss at step 18950: 0.6576
Loss at step 18960: 0.0705
Loss at step 18970: 0.7690
Loss at step 18980: 0.2444
Loss at step 18990: 0.9646
Loss at step 19000: 0.0281
Loss at step 19010: 0.4680
Loss at step 19020: 0.1566
Loss at step 19030: 0.7989
Loss at step 19040: 0.0415
Loss at step 19050: 0.1249
Loss at step 19060: 0.2936
Loss at step 19070: 1.2070
Loss at step 19080: 0.0784
Loss at step 19090: 0.1208
Loss at step 19100: 0.3538
Loss at step 19110: 0.2760
Loss at step 19120: 0.1366
Loss at step 19130: 1.5251
Loss at step 19140: 0.3782
Loss at step 19150: 0.4720
Loss at step 19160: 0.5961
Loss at step 19170: 0.1602
Loss at step 19180: 0.1247
Loss at step 19190: 0.0252
Loss at step 19200: 0.0810
Loss at step 19210: 0.4203
Loss at step 19220: 0.0430
Loss at step 19230: 0.0325
Loss at step 19240: 0.5198
Loss at step 19250: 0.3238
Loss at step 19260: 0.0094
Loss at step 19270: 0.9015
Loss at step 19280: 0.0038
Loss at step 19290: 0.0189
Loss at step 19300: 0.4233
Loss at step 19310: 0.3454
Loss at step 19320: 0.2670
Loss at step 19330: 1.0659
Loss at step 19340: 0.6990
Loss at step 19350: 0.7697
Loss at step 19360: 0.0000
Loss at step 19370: 0.0018
Loss at step 19380: 0.0336
Loss at step 19390: 0.5873
Loss at step 19400: 0.8794
Loss at step 19410: 1.1947
Loss at step 19420: 0.8961
Loss at step 19430: 0.3025
Loss at step 19440: 0.0013
Loss at step 19450: 0.0017
Loss at step 19460: 0.3612
Loss at step 19470: 0.1079
Loss at step 19480: 0.0000
Loss at step 19490: 0.3061
Loss at step 19500: 0.2587
Loss at step 19510: 0.2484
Loss at step 19520: 0.6714
Loss at step 19530: 0.0084
Loss at step 19540: 0.0088
Loss at step 19550: 0.6845
Loss at step 19560: 0.3196
Loss at step 19570: 0.6472
Loss at step 19580: 0.0913
Loss at step 19590: 0.6640
Loss at step 19600: 0.7691
Loss at step 19610: 0.0142
Loss at step 19620: 0.0029
Loss at step 19630: 0.1587
Loss at step 19640: 0.4198
Loss at step 19650: 0.0005
Loss at step 19660: 0.1418
Loss at step 19670: 0.0014
Loss at step 19680: 1.0113
Loss at step 19690: 0.4060
Loss at step 19700: 0.2563
Loss at step 19710: 0.0049
Loss at step 19720: 0.5887
Loss at step 19730: 0.0128
Loss at step 19740: 0.0606
Loss at step 19750: 0.3043
Loss at step 19760: 0.5388
Loss at step 19770: 0.5237
Loss at step 19780: 0.5862
Loss at step 19790: 0.0454
Loss at step 19800: 0.3084
Loss at step 19810: 0.2744
Loss at step 19820: 0.4342
Loss at step 19830: 0.4433
Loss at step 19840: 0.3846
Loss at step 19850: 0.2415
Loss at step 19860: 0.2185
Loss at step 19870: 0.6039
Loss at step 19880: 1.0753
Loss at step 19890: 0.7149
Loss at step 19900: 0.0243
Loss at step 19910: 0.0392
Loss at step 19920: 0.1951
Loss at step 19930: 0.4971
Loss at step 19940: 0.7622
Loss at step 19950: 0.0734
Loss at step 19960: 0.0152
Loss at step 19970: 0.4591
Loss at step 19980: 0.1883
Loss at step 19990: 0.1310
Loss at step 20000: 0.5828
Loss at step 20010: 0.2237
Loss at step 20020: 0.3875
Loss at step 20030: 0.2290
Loss at step 20040: 0.6551
Loss at step 20050: 0.0479
Loss at step 20060: 0.0866
Loss at step 20070: 0.3031
Loss at step 20080: 0.3717
Loss at step 20090: 0.0641
Loss at step 20100: 0.0046
Loss at step 20110: 1.0221
Loss at step 20120: 0.3675
Loss at step 20130: 0.5429
Loss at step 20140: 0.0247
Loss at step 20150: 0.0574
Loss at step 20160: 0.1252
Loss at step 20170: 0.0964
Loss at step 20180: 0.2993
Loss at step 20190: 0.7927
Loss at step 20200: 0.2004
Loss at step 20210: 0.3113
Loss at step 20220: 0.0396
Loss at step 20230: 0.1348
Loss at step 20240: 0.0821
Loss at step 20250: 0.2247
Loss at step 20260: 0.0006
Loss at step 20270: 0.0008
Loss at step 20280: 1.4592
Loss at step 20290: 1.0503
Loss at step 20300: 0.4189
Loss at step 20310: 0.0356
Loss at step 20320: 0.7765
Loss at step 20330: 0.2339
Loss at step 20340: 1.0924
Loss at step 20350: 0.1768
Loss at step 20360: 0.1363
Loss at step 20370: 0.0407
Loss at step 20380: 0.1803
Loss at step 20390: 0.5711
Loss at step 20400: 0.2323
Loss at step 20410: 0.0013
Loss at step 20420: 0.1138
Loss at step 20430: 0.7924
Loss at step 20440: 0.3812
Loss at step 20450: 0.1171
Loss at step 20460: 0.0615
Loss at step 20470: 0.1076
Loss at step 20480: 0.0455
Loss at step 20490: 1.1204
Loss at step 20500: 0.0333
Loss at step 20510: 0.4649
Loss at step 20520: 0.5074
Loss at step 20530: 0.0911
Loss at step 20540: 0.0246
Loss at step 20550: 0.2296
Loss at step 20560: 0.0038
Loss at step 20570: 0.0176
Loss at step 20580: 0.0177
Loss at step 20590: 0.6654
Loss at step 20600: 0.1056
Loss at step 20610: 0.2765
Loss at step 20620: 0.0006
Loss at step 20630: 0.0000
Loss at step 20640: 0.2021
Loss at step 20650: 0.1329
Loss at step 20660: 0.4300
Loss at step 20670: 0.3696
Loss at step 20680: 0.1051
Loss at step 20690: 0.1951
Loss at step 20700: 0.0438
Loss at step 20710: 0.4442
Loss at step 20720: 0.2524
Loss at step 20730: 0.0015
Loss at step 20740: 0.4649
Loss at step 20750: 0.0488
Loss at step 20760: 0.0002
Loss at step 20770: 0.4250
Loss at step 20780: 0.0000
Loss at step 20790: 0.0853
Loss at step 20800: 1.8789
Loss at step 20810: 0.0526
Loss at step 20820: 0.0337
Loss at step 20830: 0.0036
Loss at step 20840: 0.0000
Loss at step 20850: 0.1891
Loss at step 20860: 0.2787
Loss at step 20870: 0.3427
Loss at step 20880: 0.2320
Loss at step 20890: 1.0744
Loss at step 20900: 1.8122
Loss at step 20910: 0.1190
Loss at step 20920: 0.0017
Loss at step 20930: 0.0693
Loss at step 20940: 1.0440
Loss at step 20950: 0.5468
Loss at step 20960: 1.0570
Loss at step 20970: 0.2989
Loss at step 20980: 0.3537
Loss at step 20990: 0.0017
Loss at step 21000: 0.0432
Loss at step 21010: 0.5167
Loss at step 21020: 0.1229
Loss at step 21030: 0.4677
Loss at step 21040: 0.0476
Loss at step 21050: 0.0230
Loss at step 21060: 0.2975
Loss at step 21070: 0.2679
Loss at step 21080: 0.6237
Loss at step 21090: 0.0002
Loss at step 21100: 0.3442
Loss at step 21110: 0.0004
Loss at step 21120: 0.0182
Loss at step 21130: 0.6532
Loss at step 21140: 0.0064
Loss at step 21150: 0.0158
Loss at step 21160: 0.6496
Loss at step 21170: 0.2130
Loss at step 21180: 0.1157
Loss at step 21190: 0.0000
Loss at step 21200: 0.2666
Loss at step 21210: 0.4189
Loss at step 21220: 0.0120
Loss at step 21230: 0.1531
Loss at step 21240: 0.0004
Loss at step 21250: 0.0102
Loss at step 21260: 0.6937
Loss at step 21270: 0.2992
Loss at step 21280: 0.3574
Loss at step 21290: 0.5195
Loss at step 21300: 0.3929
Loss at step 21310: 0.0388
Loss at step 21320: 0.4073
Loss at step 21330: 1.5659
Loss at step 21340: 0.3535
Loss at step 21350: 0.0169
Loss at step 21360: 0.0770
Loss at step 21370: 0.0229
Loss at step 21380: 0.7104
Loss at step 21390: 0.6140
Loss at step 21400: 0.1462
Loss at step 21410: 0.0777
Loss at step 21420: 0.3817
Loss at step 21430: 0.0010
Loss at step 21440: 0.4004
Loss at step 21450: 0.9127
Loss at step 21460: 0.5835
Loss at step 21470: 0.2111
Loss at step 21480: 0.2183
Loss at step 21490: 0.0349
Loss at step 21500: 0.4392
Loss at step 21510: 0.2229
Loss at step 21520: 2.5946
Loss at step 21530: 0.0353
Loss at step 21540: 0.1951
Loss at step 21550: 0.6555
Loss at step 21560: 0.0834
Loss at step 21570: 1.1152
Loss at step 21580: 0.3366
Loss at step 21590: 0.0000
Loss at step 21600: 0.0103
Loss at step 21610: 0.6780
Loss at step 21620: 0.1636
Loss at step 21630: 0.2649
Loss at step 21640: 0.0003
Loss at step 21650: 0.0674
Loss at step 21660: 0.4245
Loss at step 21670: 1.1014
Loss at step 21680: 0.5085
Loss at step 21690: 0.0281
Loss at step 21700: 0.0025
Loss at step 21710: 1.2174
Loss at step 21720: 0.3882
Loss at step 21730: 0.1367
Loss at step 21740: 0.0990
Loss at step 21750: 0.5841
Loss at step 21760: 0.0802
Loss at step 21770: 0.0355
Loss at step 21780: 0.0009
Loss at step 21790: 1.1954
Loss at step 21800: 0.1022
Loss at step 21810: 0.7475
Loss at step 21820: 0.1369
Loss at step 21830: 0.0999
Loss at step 21840: 0.3210
Loss at step 21850: 0.0050
Loss at step 21860: 0.5661
Loss at step 21870: 1.1694
Loss at step 21880: 0.0151
Loss at step 21890: 0.2368
Loss at step 21900: 0.5192
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.709421, 'precision': [0.862071, 0.387585, 0.570618], 'recall': [0.846947, 0.556804, 0.304848], 'f1': [0.854442, 0.457034, 0.397393]}
{'accuracy': 0.859031, 'precision': 0.570618, 'recall': 0.304848, 'f1': 0.397393, 'WordR': 0.014344}
Loss at step 21910: 0.0034
Loss at step 21920: 0.2111
Loss at step 21930: 0.0409
Loss at step 21940: 0.0432
Loss at step 21950: 0.3285
Loss at step 21960: 0.0021
Loss at step 21970: 0.3931
Loss at step 21980: 0.2695
Loss at step 21990: 0.6990
Loss at step 22000: 1.3818
Loss at step 22010: 0.2479
Loss at step 22020: 1.1221
Loss at step 22030: 0.1477
Loss at step 22040: 0.0597
Loss at step 22050: 0.0039
Loss at step 22060: 0.0046
Loss at step 22070: 0.3804
Loss at step 22080: 0.0046
Loss at step 22090: 0.0464
Loss at step 22100: 0.0002
Loss at step 22110: 0.4874
Loss at step 22120: 0.1952
Loss at step 22130: 0.0133
Loss at step 22140: 0.0191
Loss at step 22150: 0.0221
Loss at step 22160: 3.2716
Loss at step 22170: 0.1446
Loss at step 22180: 0.0406
Loss at step 22190: 0.0069
Loss at step 22200: 0.8748
Loss at step 22210: 0.5738
Loss at step 22220: 0.0002
Loss at step 22230: 0.6716
Loss at step 22240: 0.1408
Loss at step 22250: 0.0461
Loss at step 22260: 0.0002
Loss at step 22270: 0.0333
Loss at step 22280: 0.1924
Loss at step 22290: 0.0015
Loss at step 22300: 0.0040
Loss at step 22310: 0.2287
Loss at step 22320: 0.2046
Loss at step 22330: 0.2047
Loss at step 22340: 0.7449
Loss at step 22350: 0.0001
Loss at step 22360: 1.0756
Loss at step 22370: 0.1772
Loss at step 22380: 0.0191
Loss at step 22390: 0.1420
Loss at step 22400: 0.7116
Loss at step 22410: 0.0418
Loss at step 22420: 0.1714
Loss at step 22430: 0.0184
Loss at step 22440: 0.0829
Loss at step 22450: 0.0013
Loss at step 22460: 0.0013
Loss at step 22470: 0.0000
Loss at step 22480: 0.0001
Loss at step 22490: 0.0398
Loss at step 22500: 0.0297
Loss at step 22510: 0.0192
Loss at step 22520: 0.5428
Loss at step 22530: 0.8382
Loss at step 22540: 0.1829
Loss at step 22550: 0.0898
Loss at step 22560: 0.1104
Loss at step 22570: 0.0002
Loss at step 22580: 0.0000
Loss at step 22590: 0.0558
Loss at step 22600: 0.5946
Loss at step 22610: 0.0439
Loss at step 22620: 0.0000
Loss at step 22630: 0.0048
Loss at step 22640: 0.1772
Loss at step 22650: 0.0159
Loss at step 22660: 0.2403
Loss at step 22670: 0.1734
Loss at step 22680: 0.1059
Loss at step 22690: 0.0122
Loss at step 22700: 0.3567
Loss at step 22710: 0.0000
Loss at step 22720: 0.0001
Loss at step 22730: 0.0587
Loss at step 22740: 0.0001
Loss at step 22750: 0.0919
Loss at step 22760: 0.0066
Loss at step 22770: 0.3837
Loss at step 22780: 0.8804
Loss at step 22790: 0.0231
Loss at step 22800: 0.0044
Loss at step 22810: 0.1920
Loss at step 22820: 0.0110
Loss at step 22830: 0.0611
Loss at step 22840: 0.0006
Loss at step 22850: 2.8352
Loss at step 22860: 0.2045
Loss at step 22870: 0.0003
Loss at step 22880: 0.0006
Loss at step 22890: 0.0382
Loss at step 22900: 0.3279
Loss at step 22910: 0.8643
Loss at step 22920: 0.0014
Loss at step 22930: 0.0090
Loss at step 22940: 0.0000
Loss at step 22950: 0.0000
Loss at step 22960: 0.0000
Loss at step 22970: 0.0947
Loss at step 22980: 0.0000
Loss at step 22990: 0.0003
Loss at step 23000: 0.0014
Loss at step 23010: 0.0004
Loss at step 23020: 0.2257
Loss at step 23030: 0.0001
Loss at step 23040: 0.2642
Loss at step 23050: 0.0215
Loss at step 23060: 0.0095
Loss at step 23070: 0.0050
Loss at step 23080: 0.0001
Loss at step 23090: 0.2694
Loss at step 23100: 0.1638
Loss at step 23110: 0.7701
Loss at step 23120: 0.0006
Loss at step 23130: 0.1390
Loss at step 23140: 0.0119
Loss at step 23150: 0.0117
Loss at step 23160: 0.0005
Loss at step 23170: 0.0375
Loss at step 23180: 0.0002
Loss at step 23190: 0.0484
Loss at step 23200: 0.0002
Loss at step 23210: 0.0006
Loss at step 23220: 0.0598
Loss at step 23230: 0.4052
Loss at step 23240: 0.0001
Loss at step 23250: 0.2795
Loss at step 23260: 0.0301
Loss at step 23270: 0.0081
Loss at step 23280: 0.2436
Loss at step 23290: 0.4107
Loss at step 23300: 0.0186
Loss at step 23310: 0.0001
Loss at step 23320: 0.0474
Loss at step 23330: 0.3192
Loss at step 23340: 0.4773
Loss at step 23350: 0.4622
Loss at step 23360: 0.0566
Loss at step 23370: 0.0789
Loss at step 23380: 1.8361
Loss at step 23390: 0.0012
Loss at step 23400: 0.0031
Loss at step 23410: 0.7725
Loss at step 23420: 0.0000
Loss at step 23430: 0.1150
Loss at step 23440: 0.6775
Loss at step 23450: 0.3211
Loss at step 23460: 0.4709
Loss at step 23470: 0.0578
Loss at step 23480: 0.0831
Loss at step 23490: 0.0004
Loss at step 23500: 0.6852
Loss at step 23510: 0.1877
Loss at step 23520: 0.2495
Loss at step 23530: 0.0000
Loss at step 23540: 0.0056
Loss at step 23550: 0.0010
Loss at step 23560: 0.1182
Loss at step 23570: 0.0000
Loss at step 23580: 0.0947
Loss at step 23590: 0.0084
Loss at step 23600: 0.0000
Loss at step 23610: 0.1013
Loss at step 23620: 0.0008
Loss at step 23630: 0.0823
Loss at step 23640: 0.3999
Loss at step 23650: 0.0071
Loss at step 23660: 0.2102
Loss at step 23670: 0.0149
Loss at step 23680: 0.0052
Loss at step 23690: 0.0285
Loss at step 23700: 0.1129
Loss at step 23710: 0.0038
Loss at step 23720: 0.0076
Loss at step 23730: 0.0007
Loss at step 23740: 0.0004
Loss at step 23750: 0.1453
Loss at step 23760: 0.0018
Loss at step 23770: 0.2760
Loss at step 23780: 0.0078
Loss at step 23790: 0.2210
Loss at step 23800: 0.0793
Loss at step 23810: 0.4536
Loss at step 23820: 0.0001
Loss at step 23830: 0.0022
Loss at step 23840: 0.2411
Loss at step 23850: 0.0872
Loss at step 23860: 0.1899
Loss at step 23870: 0.3122
Loss at step 23880: 0.0150
Loss at step 23890: 0.0000
Loss at step 23900: 0.0001
Loss at step 23910: 0.0462
Loss at step 23920: 0.2485
Loss at step 23930: 0.4790
Loss at step 23940: 0.6100
Loss at step 23950: 0.0010
Loss at step 23960: 1.2035
Loss at step 23970: 1.9109
Loss at step 23980: 0.0001
Loss at step 23990: 0.0358
Loss at step 24000: 0.1729
Loss at step 24010: 0.0336
Loss at step 24020: 0.0282
Loss at step 24030: 0.2800
Loss at step 24040: 0.2094
Loss at step 24050: 0.0459
Loss at step 24060: 0.0015
Loss at step 24070: 0.0004
Loss at step 24080: 2.4879
Loss at step 24090: 0.0000
Loss at step 24100: 0.9813
Loss at step 24110: 0.0003
Loss at step 24120: 0.0634
Loss at step 24130: 0.5882
Loss at step 24140: 0.1139
Loss at step 24150: 0.2273
Loss at step 24160: 0.2114
Loss at step 24170: 0.6156
Loss at step 24180: 0.3899
Loss at step 24190: 0.3421
Loss at step 24200: 1.6838
Loss at step 24210: 0.4402
Loss at step 24220: 0.0018
Loss at step 24230: 0.0376
Loss at step 24240: 1.2370
Loss at step 24250: 0.0000
Loss at step 24260: 0.0472
Loss at step 24270: 0.0043
Loss at step 24280: 0.2546
Loss at step 24290: 0.0800
Loss at step 24300: 0.2690
Loss at step 24310: 1.0833
Loss at step 24320: 0.1767
Loss at step 24330: 0.2967
Loss at step 24340: 0.4322
Loss at step 24350: 0.2895
Loss at step 24360: 0.0428
Loss at step 24370: 0.0050
Loss at step 24380: 0.0000
Loss at step 24390: 0.0005
Loss at step 24400: 0.0000
Loss at step 24410: 0.0419
Loss at step 24420: 0.3608
Loss at step 24430: 2.6276
Loss at step 24440: 0.1280
Loss at step 24450: 0.4054
Loss at step 24460: 0.0008
Loss at step 24470: 0.7991
Loss at step 24480: 0.0849
Loss at step 24490: 0.0000
Loss at step 24500: 0.0549
Loss at step 24510: 0.6751
Loss at step 24520: 0.0079
Loss at step 24530: 0.0065
Loss at step 24540: 0.0007
Loss at step 24550: 0.6024
Loss at step 24560: 0.0685
Loss at step 24570: 0.0494
Loss at step 24580: 0.0028
Loss at step 24590: 0.0003
Loss at step 24600: 0.0028
Loss at step 24610: 0.0002
Loss at step 24620: 0.0001
Loss at step 24630: 0.0025
Loss at step 24640: 0.6255
Loss at step 24650: 0.1336
Loss at step 24660: 0.2992
Loss at step 24670: 0.0590
Loss at step 24680: 0.0286
Loss at step 24690: 0.0000
Loss at step 24700: 0.0075
Loss at step 24710: 0.0580
Loss at step 24720: 0.2118
Loss at step 24730: 0.0008
Loss at step 24740: 0.1613
Loss at step 24750: 0.2799
Loss at step 24760: 0.2158
Loss at step 24770: 0.0010
Loss at step 24780: 0.0245
Loss at step 24790: 0.0002
Loss at step 24800: 0.0007
Loss at step 24810: 0.7902
Loss at step 24820: 0.0036
Loss at step 24830: 0.4445
Loss at step 24840: 0.0005
Loss at step 24850: 0.9972
Loss at step 24860: 0.0000
Loss at step 24870: 0.2915
Loss at step 24880: 0.0000
Loss at step 24890: 0.1334
Loss at step 24900: 0.0001
Loss at step 24910: 0.2919
Loss at step 24920: 0.0139
Loss at step 24930: 0.0354
Loss at step 24940: 0.2997
Loss at step 24950: 0.0579
Loss at step 24960: 0.6874
Loss at step 24970: 0.0030
Loss at step 24980: 0.6602
Loss at step 24990: 0.0226
Loss at step 25000: 0.0000
Loss at step 25010: 0.0000
Loss at step 25020: 0.0003
Loss at step 25030: 0.2468
Loss at step 25040: 0.0352
Loss at step 25050: 0.2162
Loss at step 25060: 0.0507
Loss at step 25070: 0.0018
Loss at step 25080: 0.9114
Loss at step 25090: 0.0090
Loss at step 25100: 0.0003
Loss at step 25110: 0.0045
Loss at step 25120: 0.0005
Loss at step 25130: 2.7036
Loss at step 25140: 0.1463
Loss at step 25150: 0.2228
Loss at step 25160: 0.4133
Loss at step 25170: 0.0236
Loss at step 25180: 0.9306
Loss at step 25190: 0.0628
Loss at step 25200: 0.0012
Loss at step 25210: 0.5621
Loss at step 25220: 0.0332
Loss at step 25230: 0.1972
Loss at step 25240: 0.4851
Loss at step 25250: 0.3658
Loss at step 25260: 0.0003
Loss at step 25270: 0.0000
Loss at step 25280: 0.0023
Loss at step 25290: 0.1678
Loss at step 25300: 0.6441
Loss at step 25310: 0.0114
Loss at step 25320: 0.0000
Loss at step 25330: 0.1059
Loss at step 25340: 0.0152
Loss at step 25350: 0.2084
Loss at step 25360: 0.0043
Loss at step 25370: 0.0128
Loss at step 25380: 0.0067
Loss at step 25390: 0.0055
Loss at step 25400: 0.0072
Loss at step 25410: 0.0002
Loss at step 25420: 4.8486
Loss at step 25430: 0.0052
Loss at step 25440: 0.2667
Loss at step 25450: 0.8061
Loss at step 25460: 0.0522
Loss at step 25470: 0.0127
Loss at step 25480: 1.2575
Loss at step 25490: 0.1189
Loss at step 25500: 0.7638
Loss at step 25510: 0.0043
Loss at step 25520: 0.0000
Loss at step 25530: 0.4432
Loss at step 25540: 0.0242
Loss at step 25550: 0.0005
Loss at step 25560: 0.2832
Loss at step 25570: 0.1941
Loss at step 25580: 0.0035
Loss at step 25590: 0.5052
Loss at step 25600: 0.2014
Loss at step 25610: 0.0000
Loss at step 25620: 0.2095
Loss at step 25630: 2.9260
Loss at step 25640: 0.0015
Loss at step 25650: 0.1113
Loss at step 25660: 0.0536
Loss at step 25670: 0.3337
Loss at step 25680: 0.5672
Loss at step 25690: 0.0000
Loss at step 25700: 0.0084
Loss at step 25710: 0.0230
Loss at step 25720: 0.3307
Loss at step 25730: 0.0001
Loss at step 25740: 0.0101
Loss at step 25750: 0.0902
Loss at step 25760: 0.2126
Loss at step 25770: 0.0020
Loss at step 25780: 0.0002
Loss at step 25790: 0.0006
Loss at step 25800: 0.0048
Loss at step 25810: 0.0006
Loss at step 25820: 0.0415
Loss at step 25830: 0.4800
Loss at step 25840: 0.0000
Loss at step 25850: 2.2233
Loss at step 25860: 0.1008
Loss at step 25870: 0.0017
Loss at step 25880: 0.1083
Loss at step 25890: 0.0016
Loss at step 25900: 2.2014
Loss at step 25910: 0.0003
Loss at step 25920: 0.0553
Loss at step 25930: 0.5875
Loss at step 25940: 0.2384
Loss at step 25950: 0.0892
Loss at step 25960: 0.6454
Loss at step 25970: 0.0076
Loss at step 25980: 0.0060
Loss at step 25990: 0.0204
Loss at step 26000: 0.4638
Loss at step 26010: 0.9176
Loss at step 26020: 0.0535
Loss at step 26030: 0.2371
Loss at step 26040: 0.7933
Loss at step 26050: 0.5212
Loss at step 26060: 0.0060
Loss at step 26070: 0.0288
Loss at step 26080: 0.6951
Loss at step 26090: 0.0888
Loss at step 26100: 0.8121
Loss at step 26110: 0.0484
Loss at step 26120: 0.1774
Loss at step 26130: 0.2559
Loss at step 26140: 0.0012
Loss at step 26150: 0.0318
Loss at step 26160: 0.2156
Loss at step 26170: 0.0000
Loss at step 26180: 0.2157
Loss at step 26190: 0.6464
Loss at step 26200: 2.4578
Loss at step 26210: 0.0043
Loss at step 26220: 0.3050
Loss at step 26230: 0.1823
Loss at step 26240: 0.0466
Loss at step 26250: 0.0000
Loss at step 26260: 0.7047
Loss at step 26270: 0.0141
Loss at step 26280: 0.6645
Loss at step 26290: 0.0001
Loss at step 26300: 0.0007
Loss at step 26310: 0.0001
Loss at step 26320: 0.9062
Loss at step 26330: 0.0003
Loss at step 26340: 0.4716
Loss at step 26350: 0.0842
Loss at step 26360: 0.0034
Loss at step 26370: 0.0000
Loss at step 26380: 0.0453
Loss at step 26390: 0.3778
Loss at step 26400: 0.4500
Loss at step 26410: 0.0002
Loss at step 26420: 0.2376
Loss at step 26430: 0.1497
Loss at step 26440: 0.0004
Loss at step 26450: 0.3044
Loss at step 26460: 0.0028
Loss at step 26470: 0.6147
Loss at step 26480: 0.0774
Loss at step 26490: 0.0000
Loss at step 26500: 0.4959
Loss at step 26510: 0.0169
Loss at step 26520: 0.0996
Loss at step 26530: 0.0105
Loss at step 26540: 0.0000
Loss at step 26550: 0.0001
Loss at step 26560: 0.0045
Loss at step 26570: 0.0000
Loss at step 26580: 0.2298
Loss at step 26590: 0.0774
Loss at step 26600: 0.0220
Loss at step 26610: 0.5091
Loss at step 26620: 0.1354
Loss at step 26630: 0.4983
Loss at step 26640: 0.6509
Loss at step 26650: 0.2298
Loss at step 26660: 0.0006
Loss at step 26670: 0.5357
Loss at step 26680: 0.0009
Loss at step 26690: 0.0000
Loss at step 26700: 0.0001
Loss at step 26710: 0.0735
Loss at step 26720: 0.4235
Loss at step 26730: 0.0016
Loss at step 26740: 0.0182
Loss at step 26750: 0.0012
Loss at step 26760: 0.3085
Loss at step 26770: 0.1117
Loss at step 26780: 0.0063
Loss at step 26790: 0.0013
Loss at step 26800: 0.0077
Loss at step 26810: 0.0031
Loss at step 26820: 0.0000
Loss at step 26830: 0.0030
Loss at step 26840: 0.0272
Loss at step 26850: 0.0041
Loss at step 26860: 0.0685
Loss at step 26870: 0.0111
Loss at step 26880: 0.0006
Loss at step 26890: 0.0035
Loss at step 26900: 0.4712
Loss at step 26910: 0.0015
Loss at step 26920: 0.0002
Loss at step 26930: 0.0035
Loss at step 26940: 0.0000
Loss at step 26950: 0.0484
Loss at step 26960: 0.0397
Loss at step 26970: 0.3311
Loss at step 26980: 0.4507
Loss at step 26990: 0.0356
Loss at step 27000: 0.0002
Loss at step 27010: 0.0005
Loss at step 27020: 0.0003
Loss at step 27030: 0.1110
Loss at step 27040: 0.0033
Loss at step 27050: 1.8669
Loss at step 27060: 0.4214
Loss at step 27070: 0.2346
Loss at step 27080: 0.0000
Loss at step 27090: 0.0413
Loss at step 27100: 0.5258
Loss at step 27110: 0.0312
Loss at step 27120: 0.0066
Loss at step 27130: 0.1645
Loss at step 27140: 0.0001
Loss at step 27150: 0.0400
Loss at step 27160: 0.0006
Loss at step 27170: 0.0041
Loss at step 27180: 0.4874
Loss at step 27190: 0.3201
Loss at step 27200: 0.2003
Loss at step 27210: 0.0090
Loss at step 27220: 0.0149
Loss at step 27230: 0.0002
Loss at step 27240: 0.0002
Loss at step 27250: 0.0000
Loss at step 27260: 0.0006
Loss at step 27270: 0.0002
Loss at step 27280: 0.1305
Loss at step 27290: 0.0008
Loss at step 27300: 0.0000
Loss at step 27310: 0.0002
Loss at step 27320: 0.1605
Loss at step 27330: 0.0003
Loss at step 27340: 0.0526
Loss at step 27350: 0.6727
Loss at step 27360: 1.2615
Loss at step 27370: 0.3242
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.715012, 'precision': [0.855127, 0.400742, 0.545095], 'recall': [0.854947, 0.501588, 0.375455], 'f1': [0.855037, 0.44553, 0.444644]}
{'accuracy': 0.856998, 'precision': 0.545095, 'recall': 0.375455, 'f1': 0.444644, 'WordR': 0.131316}
