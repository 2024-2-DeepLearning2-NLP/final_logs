Parameter 'function'=<function get_post_edit_training_datasets.<locals>.preprocess_function_train at 0x7f89b61db5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 3056 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 10044, 227, 9667, 8, 11211, 454, 2219, 13470, 35, 541, 849, 34788, 1409, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 4, 50118, 45443, 161, 14, 5, 544, 16, 855, 878, 7, 3078, 8, 14, 5, 29823, 696, 16, 855, 3735, 6091, 23, 5851, 3121, 1399, 4, 2, 2, 1039, 11535, 23382, 225, 1343, 2290, 5, 231, 4, 3103, 31, 9667, 7, 469, 28, 15, 86, 4, 2486, 3251, 23720, 16, 2018, 117, 4646, 50118, 1039, 37932, 21948, 6802, 52, 58, 129, 156, 2542, 9, 5, 696, 95, 71, 5, 2341, 16238, 31, 12813, 1399, 6, 617, 335, 64, 36, 134, 43, 2, 2, 26402, 26858, 227, 9667, 8, 11211, 454, 2219, 13470, 35, 541, 849, 34788, 1409, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 50118, 1039, 37932, 21948, 12289, 89, 6, 5, 544, 16, 855, 878, 7, 3078, 4, 50118, 1039, 11535, 23382, 225, 1343, 9154, 10578, 7, 524, 10129, 162, 141, 4091, 2459, 10417, 110, 467, 13, 2207, 6091, 16, 4, 1398, 52, 32, 23, 231, 4, 2881, 8, 24, 18, 585, 25, 5943, 4, 121, 34667, 4, 50118, 1039, 37932, 21948, 6802, 10, 29823, 696, 16, 855, 3735, 6091, 23, 5851, 3121, 1399, 4, 19719, 13, 143, 24109, 1726, 4, 50118, 1039, 11535, 23382, 225, 1343, 280, 630, 75, 464, 5, 754, 14, 24, 21, 2018, 25, 15, 86, 454, 112, 2289, 137, 24, 21, 528, 4, 2612, 98, 628, 7, 905, 82, 216, 116, 50118, 1039, 37932, 21948, 1456, 303, 259, 111, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 44799, 16, 13689, 59, 5, 4646, 9, 5, 2341, 147, 11, 51, 399, 75, 3978, 2052, 4, 18497, 1286, 5, 3104, 7, 1649, 5, 335, 59, 5, 15679, 1033, 12113, 51, 300, 5, 335, 95, 5, 2341, 16238, 31, 12813, 1399, 4, 2]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1375
Parameter 'function'=<function get_post_edit_training_datasets.<locals>.preprocess_function_train at 0x703feae4e5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 3056 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 10044, 227, 9667, 8, 11211, 454, 2219, 13470, 35, 541, 849, 34788, 1409, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 4, 50118, 45443, 161, 14, 5, 544, 16, 855, 878, 7, 3078, 8, 14, 5, 29823, 696, 16, 855, 3735, 6091, 23, 5851, 3121, 1399, 4, 2, 2, 1039, 11535, 23382, 225, 1343, 2290, 5, 231, 4, 3103, 31, 9667, 7, 469, 28, 15, 86, 4, 2486, 3251, 23720, 16, 2018, 117, 4646, 50118, 1039, 37932, 21948, 6802, 52, 58, 129, 156, 2542, 9, 5, 696, 95, 71, 5, 2341, 16238, 31, 12813, 1399, 6, 617, 335, 64, 36, 134, 43, 2, 2, 26402, 26858, 227, 9667, 8, 11211, 454, 2219, 13470, 35, 541, 849, 34788, 1409, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 50118, 1039, 37932, 21948, 12289, 89, 6, 5, 544, 16, 855, 878, 7, 3078, 4, 50118, 1039, 11535, 23382, 225, 1343, 9154, 10578, 7, 524, 10129, 162, 141, 4091, 2459, 10417, 110, 467, 13, 2207, 6091, 16, 4, 1398, 52, 32, 23, 231, 4, 2881, 8, 24, 18, 585, 25, 5943, 4, 121, 34667, 4, 50118, 1039, 37932, 21948, 6802, 10, 29823, 696, 16, 855, 3735, 6091, 23, 5851, 3121, 1399, 4, 19719, 13, 143, 24109, 1726, 4, 50118, 1039, 11535, 23382, 225, 1343, 280, 630, 75, 464, 5, 754, 14, 24, 21, 2018, 25, 15, 86, 454, 112, 2289, 137, 24, 21, 528, 4, 2612, 98, 628, 7, 905, 82, 216, 116, 50118, 1039, 37932, 21948, 1456, 303, 259, 111, 1205, 640, 90, 4, 876, 73, 448, 406, 40099, 7912, 246, 14071, 401, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 44799, 16, 13689, 59, 5, 4646, 9, 5, 2341, 147, 11, 51, 399, 75, 3978, 2052, 4, 18497, 1286, 5, 3104, 7, 1649, 5, 335, 59, 5, 15679, 1033, 12113, 51, 300, 5, 335, 95, 5, 2341, 16238, 31, 12813, 1399, 4, 2]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1375
Parameter 'function'=<function get_post_edit_training_datasets.<locals>.preprocess_function_train at 0x7338da1ce5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7102 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 2373, 544, 61, 16, 45, 447, 8, 51, 33, 5372, 13, 132, 722, 4, 50118, 45443, 553, 5, 2111, 7, 1407, 106, 10, 2789, 356, 88, 5, 576, 3104, 98, 14, 51, 64, 356, 88, 42, 4, 2, 2, 1039, 298, 12709, 1215, 22930, 39226, 7, 4615, 55, 87, 10, 367, 2397, 6, 8, 2903, 323, 1302, 45, 447, 15, 395, 48329, 50118, 1039, 3079, 2545, 3118, 19719, 13, 5, 1143, 3605, 328, 6834, 2187, 32, 47, 634, 116, 318, 47, 2220, 75, 416, 6, 860, 542, 33142, 3923, 13, 10, 367, 27659, 8, 12721, 154, 4, 50118, 1039, 298, 12709, 1215, 22930, 1832, 47, 33, 2903, 323, 15, 395, 116, 38, 314, 5, 7359, 2931, 490, 70, 183, 8, 393, 300, 1263, 4, 12091, 26, 89, 21, 10, 2248, 2289, 2067, 4, 2, 2, 1039, 298, 12709, 1215, 22930, 8630, 3490, 113, 3797, 5900, 7951, 2915, 4, 85, 1364, 22892, 12445, 19, 5, 3909, 1553, 4, 7987, 7, 395, 24, 74, 912, 22422, 59, 683, 41, 1946, 4, 50118, 1039, 298, 12709, 1215, 22930, 125, 15, 395, 24, 6897, 71, 10, 367, 2397, 4, 50118, 1039, 298, 12709, 1215, 22930, 1012, 16, 26977, 2024, 7, 41, 28359, 30577, 6, 8, 38, 437, 5, 129, 2888, 3018, 4, 50118, 1039, 3079, 2545, 3118, 4557, 13, 5, 1254, 328, 6319, 47, 1006, 149, 5, 2402, 259, 35, 1205, 640, 90, 4, 876, 73, 448, 90, 246, 530, 510, 705, 104, 705, 1343, 219, 116, 50118, 1039, 298, 12709, 1215, 22930, 374, 395, 24, 74, 45, 4615, 55, 87, 10, 367, 2397, 4, 374, 302, 24, 21, 95, 5, 5483, 6, 816, 70, 183, 662, 7, 363, 19, 129, 10, 367, 50118, 1039, 298, 12709, 1215, 22930, 22749, 2485, 4, 38, 8869, 24, 2024, 142, 59, 683, 10, 353, 5, 1012, 74, 2217, 5, 14844, 4, 1491, 686, 114, 14, 16, 31, 10, 476, 2988, 4, 50118, 1039, 3079, 2545, 3118, 19719, 13, 5, 251, 2067, 328, 12091, 73, 29465, 323, 16, 577, 706, 73, 406, 4, 18258, 47, 441, 7, 860, 70, 9, 5, 2402, 14, 52, 348, 1286, 98, 444, 116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 2111, 16, 2114, 5, 936, 14, 37, 16, 3276, 7, 4615, 55, 87, 367, 2397, 8, 161, 14, 5, 2903, 323, 1302, 45, 447, 15, 395, 4, 20, 2936, 6990, 61, 2187, 5, 2111, 21, 634, 8, 172, 6990, 7, 860, 542, 33142, 3923, 13, 10, 367, 5251, 8, 172, 7, 12721, 8, 3649, 7, 12091, 73, 29465, 323, 4, 2]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 1375
Parameter 'function'=<function get_post_edit_training_datasets.<locals>.preprocess_function_train at 0x7e1f2804e5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7102 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 2373, 544, 61, 16, 45, 447, 8, 51, 33, 5372, 13, 132, 722, 4, 50118, 45443, 553, 5, 2111, 7, 1407, 106, 10, 2789, 356, 88, 5, 576, 3104, 98, 14, 51, 64, 356, 88, 42, 4, 2, 2, 1039, 298, 12709, 1215, 22930, 39226, 7, 4615, 55, 87, 10, 367, 2397, 6, 8, 2903, 323, 1302, 45, 447, 15, 395, 48329, 50118, 1039, 3079, 2545, 3118, 19719, 13, 5, 1143, 3605, 328, 6834, 2187, 32, 47, 634, 116, 318, 47, 2220, 75, 416, 6, 860, 542, 33142, 3923, 13, 10, 367, 27659, 8, 12721, 154, 4, 50118, 1039, 298, 12709, 1215, 22930, 1832, 47, 33, 2903, 323, 15, 395, 116, 38, 314, 5, 7359, 2931, 490, 70, 183, 8, 393, 300, 1263, 4, 12091, 26, 89, 21, 10, 2248, 2289, 2067, 4, 2, 2, 1039, 298, 12709, 1215, 22930, 8630, 3490, 113, 3797, 5900, 7951, 2915, 4, 85, 1364, 22892, 12445, 19, 5, 3909, 1553, 4, 7987, 7, 395, 24, 74, 912, 22422, 59, 683, 41, 1946, 4, 50118, 1039, 298, 12709, 1215, 22930, 125, 15, 395, 24, 6897, 71, 10, 367, 2397, 4, 50118, 1039, 298, 12709, 1215, 22930, 1012, 16, 26977, 2024, 7, 41, 28359, 30577, 6, 8, 38, 437, 5, 129, 2888, 3018, 4, 50118, 1039, 3079, 2545, 3118, 4557, 13, 5, 1254, 328, 6319, 47, 1006, 149, 5, 2402, 259, 35, 1205, 640, 90, 4, 876, 73, 448, 90, 246, 530, 510, 705, 104, 705, 1343, 219, 116, 50118, 1039, 298, 12709, 1215, 22930, 374, 395, 24, 74, 45, 4615, 55, 87, 10, 367, 2397, 4, 374, 302, 24, 21, 95, 5, 5483, 6, 816, 70, 183, 662, 7, 363, 19, 129, 10, 367, 50118, 1039, 298, 12709, 1215, 22930, 22749, 2485, 4, 38, 8869, 24, 2024, 142, 59, 683, 10, 353, 5, 1012, 74, 2217, 5, 14844, 4, 1491, 686, 114, 14, 16, 31, 10, 476, 2988, 4, 50118, 1039, 3079, 2545, 3118, 19719, 13, 5, 251, 2067, 328, 12091, 73, 29465, 323, 16, 577, 706, 73, 406, 4, 18258, 47, 441, 7, 860, 70, 9, 5, 2402, 14, 52, 348, 1286, 98, 444, 116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 133, 2111, 16, 2114, 5, 936, 14, 37, 16, 3276, 7, 4615, 55, 87, 367, 2397, 8, 161, 14, 5, 2903, 323, 1302, 45, 447, 15, 395, 4, 20, 2936, 6990, 61, 2187, 5, 2111, 21, 634, 8, 172, 6990, 7, 860, 542, 33142, 3923, 13, 10, 367, 5251, 8, 172, 7, 12721, 8, 3649, 7, 12091, 73, 29465, 323, 4, 2]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 2750
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 58.9719, 'rouge2': 34.4443, 'rougeL': 49.5955, 'rougeLsum': 53.8206}, 'ppl': {'perplexity': 8.7379, 'ref_perplexity': 7.3356}, 'bertscore': {'precision': 90.5778, 'recall': 91.406, 'f1': 90.9798}}
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 57.3787, 'rouge2': 31.9472, 'rougeL': 47.5262, 'rougeLsum': 51.9957}, 'ppl': {'perplexity': 11.0095, 'ref_perplexity': 14.0507}, 'bertscore': {'precision': 90.7231, 'recall': 90.7799, 'f1': 90.7403}}
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 57.3714, 'rouge2': 32.1355, 'rougeL': 47.663, 'rougeLsum': 52.1442}, 'ppl': {'perplexity': 4.0964, 'ref_perplexity': 19.3561}, 'bertscore': {'precision': 90.4336, 'recall': 90.9502, 'f1': 90.6799}}
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 57.0256, 'rouge2': 31.5116, 'rougeL': 47.4077, 'rougeLsum': 51.7301}, 'ppl': {'perplexity': 35.9036, 'ref_perplexity': 21.9552}, 'bertscore': {'precision': 90.6676, 'recall': 90.6945, 'f1': 90.6707}}
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 56.5666, 'rouge2': 31.2343, 'rougeL': 46.8671, 'rougeLsum': 51.1677}, 'ppl': {'perplexity': 10.4904, 'ref_perplexity': 22.2896}, 'bertscore': {'precision': 90.5671, 'recall': 90.6582, 'f1': 90.601}}
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
Using default tokenizer.
{'rouge': {'rouge1': 53.7281, 'rouge2': 28.5923, 'rougeL': 43.8568, 'rougeLsum': 48.3973}, 'ppl': {'perplexity': 8.6295, 'ref_perplexity': 9.3137}, 'bertscore': {'precision': 89.6695, 'recall': 90.5527, 'f1': 90.0952}}
