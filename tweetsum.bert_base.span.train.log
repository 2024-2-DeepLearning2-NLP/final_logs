Parameter 'function'=<function get_omission_datasets.<locals>.span_func at 0x78d96359f700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 5977 of the training set: {'input_ids': [101, 1996, 8013, 5176, 3251, 2045, 2003, 2151, 3114, 2010, 6097, 2891, 10439, 4136, 2032, 1008, 2296, 1008, 2051, 2002, 9573, 2055, 1000, 6047, 26351, 1000, 2893, 3492, 4365, 15703, 1012, 1996, 4005, 5176, 3251, 1996, 26828, 2003, 6037, 2004, 1037, 3769, 1011, 2039, 1010, 2030, 1999, 1996, 4530, 8758, 15363, 4646, 1998, 5176, 2000, 1040, 2213, 1996, 12117, 12326, 1997, 1996, 4471, 2061, 2008, 2027, 2064, 8556, 2023, 3043, 2582, 1012, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2003, 2045, 2151, 3114, 2115, 6097, 2891, 10439, 4136, 2033, 1008, 2296, 1008, 2051, 1045, 9573, 2055, 1523, 6047, 26351, 1524, 2893, 3492, 4365, 15703, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 3782, 1010, 2071, 2017, 2292, 2149, 2113, 2062, 2055, 2023, 26828, 1029, 2003, 2009, 6037, 2004, 1037, 3769, 1011, 2039, 1010, 2030, 1999, 1996, 4530, 8758, 15363, 4646, 1029, 3531, 1040, 2213, 2149, 1037, 12117, 12326, 1997, 1996, 4471, 2061, 2008, 2057, 2064, 8556, 2023, 3043, 2582, 1010, 4283, 999, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2009, 2003, 2023, 1010, 2296, 2051, 1045, 2128, 27927, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1048, 3372, 23706, 2575, 9103, 4305, 102, 101, 1030, 25162, 2620, 19317, 4931, 2153, 3782, 1010, 2064, 2057, 7868, 2008, 2017, 1521, 2310, 2908, 2083, 1996, 1523, 4553, 2062, 1524, 2030, 2079, 2017, 2467, 1523, 2485, 1524, 1996, 26828, 8874, 1029, 4283, 1999, 5083, 999, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2031, 5791, 13886, 1996, 4553, 2062, 1012, 3243, 1037, 2261, 2335, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 2153, 1010, 2017, 2323, 2022, 2583, 2000, 4487, 19150, 1996, 26828, 2015, 1997, 1996, 4530, 8758, 15363, 4646, 2478, 1996, 6097, 10906, 1999, 1996, 26828, 21628, 2000, 4652, 2582, 25732, 2015, 1999, 2925, 1012, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 1045, 19148, 1045, 2064, 2079, 2008, 1010, 1045, 2001, 17704, 2667, 2025, 2000, 2735, 2125, 1996, 2878, 3444, 2349, 2000, 2028, 26828, 10694, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 2153, 1010, 2028, 2062, 2518, 2057, 1521, 1040, 2066, 2000, 4638, 1010, 2065, 2017, 11562, 1996, 4530, 8758, 15363, 4646, 2013, 1996, 12183, 3347, 1010, 1996, 1520, 26828, 2015, 1013, 3522, 2015, 1521, 2323, 3711, 1010, 22042, 4553, 2062, 2013, 2008, 2181, 2323, 19776, 2009, 8642, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [7, -1, 0, 1, 3, 6]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 21975
Loss at step 10: 3.0301
Loss at step 20: 3.0286
Loss at step 30: 3.0507
Loss at step 40: 2.0229
Loss at step 50: 3.0113
Loss at step 60: 2.4973
Loss at step 70: 3.3442
Loss at step 80: 2.9627
Loss at step 90: 2.6954
Loss at step 100: 2.3230
Loss at step 110: 2.6923
Loss at step 120: 1.8846
Loss at step 130: 2.4389
Loss at step 140: 3.2612
Loss at step 150: 2.1884
Loss at step 160: 1.5731
Loss at step 170: 1.8912
Loss at step 180: 2.1500
Loss at step 190: 1.9901
Loss at step 200: 2.0010
Loss at step 210: 2.3608
Loss at step 220: 1.5950
Loss at step 230: 2.0880
Loss at step 240: 2.1489
Loss at step 250: 2.3274
Loss at step 260: 1.7784
Loss at step 270: 2.6048
Loss at step 280: 1.8869
Loss at step 290: 2.3644
Loss at step 300: 2.0921
Loss at step 310: 1.7853
Loss at step 320: 2.8085
Loss at step 330: 1.9660
Loss at step 340: 2.2005
Loss at step 350: 2.7839
Loss at step 360: 2.4848
Loss at step 370: 2.4465
Loss at step 380: 1.6740
Loss at step 390: 2.3793
Loss at step 400: 3.5244
Loss at step 410: 2.6523
Loss at step 420: 1.8041
Loss at step 430: 1.9240
Loss at step 440: 2.3022
Loss at step 450: 2.2282
Loss at step 460: 1.7407
Loss at step 470: 1.5545
Loss at step 480: 3.8970
Loss at step 490: 2.0699
Loss at step 500: 1.8138
Loss at step 510: 1.8003
Loss at step 520: 1.8656
Loss at step 530: 1.6291
Loss at step 540: 1.8555
Loss at step 550: 2.1571
Loss at step 560: 2.3040
Loss at step 570: 1.8095
Loss at step 580: 1.7568
Loss at step 590: 2.3551
Loss at step 600: 2.1007
Loss at step 610: 2.3704
Loss at step 620: 2.4161
Loss at step 630: 2.0396
Loss at step 640: 1.4888
Loss at step 650: 1.9540
Loss at step 660: 2.5070
Loss at step 670: 2.4420
Loss at step 680: 1.8489
Loss at step 690: 1.9428
Loss at step 700: 1.6587
Loss at step 710: 2.4384
Loss at step 720: 1.5722
Loss at step 730: 2.5996
Loss at step 740: 1.7136
Loss at step 750: 2.0188
Loss at step 760: 1.8430
Loss at step 770: 1.6211
Loss at step 780: 1.7772
Loss at step 790: 2.3966
Loss at step 800: 1.8272
Loss at step 810: 1.8171
Loss at step 820: 2.3562
Loss at step 830: 1.8870
Loss at step 840: 2.0254
Loss at step 850: 2.0445
Loss at step 860: 1.9632
Loss at step 870: 2.2214
Loss at step 880: 2.3632
Loss at step 890: 1.6246
Loss at step 900: 1.9154
Loss at step 910: 2.0721
Loss at step 920: 1.9556
Loss at step 930: 1.8497
Loss at step 940: 2.1821
Loss at step 950: 2.1285
Loss at step 960: 2.1208
Loss at step 970: 2.5345
Loss at step 980: 2.2689
Loss at step 990: 1.8670
Loss at step 1000: 1.8731
Loss at step 1010: 2.5702
Loss at step 1020: 1.7564
Loss at step 1030: 1.4518
Loss at step 1040: 2.0607
Loss at step 1050: 1.6731
Loss at step 1060: 2.1837
Loss at step 1070: 1.8041
Loss at step 1080: 1.5403
Loss at step 1090: 2.2891
Loss at step 1100: 1.7498
Loss at step 1110: 1.5989
Loss at step 1120: 1.8788
Loss at step 1130: 1.7385
Loss at step 1140: 1.6511
Loss at step 1150: 1.6392
Loss at step 1160: 1.3676
Loss at step 1170: 1.8629
Loss at step 1180: 1.5326
Loss at step 1190: 1.5349
Loss at step 1200: 1.4864
Loss at step 1210: 1.4823
Loss at step 1220: 2.5110
Loss at step 1230: 1.6894
Loss at step 1240: 1.6250
Loss at step 1250: 1.5468
Loss at step 1260: 1.5997
Loss at step 1270: 2.0691
Loss at step 1280: 1.2502
Loss at step 1290: 1.3488
Loss at step 1300: 1.9456
Loss at step 1310: 1.7561
Loss at step 1320: 1.6714
Loss at step 1330: 1.3409
Loss at step 1340: 1.5470
Loss at step 1350: 3.1191
Loss at step 1360: 2.0540
Loss at step 1370: 1.6034
Loss at step 1380: 1.4851
Loss at step 1390: 1.9807
Loss at step 1400: 1.3676
Loss at step 1410: 2.1047
Loss at step 1420: 1.6644
Loss at step 1430: 2.3442
Loss at step 1440: 1.5077
Loss at step 1450: 2.0544
Loss at step 1460: 1.7911
Loss at step 1470: 1.3398
Loss at step 1480: 1.8884
Loss at step 1490: 1.6456
Loss at step 1500: 1.5259
Loss at step 1510: 2.5558
Loss at step 1520: 1.7619
Loss at step 1530: 1.6609
Loss at step 1540: 1.6327
Loss at step 1550: 1.7509
Loss at step 1560: 1.7118
Loss at step 1570: 2.0862
Loss at step 1580: 1.0179
Loss at step 1590: 1.7749
Loss at step 1600: 1.5921
Loss at step 1610: 1.4815
Loss at step 1620: 1.2673
Loss at step 1630: 1.2550
Loss at step 1640: 1.5878
Loss at step 1650: 1.6897
Loss at step 1660: 1.8320
Loss at step 1670: 1.6578
Loss at step 1680: 1.2113
Loss at step 1690: 1.8852
Loss at step 1700: 1.4789
Loss at step 1710: 1.6837
Loss at step 1720: 1.1370
Loss at step 1730: 1.2804
Loss at step 1740: 1.7642
Loss at step 1750: 1.5268
Loss at step 1760: 1.4863
Loss at step 1770: 1.6976
Loss at step 1780: 1.3773
Loss at step 1790: 1.4937
Loss at step 1800: 1.2523
Loss at step 1810: 2.3456
Loss at step 1820: 1.7195
Loss at step 1830: 1.6460
Loss at step 1840: 1.6676
Loss at step 1850: 1.8382
Loss at step 1860: 0.9467
Loss at step 1870: 1.7433
Loss at step 1880: 1.5762
Loss at step 1890: 1.2017
Loss at step 1900: 1.6528
Loss at step 1910: 1.4332
Loss at step 1920: 1.2800
Loss at step 1930: 1.4017
Loss at step 1940: 1.1414
Loss at step 1950: 1.3830
Loss at step 1960: 1.2276
Loss at step 1970: 1.2982
Loss at step 1980: 1.6239
Loss at step 1990: 1.4458
Loss at step 2000: 1.0729
Loss at step 2010: 1.6527
Loss at step 2020: 1.8053
Loss at step 2030: 1.3426
Loss at step 2040: 1.3020
Loss at step 2050: 1.1676
Loss at step 2060: 1.9752
Loss at step 2070: 1.4349
Loss at step 2080: 1.2510
Loss at step 2090: 1.1902
Loss at step 2100: 1.3328
Loss at step 2110: 1.5676
Loss at step 2120: 1.7775
Loss at step 2130: 1.2027
Loss at step 2140: 1.3108
Loss at step 2150: 0.7447
Loss at step 2160: 1.5199
Loss at step 2170: 1.5169
Loss at step 2180: 1.3830
Loss at step 2190: 1.6005
Loss at step 2200: 1.3816
Loss at step 2210: 1.2908
Loss at step 2220: 1.0239
Loss at step 2230: 1.3825
Loss at step 2240: 1.2088
Loss at step 2250: 1.6005
Loss at step 2260: 1.3483
Loss at step 2270: 1.8976
Loss at step 2280: 1.2993
Loss at step 2290: 1.5410
Loss at step 2300: 1.4281
Loss at step 2310: 1.3775
Loss at step 2320: 1.8881
Loss at step 2330: 1.3596
Loss at step 2340: 1.6814
Loss at step 2350: 1.4487
Loss at step 2360: 0.9417
Loss at step 2370: 1.8684
Loss at step 2380: 1.1359
Loss at step 2390: 1.3867
Loss at step 2400: 1.2465
Loss at step 2410: 1.0107
Loss at step 2420: 1.1147
Loss at step 2430: 0.9528
Loss at step 2440: 0.8040
Loss at step 2450: 2.1045
Loss at step 2460: 1.5572
Loss at step 2470: 1.5744
Loss at step 2480: 1.7525
Loss at step 2490: 1.2300
Loss at step 2500: 1.3598
Loss at step 2510: 1.2492
Loss at step 2520: 1.1907
Loss at step 2530: 2.3859
Loss at step 2540: 1.2101
Loss at step 2550: 0.8581
Loss at step 2560: 1.4942
Loss at step 2570: 0.8340
Loss at step 2580: 1.2073
Loss at step 2590: 1.5682
Loss at step 2600: 1.0773
Loss at step 2610: 1.6490
Loss at step 2620: 2.6538
Loss at step 2630: 0.9910
Loss at step 2640: 1.6120
Loss at step 2650: 1.6145
Loss at step 2660: 0.9045
Loss at step 2670: 1.1100
Loss at step 2680: 1.3776
Loss at step 2690: 1.5550
Loss at step 2700: 1.6285
Loss at step 2710: 1.7565
Loss at step 2720: 1.5319
Loss at step 2730: 1.1818
Loss at step 2740: 1.3177
Loss at step 2750: 0.7457
Loss at step 2760: 0.9759
Loss at step 2770: 1.2422
Loss at step 2780: 2.2442
Loss at step 2790: 1.4989
Loss at step 2800: 0.9579
Loss at step 2810: 1.0586
Loss at step 2820: 1.3756
Loss at step 2830: 1.6477
Loss at step 2840: 1.4712
Loss at step 2850: 1.0354
Loss at step 2860: 1.9153
Loss at step 2870: 1.3628
Loss at step 2880: 1.5783
Loss at step 2890: 1.0405
Loss at step 2900: 1.8917
Loss at step 2910: 1.7042
Loss at step 2920: 0.9585
Loss at step 2930: 0.9636
Loss at step 2940: 1.0065
Loss at step 2950: 1.2000
Loss at step 2960: 1.0331
Loss at step 2970: 1.1636
Loss at step 2980: 1.7705
Loss at step 2990: 1.4235
Loss at step 3000: 0.8972
Loss at step 3010: 1.2450
Loss at step 3020: 1.3533
Loss at step 3030: 1.3263
Loss at step 3040: 1.0545
Loss at step 3050: 0.9227
Loss at step 3060: 1.1996
Loss at step 3070: 0.9187
Loss at step 3080: 0.7911
Loss at step 3090: 0.8784
Loss at step 3100: 1.0525
Loss at step 3110: 0.9652
Loss at step 3120: 1.1249
Loss at step 3130: 1.3760
Loss at step 3140: 1.1417
Loss at step 3150: 0.7879
Loss at step 3160: 1.6670
Loss at step 3170: 1.1422
Loss at step 3180: 1.6437
Loss at step 3190: 1.0235
Loss at step 3200: 0.9653
Loss at step 3210: 1.4430
Loss at step 3220: 1.0174
Loss at step 3230: 1.6790
Loss at step 3240: 0.9910
Loss at step 3250: 0.8410
Loss at step 3260: 1.0582
Loss at step 3270: 1.3730
Loss at step 3280: 1.1702
Loss at step 3290: 0.9150
Loss at step 3300: 1.0382
Loss at step 3310: 1.5166
Loss at step 3320: 0.8715
Loss at step 3330: 1.4276
Loss at step 3340: 1.4510
Loss at step 3350: 1.1629
Loss at step 3360: 1.2854
Loss at step 3370: 0.7834
Loss at step 3380: 1.0965
Loss at step 3390: 1.1276
Loss at step 3400: 0.7658
Loss at step 3410: 0.9728
Loss at step 3420: 1.0074
Loss at step 3430: 1.3395
Loss at step 3440: 1.2321
Loss at step 3450: 1.0248
Loss at step 3460: 0.5289
Loss at step 3470: 1.1625
Loss at step 3480: 1.4117
Loss at step 3490: 0.9901
Loss at step 3500: 0.8320
Loss at step 3510: 0.9863
Loss at step 3520: 1.2707
Loss at step 3530: 1.2995
Loss at step 3540: 1.3959
Loss at step 3550: 1.2099
Loss at step 3560: 1.2347
Loss at step 3570: 0.9459
Loss at step 3580: 1.6090
Loss at step 3590: 1.2945
Loss at step 3600: 1.2722
Loss at step 3610: 0.7914
Loss at step 3620: 0.9207
Loss at step 3630: 1.4924
Loss at step 3640: 1.0738
Loss at step 3650: 1.2907
Loss at step 3660: 1.0570
Loss at step 3670: 1.0619
Loss at step 3680: 1.2173
Loss at step 3690: 1.1939
Loss at step 3700: 0.8575
Loss at step 3710: 0.6355
Loss at step 3720: 1.0398
Loss at step 3730: 1.0548
Loss at step 3740: 1.2538
Loss at step 3750: 0.8557
Loss at step 3760: 1.3729
Loss at step 3770: 1.1599
Loss at step 3780: 0.7301
Loss at step 3790: 0.9225
Loss at step 3800: 0.9991
Loss at step 3810: 1.4824
Loss at step 3820: 0.9793
Loss at step 3830: 1.1557
Loss at step 3840: 0.9292
Loss at step 3850: 0.9809
Loss at step 3860: 0.7232
Loss at step 3870: 0.6846
Loss at step 3880: 0.9835
Loss at step 3890: 1.1180
Loss at step 3900: 0.9778
Loss at step 3910: 1.3666
Loss at step 3920: 1.9239
Loss at step 3930: 0.8681
Loss at step 3940: 2.1728
Loss at step 3950: 1.5772
Loss at step 3960: 1.3839
Loss at step 3970: 1.0256
Loss at step 3980: 0.7824
Loss at step 3990: 1.3008
Loss at step 4000: 0.7936
Loss at step 4010: 0.9926
Loss at step 4020: 0.6800
Loss at step 4030: 0.8392
Loss at step 4040: 1.0896
Loss at step 4050: 0.7520
Loss at step 4060: 0.7425
Loss at step 4070: 0.9573
Loss at step 4080: 0.9780
Loss at step 4090: 1.6487
Loss at step 4100: 0.8742
Loss at step 4110: 0.5266
Loss at step 4120: 1.1426
Loss at step 4130: 0.8118
Loss at step 4140: 1.0236
Loss at step 4150: 0.7501
Loss at step 4160: 0.9348
Loss at step 4170: 0.8684
Loss at step 4180: 0.6489
Loss at step 4190: 0.6232
Loss at step 4200: 0.6808
Loss at step 4210: 1.4342
Loss at step 4220: 1.5015
Loss at step 4230: 1.2202
Loss at step 4240: 0.6012
Loss at step 4250: 0.6742
Loss at step 4260: 0.9218
Loss at step 4270: 0.9967
Loss at step 4280: 0.8519
Loss at step 4290: 1.1691
Loss at step 4300: 1.1709
Loss at step 4310: 1.0676
Loss at step 4320: 1.2641
Loss at step 4330: 0.6825
Loss at step 4340: 0.8008
Loss at step 4350: 0.8982
Loss at step 4360: 1.2724
Loss at step 4370: 1.4194
Loss at step 4380: 1.0196
Loss at step 4390: 1.0371
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.486928, 'precision': [0.491663, 0.453027, 0.603133], 'recall': [0.870082, 0.353564, 0.134146], 'f1': [0.628292, 0.397163, 0.219477]}
{'accuracy': 0.7517, 'precision': 0.603133, 'recall': 0.134146, 'f1': 0.219477, 'WordR': 0.018182}
Loss at step 4400: 0.4649
Loss at step 4410: 0.9189
Loss at step 4420: 1.3469
Loss at step 4430: 0.5412
Loss at step 4440: 0.6272
Loss at step 4450: 0.7204
Loss at step 4460: 0.6659
Loss at step 4470: 0.7234
Loss at step 4480: 1.0050
Loss at step 4490: 1.0649
Loss at step 4500: 0.5907
Loss at step 4510: 0.9891
Loss at step 4520: 0.7060
Loss at step 4530: 1.3972
Loss at step 4540: 0.8577
Loss at step 4550: 0.4052
Loss at step 4560: 0.7956
Loss at step 4570: 1.4051
Loss at step 4580: 1.9044
Loss at step 4590: 1.1939
Loss at step 4600: 1.0430
Loss at step 4610: 0.6281
Loss at step 4620: 1.1267
Loss at step 4630: 0.5678
Loss at step 4640: 0.7792
Loss at step 4650: 0.7152
Loss at step 4660: 0.9767
Loss at step 4670: 0.5837
Loss at step 4680: 0.5491
Loss at step 4690: 1.0894
Loss at step 4700: 0.4778
Loss at step 4710: 1.0444
Loss at step 4720: 0.6758
Loss at step 4730: 0.7758
Loss at step 4740: 1.0292
Loss at step 4750: 0.9284
Loss at step 4760: 0.5370
Loss at step 4770: 1.5570
Loss at step 4780: 0.9602
Loss at step 4790: 0.8604
Loss at step 4800: 0.7002
Loss at step 4810: 0.6335
Loss at step 4820: 1.3966
Loss at step 4830: 0.8583
Loss at step 4840: 0.7118
Loss at step 4850: 0.6538
Loss at step 4860: 0.3719
Loss at step 4870: 1.1979
Loss at step 4880: 1.0925
Loss at step 4890: 1.0451
Loss at step 4900: 0.6921
Loss at step 4910: 1.2093
Loss at step 4920: 0.5235
Loss at step 4930: 1.0224
Loss at step 4940: 0.9042
Loss at step 4950: 1.0177
Loss at step 4960: 0.4735
Loss at step 4970: 1.3379
Loss at step 4980: 0.7076
Loss at step 4990: 1.2359
Loss at step 5000: 0.4284
Loss at step 5010: 0.5327
Loss at step 5020: 0.5147
Loss at step 5030: 0.7525
Loss at step 5040: 1.4186
Loss at step 5050: 1.0380
Loss at step 5060: 0.8751
Loss at step 5070: 0.3847
Loss at step 5080: 0.7313
Loss at step 5090: 0.7215
Loss at step 5100: 0.8598
Loss at step 5110: 1.2207
Loss at step 5120: 0.6266
Loss at step 5130: 0.9225
Loss at step 5140: 0.9440
Loss at step 5150: 0.5597
Loss at step 5160: 1.2655
Loss at step 5170: 1.0381
Loss at step 5180: 0.8664
Loss at step 5190: 1.2982
Loss at step 5200: 0.7684
Loss at step 5210: 1.1707
Loss at step 5220: 0.5838
Loss at step 5230: 1.4758
Loss at step 5240: 0.5747
Loss at step 5250: 1.0065
Loss at step 5260: 0.7766
Loss at step 5270: 1.2739
Loss at step 5280: 0.5642
Loss at step 5290: 0.5183
Loss at step 5300: 0.6689
Loss at step 5310: 0.5554
Loss at step 5320: 0.8812
Loss at step 5330: 0.8025
Loss at step 5340: 0.8988
Loss at step 5350: 0.7836
Loss at step 5360: 0.6866
Loss at step 5370: 0.6482
Loss at step 5380: 0.8812
Loss at step 5390: 0.7450
Loss at step 5400: 0.9077
Loss at step 5410: 0.6673
Loss at step 5420: 1.0492
Loss at step 5430: 0.4174
Loss at step 5440: 1.0642
Loss at step 5450: 0.7385
Loss at step 5460: 0.3468
Loss at step 5470: 0.4868
Loss at step 5480: 1.5439
Loss at step 5490: 1.2782
Loss at step 5500: 0.8022
Loss at step 5510: 1.0028
Loss at step 5520: 0.5005
Loss at step 5530: 0.8860
Loss at step 5540: 0.4672
Loss at step 5550: 0.8227
Loss at step 5560: 0.8329
Loss at step 5570: 1.3947
Loss at step 5580: 0.6729
Loss at step 5590: 0.6245
Loss at step 5600: 0.5289
Loss at step 5610: 1.3113
Loss at step 5620: 0.5561
Loss at step 5630: 0.8401
Loss at step 5640: 0.5812
Loss at step 5650: 0.8082
Loss at step 5660: 0.6707
Loss at step 5670: 0.6982
Loss at step 5680: 0.9415
Loss at step 5690: 0.6137
Loss at step 5700: 0.4369
Loss at step 5710: 1.4806
Loss at step 5720: 0.5092
Loss at step 5730: 0.7693
Loss at step 5740: 0.9899
Loss at step 5750: 0.8538
Loss at step 5760: 0.9925
Loss at step 5770: 0.6687
Loss at step 5780: 0.6197
Loss at step 5790: 0.6228
Loss at step 5800: 0.8738
Loss at step 5810: 0.7043
Loss at step 5820: 1.1948
Loss at step 5830: 1.9562
Loss at step 5840: 0.8609
Loss at step 5850: 0.6842
Loss at step 5860: 0.9099
Loss at step 5870: 0.7950
Loss at step 5880: 1.2840
Loss at step 5890: 0.4409
Loss at step 5900: 1.2179
Loss at step 5910: 0.9057
Loss at step 5920: 1.0152
Loss at step 5930: 0.8151
Loss at step 5940: 0.6160
Loss at step 5950: 0.8122
Loss at step 5960: 0.9219
Loss at step 5970: 0.1481
Loss at step 5980: 0.4812
Loss at step 5990: 0.7831
Loss at step 6000: 0.6218
Loss at step 6010: 0.9829
Loss at step 6020: 1.1079
Loss at step 6030: 0.2939
Loss at step 6040: 0.7462
Loss at step 6050: 0.8142
Loss at step 6060: 0.2804
Loss at step 6070: 0.5633
Loss at step 6080: 0.6038
Loss at step 6090: 0.3422
Loss at step 6100: 1.5295
Loss at step 6110: 0.5662
Loss at step 6120: 1.2777
Loss at step 6130: 0.6662
Loss at step 6140: 0.6562
Loss at step 6150: 1.2316
Loss at step 6160: 0.6222
Loss at step 6170: 0.4576
Loss at step 6180: 1.8477
Loss at step 6190: 0.7284
Loss at step 6200: 0.6625
Loss at step 6210: 0.6915
Loss at step 6220: 0.8817
Loss at step 6230: 1.0566
Loss at step 6240: 1.1936
Loss at step 6250: 0.5465
Loss at step 6260: 0.6674
Loss at step 6270: 0.8661
Loss at step 6280: 1.2051
Loss at step 6290: 0.4956
Loss at step 6300: 0.5055
Loss at step 6310: 0.9132
Loss at step 6320: 0.5538
Loss at step 6330: 0.3546
Loss at step 6340: 0.4505
Loss at step 6350: 0.8967
Loss at step 6360: 0.4499
Loss at step 6370: 0.5953
Loss at step 6380: 0.3233
Loss at step 6390: 0.4381
Loss at step 6400: 0.5031
Loss at step 6410: 0.6930
Loss at step 6420: 0.4112
Loss at step 6430: 0.7823
Loss at step 6440: 0.7487
Loss at step 6450: 0.2054
Loss at step 6460: 0.5552
Loss at step 6470: 0.9435
Loss at step 6480: 0.8090
Loss at step 6490: 1.0787
Loss at step 6500: 0.3304
Loss at step 6510: 0.6672
Loss at step 6520: 0.4802
Loss at step 6530: 1.3840
Loss at step 6540: 1.1578
Loss at step 6550: 1.1651
Loss at step 6560: 0.8330
Loss at step 6570: 1.0504
Loss at step 6580: 0.3866
Loss at step 6590: 0.3060
Loss at step 6600: 0.4970
Loss at step 6610: 0.5896
Loss at step 6620: 0.4425
Loss at step 6630: 0.1883
Loss at step 6640: 0.7618
Loss at step 6650: 0.8701
Loss at step 6660: 0.7996
Loss at step 6670: 0.8189
Loss at step 6680: 0.8585
Loss at step 6690: 0.6893
Loss at step 6700: 1.0624
Loss at step 6710: 0.3554
Loss at step 6720: 1.0186
Loss at step 6730: 0.2965
Loss at step 6740: 0.5071
Loss at step 6750: 0.2795
Loss at step 6760: 0.3446
Loss at step 6770: 0.4490
Loss at step 6780: 0.9297
Loss at step 6790: 0.5720
Loss at step 6800: 0.8970
Loss at step 6810: 1.0207
Loss at step 6820: 0.3458
Loss at step 6830: 0.9972
Loss at step 6840: 0.9071
Loss at step 6850: 1.1282
Loss at step 6860: 1.0514
Loss at step 6870: 0.9303
Loss at step 6880: 0.4594
Loss at step 6890: 0.9308
Loss at step 6900: 0.5617
Loss at step 6910: 0.4188
Loss at step 6920: 0.9559
Loss at step 6930: 0.7380
Loss at step 6940: 0.3961
Loss at step 6950: 0.4733
Loss at step 6960: 0.9059
Loss at step 6970: 0.6418
Loss at step 6980: 0.4486
Loss at step 6990: 0.9091
Loss at step 7000: 0.7345
Loss at step 7010: 0.8239
Loss at step 7020: 1.1870
Loss at step 7030: 0.5505
Loss at step 7040: 0.2011
Loss at step 7050: 0.6344
Loss at step 7060: 1.3575
Loss at step 7070: 0.5579
Loss at step 7080: 0.9862
Loss at step 7090: 1.4651
Loss at step 7100: 0.8222
Loss at step 7110: 0.3374
Loss at step 7120: 0.5021
Loss at step 7130: 0.8319
Loss at step 7140: 0.5459
Loss at step 7150: 0.2927
Loss at step 7160: 0.5179
Loss at step 7170: 0.7221
Loss at step 7180: 0.5125
Loss at step 7190: 0.1540
Loss at step 7200: 0.8302
Loss at step 7210: 0.4916
Loss at step 7220: 0.5450
Loss at step 7230: 0.7045
Loss at step 7240: 0.8237
Loss at step 7250: 0.5733
Loss at step 7260: 0.1414
Loss at step 7270: 0.9599
Loss at step 7280: 0.7516
Loss at step 7290: 0.8948
Loss at step 7300: 0.6239
Loss at step 7310: 0.8074
Loss at step 7320: 0.2396
Loss at step 7330: 0.9969
Loss at step 7340: 0.6806
Loss at step 7350: 0.4673
Loss at step 7360: 0.4381
Loss at step 7370: 1.2086
Loss at step 7380: 0.3976
Loss at step 7390: 1.0782
Loss at step 7400: 1.2208
Loss at step 7410: 0.1563
Loss at step 7420: 0.6302
Loss at step 7430: 0.2712
Loss at step 7440: 0.2167
Loss at step 7450: 0.6849
Loss at step 7460: 1.0205
Loss at step 7470: 0.4712
Loss at step 7480: 0.7718
Loss at step 7490: 0.6267
Loss at step 7500: 0.5184
Loss at step 7510: 0.4631
Loss at step 7520: 1.0891
Loss at step 7530: 0.4748
Loss at step 7540: 0.3942
Loss at step 7550: 0.5486
Loss at step 7560: 0.6758
Loss at step 7570: 0.8219
Loss at step 7580: 0.3408
Loss at step 7590: 0.6235
Loss at step 7600: 0.4558
Loss at step 7610: 1.0045
Loss at step 7620: 0.3848
Loss at step 7630: 0.5702
Loss at step 7640: 0.8714
Loss at step 7650: 0.1981
Loss at step 7660: 1.0894
Loss at step 7670: 1.5381
Loss at step 7680: 0.5771
Loss at step 7690: 0.4824
Loss at step 7700: 0.8334
Loss at step 7710: 0.3407
Loss at step 7720: 1.2885
Loss at step 7730: 1.3026
Loss at step 7740: 0.3846
Loss at step 7750: 0.1383
Loss at step 7760: 1.0540
Loss at step 7770: 0.3770
Loss at step 7780: 0.2161
Loss at step 7790: 0.2480
Loss at step 7800: 0.4080
Loss at step 7810: 0.9269
Loss at step 7820: 0.6368
Loss at step 7830: 0.2701
Loss at step 7840: 0.5370
Loss at step 7850: 0.2806
Loss at step 7860: 0.1648
Loss at step 7870: 0.7243
Loss at step 7880: 0.8533
Loss at step 7890: 0.3931
Loss at step 7900: 0.7703
Loss at step 7910: 1.0292
Loss at step 7920: 0.5086
Loss at step 7930: 0.6498
Loss at step 7940: 0.4682
Loss at step 7950: 0.4985
Loss at step 7960: 0.3858
Loss at step 7970: 0.2176
Loss at step 7980: 1.0287
Loss at step 7990: 0.3086
Loss at step 8000: 0.6636
Loss at step 8010: 0.5669
Loss at step 8020: 1.0765
Loss at step 8030: 0.7950
Loss at step 8040: 0.3454
Loss at step 8050: 0.4284
Loss at step 8060: 0.1347
Loss at step 8070: 0.3129
Loss at step 8080: 0.6537
Loss at step 8090: 0.5363
Loss at step 8100: 0.5314
Loss at step 8110: 0.4226
Loss at step 8120: 0.5924
Loss at step 8130: 0.7754
Loss at step 8140: 0.2511
Loss at step 8150: 1.2850
Loss at step 8160: 0.8436
Loss at step 8170: 0.1622
Loss at step 8180: 0.4301
Loss at step 8190: 0.2944
Loss at step 8200: 0.1836
Loss at step 8210: 0.3531
Loss at step 8220: 0.3274
Loss at step 8230: 1.2612
Loss at step 8240: 0.4145
Loss at step 8250: 0.8203
Loss at step 8260: 0.1172
Loss at step 8270: 0.2467
Loss at step 8280: 0.8515
Loss at step 8290: 0.3558
Loss at step 8300: 0.7198
Loss at step 8310: 0.3208
Loss at step 8320: 1.5590
Loss at step 8330: 0.4308
Loss at step 8340: 0.5803
Loss at step 8350: 0.5425
Loss at step 8360: 0.6708
Loss at step 8370: 0.3380
Loss at step 8380: 0.5906
Loss at step 8390: 0.4226
Loss at step 8400: 0.2988
Loss at step 8410: 0.5034
Loss at step 8420: 0.5826
Loss at step 8430: 0.7835
Loss at step 8440: 0.5352
Loss at step 8450: 0.7878
Loss at step 8460: 0.3850
Loss at step 8470: 0.6100
Loss at step 8480: 1.0061
Loss at step 8490: 0.2717
Loss at step 8500: 0.5062
Loss at step 8510: 0.1238
Loss at step 8520: 0.4622
Loss at step 8530: 0.7511
Loss at step 8540: 0.3510
Loss at step 8550: 0.4482
Loss at step 8560: 0.4109
Loss at step 8570: 0.0644
Loss at step 8580: 0.3480
Loss at step 8590: 1.0483
Loss at step 8600: 0.4183
Loss at step 8610: 0.4982
Loss at step 8620: 0.7936
Loss at step 8630: 0.2588
Loss at step 8640: 0.8115
Loss at step 8650: 0.8158
Loss at step 8660: 0.8689
Loss at step 8670: 0.3469
Loss at step 8680: 0.3346
Loss at step 8690: 0.6070
Loss at step 8700: 0.6301
Loss at step 8710: 0.4509
Loss at step 8720: 0.6789
Loss at step 8730: 0.4565
Loss at step 8740: 0.3155
Loss at step 8750: 0.6529
Loss at step 8760: 0.3094
Loss at step 8770: 0.1348
Loss at step 8780: 0.2122
Loss at step 8790: 0.0507
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.52486, 'precision': [0.535858, 0.501946, 0.533023], 'recall': [0.765574, 0.420367, 0.332753], 'f1': [0.630442, 0.457548, 0.409725]}
{'accuracy': 0.750491, 'precision': 0.533023, 'recall': 0.332753, 'f1': 0.409725, 'WordR': 0.177766}
Loss at step 8800: 0.4032
Loss at step 8810: 0.4874
Loss at step 8820: 0.0255
Loss at step 8830: 0.4224
Loss at step 8840: 0.6556
Loss at step 8850: 0.0575
Loss at step 8860: 0.1299
Loss at step 8870: 0.0513
Loss at step 8880: 0.3439
Loss at step 8890: 0.1203
Loss at step 8900: 0.5645
Loss at step 8910: 0.3077
Loss at step 8920: 0.9854
Loss at step 8930: 0.5692
Loss at step 8940: 0.3341
Loss at step 8950: 1.3445
Loss at step 8960: 1.0079
Loss at step 8970: 0.6468
Loss at step 8980: 0.1226
Loss at step 8990: 0.2420
Loss at step 9000: 0.1562
Loss at step 9010: 0.9660
Loss at step 9020: 0.2206
Loss at step 9030: 0.2444
Loss at step 9040: 0.4334
Loss at step 9050: 1.2240
Loss at step 9060: 0.2977
Loss at step 9070: 0.0120
Loss at step 9080: 0.4693
Loss at step 9090: 0.4315
Loss at step 9100: 0.2302
Loss at step 9110: 0.1993
Loss at step 9120: 0.3561
Loss at step 9130: 0.2272
Loss at step 9140: 0.0638
Loss at step 9150: 0.4986
Loss at step 9160: 0.8939
Loss at step 9170: 0.5670
Loss at step 9180: 0.4626
Loss at step 9190: 0.8738
Loss at step 9200: 0.9021
Loss at step 9210: 0.0062
Loss at step 9220: 0.0878
Loss at step 9230: 1.1045
Loss at step 9240: 0.1901
Loss at step 9250: 0.8126
Loss at step 9260: 0.4156
Loss at step 9270: 0.7024
Loss at step 9280: 0.4056
Loss at step 9290: 0.2999
Loss at step 9300: 0.5548
Loss at step 9310: 0.3015
Loss at step 9320: 0.6862
Loss at step 9330: 1.0419
Loss at step 9340: 0.2713
Loss at step 9350: 0.4592
Loss at step 9360: 0.9077
Loss at step 9370: 0.4299
Loss at step 9380: 0.9468
Loss at step 9390: 0.4002
Loss at step 9400: 0.6742
Loss at step 9410: 0.8459
Loss at step 9420: 0.2391
Loss at step 9430: 0.2240
Loss at step 9440: 0.2790
Loss at step 9450: 0.4122
Loss at step 9460: 0.4378
Loss at step 9470: 0.4982
Loss at step 9480: 0.7803
Loss at step 9490: 0.3815
Loss at step 9500: 0.8600
Loss at step 9510: 0.0407
Loss at step 9520: 0.5669
Loss at step 9530: 0.0476
Loss at step 9540: 0.1817
Loss at step 9550: 0.5435
Loss at step 9560: 0.5307
Loss at step 9570: 0.1739
Loss at step 9580: 0.6877
Loss at step 9590: 1.0012
Loss at step 9600: 0.8285
Loss at step 9610: 0.3610
Loss at step 9620: 0.0462
Loss at step 9630: 1.0091
Loss at step 9640: 0.1989
Loss at step 9650: 0.8764
Loss at step 9660: 0.1702
Loss at step 9670: 0.2328
Loss at step 9680: 0.2196
Loss at step 9690: 0.3879
Loss at step 9700: 0.3945
Loss at step 9710: 0.3702
Loss at step 9720: 0.5584
Loss at step 9730: 0.0522
Loss at step 9740: 0.3586
Loss at step 9750: 0.5123
Loss at step 9760: 0.2809
Loss at step 9770: 0.3155
Loss at step 9780: 0.2164
Loss at step 9790: 1.0447
Loss at step 9800: 0.7934
Loss at step 9810: 0.1436
Loss at step 9820: 0.0658
Loss at step 9830: 0.6512
Loss at step 9840: 0.4009
Loss at step 9850: 0.3258
Loss at step 9860: 0.2803
Loss at step 9870: 0.5566
Loss at step 9880: 0.4258
Loss at step 9890: 0.2802
Loss at step 9900: 0.4441
Loss at step 9910: 0.3774
Loss at step 9920: 0.1724
Loss at step 9930: 0.3308
Loss at step 9940: 0.2156
Loss at step 9950: 0.2369
Loss at step 9960: 0.3477
Loss at step 9970: 1.2504
Loss at step 9980: 0.4598
Loss at step 9990: 0.7396
Loss at step 10000: 1.0399
Loss at step 10010: 0.1438
Loss at step 10020: 0.4681
Loss at step 10030: 0.7508
Loss at step 10040: 0.4592
Loss at step 10050: 0.4402
Loss at step 10060: 0.1782
Loss at step 10070: 0.0590
Loss at step 10080: 0.8173
Loss at step 10090: 0.5471
Loss at step 10100: 0.9058
Loss at step 10110: 0.2564
Loss at step 10120: 0.1652
Loss at step 10130: 0.1880
Loss at step 10140: 1.1049
Loss at step 10150: 0.5502
Loss at step 10160: 0.9277
Loss at step 10170: 0.1501
Loss at step 10180: 0.3357
Loss at step 10190: 0.7392
Loss at step 10200: 0.5542
Loss at step 10210: 0.2697
Loss at step 10220: 0.3677
Loss at step 10230: 0.3093
Loss at step 10240: 1.1403
Loss at step 10250: 0.7003
Loss at step 10260: 0.1078
Loss at step 10270: 0.1972
Loss at step 10280: 0.2308
Loss at step 10290: 0.0641
Loss at step 10300: 0.4810
Loss at step 10310: 0.1057
Loss at step 10320: 0.8708
Loss at step 10330: 0.6214
Loss at step 10340: 0.3862
Loss at step 10350: 0.1295
Loss at step 10360: 0.4640
Loss at step 10370: 0.4203
Loss at step 10380: 0.2729
Loss at step 10390: 0.4637
Loss at step 10400: 0.6083
Loss at step 10410: 1.1999
Loss at step 10420: 0.2226
Loss at step 10430: 0.1021
Loss at step 10440: 0.2513
Loss at step 10450: 0.3201
Loss at step 10460: 0.4249
Loss at step 10470: 0.5051
Loss at step 10480: 0.5310
Loss at step 10490: 0.0593
Loss at step 10500: 0.4339
Loss at step 10510: 0.4628
Loss at step 10520: 0.2653
Loss at step 10530: 0.5479
Loss at step 10540: 0.0520
Loss at step 10550: 0.2094
Loss at step 10560: 0.1622
Loss at step 10570: 0.1761
Loss at step 10580: 0.1945
Loss at step 10590: 0.3052
Loss at step 10600: 0.9767
Loss at step 10610: 1.3727
Loss at step 10620: 0.0095
Loss at step 10630: 0.3468
Loss at step 10640: 0.3102
Loss at step 10650: 0.1303
Loss at step 10660: 0.2863
Loss at step 10670: 0.6608
Loss at step 10680: 0.0552
Loss at step 10690: 0.0424
Loss at step 10700: 0.7997
Loss at step 10710: 0.7522
Loss at step 10720: 0.0652
Loss at step 10730: 0.5507
Loss at step 10740: 0.0284
Loss at step 10750: 0.2442
Loss at step 10760: 0.0584
Loss at step 10770: 0.1129
Loss at step 10780: 2.7461
Loss at step 10790: 0.5870
Loss at step 10800: 0.0524
Loss at step 10810: 0.2255
Loss at step 10820: 0.3549
Loss at step 10830: 0.9772
Loss at step 10840: 0.0189
Loss at step 10850: 1.0671
Loss at step 10860: 0.7924
Loss at step 10870: 0.5601
Loss at step 10880: 0.1596
Loss at step 10890: 0.2246
Loss at step 10900: 0.3705
Loss at step 10910: 0.5110
Loss at step 10920: 1.6584
Loss at step 10930: 0.3346
Loss at step 10940: 0.5110
Loss at step 10950: 0.6516
Loss at step 10960: 0.4124
Loss at step 10970: 0.3683
Loss at step 10980: 0.1406
Loss at step 10990: 0.4004
Loss at step 11000: 0.5197
Loss at step 11010: 0.8123
Loss at step 11020: 0.2843
Loss at step 11030: 0.1114
Loss at step 11040: 0.1502
Loss at step 11050: 0.6894
Loss at step 11060: 0.4612
Loss at step 11070: 0.9579
Loss at step 11080: 0.0435
Loss at step 11090: 0.2481
Loss at step 11100: 0.1915
Loss at step 11110: 0.7903
Loss at step 11120: 1.0387
Loss at step 11130: 0.3283
Loss at step 11140: 0.2818
Loss at step 11150: 0.2122
Loss at step 11160: 0.3107
Loss at step 11170: 0.0164
Loss at step 11180: 0.5522
Loss at step 11190: 0.4219
Loss at step 11200: 0.2010
Loss at step 11210: 0.2053
Loss at step 11220: 0.1507
Loss at step 11230: 0.5320
Loss at step 11240: 0.7417
Loss at step 11250: 0.0771
Loss at step 11260: 0.4417
Loss at step 11270: 0.1665
Loss at step 11280: 0.0064
Loss at step 11290: 0.2082
Loss at step 11300: 0.1669
Loss at step 11310: 0.4438
Loss at step 11320: 0.0347
Loss at step 11330: 0.3271
Loss at step 11340: 0.2700
Loss at step 11350: 0.7856
Loss at step 11360: 0.0458
Loss at step 11370: 0.0630
Loss at step 11380: 0.9114
Loss at step 11390: 0.1283
Loss at step 11400: 0.0900
Loss at step 11410: 0.7801
Loss at step 11420: 0.5629
Loss at step 11430: 0.6376
Loss at step 11440: 1.9651
Loss at step 11450: 0.7140
Loss at step 11460: 0.1718
Loss at step 11470: 0.1427
Loss at step 11480: 0.0956
Loss at step 11490: 2.0328
Loss at step 11500: 0.0671
Loss at step 11510: 0.7692
Loss at step 11520: 0.2192
Loss at step 11530: 0.8002
Loss at step 11540: 0.3243
Loss at step 11550: 1.4373
Loss at step 11560: 0.4833
Loss at step 11570: 0.3540
Loss at step 11580: 0.1092
Loss at step 11590: 0.4161
Loss at step 11600: 0.8346
Loss at step 11610: 0.5806
Loss at step 11620: 0.0194
Loss at step 11630: 0.1438
Loss at step 11640: 0.0854
Loss at step 11650: 0.0188
Loss at step 11660: 0.6518
Loss at step 11670: 0.0254
Loss at step 11680: 0.0890
Loss at step 11690: 0.0771
Loss at step 11700: 0.0441
Loss at step 11710: 0.6021
Loss at step 11720: 0.6742
Loss at step 11730: 1.0150
Loss at step 11740: 0.5017
Loss at step 11750: 0.0064
Loss at step 11760: 0.2524
Loss at step 11770: 0.5417
Loss at step 11780: 0.1836
Loss at step 11790: 0.0210
Loss at step 11800: 0.5489
Loss at step 11810: 0.2263
Loss at step 11820: 0.1833
Loss at step 11830: 0.4022
Loss at step 11840: 0.4712
Loss at step 11850: 0.2306
Loss at step 11860: 0.2167
Loss at step 11870: 0.4041
Loss at step 11880: 0.7803
Loss at step 11890: 0.2715
Loss at step 11900: 1.0953
Loss at step 11910: 0.1875
Loss at step 11920: 0.1488
Loss at step 11930: 0.5054
Loss at step 11940: 0.0797
Loss at step 11950: 0.0245
Loss at step 11960: 0.0353
Loss at step 11970: 0.5053
Loss at step 11980: 0.3037
Loss at step 11990: 0.1814
Loss at step 12000: 0.0503
Loss at step 12010: 0.2278
Loss at step 12020: 0.1543
Loss at step 12030: 0.2587
Loss at step 12040: 0.1432
Loss at step 12050: 0.0299
Loss at step 12060: 0.1588
Loss at step 12070: 0.7401
Loss at step 12080: 0.5842
Loss at step 12090: 0.1465
Loss at step 12100: 0.1627
Loss at step 12110: 0.0526
Loss at step 12120: 0.0572
Loss at step 12130: 0.0940
Loss at step 12140: 0.9626
Loss at step 12150: 0.0253
Loss at step 12160: 1.0060
Loss at step 12170: 0.9677
Loss at step 12180: 0.3489
Loss at step 12190: 0.2403
Loss at step 12200: 0.2886
Loss at step 12210: 0.3238
Loss at step 12220: 0.2666
Loss at step 12230: 0.5317
Loss at step 12240: 0.1136
Loss at step 12250: 0.1231
Loss at step 12260: 1.8710
Loss at step 12270: 0.1077
Loss at step 12280: 1.1788
Loss at step 12290: 0.5668
Loss at step 12300: 0.4857
Loss at step 12310: 0.1176
Loss at step 12320: 0.0391
Loss at step 12330: 0.0710
Loss at step 12340: 0.3127
Loss at step 12350: 0.0387
Loss at step 12360: 0.0738
Loss at step 12370: 0.0128
Loss at step 12380: 0.2338
Loss at step 12390: 0.4034
Loss at step 12400: 0.0167
Loss at step 12410: 0.3024
Loss at step 12420: 0.4865
Loss at step 12430: 0.8307
Loss at step 12440: 0.1819
Loss at step 12450: 0.5532
Loss at step 12460: 0.1494
Loss at step 12470: 0.0798
Loss at step 12480: 0.1314
Loss at step 12490: 0.0137
Loss at step 12500: 0.0918
Loss at step 12510: 0.2843
Loss at step 12520: 0.2313
Loss at step 12530: 0.3570
Loss at step 12540: 0.3607
Loss at step 12550: 0.1699
Loss at step 12560: 0.0956
Loss at step 12570: 0.3242
Loss at step 12580: 0.3879
Loss at step 12590: 0.2762
Loss at step 12600: 0.2832
Loss at step 12610: 0.4731
Loss at step 12620: 0.0107
Loss at step 12630: 0.3337
Loss at step 12640: 0.1084
Loss at step 12650: 0.0077
Loss at step 12660: 0.3767
Loss at step 12670: 0.1004
Loss at step 12680: 0.2032
Loss at step 12690: 0.0021
Loss at step 12700: 0.3192
Loss at step 12710: 0.1399
Loss at step 12720: 0.4712
Loss at step 12730: 0.8260
Loss at step 12740: 0.0134
Loss at step 12750: 0.0783
Loss at step 12760: 0.0440
Loss at step 12770: 0.6324
Loss at step 12780: 0.1183
Loss at step 12790: 0.0647
Loss at step 12800: 0.2420
Loss at step 12810: 0.6203
Loss at step 12820: 0.2653
Loss at step 12830: 0.6801
Loss at step 12840: 0.6980
Loss at step 12850: 0.1825
Loss at step 12860: 0.0884
Loss at step 12870: 0.0033
Loss at step 12880: 1.3364
Loss at step 12890: 0.1200
Loss at step 12900: 0.0247
Loss at step 12910: 0.0095
Loss at step 12920: 1.5415
Loss at step 12930: 0.6716
Loss at step 12940: 0.1404
Loss at step 12950: 0.3114
Loss at step 12960: 0.8316
Loss at step 12970: 0.7376
Loss at step 12980: 0.0432
Loss at step 12990: 0.4725
Loss at step 13000: 0.0555
Loss at step 13010: 0.1020
Loss at step 13020: 0.0330
Loss at step 13030: 0.4176
Loss at step 13040: 0.9002
Loss at step 13050: 0.7923
Loss at step 13060: 0.4905
Loss at step 13070: 0.0259
Loss at step 13080: 0.2108
Loss at step 13090: 0.4903
Loss at step 13100: 0.0310
Loss at step 13110: 0.7590
Loss at step 13120: 0.7184
Loss at step 13130: 0.0082
Loss at step 13140: 0.5686
Loss at step 13150: 0.2013
Loss at step 13160: 0.3428
Loss at step 13170: 0.2136
Loss at step 13180: 0.0471
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.545262, 'precision': [0.647989, 0.471668, 0.549398], 'recall': [0.587705, 0.606925, 0.397213], 'f1': [0.616377, 0.530816, 0.461072]}
{'accuracy': 0.75835, 'precision': 0.549398, 'recall': 0.397213, 'f1': 0.461072, 'WordR': 0.177766}
Loss at step 13190: 0.0284
Loss at step 13200: 0.1594
Loss at step 13210: 0.4516
Loss at step 13220: 0.0342
Loss at step 13230: 0.0026
Loss at step 13240: 0.0006
Loss at step 13250: 0.0085
Loss at step 13260: 0.2061
Loss at step 13270: 0.1130
Loss at step 13280: 0.3278
Loss at step 13290: 0.0200
Loss at step 13300: 0.6045
Loss at step 13310: 0.6206
Loss at step 13320: 0.0052
Loss at step 13330: 0.2437
Loss at step 13340: 0.0218
Loss at step 13350: 0.0860
Loss at step 13360: 0.0195
Loss at step 13370: 0.0114
Loss at step 13380: 0.2638
Loss at step 13390: 0.0352
Loss at step 13400: 0.1715
Loss at step 13410: 0.0830
Loss at step 13420: 0.6965
Loss at step 13430: 0.2916
Loss at step 13440: 0.0237
Loss at step 13450: 0.2102
Loss at step 13460: 0.2823
Loss at step 13470: 0.0154
Loss at step 13480: 0.0842
Loss at step 13490: 0.1698
Loss at step 13500: 0.6695
Loss at step 13510: 0.0362
Loss at step 13520: 0.0033
Loss at step 13530: 0.0196
Loss at step 13540: 0.0079
Loss at step 13550: 0.0026
Loss at step 13560: 0.1090
Loss at step 13570: 0.0006
Loss at step 13580: 0.0791
Loss at step 13590: 0.0007
Loss at step 13600: 0.1203
Loss at step 13610: 0.2384
Loss at step 13620: 0.0190
Loss at step 13630: 0.2817
Loss at step 13640: 0.3861
Loss at step 13650: 0.4430
Loss at step 13660: 0.3201
Loss at step 13670: 0.2594
Loss at step 13680: 0.0093
Loss at step 13690: 0.0003
Loss at step 13700: 0.5947
Loss at step 13710: 1.4433
Loss at step 13720: 0.0081
Loss at step 13730: 0.0001
Loss at step 13740: 0.4355
Loss at step 13750: 0.0854
Loss at step 13760: 0.0068
Loss at step 13770: 0.1487
Loss at step 13780: 0.0339
Loss at step 13790: 0.0712
Loss at step 13800: 0.0795
Loss at step 13810: 0.0274
Loss at step 13820: 0.1668
Loss at step 13830: 0.3575
Loss at step 13840: 0.0887
Loss at step 13850: 0.0001
Loss at step 13860: 0.9272
Loss at step 13870: 0.2767
Loss at step 13880: 0.3632
Loss at step 13890: 0.2552
Loss at step 13900: 0.0259
Loss at step 13910: 0.4952
Loss at step 13920: 0.0101
Loss at step 13930: 0.1015
Loss at step 13940: 0.5608
Loss at step 13950: 0.0139
Loss at step 13960: 0.0075
Loss at step 13970: 0.2407
Loss at step 13980: 0.1202
Loss at step 13990: 0.0781
Loss at step 14000: 0.5250
Loss at step 14010: 0.6463
Loss at step 14020: 0.1973
Loss at step 14030: 0.6352
Loss at step 14040: 0.1067
Loss at step 14050: 0.0661
Loss at step 14060: 0.5978
Loss at step 14070: 0.3819
Loss at step 14080: 0.1643
Loss at step 14090: 0.1310
Loss at step 14100: 0.0072
Loss at step 14110: 0.0304
Loss at step 14120: 0.0050
Loss at step 14130: 0.4513
Loss at step 14140: 0.0724
Loss at step 14150: 0.4773
Loss at step 14160: 0.1263
Loss at step 14170: 0.0088
Loss at step 14180: 0.0347
Loss at step 14190: 0.2522
Loss at step 14200: 0.0425
Loss at step 14210: 0.0727
Loss at step 14220: 0.0001
Loss at step 14230: 0.6632
Loss at step 14240: 0.0238
Loss at step 14250: 0.2114
Loss at step 14260: 0.0023
Loss at step 14270: 0.0195
Loss at step 14280: 0.0103
Loss at step 14290: 0.0794
Loss at step 14300: 0.1133
Loss at step 14310: 0.4508
Loss at step 14320: 0.5270
Loss at step 14330: 0.7677
Loss at step 14340: 0.0236
Loss at step 14350: 0.0110
Loss at step 14360: 0.0968
Loss at step 14370: 0.5132
Loss at step 14380: 0.0368
Loss at step 14390: 0.1849
Loss at step 14400: 0.0150
Loss at step 14410: 0.0529
Loss at step 14420: 0.0062
Loss at step 14430: 0.1333
Loss at step 14440: 0.4643
Loss at step 14450: 0.4217
Loss at step 14460: 0.6410
Loss at step 14470: 0.1993
Loss at step 14480: 0.5230
Loss at step 14490: 0.0952
Loss at step 14500: 0.1842
Loss at step 14510: 0.0024
Loss at step 14520: 1.1598
Loss at step 14530: 0.4064
Loss at step 14540: 0.0016
Loss at step 14550: 0.0078
Loss at step 14560: 0.2300
Loss at step 14570: 0.0880
Loss at step 14580: 0.8190
Loss at step 14590: 0.4418
Loss at step 14600: 0.0155
Loss at step 14610: 0.4502
Loss at step 14620: 0.3482
Loss at step 14630: 0.2468
Loss at step 14640: 0.0037
Loss at step 14650: 0.0011
Loss at step 14660: 0.1773
Loss at step 14670: 0.0001
Loss at step 14680: 0.0189
Loss at step 14690: 0.6552
Loss at step 14700: 0.0862
Loss at step 14710: 0.1722
Loss at step 14720: 0.0489
Loss at step 14730: 0.5450
Loss at step 14740: 0.4644
Loss at step 14750: 1.1169
Loss at step 14760: 0.8510
Loss at step 14770: 0.1499
Loss at step 14780: 0.4156
Loss at step 14790: 0.6664
Loss at step 14800: 0.2444
Loss at step 14810: 0.0209
Loss at step 14820: 0.1670
Loss at step 14830: 0.0028
Loss at step 14840: 0.4205
Loss at step 14850: 0.2278
Loss at step 14860: 0.1454
Loss at step 14870: 0.1243
Loss at step 14880: 0.0851
Loss at step 14890: 0.5533
Loss at step 14900: 0.0048
Loss at step 14910: 0.6038
Loss at step 14920: 0.2131
Loss at step 14930: 1.8808
Loss at step 14940: 0.0962
Loss at step 14950: 0.1224
Loss at step 14960: 0.4813
Loss at step 14970: 0.0247
Loss at step 14980: 0.0180
Loss at step 14990: 0.0309
Loss at step 15000: 0.1990
Loss at step 15010: 0.1852
Loss at step 15020: 0.1798
Loss at step 15030: 1.0671
Loss at step 15040: 0.0472
Loss at step 15050: 0.2312
Loss at step 15060: 0.0314
Loss at step 15070: 0.2169
Loss at step 15080: 0.3184
Loss at step 15090: 0.0541
Loss at step 15100: 0.3811
Loss at step 15110: 0.0028
Loss at step 15120: 0.3886
Loss at step 15130: 0.0011
Loss at step 15140: 1.1753
Loss at step 15150: 0.0256
Loss at step 15160: 0.1014
Loss at step 15170: 0.3096
Loss at step 15180: 0.0022
Loss at step 15190: 0.0198
Loss at step 15200: 0.0420
Loss at step 15210: 0.0984
Loss at step 15220: 0.0100
Loss at step 15230: 0.0256
Loss at step 15240: 0.9446
Loss at step 15250: 1.1169
Loss at step 15260: 0.4824
Loss at step 15270: 1.7046
Loss at step 15280: 0.3233
Loss at step 15290: 0.0131
Loss at step 15300: 0.2110
Loss at step 15310: 0.3678
Loss at step 15320: 0.0458
Loss at step 15330: 0.0284
Loss at step 15340: 0.0145
Loss at step 15350: 0.0587
Loss at step 15360: 0.9887
Loss at step 15370: 0.0020
Loss at step 15380: 0.0246
Loss at step 15390: 0.0020
Loss at step 15400: 0.4682
Loss at step 15410: 0.0241
Loss at step 15420: 1.0744
Loss at step 15430: 0.2264
Loss at step 15440: 0.3288
Loss at step 15450: 0.1840
Loss at step 15460: 0.0020
Loss at step 15470: 0.0187
Loss at step 15480: 0.0026
Loss at step 15490: 0.0932
Loss at step 15500: 0.0062
Loss at step 15510: 0.0160
Loss at step 15520: 0.0009
Loss at step 15530: 0.5221
Loss at step 15540: 0.0031
Loss at step 15550: 0.2332
Loss at step 15560: 0.1370
Loss at step 15570: 0.6080
Loss at step 15580: 0.1371
Loss at step 15590: 0.4471
Loss at step 15600: 0.0532
Loss at step 15610: 0.0839
Loss at step 15620: 0.0622
Loss at step 15630: 0.0657
Loss at step 15640: 0.8072
Loss at step 15650: 0.0026
Loss at step 15660: 0.3910
Loss at step 15670: 0.2076
Loss at step 15680: 0.2290
Loss at step 15690: 0.2749
Loss at step 15700: 0.0050
Loss at step 15710: 0.1847
Loss at step 15720: 0.4217
Loss at step 15730: 0.1531
Loss at step 15740: 0.0079
Loss at step 15750: 0.4721
Loss at step 15760: 0.0082
Loss at step 15770: 0.1420
Loss at step 15780: 0.0247
Loss at step 15790: 0.0011
Loss at step 15800: 0.0352
Loss at step 15810: 0.0846
Loss at step 15820: 0.0002
Loss at step 15830: 0.0588
Loss at step 15840: 0.0435
Loss at step 15850: 0.3483
Loss at step 15860: 0.1372
Loss at step 15870: 0.0005
Loss at step 15880: 0.0266
Loss at step 15890: 0.0001
Loss at step 15900: 0.0000
Loss at step 15910: 0.0132
Loss at step 15920: 0.0005
Loss at step 15930: 0.0018
Loss at step 15940: 0.1281
Loss at step 15950: 0.0043
Loss at step 15960: 0.0002
Loss at step 15970: 0.0146
Loss at step 15980: 0.0013
Loss at step 15990: 0.0044
Loss at step 16000: 0.0021
Loss at step 16010: 0.3286
Loss at step 16020: 0.6718
Loss at step 16030: 0.0023
Loss at step 16040: 0.2115
Loss at step 16050: 0.0216
Loss at step 16060: 0.1366
Loss at step 16070: 0.0707
Loss at step 16080: 0.0598
Loss at step 16090: 0.1068
Loss at step 16100: 0.0339
Loss at step 16110: 0.0001
Loss at step 16120: 0.1355
Loss at step 16130: 0.0609
Loss at step 16140: 0.0173
Loss at step 16150: 0.0913
Loss at step 16160: 0.2896
Loss at step 16170: 0.5845
Loss at step 16180: 0.0025
Loss at step 16190: 0.0501
Loss at step 16200: 0.4225
Loss at step 16210: 0.0011
Loss at step 16220: 0.0006
Loss at step 16230: 0.0180
Loss at step 16240: 0.1291
Loss at step 16250: 0.5605
Loss at step 16260: 0.0282
Loss at step 16270: 0.0001
Loss at step 16280: 0.0002
Loss at step 16290: 0.0014
Loss at step 16300: 0.3181
Loss at step 16310: 0.0387
Loss at step 16320: 0.0003
Loss at step 16330: 0.3909
Loss at step 16340: 0.1384
Loss at step 16350: 0.0243
Loss at step 16360: 0.3295
Loss at step 16370: 0.0846
Loss at step 16380: 0.0001
Loss at step 16390: 0.0899
Loss at step 16400: 0.0131
Loss at step 16410: 0.8985
Loss at step 16420: 0.4433
Loss at step 16430: 0.0065
Loss at step 16440: 0.0230
Loss at step 16450: 0.0149
Loss at step 16460: 0.1679
Loss at step 16470: 0.1069
Loss at step 16480: 0.2441
Loss at step 16490: 0.0667
Loss at step 16500: 0.2865
Loss at step 16510: 0.0017
Loss at step 16520: 0.8462
Loss at step 16530: 0.3389
Loss at step 16540: 0.0434
Loss at step 16550: 0.0097
Loss at step 16560: 0.9101
Loss at step 16570: 0.0467
Loss at step 16580: 0.0058
Loss at step 16590: 0.4601
Loss at step 16600: 0.6848
Loss at step 16610: 0.1127
Loss at step 16620: 0.0221
Loss at step 16630: 0.2249
Loss at step 16640: 0.0830
Loss at step 16650: 0.9665
Loss at step 16660: 0.0326
Loss at step 16670: 0.4266
Loss at step 16680: 0.4857
Loss at step 16690: 0.8882
Loss at step 16700: 0.1639
Loss at step 16710: 0.4663
Loss at step 16720: 0.5081
Loss at step 16730: 0.0009
Loss at step 16740: 0.0005
Loss at step 16750: 0.3406
Loss at step 16760: 0.1705
Loss at step 16770: 0.0000
Loss at step 16780: 0.0316
Loss at step 16790: 0.4993
Loss at step 16800: 0.0242
Loss at step 16810: 0.3896
Loss at step 16820: 0.3342
Loss at step 16830: 0.0325
Loss at step 16840: 0.0120
Loss at step 16850: 0.0787
Loss at step 16860: 0.1604
Loss at step 16870: 0.0001
Loss at step 16880: 0.0218
Loss at step 16890: 0.6611
Loss at step 16900: 2.2952
Loss at step 16910: 0.1033
Loss at step 16920: 0.0040
Loss at step 16930: 0.7210
Loss at step 16940: 0.0004
Loss at step 16950: 0.0001
Loss at step 16960: 0.0657
Loss at step 16970: 0.0001
Loss at step 16980: 0.0777
Loss at step 16990: 0.3325
Loss at step 17000: 0.0153
Loss at step 17010: 0.0018
Loss at step 17020: 0.2357
Loss at step 17030: 0.0182
Loss at step 17040: 0.3773
Loss at step 17050: 0.4987
Loss at step 17060: 0.4369
Loss at step 17070: 0.0945
Loss at step 17080: 0.0040
Loss at step 17090: 0.0009
Loss at step 17100: 0.2685
Loss at step 17110: 0.0010
Loss at step 17120: 0.0601
Loss at step 17130: 0.0000
Loss at step 17140: 0.0023
Loss at step 17150: 0.1886
Loss at step 17160: 0.1023
Loss at step 17170: 1.5624
Loss at step 17180: 0.1265
Loss at step 17190: 0.0082
Loss at step 17200: 0.0000
Loss at step 17210: 0.0001
Loss at step 17220: 0.0347
Loss at step 17230: 0.0018
Loss at step 17240: 1.9163
Loss at step 17250: 0.0316
Loss at step 17260: 0.0042
Loss at step 17270: 0.0146
Loss at step 17280: 0.0396
Loss at step 17290: 0.0001
Loss at step 17300: 0.0155
Loss at step 17310: 0.3990
Loss at step 17320: 0.0520
Loss at step 17330: 0.2888
Loss at step 17340: 0.1715
Loss at step 17350: 0.4098
Loss at step 17360: 0.0005
Loss at step 17370: 0.2817
Loss at step 17380: 0.1571
Loss at step 17390: 0.0313
Loss at step 17400: 0.7593
Loss at step 17410: 0.0036
Loss at step 17420: 0.2365
Loss at step 17430: 0.4662
Loss at step 17440: 0.0115
Loss at step 17450: 0.0415
Loss at step 17460: 0.0190
Loss at step 17470: 0.0037
Loss at step 17480: 0.2619
Loss at step 17490: 0.4432
Loss at step 17500: 0.0031
Loss at step 17510: 0.0022
Loss at step 17520: 0.1514
Loss at step 17530: 0.0381
Loss at step 17540: 0.1208
Loss at step 17550: 0.1692
Loss at step 17560: 0.0295
Loss at step 17570: 0.6088
Loss at step 17580: 0.0785
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.556143, 'precision': [0.639647, 0.489022, 0.558537], 'recall': [0.62418, 0.598778, 0.398955], 'f1': [0.631819, 0.538363, 0.465447]}
{'accuracy': 0.761523, 'precision': 0.558537, 'recall': 0.398955, 'f1': 0.465447, 'WordR': 0.177766}
Parameter 'function'=<function get_omission_datasets.<locals>.span_func at 0x76846439d700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 5977 of the training set: {'input_ids': [101, 1996, 8013, 5176, 3251, 2045, 2003, 2151, 3114, 2010, 6097, 2891, 10439, 4136, 2032, 1008, 2296, 1008, 2051, 2002, 9573, 2055, 1000, 6047, 26351, 1000, 2893, 3492, 4365, 15703, 1012, 1996, 4005, 5176, 3251, 1996, 26828, 2003, 6037, 2004, 1037, 3769, 1011, 2039, 1010, 2030, 1999, 1996, 4530, 8758, 15363, 4646, 1998, 5176, 2000, 1040, 2213, 1996, 12117, 12326, 1997, 1996, 4471, 2061, 2008, 2027, 2064, 8556, 2023, 3043, 2582, 1012, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2003, 2045, 2151, 3114, 2115, 6097, 2891, 10439, 4136, 2033, 1008, 2296, 1008, 2051, 1045, 9573, 2055, 1523, 6047, 26351, 1524, 2893, 3492, 4365, 15703, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 3782, 1010, 2071, 2017, 2292, 2149, 2113, 2062, 2055, 2023, 26828, 1029, 2003, 2009, 6037, 2004, 1037, 3769, 1011, 2039, 1010, 2030, 1999, 1996, 4530, 8758, 15363, 4646, 1029, 3531, 1040, 2213, 2149, 1037, 12117, 12326, 1997, 1996, 4471, 2061, 2008, 2057, 2064, 8556, 2023, 3043, 2582, 1010, 4283, 999, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2009, 2003, 2023, 1010, 2296, 2051, 1045, 2128, 27927, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1048, 3372, 23706, 2575, 9103, 4305, 102, 101, 1030, 25162, 2620, 19317, 4931, 2153, 3782, 1010, 2064, 2057, 7868, 2008, 2017, 1521, 2310, 2908, 2083, 1996, 1523, 4553, 2062, 1524, 2030, 2079, 2017, 2467, 1523, 2485, 1524, 1996, 26828, 8874, 1029, 4283, 1999, 5083, 999, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 2031, 5791, 13886, 1996, 4553, 2062, 1012, 3243, 1037, 2261, 2335, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 2153, 1010, 2017, 2323, 2022, 2583, 2000, 4487, 19150, 1996, 26828, 2015, 1997, 1996, 4530, 8758, 15363, 4646, 2478, 1996, 6097, 10906, 1999, 1996, 26828, 21628, 2000, 4652, 2582, 25732, 2015, 1999, 2925, 1012, 102, 101, 1030, 4530, 8758, 6342, 9397, 11589, 1045, 19148, 1045, 2064, 2079, 2008, 1010, 1045, 2001, 17704, 2667, 2025, 2000, 2735, 2125, 1996, 2878, 3444, 2349, 2000, 2028, 26828, 10694, 1012, 102, 101, 1030, 25162, 2620, 19317, 7632, 2153, 1010, 2028, 2062, 2518, 2057, 1521, 1040, 2066, 2000, 4638, 1010, 2065, 2017, 11562, 1996, 4530, 8758, 15363, 4646, 2013, 1996, 12183, 3347, 1010, 1996, 1520, 26828, 2015, 1013, 3522, 2015, 1521, 2323, 3711, 1010, 22042, 4553, 2062, 2013, 2008, 2181, 2323, 19776, 2009, 8642, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [7, -1, 0, 1, 3, 6]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 21975
Loss at step 10: 3.0301
Loss at step 20: 3.0286
Loss at step 30: 3.0507
Loss at step 40: 2.0230
Loss at step 50: 3.0114
Loss at step 60: 2.4965
Loss at step 70: 3.3411
Loss at step 80: 2.9597
Loss at step 90: 2.6949
Loss at step 100: 2.3227
Loss at step 110: 2.3222
Loss at step 120: 2.8708
Loss at step 130: 2.0528
Loss at step 140: 2.1444
Loss at step 150: 3.2734
Loss at step 160: 2.6442
Loss at step 170: 2.2687
Loss at step 180: 2.2320
Loss at step 190: 2.2105
Loss at step 200: 2.0698
Loss at step 210: 2.1675
Loss at step 220: 1.7872
Loss at step 230: 1.9797
Loss at step 240: 2.0943
Loss at step 250: 2.0118
Loss at step 260: 2.6505
Loss at step 270: 2.3630
Loss at step 280: 1.9379
Loss at step 290: 2.3478
Loss at step 300: 2.2015
Loss at step 310: 2.5143
Loss at step 320: 2.0933
Loss at step 330: 2.2205
Loss at step 340: 2.0698
Loss at step 350: 2.2740
Loss at step 360: 3.8632
Loss at step 370: 2.7174
Loss at step 380: 1.7278
Loss at step 390: 2.1104
Loss at step 400: 1.7675
Loss at step 410: 2.5187
Loss at step 420: 1.6117
Loss at step 430: 1.9194
Loss at step 440: 2.0639
Loss at step 450: 1.7926
Loss at step 460: 1.7811
Loss at step 470: 1.9880
Loss at step 480: 1.7085
Loss at step 490: 2.1581
Loss at step 500: 1.9780
Loss at step 510: 2.5745
Loss at step 520: 1.7447
Loss at step 530: 2.7712
Loss at step 540: 2.1701
Loss at step 550: 2.0561
Loss at step 560: 1.6057
Loss at step 570: 3.0227
Loss at step 580: 2.0208
Loss at step 590: 1.8730
Loss at step 600: 1.8811
Loss at step 610: 2.2217
Loss at step 620: 2.0687
Loss at step 630: 2.2921
Loss at step 640: 1.2098
Loss at step 650: 1.6598
Loss at step 660: 2.0412
Loss at step 670: 2.7254
Loss at step 680: 2.0135
Loss at step 690: 2.0905
Loss at step 700: 1.6638
Loss at step 710: 2.1620
Loss at step 720: 1.4975
Loss at step 730: 2.1563
Loss at step 740: 1.6603
Loss at step 750: 2.0495
Loss at step 760: 1.5433
Loss at step 770: 1.3517
Loss at step 780: 1.6285
Loss at step 790: 1.9019
Loss at step 800: 2.0665
Loss at step 810: 1.7811
Loss at step 820: 1.8564
Loss at step 830: 1.5387
Loss at step 840: 2.1812
Loss at step 850: 2.0165
Loss at step 860: 2.0238
Loss at step 870: 2.1484
Loss at step 880: 3.1710
Loss at step 890: 1.4962
Loss at step 900: 1.7042
Loss at step 910: 1.2766
Loss at step 920: 1.6196
Loss at step 930: 1.5930
Loss at step 940: 2.0039
Loss at step 950: 2.0745
Loss at step 960: 1.8011
Loss at step 970: 1.6475
Loss at step 980: 1.9989
Loss at step 990: 2.0319
Loss at step 1000: 1.7245
Loss at step 1010: 2.5372
Loss at step 1020: 1.6469
Loss at step 1030: 1.4695
Loss at step 1040: 2.2855
Loss at step 1050: 1.9263
Loss at step 1060: 2.7220
Loss at step 1070: 1.6237
Loss at step 1080: 1.7527
Loss at step 1090: 2.6368
Loss at step 1100: 1.7732
Loss at step 1110: 1.4563
Loss at step 1120: 2.0694
Loss at step 1130: 1.9997
Loss at step 1140: 2.2740
Loss at step 1150: 1.6296
Loss at step 1160: 1.1990
Loss at step 1170: 2.1187
Loss at step 1180: 1.6465
Loss at step 1190: 1.3308
Loss at step 1200: 1.9909
Loss at step 1210: 1.1352
Loss at step 1220: 2.1923
Loss at step 1230: 1.7434
Loss at step 1240: 1.5311
Loss at step 1250: 1.7071
Loss at step 1260: 1.7272
Loss at step 1270: 2.2312
Loss at step 1280: 1.1989
Loss at step 1290: 1.1168
Loss at step 1300: 1.6912
Loss at step 1310: 1.3946
Loss at step 1320: 1.5447
Loss at step 1330: 1.2242
Loss at step 1340: 1.1871
Loss at step 1350: 2.1432
Loss at step 1360: 1.6954
Loss at step 1370: 2.0253
Loss at step 1380: 1.5661
Loss at step 1390: 1.7950
Loss at step 1400: 1.2654
Loss at step 1410: 1.7757
Loss at step 1420: 1.5351
Loss at step 1430: 2.1693
Loss at step 1440: 1.4150
Loss at step 1450: 1.7165
Loss at step 1460: 1.7259
Loss at step 1470: 1.3871
Loss at step 1480: 2.2188
Loss at step 1490: 1.6337
Loss at step 1500: 1.4813
Loss at step 1510: 1.7682
Loss at step 1520: 1.7759
Loss at step 1530: 1.3641
Loss at step 1540: 1.6285
Loss at step 1550: 1.1967
Loss at step 1560: 1.2455
Loss at step 1570: 1.8732
Loss at step 1580: 0.5463
Loss at step 1590: 1.5303
Loss at step 1600: 1.5355
Loss at step 1610: 1.6489
Loss at step 1620: 1.2062
Loss at step 1630: 1.3800
Loss at step 1640: 2.2401
Loss at step 1650: 1.4031
Loss at step 1660: 1.5085
Loss at step 1670: 1.5876
Loss at step 1680: 1.3991
Loss at step 1690: 1.4505
Loss at step 1700: 1.3241
Loss at step 1710: 1.4054
Loss at step 1720: 1.0456
Loss at step 1730: 1.2762
Loss at step 1740: 1.6944
Loss at step 1750: 1.6124
Loss at step 1760: 1.2375
Loss at step 1770: 1.6443
Loss at step 1780: 1.4169
Loss at step 1790: 1.3995
Loss at step 1800: 1.1191
Loss at step 1810: 1.2833
Loss at step 1820: 1.2655
Loss at step 1830: 1.3055
Loss at step 1840: 1.4610
Loss at step 1850: 1.4557
Loss at step 1860: 0.8959
Loss at step 1870: 1.5048
Loss at step 1880: 1.5554
Loss at step 1890: 1.0876
Loss at step 1900: 1.4695
Loss at step 1910: 1.4259
Loss at step 1920: 1.2190
Loss at step 1930: 2.0891
Loss at step 1940: 1.1421
Loss at step 1950: 1.3422
Loss at step 1960: 1.1186
Loss at step 1970: 1.1541
Loss at step 1980: 1.7287
Loss at step 1990: 1.3274
Loss at step 2000: 1.2984
Loss at step 2010: 1.2743
Loss at step 2020: 1.7430
Loss at step 2030: 1.3339
Loss at step 2040: 1.3901
Loss at step 2050: 0.9149
Loss at step 2060: 1.9680
Loss at step 2070: 1.3664
Loss at step 2080: 1.2528
Loss at step 2090: 1.5368
Loss at step 2100: 1.5079
Loss at step 2110: 1.7404
Loss at step 2120: 1.7148
Loss at step 2130: 0.9005
Loss at step 2140: 1.2889
Loss at step 2150: 0.8175
Loss at step 2160: 1.3997
Loss at step 2170: 1.4644
Loss at step 2180: 1.2687
Loss at step 2190: 1.5089
Loss at step 2200: 1.3264
Loss at step 2210: 2.2946
Loss at step 2220: 0.9039
Loss at step 2230: 1.2713
Loss at step 2240: 1.1563
Loss at step 2250: 1.4631
Loss at step 2260: 0.8434
Loss at step 2270: 1.5648
Loss at step 2280: 1.5017
Loss at step 2290: 1.4839
Loss at step 2300: 1.1674
Loss at step 2310: 1.0291
Loss at step 2320: 1.4925
Loss at step 2330: 1.4770
Loss at step 2340: 1.5836
Loss at step 2350: 1.2847
Loss at step 2360: 0.8376
Loss at step 2370: 1.8182
Loss at step 2380: 0.9767
Loss at step 2390: 1.2528
Loss at step 2400: 1.2076
Loss at step 2410: 0.9756
Loss at step 2420: 1.1121
Loss at step 2430: 0.9114
Loss at step 2440: 0.8669
Loss at step 2450: 1.5767
Loss at step 2460: 1.6159
Loss at step 2470: 1.3509
Loss at step 2480: 2.3788
Loss at step 2490: 0.9826
Loss at step 2500: 1.3351
Loss at step 2510: 1.3110
Loss at step 2520: 1.1118
Loss at step 2530: 2.0247
Loss at step 2540: 1.6638
Loss at step 2550: 0.8642
Loss at step 2560: 1.6238
Loss at step 2570: 0.9153
Loss at step 2580: 1.0991
Loss at step 2590: 2.2145
Loss at step 2600: 1.1787
Loss at step 2610: 1.5396
Loss at step 2620: 2.8322
Loss at step 2630: 0.9077
Loss at step 2640: 1.4424
Loss at step 2650: 1.5866
Loss at step 2660: 0.9749
Loss at step 2670: 0.7528
Loss at step 2680: 0.9037
Loss at step 2690: 1.4581
Loss at step 2700: 1.5026
Loss at step 2710: 1.3233
Loss at step 2720: 1.2280
Loss at step 2730: 0.7861
Loss at step 2740: 1.2982
Loss at step 2750: 0.5874
Loss at step 2760: 0.4648
Loss at step 2770: 1.6246
Loss at step 2780: 2.1836
Loss at step 2790: 1.2780
Loss at step 2800: 1.1079
Loss at step 2810: 0.9303
Loss at step 2820: 1.3576
Loss at step 2830: 1.7143
Loss at step 2840: 1.0076
Loss at step 2850: 0.8521
Loss at step 2860: 1.7095
Loss at step 2870: 1.3343
Loss at step 2880: 1.6383
Loss at step 2890: 1.1149
Loss at step 2900: 1.5734
Loss at step 2910: 1.4318
Loss at step 2920: 0.9647
Loss at step 2930: 0.6700
Loss at step 2940: 1.1817
Loss at step 2950: 1.1965
Loss at step 2960: 0.8823
Loss at step 2970: 0.7072
Loss at step 2980: 1.8289
Loss at step 2990: 1.0228
Loss at step 3000: 0.9335
Loss at step 3010: 0.9602
Loss at step 3020: 0.9592
Loss at step 3030: 1.2438
Loss at step 3040: 1.0806
Loss at step 3050: 1.0223
Loss at step 3060: 1.1504
Loss at step 3070: 0.9143
Loss at step 3080: 0.9505
Loss at step 3090: 0.9855
Loss at step 3100: 0.8000
Loss at step 3110: 0.9800
Loss at step 3120: 0.9357
Loss at step 3130: 1.2102
Loss at step 3140: 1.0640
Loss at step 3150: 0.8092
Loss at step 3160: 1.2349
Loss at step 3170: 1.0836
Loss at step 3180: 1.3663
Loss at step 3190: 1.0306
Loss at step 3200: 0.9690
Loss at step 3210: 1.1426
Loss at step 3220: 1.1980
Loss at step 3230: 1.3171
Loss at step 3240: 0.7042
Loss at step 3250: 0.6336
Loss at step 3260: 0.8534
Loss at step 3270: 0.9124
Loss at step 3280: 1.0477
Loss at step 3290: 0.8838
Loss at step 3300: 0.6858
Loss at step 3310: 1.8933
Loss at step 3320: 0.9079
Loss at step 3330: 0.8008
Loss at step 3340: 0.8888
Loss at step 3350: 1.1130
Loss at step 3360: 1.0453
Loss at step 3370: 0.6135
Loss at step 3380: 0.5786
Loss at step 3390: 0.9185
Loss at step 3400: 0.4128
Loss at step 3410: 1.0049
Loss at step 3420: 0.7730
Loss at step 3430: 1.0710
Loss at step 3440: 0.8570
Loss at step 3450: 0.6974
Loss at step 3460: 0.5385
Loss at step 3470: 1.1110
Loss at step 3480: 1.2363
Loss at step 3490: 1.1167
Loss at step 3500: 0.8187
Loss at step 3510: 1.3428
Loss at step 3520: 0.9757
Loss at step 3530: 1.2457
Loss at step 3540: 1.0172
Loss at step 3550: 0.9348
Loss at step 3560: 1.6124
Loss at step 3570: 0.9716
Loss at step 3580: 1.3064
Loss at step 3590: 0.9510
Loss at step 3600: 1.2242
Loss at step 3610: 0.4710
Loss at step 3620: 1.0346
Loss at step 3630: 1.3759
Loss at step 3640: 0.8663
Loss at step 3650: 1.2125
Loss at step 3660: 0.9135
Loss at step 3670: 0.8502
Loss at step 3680: 1.0631
Loss at step 3690: 1.1008
Loss at step 3700: 0.8419
Loss at step 3710: 0.3518
Loss at step 3720: 0.7302
Loss at step 3730: 0.8622
Loss at step 3740: 1.1198
Loss at step 3750: 1.1433
Loss at step 3760: 1.1459
Loss at step 3770: 0.7750
Loss at step 3780: 0.7147
Loss at step 3790: 0.6379
Loss at step 3800: 1.1792
Loss at step 3810: 1.2754
Loss at step 3820: 0.6970
Loss at step 3830: 1.0756
Loss at step 3840: 0.9950
Loss at step 3850: 0.8024
Loss at step 3860: 0.6511
Loss at step 3870: 0.3797
Loss at step 3880: 0.6740
Loss at step 3890: 1.2720
Loss at step 3900: 0.7529
Loss at step 3910: 1.1618
Loss at step 3920: 1.7661
Loss at step 3930: 0.5008
Loss at step 3940: 1.6311
Loss at step 3950: 1.2079
Loss at step 3960: 1.1352
Loss at step 3970: 0.6806
Loss at step 3980: 0.7391
Loss at step 3990: 1.5283
Loss at step 4000: 0.7465
Loss at step 4010: 0.7016
Loss at step 4020: 0.8148
Loss at step 4030: 0.6606
Loss at step 4040: 0.8543
Loss at step 4050: 0.5352
Loss at step 4060: 0.7279
Loss at step 4070: 0.9009
Loss at step 4080: 0.9447
Loss at step 4090: 1.1815
Loss at step 4100: 0.9679
Loss at step 4110: 0.6079
Loss at step 4120: 0.8635
Loss at step 4130: 0.3263
Loss at step 4140: 1.0996
Loss at step 4150: 0.7386
Loss at step 4160: 0.5653
Loss at step 4170: 0.7495
Loss at step 4180: 0.5444
Loss at step 4190: 0.4174
Loss at step 4200: 0.4432
Loss at step 4210: 0.8549
Loss at step 4220: 0.9777
Loss at step 4230: 1.3696
Loss at step 4240: 0.4248
Loss at step 4250: 0.2831
Loss at step 4260: 0.8890
Loss at step 4270: 0.7604
Loss at step 4280: 0.6778
Loss at step 4290: 0.8438
Loss at step 4300: 0.6237
Loss at step 4310: 0.9766
Loss at step 4320: 1.4199
Loss at step 4330: 0.8493
Loss at step 4340: 0.6750
Loss at step 4350: 0.7463
Loss at step 4360: 0.6547
Loss at step 4370: 1.3395
Loss at step 4380: 0.6064
Loss at step 4390: 0.7622
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.496146, 'precision': [0.506517, 0.443122, 0.678899], 'recall': [0.812295, 0.409369, 0.171893], 'f1': [0.623957, 0.425577, 0.274328]}
{'accuracy': 0.763337, 'precision': 0.678899, 'recall': 0.171893, 'f1': 0.274328, 'WordR': 0.018182}
Loss at step 4400: 0.5128
Loss at step 4410: 0.9971
Loss at step 4420: 0.9827
Loss at step 4430: 0.2628
Loss at step 4440: 0.3005
Loss at step 4450: 0.7398
Loss at step 4460: 1.0437
Loss at step 4470: 0.7003
Loss at step 4480: 0.6285
Loss at step 4490: 0.7837
Loss at step 4500: 0.8455
Loss at step 4510: 0.4378
Loss at step 4520: 0.7484
Loss at step 4530: 1.0811
Loss at step 4540: 0.5669
Loss at step 4550: 0.2006
Loss at step 4560: 1.2012
Loss at step 4570: 1.2676
Loss at step 4580: 1.1552
Loss at step 4590: 0.9434
Loss at step 4600: 0.5072
Loss at step 4610: 0.4051
Loss at step 4620: 0.9557
Loss at step 4630: 0.6687
Loss at step 4640: 0.7095
Loss at step 4650: 0.2206
Loss at step 4660: 1.1198
Loss at step 4670: 0.4927
Loss at step 4680: 0.4972
Loss at step 4690: 1.2058
Loss at step 4700: 0.4241
Loss at step 4710: 0.6264
Loss at step 4720: 0.3539
Loss at step 4730: 0.5727
Loss at step 4740: 0.5708
Loss at step 4750: 0.7699
Loss at step 4760: 0.3027
Loss at step 4770: 0.6717
Loss at step 4780: 0.8003
Loss at step 4790: 1.0750
Loss at step 4800: 0.5834
Loss at step 4810: 0.7262
Loss at step 4820: 0.8972
Loss at step 4830: 0.6585
Loss at step 4840: 0.4039
Loss at step 4850: 0.4264
Loss at step 4860: 0.3388
Loss at step 4870: 0.7861
Loss at step 4880: 0.8655
Loss at step 4890: 0.4932
Loss at step 4900: 0.8183
Loss at step 4910: 0.7383
Loss at step 4920: 0.3815
Loss at step 4930: 0.7710
Loss at step 4940: 0.6004
Loss at step 4950: 0.6756
Loss at step 4960: 0.4016
Loss at step 4970: 0.9747
Loss at step 4980: 0.5863
Loss at step 4990: 1.4435
Loss at step 5000: 0.4333
Loss at step 5010: 0.4298
Loss at step 5020: 0.4901
Loss at step 5030: 0.5541
Loss at step 5040: 1.1802
Loss at step 5050: 0.5804
Loss at step 5060: 0.5721
Loss at step 5070: 0.4788
Loss at step 5080: 1.1296
Loss at step 5090: 0.4708
Loss at step 5100: 0.7174
Loss at step 5110: 1.1367
Loss at step 5120: 0.5240
Loss at step 5130: 0.5849
Loss at step 5140: 1.0031
Loss at step 5150: 0.3102
Loss at step 5160: 1.0022
Loss at step 5170: 0.7277
Loss at step 5180: 0.7357
Loss at step 5190: 1.0779
Loss at step 5200: 0.4866
Loss at step 5210: 1.2913
Loss at step 5220: 0.5032
Loss at step 5230: 0.7903
Loss at step 5240: 0.4029
Loss at step 5250: 0.9029
Loss at step 5260: 1.3389
Loss at step 5270: 0.7390
Loss at step 5280: 0.5554
Loss at step 5290: 0.7491
Loss at step 5300: 0.3827
Loss at step 5310: 1.1824
Loss at step 5320: 1.2612
Loss at step 5330: 0.7201
Loss at step 5340: 0.7081
Loss at step 5350: 0.5368
Loss at step 5360: 0.5737
Loss at step 5370: 0.4503
Loss at step 5380: 0.3981
Loss at step 5390: 0.4692
Loss at step 5400: 0.5313
Loss at step 5410: 1.0174
Loss at step 5420: 0.8693
Loss at step 5430: 0.1758
Loss at step 5440: 0.4116
Loss at step 5450: 0.7321
Loss at step 5460: 0.3632
Loss at step 5470: 0.8881
Loss at step 5480: 0.6009
Loss at step 5490: 0.5656
Loss at step 5500: 0.8993
Loss at step 5510: 0.5714
Loss at step 5520: 0.7474
Loss at step 5530: 0.7023
Loss at step 5540: 0.6288
Loss at step 5550: 2.4687
Loss at step 5560: 0.5451
Loss at step 5570: 1.0445
Loss at step 5580: 0.7049
Loss at step 5590: 0.1757
Loss at step 5600: 0.2885
Loss at step 5610: 0.8371
Loss at step 5620: 0.3542
Loss at step 5630: 0.7226
Loss at step 5640: 0.6609
Loss at step 5650: 0.3211
Loss at step 5660: 0.4722
Loss at step 5670: 0.6838
Loss at step 5680: 0.7893
Loss at step 5690: 0.5342
Loss at step 5700: 0.1549
Loss at step 5710: 0.8841
Loss at step 5720: 0.4788
Loss at step 5730: 0.7506
Loss at step 5740: 0.5345
Loss at step 5750: 0.8168
Loss at step 5760: 1.5523
Loss at step 5770: 0.4179
Loss at step 5780: 0.7757
Loss at step 5790: 0.4628
Loss at step 5800: 0.2712
Loss at step 5810: 0.1556
Loss at step 5820: 0.7830
Loss at step 5830: 0.7523
Loss at step 5840: 0.9425
Loss at step 5850: 0.9882
Loss at step 5860: 0.6920
Loss at step 5870: 0.5010
Loss at step 5880: 0.6000
Loss at step 5890: 0.2283
Loss at step 5900: 1.2160
Loss at step 5910: 0.7231
Loss at step 5920: 0.8309
Loss at step 5930: 0.9233
Loss at step 5940: 0.4933
Loss at step 5950: 0.8973
Loss at step 5960: 0.6881
Loss at step 5970: 0.1424
Loss at step 5980: 0.1732
Loss at step 5990: 0.3627
Loss at step 6000: 0.6349
Loss at step 6010: 0.5769
Loss at step 6020: 0.8347
Loss at step 6030: 0.4569
Loss at step 6040: 0.9763
Loss at step 6050: 1.0535
Loss at step 6060: 0.3946
Loss at step 6070: 0.4615
Loss at step 6080: 0.3528
Loss at step 6090: 0.0997
Loss at step 6100: 0.5278
Loss at step 6110: 0.8146
Loss at step 6120: 0.9895
Loss at step 6130: 0.3861
Loss at step 6140: 0.6912
Loss at step 6150: 1.2057
Loss at step 6160: 0.9415
Loss at step 6170: 0.3427
Loss at step 6180: 0.6790
Loss at step 6190: 0.4414
Loss at step 6200: 0.4329
Loss at step 6210: 0.7149
Loss at step 6220: 0.5467
Loss at step 6230: 0.4204
Loss at step 6240: 0.7200
Loss at step 6250: 0.4123
Loss at step 6260: 0.3588
Loss at step 6270: 0.5974
Loss at step 6280: 0.6947
Loss at step 6290: 0.3415
Loss at step 6300: 0.5048
Loss at step 6310: 0.5165
Loss at step 6320: 0.3816
Loss at step 6330: 0.0429
Loss at step 6340: 0.5540
Loss at step 6350: 0.5317
Loss at step 6360: 0.5212
Loss at step 6370: 0.5170
Loss at step 6380: 0.4447
Loss at step 6390: 0.8122
Loss at step 6400: 0.4051
Loss at step 6410: 0.4287
Loss at step 6420: 0.5041
Loss at step 6430: 0.6789
Loss at step 6440: 0.6742
Loss at step 6450: 0.1453
Loss at step 6460: 0.4255
Loss at step 6470: 0.4154
Loss at step 6480: 0.6891
Loss at step 6490: 0.8009
Loss at step 6500: 0.4120
Loss at step 6510: 0.4939
Loss at step 6520: 0.2126
Loss at step 6530: 0.8344
Loss at step 6540: 0.2824
Loss at step 6550: 1.0060
Loss at step 6560: 0.4551
Loss at step 6570: 1.2296
Loss at step 6580: 0.3911
Loss at step 6590: 0.1311
Loss at step 6600: 0.0738
Loss at step 6610: 0.5544
Loss at step 6620: 0.7554
Loss at step 6630: 0.2404
Loss at step 6640: 0.2198
Loss at step 6650: 0.4128
Loss at step 6660: 0.9722
Loss at step 6670: 0.4601
Loss at step 6680: 0.4143
Loss at step 6690: 0.4816
Loss at step 6700: 1.1414
Loss at step 6710: 0.2320
Loss at step 6720: 0.5750
Loss at step 6730: 0.1100
Loss at step 6740: 0.7641
Loss at step 6750: 0.1192
Loss at step 6760: 0.2440
Loss at step 6770: 0.2667
Loss at step 6780: 0.4387
Loss at step 6790: 0.5028
Loss at step 6800: 0.7177
Loss at step 6810: 1.0557
Loss at step 6820: 0.9564
Loss at step 6830: 0.2948
Loss at step 6840: 0.0914
Loss at step 6850: 0.6058
Loss at step 6860: 1.1690
Loss at step 6870: 0.4855
Loss at step 6880: 0.4146
Loss at step 6890: 0.7861
Loss at step 6900: 0.2730
Loss at step 6910: 0.1490
Loss at step 6920: 0.4280
Loss at step 6930: 0.5309
Loss at step 6940: 0.3935
Loss at step 6950: 0.1905
Loss at step 6960: 0.1989
Loss at step 6970: 0.6182
Loss at step 6980: 0.5602
Loss at step 6990: 0.4663
Loss at step 7000: 0.5148
Loss at step 7010: 0.3856
Loss at step 7020: 1.1334
Loss at step 7030: 0.3969
Loss at step 7040: 0.2341
Loss at step 7050: 0.8208
Loss at step 7060: 2.0817
Loss at step 7070: 0.5921
Loss at step 7080: 0.6074
Loss at step 7090: 0.7782
Loss at step 7100: 0.5932
Loss at step 7110: 1.2235
Loss at step 7120: 0.4332
Loss at step 7130: 0.4356
Loss at step 7140: 0.3574
Loss at step 7150: 0.0821
Loss at step 7160: 0.1859
Loss at step 7170: 0.5752
Loss at step 7180: 0.7767
Loss at step 7190: 0.0795
Loss at step 7200: 0.5540
Loss at step 7210: 0.4717
Loss at step 7220: 0.5999
Loss at step 7230: 0.8494
Loss at step 7240: 0.3302
Loss at step 7250: 0.3138
Loss at step 7260: 0.2383
Loss at step 7270: 0.1858
Loss at step 7280: 0.7687
Loss at step 7290: 0.4843
Loss at step 7300: 0.5684
Loss at step 7310: 1.0373
Loss at step 7320: 0.1967
Loss at step 7330: 0.6829
Loss at step 7340: 1.2865
Loss at step 7350: 0.6118
Loss at step 7360: 0.5121
Loss at step 7370: 0.4499
Loss at step 7380: 0.1394
Loss at step 7390: 1.5679
Loss at step 7400: 0.5088
Loss at step 7410: 0.2991
Loss at step 7420: 0.2922
Loss at step 7430: 0.3533
Loss at step 7440: 0.0225
Loss at step 7450: 1.1445
Loss at step 7460: 0.1460
Loss at step 7470: 0.0899
Loss at step 7480: 0.2450
Loss at step 7490: 0.2952
Loss at step 7500: 0.2218
Loss at step 7510: 0.2435
Loss at step 7520: 0.5085
Loss at step 7530: 0.4464
Loss at step 7540: 0.1012
Loss at step 7550: 0.4550
Loss at step 7560: 1.2663
Loss at step 7570: 1.0183
Loss at step 7580: 0.1711
Loss at step 7590: 0.3836
Loss at step 7600: 0.3021
Loss at step 7610: 0.0923
Loss at step 7620: 0.4180
Loss at step 7630: 0.2758
Loss at step 7640: 0.5759
Loss at step 7650: 0.0145
Loss at step 7660: 0.9227
Loss at step 7670: 0.7442
Loss at step 7680: 0.4397
Loss at step 7690: 0.6085
Loss at step 7700: 0.3785
Loss at step 7710: 0.1782
Loss at step 7720: 0.8775
Loss at step 7730: 0.3431
Loss at step 7740: 0.1024
Loss at step 7750: 0.0379
Loss at step 7760: 0.6380
Loss at step 7770: 0.3004
Loss at step 7780: 0.1319
Loss at step 7790: 0.1353
Loss at step 7800: 0.0502
Loss at step 7810: 1.4176
Loss at step 7820: 0.5859
Loss at step 7830: 0.4491
Loss at step 7840: 0.2200
Loss at step 7850: 0.1308
Loss at step 7860: 0.0698
Loss at step 7870: 0.2030
Loss at step 7880: 0.6296
Loss at step 7890: 0.8254
Loss at step 7900: 0.8596
Loss at step 7910: 1.2267
Loss at step 7920: 0.2343
Loss at step 7930: 0.5683
Loss at step 7940: 0.4356
Loss at step 7950: 0.3049
Loss at step 7960: 0.3359
Loss at step 7970: 0.0485
Loss at step 7980: 0.2898
Loss at step 7990: 0.2720
Loss at step 8000: 0.3348
Loss at step 8010: 0.6855
Loss at step 8020: 0.2261
Loss at step 8030: 0.5705
Loss at step 8040: 0.1152
Loss at step 8050: 0.3531
Loss at step 8060: 0.0224
Loss at step 8070: 0.4067
Loss at step 8080: 0.3413
Loss at step 8090: 0.2287
Loss at step 8100: 0.3778
Loss at step 8110: 0.3528
Loss at step 8120: 0.1339
Loss at step 8130: 0.7311
Loss at step 8140: 0.3037
Loss at step 8150: 0.0788
Loss at step 8160: 0.3993
Loss at step 8170: 0.0871
Loss at step 8180: 0.6018
Loss at step 8190: 0.5317
Loss at step 8200: 0.2816
Loss at step 8210: 0.2087
Loss at step 8220: 0.1576
Loss at step 8230: 0.5788
Loss at step 8240: 0.0713
Loss at step 8250: 0.7130
Loss at step 8260: 0.2463
Loss at step 8270: 0.1783
Loss at step 8280: 0.8200
Loss at step 8290: 0.2974
Loss at step 8300: 0.4856
Loss at step 8310: 0.2220
Loss at step 8320: 1.0868
Loss at step 8330: 0.2244
Loss at step 8340: 0.6319
Loss at step 8350: 0.3358
Loss at step 8360: 0.4929
Loss at step 8370: 0.1467
Loss at step 8380: 0.3139
Loss at step 8390: 0.2520
Loss at step 8400: 0.0499
Loss at step 8410: 0.7227
Loss at step 8420: 0.1428
Loss at step 8430: 1.0839
Loss at step 8440: 0.8198
Loss at step 8450: 0.5665
Loss at step 8460: 0.5752
Loss at step 8470: 0.6880
Loss at step 8480: 0.6080
Loss at step 8490: 0.1873
Loss at step 8500: 0.7966
Loss at step 8510: 0.1987
Loss at step 8520: 0.4563
Loss at step 8530: 0.8419
Loss at step 8540: 0.5963
Loss at step 8550: 0.1249
Loss at step 8560: 0.0307
Loss at step 8570: 1.3438
Loss at step 8580: 0.0897
Loss at step 8590: 0.6723
Loss at step 8600: 0.2839
Loss at step 8610: 0.4559
Loss at step 8620: 0.8300
Loss at step 8630: 0.0877
Loss at step 8640: 0.0823
Loss at step 8650: 0.8571
Loss at step 8660: 0.4254
Loss at step 8670: 0.7911
Loss at step 8680: 0.6969
Loss at step 8690: 1.0234
Loss at step 8700: 0.0905
Loss at step 8710: 0.3551
Loss at step 8720: 0.4474
Loss at step 8730: 0.0490
Loss at step 8740: 0.5232
Loss at step 8750: 0.8767
Loss at step 8760: 0.7302
Loss at step 8770: 0.1247
Loss at step 8780: 0.0979
Loss at step 8790: 0.0036
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.557352, 'precision': [0.602475, 0.509286, 0.56729], 'recall': [0.678279, 0.580855, 0.352497], 'f1': [0.638134, 0.542721, 0.434814]}
{'accuracy': 0.761523, 'precision': 0.56729, 'recall': 0.352497, 'f1': 0.434814, 'WordR': 0.177766}
Loss at step 8800: 0.1219
Loss at step 8810: 0.1891
Loss at step 8820: 0.3150
Loss at step 8830: 0.3569
Loss at step 8840: 0.3241
Loss at step 8850: 0.0468
Loss at step 8860: 0.5830
Loss at step 8870: 0.0591
Loss at step 8880: 0.2057
Loss at step 8890: 0.3623
Loss at step 8900: 0.3678
Loss at step 8910: 0.3206
Loss at step 8920: 0.7433
Loss at step 8930: 0.1689
Loss at step 8940: 0.2800
Loss at step 8950: 0.1574
Loss at step 8960: 1.2883
Loss at step 8970: 0.9511
Loss at step 8980: 0.0211
Loss at step 8990: 0.1799
Loss at step 9000: 0.1695
Loss at step 9010: 0.4889
Loss at step 9020: 0.3367
Loss at step 9030: 0.4236
Loss at step 9040: 0.4108
Loss at step 9050: 0.2369
Loss at step 9060: 0.1396
Loss at step 9070: 0.3127
Loss at step 9080: 0.1948
Loss at step 9090: 0.1117
Loss at step 9100: 0.0274
Loss at step 9110: 0.4808
Loss at step 9120: 0.4902
Loss at step 9130: 0.0748
Loss at step 9140: 0.1074
Loss at step 9150: 0.7710
Loss at step 9160: 0.4135
Loss at step 9170: 0.0552
Loss at step 9180: 0.5701
Loss at step 9190: 0.4718
Loss at step 9200: 0.1389
Loss at step 9210: 0.3190
Loss at step 9220: 0.1188
Loss at step 9230: 0.1669
Loss at step 9240: 0.2077
Loss at step 9250: 0.2162
Loss at step 9260: 0.0781
Loss at step 9270: 0.8145
Loss at step 9280: 0.5938
Loss at step 9290: 0.2443
Loss at step 9300: 0.0381
Loss at step 9310: 0.4693
Loss at step 9320: 0.5470
Loss at step 9330: 0.5499
Loss at step 9340: 1.2587
Loss at step 9350: 0.2768
Loss at step 9360: 0.3012
Loss at step 9370: 0.2756
Loss at step 9380: 0.9690
Loss at step 9390: 1.0844
Loss at step 9400: 0.8626
Loss at step 9410: 0.4809
Loss at step 9420: 0.1394
Loss at step 9430: 0.1470
Loss at step 9440: 0.1493
Loss at step 9450: 0.0816
Loss at step 9460: 0.3379
Loss at step 9470: 0.0484
Loss at step 9480: 0.2186
Loss at step 9490: 0.1100
Loss at step 9500: 0.4956
Loss at step 9510: 0.0016
Loss at step 9520: 0.0496
Loss at step 9530: 0.0291
Loss at step 9540: 0.0402
Loss at step 9550: 0.3332
Loss at step 9560: 0.2228
Loss at step 9570: 0.0396
Loss at step 9580: 0.1154
Loss at step 9590: 0.5335
Loss at step 9600: 0.8406
Loss at step 9610: 0.1834
Loss at step 9620: 0.1128
Loss at step 9630: 0.2783
Loss at step 9640: 0.0865
Loss at step 9650: 0.3266
Loss at step 9660: 0.1374
Loss at step 9670: 0.0276
Loss at step 9680: 0.2014
Loss at step 9690: 0.3038
Loss at step 9700: 1.7478
Loss at step 9710: 0.0042
Loss at step 9720: 0.1444
Loss at step 9730: 0.0392
Loss at step 9740: 0.3726
Loss at step 9750: 0.1765
Loss at step 9760: 0.7086
Loss at step 9770: 0.6207
Loss at step 9780: 0.2305
Loss at step 9790: 0.4795
Loss at step 9800: 0.3009
Loss at step 9810: 0.1218
Loss at step 9820: 0.0101
Loss at step 9830: 0.1061
Loss at step 9840: 0.1614
Loss at step 9850: 0.1674
Loss at step 9860: 0.0357
Loss at step 9870: 0.0742
Loss at step 9880: 0.1733
Loss at step 9890: 0.5685
Loss at step 9900: 0.2761
Loss at step 9910: 0.4197
Loss at step 9920: 0.1014
Loss at step 9930: 0.3441
Loss at step 9940: 0.3496
Loss at step 9950: 0.6227
Loss at step 9960: 0.1472
Loss at step 9970: 1.1462
Loss at step 9980: 0.3668
Loss at step 9990: 0.1247
Loss at step 10000: 0.4793
Loss at step 10010: 0.1950
Loss at step 10020: 0.5921
Loss at step 10030: 0.2589
Loss at step 10040: 0.1935
Loss at step 10050: 0.4907
Loss at step 10060: 0.1483
Loss at step 10070: 0.3051
Loss at step 10080: 0.7570
Loss at step 10090: 0.4102
Loss at step 10100: 0.5151
Loss at step 10110: 0.0666
Loss at step 10120: 0.0537
Loss at step 10130: 0.1534
Loss at step 10140: 0.6297
Loss at step 10150: 1.0067
Loss at step 10160: 0.4512
Loss at step 10170: 0.0286
Loss at step 10180: 1.0821
Loss at step 10190: 1.1625
Loss at step 10200: 0.8438
Loss at step 10210: 0.0210
Loss at step 10220: 0.2224
Loss at step 10230: 0.9891
Loss at step 10240: 0.8390
Loss at step 10250: 0.3340
Loss at step 10260: 0.7489
Loss at step 10270: 0.0055
Loss at step 10280: 0.4699
Loss at step 10290: 0.0316
Loss at step 10300: 0.6348
Loss at step 10310: 0.2171
Loss at step 10320: 0.4595
Loss at step 10330: 0.2907
Loss at step 10340: 0.3592
Loss at step 10350: 0.0060
Loss at step 10360: 0.2619
Loss at step 10370: 0.0981
Loss at step 10380: 0.2943
Loss at step 10390: 0.5978
Loss at step 10400: 0.7381
Loss at step 10410: 0.0527
Loss at step 10420: 0.0173
Loss at step 10430: 0.2183
Loss at step 10440: 0.2546
Loss at step 10450: 0.1243
Loss at step 10460: 0.1044
Loss at step 10470: 0.6584
Loss at step 10480: 0.4056
Loss at step 10490: 0.0983
Loss at step 10500: 0.0875
Loss at step 10510: 0.5523
Loss at step 10520: 0.9273
Loss at step 10530: 0.5029
Loss at step 10540: 0.1184
Loss at step 10550: 0.0003
Loss at step 10560: 0.0194
Loss at step 10570: 0.1402
Loss at step 10580: 1.2122
Loss at step 10590: 0.8022
Loss at step 10600: 0.6605
Loss at step 10610: 0.0854
Loss at step 10620: 0.0106
Loss at step 10630: 0.4101
Loss at step 10640: 0.2147
Loss at step 10650: 0.0419
Loss at step 10660: 0.6791
Loss at step 10670: 0.1444
Loss at step 10680: 0.0233
Loss at step 10690: 0.0134
Loss at step 10700: 0.3134
Loss at step 10710: 1.1465
Loss at step 10720: 0.0370
Loss at step 10730: 0.5184
Loss at step 10740: 0.2390
Loss at step 10750: 0.1449
Loss at step 10760: 0.0024
Loss at step 10770: 0.0094
Loss at step 10780: 0.4366
Loss at step 10790: 0.5790
Loss at step 10800: 0.3453
Loss at step 10810: 0.1317
Loss at step 10820: 0.0367
Loss at step 10830: 0.3006
Loss at step 10840: 0.0011
Loss at step 10850: 0.4516
Loss at step 10860: 0.3309
Loss at step 10870: 0.5136
Loss at step 10880: 0.1552
Loss at step 10890: 0.0563
Loss at step 10900: 0.6145
Loss at step 10910: 0.2225
Loss at step 10920: 0.0987
Loss at step 10930: 0.3354
Loss at step 10940: 0.0941
Loss at step 10950: 0.1213
Loss at step 10960: 0.0272
Loss at step 10970: 0.0732
Loss at step 10980: 0.0040
Loss at step 10990: 0.3036
Loss at step 11000: 0.0214
Loss at step 11010: 0.7953
Loss at step 11020: 0.0983
Loss at step 11030: 0.0215
Loss at step 11040: 0.0047
Loss at step 11050: 1.1849
Loss at step 11060: 0.2596
Loss at step 11070: 0.5106
Loss at step 11080: 0.0011
Loss at step 11090: 0.1372
Loss at step 11100: 0.9398
Loss at step 11110: 0.3582
Loss at step 11120: 0.9873
Loss at step 11130: 0.2433
Loss at step 11140: 0.4355
Loss at step 11150: 0.0536
Loss at step 11160: 0.2631
Loss at step 11170: 0.0010
Loss at step 11180: 0.1175
Loss at step 11190: 0.1245
Loss at step 11200: 0.1598
Loss at step 11210: 0.6667
Loss at step 11220: 0.0504
Loss at step 11230: 0.1265
Loss at step 11240: 0.3639
Loss at step 11250: 0.2086
Loss at step 11260: 0.2987
Loss at step 11270: 1.0847
Loss at step 11280: 0.0018
Loss at step 11290: 0.3653
Loss at step 11300: 0.4367
Loss at step 11310: 0.1491
Loss at step 11320: 0.0317
Loss at step 11330: 0.9522
Loss at step 11340: 0.0400
Loss at step 11350: 0.4926
Loss at step 11360: 0.0022
Loss at step 11370: 0.2470
Loss at step 11380: 0.2263
Loss at step 11390: 0.2728
Loss at step 11400: 0.0012
Loss at step 11410: 0.2990
Loss at step 11420: 0.5383
Loss at step 11430: 0.0004
Loss at step 11440: 1.4258
Loss at step 11450: 0.3146
Loss at step 11460: 0.0036
Loss at step 11470: 0.8212
Loss at step 11480: 0.0075
Loss at step 11490: 0.7753
Loss at step 11500: 0.3396
Loss at step 11510: 1.0107
Loss at step 11520: 0.3358
Loss at step 11530: 1.1080
Loss at step 11540: 1.2511
Loss at step 11550: 1.3960
Loss at step 11560: 0.1806
Loss at step 11570: 0.0345
Loss at step 11580: 0.0121
Loss at step 11590: 0.3042
Loss at step 11600: 0.0363
Loss at step 11610: 0.8374
Loss at step 11620: 0.0000
Loss at step 11630: 0.4903
Loss at step 11640: 0.2751
Loss at step 11650: 0.0743
Loss at step 11660: 0.2424
Loss at step 11670: 0.0004
Loss at step 11680: 0.0185
Loss at step 11690: 1.1342
Loss at step 11700: 0.0196
Loss at step 11710: 0.2259
Loss at step 11720: 0.1034
Loss at step 11730: 0.6659
Loss at step 11740: 0.0082
Loss at step 11750: 0.0009
Loss at step 11760: 1.5577
Loss at step 11770: 0.2567
Loss at step 11780: 0.3487
Loss at step 11790: 0.0053
Loss at step 11800: 0.3430
Loss at step 11810: 0.0042
Loss at step 11820: 0.0006
Loss at step 11830: 0.3321
Loss at step 11840: 1.0494
Loss at step 11850: 0.1405
Loss at step 11860: 0.4888
Loss at step 11870: 0.0784
Loss at step 11880: 0.4662
Loss at step 11890: 0.5779
Loss at step 11900: 0.9531
Loss at step 11910: 0.1115
Loss at step 11920: 0.0372
Loss at step 11930: 0.3870
Loss at step 11940: 0.3987
Loss at step 11950: 0.0308
Loss at step 11960: 0.0730
Loss at step 11970: 0.5841
Loss at step 11980: 0.0645
Loss at step 11990: 0.1460
Loss at step 12000: 0.0008
Loss at step 12010: 0.3317
Loss at step 12020: 0.1166
Loss at step 12030: 0.5442
Loss at step 12040: 0.5208
Loss at step 12050: 0.0025
Loss at step 12060: 0.5093
Loss at step 12070: 0.8324
Loss at step 12080: 0.4728
Loss at step 12090: 0.0544
Loss at step 12100: 0.4640
Loss at step 12110: 0.0326
Loss at step 12120: 0.2072
Loss at step 12130: 0.0382
Loss at step 12140: 0.0902
Loss at step 12150: 0.0185
Loss at step 12160: 0.5654
Loss at step 12170: 0.0875
Loss at step 12180: 0.2645
Loss at step 12190: 0.0194
Loss at step 12200: 0.1073
Loss at step 12210: 0.2785
Loss at step 12220: 0.1830
Loss at step 12230: 0.2503
Loss at step 12240: 0.2164
Loss at step 12250: 0.0857
Loss at step 12260: 0.0124
Loss at step 12270: 0.0558
Loss at step 12280: 0.1386
Loss at step 12290: 0.7187
Loss at step 12300: 0.1253
Loss at step 12310: 0.0421
Loss at step 12320: 0.1010
Loss at step 12330: 0.0052
Loss at step 12340: 1.2824
Loss at step 12350: 0.0140
Loss at step 12360: 0.1045
Loss at step 12370: 0.0258
Loss at step 12380: 0.0013
Loss at step 12390: 0.0492
Loss at step 12400: 0.4080
Loss at step 12410: 0.5674
Loss at step 12420: 0.0299
Loss at step 12430: 0.6470
Loss at step 12440: 0.4636
Loss at step 12450: 0.2983
Loss at step 12460: 0.1208
Loss at step 12470: 0.0519
Loss at step 12480: 0.0791
Loss at step 12490: 0.1609
Loss at step 12500: 0.1963
Loss at step 12510: 0.1558
Loss at step 12520: 0.0714
Loss at step 12530: 0.5778
Loss at step 12540: 0.3723
Loss at step 12550: 0.0167
Loss at step 12560: 0.0221
Loss at step 12570: 0.2341
Loss at step 12580: 0.0576
Loss at step 12590: 0.0128
Loss at step 12600: 0.4653
Loss at step 12610: 0.0387
Loss at step 12620: 0.0005
Loss at step 12630: 0.5245
Loss at step 12640: 0.0085
Loss at step 12650: 0.0008
Loss at step 12660: 0.2300
Loss at step 12670: 0.1015
Loss at step 12680: 0.0240
Loss at step 12690: 0.0002
Loss at step 12700: 0.3100
Loss at step 12710: 0.0028
Loss at step 12720: 0.3257
Loss at step 12730: 0.0215
Loss at step 12740: 0.3131
Loss at step 12750: 0.2527
Loss at step 12760: 0.1573
Loss at step 12770: 0.2348
Loss at step 12780: 0.3073
Loss at step 12790: 0.0087
Loss at step 12800: 0.6457
Loss at step 12810: 0.4207
Loss at step 12820: 0.1656
Loss at step 12830: 0.3256
Loss at step 12840: 0.0008
Loss at step 12850: 0.0934
Loss at step 12860: 0.0594
Loss at step 12870: 0.0145
Loss at step 12880: 0.0102
Loss at step 12890: 0.1298
Loss at step 12900: 0.1409
Loss at step 12910: 0.0091
Loss at step 12920: 0.3847
Loss at step 12930: 0.2777
Loss at step 12940: 0.1463
Loss at step 12950: 0.1255
Loss at step 12960: 0.0272
Loss at step 12970: 0.4534
Loss at step 12980: 0.1322
Loss at step 12990: 0.1430
Loss at step 13000: 0.0057
Loss at step 13010: 0.0424
Loss at step 13020: 0.0394
Loss at step 13030: 0.4689
Loss at step 13040: 1.6876
Loss at step 13050: 0.0209
Loss at step 13060: 0.2413
Loss at step 13070: 0.0074
Loss at step 13080: 0.0185
Loss at step 13090: 0.1841
Loss at step 13100: 0.0011
Loss at step 13110: 2.4825
Loss at step 13120: 0.0630
Loss at step 13130: 0.0393
Loss at step 13140: 0.5397
Loss at step 13150: 0.0439
Loss at step 13160: 0.3320
Loss at step 13170: 0.0123
Loss at step 13180: 0.0027
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.56506, 'precision': [0.642092, 0.511479, 0.549168], 'recall': [0.613934, 0.617108, 0.421603], 'f1': [0.627697, 0.55935, 0.477004]}
{'accuracy': 0.759408, 'precision': 0.549168, 'recall': 0.421603, 'f1': 0.477004, 'WordR': 0.177766}
Loss at step 13190: 0.0541
Loss at step 13200: 0.3612
Loss at step 13210: 0.3825
Loss at step 13220: 0.5426
Loss at step 13230: 0.0001
Loss at step 13240: 0.0004
Loss at step 13250: 0.0503
Loss at step 13260: 0.4180
Loss at step 13270: 0.5877
Loss at step 13280: 0.5823
Loss at step 13290: 0.0008
Loss at step 13300: 2.1715
Loss at step 13310: 0.4785
Loss at step 13320: 0.0179
Loss at step 13330: 0.0007
Loss at step 13340: 0.1515
Loss at step 13350: 0.1817
Loss at step 13360: 0.0002
Loss at step 13370: 0.0024
Loss at step 13380: 0.2814
Loss at step 13390: 0.1770
Loss at step 13400: 0.0586
Loss at step 13410: 0.3644
Loss at step 13420: 1.2948
Loss at step 13430: 0.2674
Loss at step 13440: 0.0002
Loss at step 13450: 0.2020
Loss at step 13460: 0.0399
Loss at step 13470: 0.0003
Loss at step 13480: 0.0747
Loss at step 13490: 0.0768
Loss at step 13500: 0.4113
Loss at step 13510: 0.0505
Loss at step 13520: 0.6958
Loss at step 13530: 0.0437
Loss at step 13540: 0.2079
Loss at step 13550: 0.6756
Loss at step 13560: 0.0172
Loss at step 13570: 0.0015
Loss at step 13580: 0.7333
Loss at step 13590: 0.0001
Loss at step 13600: 0.3296
Loss at step 13610: 0.1852
Loss at step 13620: 0.0918
Loss at step 13630: 0.0189
Loss at step 13640: 0.6781
Loss at step 13650: 1.0955
Loss at step 13660: 1.6200
Loss at step 13670: 0.0036
Loss at step 13680: 0.0302
Loss at step 13690: 0.0152
Loss at step 13700: 0.0216
Loss at step 13710: 0.0015
Loss at step 13720: 0.0156
Loss at step 13730: 0.0000
Loss at step 13740: 0.0126
Loss at step 13750: 0.2159
Loss at step 13760: 0.0008
Loss at step 13770: 0.0002
Loss at step 13780: 0.0001
Loss at step 13790: 0.0021
Loss at step 13800: 0.0001
Loss at step 13810: 0.0021
Loss at step 13820: 0.5583
Loss at step 13830: 0.0661
Loss at step 13840: 0.1626
Loss at step 13850: 0.0001
Loss at step 13860: 1.8083
Loss at step 13870: 0.0110
Loss at step 13880: 0.9538
Loss at step 13890: 0.4038
Loss at step 13900: 0.0088
Loss at step 13910: 0.0169
Loss at step 13920: 0.0004
Loss at step 13930: 0.0033
Loss at step 13940: 0.1506
Loss at step 13950: 0.0472
Loss at step 13960: 0.0019
Loss at step 13970: 0.3414
Loss at step 13980: 0.0000
Loss at step 13990: 0.6019
Loss at step 14000: 0.0002
Loss at step 14010: 0.0981
Loss at step 14020: 0.0413
Loss at step 14030: 0.4749
Loss at step 14040: 0.0223
Loss at step 14050: 0.0094
Loss at step 14060: 0.0007
Loss at step 14070: 0.5116
Loss at step 14080: 0.1063
Loss at step 14090: 0.0032
Loss at step 14100: 0.0002
Loss at step 14110: 0.0194
Loss at step 14120: 0.0291
Loss at step 14130: 0.0943
Loss at step 14140: 0.0007
Loss at step 14150: 0.1405
Loss at step 14160: 0.1958
Loss at step 14170: 0.0023
Loss at step 14180: 0.0005
Loss at step 14190: 0.3549
Loss at step 14200: 0.4882
Loss at step 14210: 0.7290
Loss at step 14220: 0.0063
Loss at step 14230: 1.5755
Loss at step 14240: 0.0292
Loss at step 14250: 0.2553
Loss at step 14260: 0.0005
Loss at step 14270: 0.0162
Loss at step 14280: 0.0000
Loss at step 14290: 0.0008
Loss at step 14300: 0.0132
Loss at step 14310: 0.4606
Loss at step 14320: 0.5367
Loss at step 14330: 0.0011
Loss at step 14340: 0.0038
Loss at step 14350: 0.0002
Loss at step 14360: 0.1126
Loss at step 14370: 2.3252
Loss at step 14380: 0.0605
Loss at step 14390: 0.0791
Loss at step 14400: 0.0000
Loss at step 14410: 5.2247
Loss at step 14420: 0.0278
Loss at step 14430: 0.0575
Loss at step 14440: 0.0166
Loss at step 14450: 0.0370
Loss at step 14460: 0.7233
Loss at step 14470: 0.2349
Loss at step 14480: 0.0008
Loss at step 14490: 0.9086
Loss at step 14500: 0.0206
Loss at step 14510: 0.0014
Loss at step 14520: 0.1483
Loss at step 14530: 0.0012
Loss at step 14540: 0.0909
Loss at step 14550: 0.0001
Loss at step 14560: 0.0174
Loss at step 14570: 0.0920
Loss at step 14580: 0.0038
Loss at step 14590: 0.0299
Loss at step 14600: 0.0586
Loss at step 14610: 0.5353
Loss at step 14620: 0.0134
Loss at step 14630: 0.1851
Loss at step 14640: 0.0000
Loss at step 14650: 0.0569
Loss at step 14660: 0.0692
Loss at step 14670: 0.0001
Loss at step 14680: 0.0011
Loss at step 14690: 0.1286
Loss at step 14700: 0.0052
Loss at step 14710: 0.0147
Loss at step 14720: 0.2648
Loss at step 14730: 0.0281
Loss at step 14740: 0.0249
Loss at step 14750: 1.9246
Loss at step 14760: 0.8269
Loss at step 14770: 1.0354
Loss at step 14780: 1.3435
Loss at step 14790: 0.4379
Loss at step 14800: 0.0635
Loss at step 14810: 0.0083
Loss at step 14820: 0.4209
Loss at step 14830: 0.0261
Loss at step 14840: 0.0572
Loss at step 14850: 1.0524
Loss at step 14860: 0.0040
Loss at step 14870: 0.0002
Loss at step 14880: 0.1540
Loss at step 14890: 0.2188
Loss at step 14900: 0.0008
Loss at step 14910: 0.1401
Loss at step 14920: 0.2459
Loss at step 14930: 0.2970
Loss at step 14940: 0.0059
Loss at step 14950: 0.0000
Loss at step 14960: 0.1490
Loss at step 14970: 0.0224
Loss at step 14980: 0.1714
Loss at step 14990: 0.0779
Loss at step 15000: 0.1278
Loss at step 15010: 0.0389
Loss at step 15020: 0.0003
Loss at step 15030: 0.0027
Loss at step 15040: 0.2681
Loss at step 15050: 0.0501
Loss at step 15060: 0.0281
Loss at step 15070: 0.0089
Loss at step 15080: 0.4486
Loss at step 15090: 0.7060
Loss at step 15100: 0.0335
Loss at step 15110: 0.0021
Loss at step 15120: 0.0313
Loss at step 15130: 0.0034
Loss at step 15140: 0.0004
Loss at step 15150: 0.0004
Loss at step 15160: 0.1939
Loss at step 15170: 0.0027
Loss at step 15180: 0.0263
Loss at step 15190: 0.1438
Loss at step 15200: 0.5215
Loss at step 15210: 0.0967
Loss at step 15220: 0.3697
Loss at step 15230: 0.2151
Loss at step 15240: 0.7320
Loss at step 15250: 0.3374
Loss at step 15260: 0.0046
Loss at step 15270: 0.0166
Loss at step 15280: 0.3446
Loss at step 15290: 0.0725
Loss at step 15300: 0.2097
Loss at step 15310: 0.7681
Loss at step 15320: 0.2935
Loss at step 15330: 0.2897
Loss at step 15340: 0.0034
Loss at step 15350: 0.0001
Loss at step 15360: 0.9947
Loss at step 15370: 0.0006
Loss at step 15380: 0.0001
Loss at step 15390: 0.0227
Loss at step 15400: 0.1842
Loss at step 15410: 0.0183
Loss at step 15420: 0.1340
Loss at step 15430: 0.4926
Loss at step 15440: 0.1063
Loss at step 15450: 0.0137
Loss at step 15460: 0.2905
Loss at step 15470: 0.0004
Loss at step 15480: 0.0711
Loss at step 15490: 0.1606
Loss at step 15500: 0.0006
Loss at step 15510: 0.0001
Loss at step 15520: 0.0002
Loss at step 15530: 1.5074
Loss at step 15540: 0.0019
Loss at step 15550: 0.5053
Loss at step 15560: 0.0000
Loss at step 15570: 0.5349
Loss at step 15580: 0.4953
Loss at step 15590: 0.2645
Loss at step 15600: 0.4621
Loss at step 15610: 0.0021
Loss at step 15620: 0.2421
Loss at step 15630: 0.1977
Loss at step 15640: 0.0039
Loss at step 15650: 0.0029
Loss at step 15660: 1.4018
Loss at step 15670: 0.0371
Loss at step 15680: 0.1337
Loss at step 15690: 0.0182
Loss at step 15700: 0.0001
Loss at step 15710: 0.4214
Loss at step 15720: 0.0001
Loss at step 15730: 0.1546
Loss at step 15740: 0.2330
Loss at step 15750: 0.1893
Loss at step 15760: 0.0354
Loss at step 15770: 0.3923
Loss at step 15780: 0.1541
Loss at step 15790: 0.0017
Loss at step 15800: 0.3565
Loss at step 15810: 0.0992
Loss at step 15820: 0.0000
Loss at step 15830: 0.0296
Loss at step 15840: 0.0673
Loss at step 15850: 0.0380
Loss at step 15860: 1.9541
Loss at step 15870: 0.0015
Loss at step 15880: 0.0080
Loss at step 15890: 0.0000
Loss at step 15900: 0.0010
Loss at step 15910: 0.0509
Loss at step 15920: 0.0242
Loss at step 15930: 0.0027
Loss at step 15940: 0.2237
Loss at step 15950: 0.6386
Loss at step 15960: 0.0000
Loss at step 15970: 0.7503
Loss at step 15980: 0.0001
Loss at step 15990: 0.0731
Loss at step 16000: 0.0004
Loss at step 16010: 0.0351
Loss at step 16020: 0.4791
Loss at step 16030: 0.0005
Loss at step 16040: 0.0024
Loss at step 16050: 0.0001
Loss at step 16060: 0.2649
Loss at step 16070: 0.1122
Loss at step 16080: 0.1815
Loss at step 16090: 0.0836
Loss at step 16100: 0.0246
Loss at step 16110: 0.0000
Loss at step 16120: 0.0005
Loss at step 16130: 0.2260
Loss at step 16140: 0.9984
Loss at step 16150: 0.0043
Loss at step 16160: 0.0000
Loss at step 16170: 0.6228
Loss at step 16180: 0.0116
Loss at step 16190: 0.0389
Loss at step 16200: 0.2100
Loss at step 16210: 0.0001
Loss at step 16220: 0.0003
Loss at step 16230: 0.1811
Loss at step 16240: 0.3317
Loss at step 16250: 0.6397
Loss at step 16260: 0.0078
Loss at step 16270: 0.0004
Loss at step 16280: 0.0000
Loss at step 16290: 0.0006
Loss at step 16300: 1.1965
Loss at step 16310: 0.2392
Loss at step 16320: 0.1344
Loss at step 16330: 0.0001
Loss at step 16340: 0.7378
Loss at step 16350: 0.0008
Loss at step 16360: 0.0587
Loss at step 16370: 0.1225
Loss at step 16380: 0.0000
Loss at step 16390: 0.0076
Loss at step 16400: 0.1056
Loss at step 16410: 0.0148
Loss at step 16420: 0.0019
Loss at step 16430: 0.0002
Loss at step 16440: 0.6846
Loss at step 16450: 0.0124
Loss at step 16460: 0.5960
Loss at step 16470: 0.0014
Loss at step 16480: 0.0001
Loss at step 16490: 0.0054
Loss at step 16500: 0.0033
Loss at step 16510: 0.0004
Loss at step 16520: 0.0072
Loss at step 16530: 0.0820
Loss at step 16540: 0.0009
Loss at step 16550: 0.0000
Loss at step 16560: 0.1750
Loss at step 16570: 0.0004
Loss at step 16580: 0.0001
Loss at step 16590: 0.1086
Loss at step 16600: 0.0090
Loss at step 16610: 0.3486
Loss at step 16620: 0.3634
Loss at step 16630: 0.0064
Loss at step 16640: 0.2062
Loss at step 16650: 0.5947
Loss at step 16660: 0.0102
Loss at step 16670: 1.0005
Loss at step 16680: 0.2223
Loss at step 16690: 0.4294
Loss at step 16700: 0.0655
Loss at step 16710: 0.2865
Loss at step 16720: 0.3304
Loss at step 16730: 0.0000
Loss at step 16740: 0.0019
Loss at step 16750: 1.0734
Loss at step 16760: 0.0057
Loss at step 16770: 0.0087
Loss at step 16780: 0.0004
Loss at step 16790: 0.0125
Loss at step 16800: 0.0025
Loss at step 16810: 0.3233
Loss at step 16820: 0.0172
Loss at step 16830: 0.0065
Loss at step 16840: 0.0002
Loss at step 16850: 0.0464
Loss at step 16860: 0.1681
Loss at step 16870: 0.0005
Loss at step 16880: 0.4723
Loss at step 16890: 0.7334
Loss at step 16900: 0.3236
Loss at step 16910: 0.1118
Loss at step 16920: 0.0001
Loss at step 16930: 0.0191
Loss at step 16940: 0.0040
Loss at step 16950: 0.3978
Loss at step 16960: 0.0069
Loss at step 16970: 0.0000
Loss at step 16980: 0.2199
Loss at step 16990: 0.0373
Loss at step 17000: 0.0022
Loss at step 17010: 0.0003
Loss at step 17020: 0.0622
Loss at step 17030: 0.0133
Loss at step 17040: 0.3754
Loss at step 17050: 0.5833
Loss at step 17060: 0.6371
Loss at step 17070: 0.0515
Loss at step 17080: 0.0003
Loss at step 17090: 0.0001
Loss at step 17100: 0.0012
Loss at step 17110: 0.2322
Loss at step 17120: 0.0348
Loss at step 17130: 0.0000
Loss at step 17140: 0.0019
Loss at step 17150: 0.1409
Loss at step 17160: 0.0001
Loss at step 17170: 0.0797
Loss at step 17180: 0.0018
Loss at step 17190: 0.0020
Loss at step 17200: 0.0000
Loss at step 17210: 0.1600
Loss at step 17220: 0.0276
Loss at step 17230: 0.0017
Loss at step 17240: 0.0123
Loss at step 17250: 0.0141
Loss at step 17260: 0.0001
Loss at step 17270: 0.0001
Loss at step 17280: 0.0222
Loss at step 17290: 0.0002
Loss at step 17300: 0.1985
Loss at step 17310: 0.6688
Loss at step 17320: 0.8666
Loss at step 17330: 0.0986
Loss at step 17340: 0.0393
Loss at step 17350: 0.0032
Loss at step 17360: 0.0001
Loss at step 17370: 0.3659
Loss at step 17380: 0.7393
Loss at step 17390: 0.0161
Loss at step 17400: 1.4278
Loss at step 17410: 0.0000
Loss at step 17420: 0.0002
Loss at step 17430: 0.0006
Loss at step 17440: 0.7204
Loss at step 17450: 0.0046
Loss at step 17460: 0.0127
Loss at step 17470: 0.0117
Loss at step 17480: 0.0764
Loss at step 17490: 0.9448
Loss at step 17500: 0.0001
Loss at step 17510: 0.0031
Loss at step 17520: 0.0141
Loss at step 17530: 0.2260
Loss at step 17540: 0.0326
Loss at step 17550: 0.1814
Loss at step 17560: 0.2159
Loss at step 17570: 0.6860
Loss at step 17580: 0.2310
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.560375, 'precision': [0.643896, 0.507338, 0.534993], 'recall': [0.59877, 0.591446, 0.461672], 'f1': [0.620514, 0.546173, 0.495636]}
{'accuracy': 0.755478, 'precision': 0.534993, 'recall': 0.461672, 'f1': 0.495636, 'WordR': 0.177437}
Loss at step 17590: 0.0001
Loss at step 17600: 0.0004
Loss at step 17610: 0.0011
Loss at step 17620: 0.0005
Loss at step 17630: 0.0003
Loss at step 17640: 0.0452
Loss at step 17650: 0.0004
Loss at step 17660: 0.1063
Loss at step 17670: 0.0007
Loss at step 17680: 0.3401
Loss at step 17690: 0.0610
Loss at step 17700: 0.2192
Loss at step 17710: 0.0123
Loss at step 17720: 0.0209
Loss at step 17730: 0.0011
Loss at step 17740: 0.3057
Loss at step 17750: 0.0004
Loss at step 17760: 0.0000
Loss at step 17770: 0.0000
Loss at step 17780: 0.3184
Loss at step 17790: 0.0030
Loss at step 17800: 0.2993
Loss at step 17810: 0.0018
Loss at step 17820: 0.0002
Loss at step 17830: 0.0004
Loss at step 17840: 0.0000
Loss at step 17850: 0.0001
Loss at step 17860: 0.0038
Loss at step 17870: 0.1247
Loss at step 17880: 0.0000
Loss at step 17890: 0.4783
Loss at step 17900: 0.0104
Loss at step 17910: 0.1143
Loss at step 17920: 0.0001
Loss at step 17930: 0.2950
Loss at step 17940: 0.4325
Loss at step 17950: 0.0008
Loss at step 17960: 0.0000
Loss at step 17970: 0.0010
Loss at step 17980: 0.0000
Loss at step 17990: 0.0000
Loss at step 18000: 0.1906
Loss at step 18010: 0.0000
Loss at step 18020: 0.2912
Loss at step 18030: 0.0020
Loss at step 18040: 0.2424
Loss at step 18050: 0.0445
Loss at step 18060: 0.0222
Loss at step 18070: 0.0061
Loss at step 18080: 0.0043
Loss at step 18090: 0.3971
Loss at step 18100: 0.0001
Loss at step 18110: 0.2095
Loss at step 18120: 0.0033
Loss at step 18130: 0.0039
Loss at step 18140: 0.0163
Loss at step 18150: 0.0021
Loss at step 18160: 0.0000
Loss at step 18170: 0.0009
Loss at step 18180: 0.4721
Loss at step 18190: 0.2625
Loss at step 18200: 0.0010
Loss at step 18210: 0.0001
Loss at step 18220: 0.0176
Loss at step 18230: 0.0011
Loss at step 18240: 0.8180
Loss at step 18250: 0.9494
Loss at step 18260: 0.0004
Loss at step 18270: 0.5502
Loss at step 18280: 0.1643
Loss at step 18290: 0.0001
Loss at step 18300: 0.0957
Loss at step 18310: 0.0485
Loss at step 18320: 0.0007
Loss at step 18330: 0.0032
Loss at step 18340: 0.0132
Loss at step 18350: 0.0000
Loss at step 18360: 0.0168
Loss at step 18370: 0.0001
Loss at step 18380: 0.0074
Loss at step 18390: 0.0001
Loss at step 18400: 0.0000
Loss at step 18410: 0.2370
Loss at step 18420: 1.0871
Loss at step 18430: 0.0021
Loss at step 18440: 0.0763
Loss at step 18450: 0.2234
Loss at step 18460: 0.0000
Loss at step 18470: 0.0315
Loss at step 18480: 0.0060
Loss at step 18490: 0.0000
Loss at step 18500: 0.0578
Loss at step 18510: 0.1137
Loss at step 18520: 0.0000
Loss at step 18530: 0.0006
Loss at step 18540: 0.0010
Loss at step 18550: 0.4327
Loss at step 18560: 0.1388
Loss at step 18570: 0.4240
Loss at step 18580: 0.4378
Loss at step 18590: 0.0006
Loss at step 18600: 0.1476
Loss at step 18610: 0.0000
Loss at step 18620: 0.3669
Loss at step 18630: 0.0000
Loss at step 18640: 0.0000
Loss at step 18650: 0.0907
Loss at step 18660: 0.0004
Loss at step 18670: 0.0440
Loss at step 18680: 0.0469
Loss at step 18690: 1.6124
Loss at step 18700: 0.0002
Loss at step 18710: 0.0001
Loss at step 18720: 0.0000
Loss at step 18730: 0.2623
Loss at step 18740: 0.0003
Loss at step 18750: 0.3532
Loss at step 18760: 0.1669
Loss at step 18770: 0.0011
Loss at step 18780: 0.0001
Loss at step 18790: 0.0147
Loss at step 18800: 0.1726
Loss at step 18810: 0.4484
Loss at step 18820: 0.0000
Loss at step 18830: 0.0000
Loss at step 18840: 0.0650
Loss at step 18850: 0.0095
Loss at step 18860: 0.0001
Loss at step 18870: 0.0000
Loss at step 18880: 0.0000
Loss at step 18890: 0.0000
Loss at step 18900: 0.7142
Loss at step 18910: 0.2641
Loss at step 18920: 0.0001
Loss at step 18930: 0.4826
Loss at step 18940: 0.0000
Loss at step 18950: 0.0002
Loss at step 18960: 0.0180
Loss at step 18970: 0.0000
Loss at step 18980: 0.0020
Loss at step 18990: 0.0001
Loss at step 19000: 0.0597
Loss at step 19010: 0.0000
Loss at step 19020: 0.1004
Loss at step 19030: 0.4586
Loss at step 19040: 0.0001
Loss at step 19050: 0.0019
Loss at step 19060: 1.0073
Loss at step 19070: 0.0024
Loss at step 19080: 0.0243
Loss at step 19090: 0.2827
Loss at step 19100: 0.1668
Loss at step 19110: 0.0044
Loss at step 19120: 0.6596
Loss at step 19130: 0.0244
Loss at step 19140: 0.0170
Loss at step 19150: 0.0004
Loss at step 19160: 0.0000
Loss at step 19170: 0.6459
Loss at step 19180: 0.0000
Loss at step 19190: 0.0159
Loss at step 19200: 0.2277
Loss at step 19210: 0.0002
Loss at step 19220: 0.0090
Loss at step 19230: 0.1642
Loss at step 19240: 0.0169
Loss at step 19250: 0.0004
Loss at step 19260: 0.0000
Loss at step 19270: 0.0007
Loss at step 19280: 0.0000
Loss at step 19290: 0.0000
Loss at step 19300: 0.0566
Loss at step 19310: 0.0001
Loss at step 19320: 0.1764
Loss at step 19330: 0.0009
Loss at step 19340: 0.0000
Loss at step 19350: 0.0000
Loss at step 19360: 0.0000
Loss at step 19370: 0.0007
Loss at step 19380: 0.0001
Loss at step 19390: 0.0005
Loss at step 19400: 0.0000
Loss at step 19410: 0.0051
Loss at step 19420: 0.0002
Loss at step 19430: 0.9480
Loss at step 19440: 0.1906
Loss at step 19450: 0.0004
Loss at step 19460: 0.1671
Loss at step 19470: 0.0003
Loss at step 19480: 0.0077
Loss at step 19490: 0.0000
Loss at step 19500: 0.2492
Loss at step 19510: 0.0000
Loss at step 19520: 0.0000
Loss at step 19530: 0.0000
Loss at step 19540: 0.0000
Loss at step 19550: 0.0038
Loss at step 19560: 0.7768
Loss at step 19570: 0.2121
Loss at step 19580: 0.0001
Loss at step 19590: 1.1124
Loss at step 19600: 0.0000
Loss at step 19610: 0.0406
Loss at step 19620: 0.0014
Loss at step 19630: 0.0000
Loss at step 19640: 0.0002
Loss at step 19650: 0.0744
Loss at step 19660: 0.0000
Loss at step 19670: 0.0053
Loss at step 19680: 0.6677
Loss at step 19690: 0.0000
Loss at step 19700: 0.0301
Loss at step 19710: 0.0001
Loss at step 19720: 0.0129
Loss at step 19730: 0.3219
Loss at step 19740: 0.0000
Loss at step 19750: 0.0000
Loss at step 19760: 0.0000
Loss at step 19770: 0.0001
Loss at step 19780: 0.9671
Loss at step 19790: 0.0000
Loss at step 19800: 0.1034
Loss at step 19810: 0.2437
Loss at step 19820: 0.1138
Loss at step 19830: 0.1353
Loss at step 19840: 0.0000
Loss at step 19850: 0.0001
Loss at step 19860: 0.3226
Loss at step 19870: 0.1189
Loss at step 19880: 0.0002
Loss at step 19890: 0.0002
Loss at step 19900: 0.0007
Loss at step 19910: 0.3645
Loss at step 19920: 0.0000
Loss at step 19930: 0.0008
Loss at step 19940: 0.0169
Loss at step 19950: 0.0000
Loss at step 19960: 0.0001
Loss at step 19970: 0.0008
Loss at step 19980: 0.2899
Loss at step 19990: 0.0024
Loss at step 20000: 0.0256
Loss at step 20010: 0.0537
Loss at step 20020: 0.4011
Loss at step 20030: 0.0000
Loss at step 20040: 0.1355
Loss at step 20050: 0.0003
Loss at step 20060: 0.3421
Loss at step 20070: 0.1059
Loss at step 20080: 0.0002
Loss at step 20090: 0.0000
Loss at step 20100: 0.2450
Loss at step 20110: 0.0001
Loss at step 20120: 0.0009
Loss at step 20130: 0.0001
Loss at step 20140: 0.0023
Loss at step 20150: 0.0000
Loss at step 20160: 0.5461
Loss at step 20170: 0.1615
Loss at step 20180: 0.0003
Loss at step 20190: 0.0000
Loss at step 20200: 0.3199
Loss at step 20210: 0.0001
Loss at step 20220: 0.0459
Loss at step 20230: 0.0000
Loss at step 20240: 0.0000
Loss at step 20250: 0.0001
Loss at step 20260: 0.0000
Loss at step 20270: 0.0012
Loss at step 20280: 0.6239
Loss at step 20290: 0.0047
Loss at step 20300: 0.2206
Loss at step 20310: 0.8555
Loss at step 20320: 0.0000
Loss at step 20330: 0.1682
Loss at step 20340: 0.4674
Loss at step 20350: 0.0021
Loss at step 20360: 0.3957
Loss at step 20370: 0.0001
Loss at step 20380: 0.0006
Loss at step 20390: 0.0982
Loss at step 20400: 0.0416
Loss at step 20410: 0.0111
Loss at step 20420: 0.1491
Loss at step 20430: 0.0001
Loss at step 20440: 0.1332
Loss at step 20450: 0.0002
Loss at step 20460: 0.0001
Loss at step 20470: 0.0012
Loss at step 20480: 0.0110
Loss at step 20490: 0.0021
Loss at step 20500: 0.0000
Loss at step 20510: 0.0005
Loss at step 20520: 0.0001
Loss at step 20530: 0.0051
Loss at step 20540: 0.0002
Loss at step 20550: 2.8179
Loss at step 20560: 0.0002
Loss at step 20570: 0.0000
Loss at step 20580: 0.0000
Loss at step 20590: 0.0000
Loss at step 20600: 0.0000
Loss at step 20610: 0.0007
Loss at step 20620: 0.0004
Loss at step 20630: 0.0000
Loss at step 20640: 0.0017
Loss at step 20650: 0.0003
Loss at step 20660: 0.0000
Loss at step 20670: 0.0010
Loss at step 20680: 0.0000
Loss at step 20690: 0.0043
Loss at step 20700: 0.0001
Loss at step 20710: 0.0000
Loss at step 20720: 0.0007
Loss at step 20730: 0.0013
Loss at step 20740: 0.0001
Loss at step 20750: 0.0017
Loss at step 20760: 0.0007
Loss at step 20770: 0.0001
Loss at step 20780: 0.0000
Loss at step 20790: 0.0026
Loss at step 20800: 0.0379
Loss at step 20810: 0.0237
Loss at step 20820: 0.0636
Loss at step 20830: 0.0001
Loss at step 20840: 0.0000
Loss at step 20850: 0.0001
Loss at step 20860: 0.0001
Loss at step 20870: 0.0265
Loss at step 20880: 0.0119
Loss at step 20890: 0.0000
Loss at step 20900: 0.0008
Loss at step 20910: 0.3044
Loss at step 20920: 0.0001
Loss at step 20930: 0.0000
Loss at step 20940: 0.1484
Loss at step 20950: 0.0008
Loss at step 20960: 0.0000
Loss at step 20970: 0.0000
Loss at step 20980: 0.0003
Loss at step 20990: 0.0001
Loss at step 21000: 0.1744
Loss at step 21010: 0.0071
Loss at step 21020: 0.3628
Loss at step 21030: 0.0000
Loss at step 21040: 0.0012
Loss at step 21050: 0.0000
Loss at step 21060: 0.0339
Loss at step 21070: 0.0001
Loss at step 21080: 0.0004
Loss at step 21090: 0.4957
Loss at step 21100: 0.0077
Loss at step 21110: 0.0002
Loss at step 21120: 0.0015
Loss at step 21130: 0.0291
Loss at step 21140: 0.0000
Loss at step 21150: 0.0002
Loss at step 21160: 0.0000
Loss at step 21170: 0.0000
Loss at step 21180: 0.0000
Loss at step 21190: 0.0001
Loss at step 21200: 0.0004
Loss at step 21210: 0.0026
Loss at step 21220: 0.0002
Loss at step 21230: 0.0001
Loss at step 21240: 0.0004
Loss at step 21250: 0.0459
Loss at step 21260: 0.0004
Loss at step 21270: 0.0000
Loss at step 21280: 0.0000
Loss at step 21290: 0.0000
Loss at step 21300: 0.0077
Loss at step 21310: 0.0000
Loss at step 21320: 0.0006
Loss at step 21330: 0.6616
Loss at step 21340: 0.0000
Loss at step 21350: 0.0001
Loss at step 21360: 0.0017
Loss at step 21370: 0.0003
Loss at step 21380: 0.0047
Loss at step 21390: 0.0000
Loss at step 21400: 0.0000
Loss at step 21410: 0.0002
Loss at step 21420: 0.0142
Loss at step 21430: 0.0421
Loss at step 21440: 0.0000
Loss at step 21450: 0.0000
Loss at step 21460: 0.0001
Loss at step 21470: 0.0000
Loss at step 21480: 0.1940
Loss at step 21490: 0.0007
Loss at step 21500: 0.6043
Loss at step 21510: 0.0000
Loss at step 21520: 0.0001
Loss at step 21530: 0.0000
Loss at step 21540: 0.0000
Loss at step 21550: 0.0000
Loss at step 21560: 0.0000
Loss at step 21570: 0.0001
Loss at step 21580: 0.0010
Loss at step 21590: 0.0001
Loss at step 21600: 0.0001
Loss at step 21610: 0.0003
Loss at step 21620: 0.0000
Loss at step 21630: 0.0016
Loss at step 21640: 0.0019
Loss at step 21650: 0.0000
Loss at step 21660: 0.0000
Loss at step 21670: 0.0013
Loss at step 21680: 0.0000
Loss at step 21690: 0.0084
Loss at step 21700: 0.0000
Loss at step 21710: 0.0079
Loss at step 21720: 0.0000
Loss at step 21730: 0.0002
Loss at step 21740: 0.0127
Loss at step 21750: 0.0469
Loss at step 21760: 0.0000
Loss at step 21770: 0.0043
Loss at step 21780: 0.0000
Loss at step 21790: 0.1813
Loss at step 21800: 0.0244
Loss at step 21810: 0.0000
Loss at step 21820: 0.0000
Loss at step 21830: 0.0000
Loss at step 21840: 0.1190
Loss at step 21850: 0.0001
Loss at step 21860: 0.0002
Loss at step 21870: 0.0000
Loss at step 21880: 0.2454
Loss at step 21890: 0.0007
Loss at step 21900: 0.2233
Loss at step 21910: 0.0061
Loss at step 21920: 0.0000
Loss at step 21930: 0.0001
Loss at step 21940: 0.0000
Loss at step 21950: 0.0001
Loss at step 21960: 0.0469
Loss at step 21970: 0.0024
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.565815, 'precision': [0.676647, 0.500941, 0.555088], 'recall': [0.555738, 0.650509, 0.45935], 'f1': [0.610261, 0.566011, 0.502701]}
{'accuracy': 0.763488, 'precision': 0.555088, 'recall': 0.45935, 'f1': 0.502701, 'WordR': 0.177766}
