Sample 10476 of the training set: {'input_ids': [0, 133, 3097, 3978, 5, 165, 14, 5, 165, 21, 15, 759, 508, 4, 50118, 894, 2002, 5, 165, 7, 422, 15491, 7, 1532, 61, 2433, 58, 5, 144, 505, 4, 50118, 133, 3097, 2327, 14, 5, 695, 21, 164, 88, 10, 319, 9, 430, 4, 50118, 894, 67, 2327, 14, 24, 74, 28, 769, 12, 1902, 2340, 1938, 14, 51, 956, 7, 1477, 819, 15, 99, 51, 222, 45, 14518, 4, 2, 2, 0, 2264, 222, 5, 3097, 206, 59, 5, 26794, 13433, 9, 5, 695, 116, 47385, 2, 0, 44385, 211, 35, 235, 479, 5143, 2156, 24, 630, 75, 173, 479, 407, 2156, 11, 10, 169, 2156, 14, 128, 29, 2156, 47, 216, 2156, 14, 128, 29, 2345, 9, 5, 7353, 631, 16, 14, 190, 2156, 224, 15, 5, 709, 278, 2682, 14, 52, 794, 2156, 5, 2156, 37463, 2156, 5, 1530, 14, 2156, 37463, 2156, 14, 22202, 26591, 21, 562, 77, 8348, 66, 5, 275, 881, 1530, 2156, 25522, 31375, 1536, 9834, 24303, 24, 21, 95, 25522, 417, 32062, 6920, 254, 24303, 47, 216, 2156, 24, 938, 75, 205, 615, 13, 25522, 417, 32062, 6920, 254, 24303, 13, 25522, 44970, 24303, 10, 25522, 417, 32062, 6920, 254, 24303, 10, 25522, 417, 32062, 6920, 254, 24303, 13, 10, 588, 467, 479, 2, 0, 17297, 495, 83, 35, 256, 5471, 479, 256, 119, 111, 1368, 5471, 479, 2, 0, 44385, 211, 35, 370, 25522, 417, 32062, 6920, 254, 24303, 47, 25522, 417, 32062, 6920, 254, 24303, 47, 2156, 25522, 31375, 1536, 9834, 24303, 7252, 407, 2156, 37463, 2156, 52, 202, 33, 2682, 7, 109, 479, 2, 0, 17297, 495, 83, 35, 8976, 479, 2, 0, 44385, 211, 35, 23129, 2156, 8, 2156, 37463, 38, 218, 75, 216, 407, 2156, 546, 23, 5, 414, 2156, 147, 2156, 47, 216, 25522, 417, 32062, 6920, 254, 24303, 99, 128, 29, 5, 25522, 417, 32062, 6920, 254, 24303, 99, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 99, 128, 29, 3553, 99, 128, 29, 26293, 939, 364, 11380, 2156, 38, 206, 14, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 14, 128, 29, 10, 205, 631, 479, 8901, 10, 143, 47, 33, 143, 4312, 59, 99, 1493, 25522, 31375, 1536, 9834, 24303, 1423, 47, 128, 241, 2053, 14, 47, 399, 75, 120, 7, 14, 47, 74, 101, 7, 109, 114, 47, 56, 55, 86, 17487, 23129, 2, 0, 17297, 495, 381, 35, 5534, 2156, 856, 10, 319, 9, 631, 479, 3047, 52, 667, 10, 319, 9, 579, 25522, 44970, 24303, 631, 2156, 8, 52, 630, 75, 173, 2156, 25522, 31375, 1536, 9834, 24303, 52, 3438, 209, 479, 5359, 25522, 31375, 1536, 9834, 24303, 52, 667, 456, 19, 5, 33087, 25911, 1905, 479, 38, 218, 75, 216, 2230, 142, 52, 1381, 25522, 417, 32062, 6920, 254, 24303, 52, 25522, 417, 32062, 6920, 254, 24303, 103, 25522, 417, 32062, 6920, 254, 24303, 65, 9280, 14, 630, 75, 173, 479, 12698, 2156, 18774, 24, 2156, 402, 25522, 44970, 24303, 38, 218, 75, 216, 2230, 2, 0, 44385, 211, 35, 256, 119, 111, 1368, 5471, 479, 2, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-1, 0, 1, 5]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 5
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 2
  Gradient Accumulation steps = 1
  Total optimization steps = 27375
Loss at step 10: 4.3558
Loss at step 20: 2.8760
Loss at step 30: 3.3445
Loss at step 40: 2.4118
Loss at step 50: 6.9797
Loss at step 60: 4.4306
Loss at step 70: 1.0993
Loss at step 80: 3.2705
Loss at step 90: 3.3352
Loss at step 100: 3.1517
Loss at step 110: 1.6961
Loss at step 120: 2.1319
Loss at step 130: 1.8368
Loss at step 140: 2.4337
Loss at step 150: 2.4770
Loss at step 160: 8.9474
Loss at step 170: 3.2543
Loss at step 180: 4.9638
Loss at step 190: 1.7891
Loss at step 200: 2.5141
Loss at step 210: 2.2573
Loss at step 220: 4.4822
Loss at step 230: 2.3105
Loss at step 240: 2.2284
Loss at step 250: 3.0076
Loss at step 260: 1.2925
Loss at step 270: 1.8327
Loss at step 280: 2.4181
Loss at step 290: 1.6880
Loss at step 300: 2.4188
Loss at step 310: 1.5726
Loss at step 320: 2.7714
Loss at step 330: 2.3814
Loss at step 340: 4.9772
Loss at step 350: 1.5172
Loss at step 360: 1.0879
Loss at step 370: 2.1983
Loss at step 380: 2.2918
Loss at step 390: 5.5735
Loss at step 400: 0.9849
Loss at step 410: 3.6017
Loss at step 420: 2.1131
Loss at step 430: 1.9471
Loss at step 440: 2.5484
Loss at step 450: 2.6167
Loss at step 460: 4.0042
Loss at step 470: 1.9663
Loss at step 480: 1.3972
Loss at step 490: 1.8589
Loss at step 500: 1.3851
Loss at step 510: 1.7264
Loss at step 520: 2.4833
Loss at step 530: 1.5186
Loss at step 540: 3.1307
Loss at step 550: 2.8683
Loss at step 560: 2.1389
Loss at step 570: 2.6687
Loss at step 580: 2.4300
Loss at step 590: 2.1607
Loss at step 600: 1.7922
Loss at step 610: 2.4542
Loss at step 620: 1.6480
Loss at step 630: 1.8492
Loss at step 640: 1.7644
Loss at step 650: 2.2756
Loss at step 660: 2.4048
Loss at step 670: 2.6683
Loss at step 680: 2.3717
Loss at step 690: 1.7273
Loss at step 700: 1.8463
Loss at step 710: 2.0726
Loss at step 720: 3.1109
Loss at step 730: 3.1928
Loss at step 740: 1.6329
Loss at step 750: 2.3159
Loss at step 760: 2.1939
Loss at step 770: 1.6414
Loss at step 780: 3.5227
Loss at step 790: 0.8830
Loss at step 800: 1.7891
Loss at step 810: 2.3741
Loss at step 820: 1.6334
Loss at step 830: 1.6070
Loss at step 840: 1.7373
Loss at step 850: 2.1545
Loss at step 860: 2.4234
Loss at step 870: 2.4230
Loss at step 880: 0.8353
Loss at step 890: 2.2546
Loss at step 900: 1.8253
Loss at step 910: 1.8430
Loss at step 920: 0.8294
Loss at step 930: 2.4933
Loss at step 940: 2.2055
Loss at step 950: 2.6955
Loss at step 960: 2.5146
Loss at step 970: 1.5785
Loss at step 980: 2.7024
Loss at step 990: 2.7718
Loss at step 1000: 2.6290
Loss at step 1010: 1.0223
Loss at step 1020: 1.7017
Loss at step 1030: 2.3796
Loss at step 1040: 1.9366
Loss at step 1050: 1.8885
Loss at step 1060: 2.1527
Loss at step 1070: 2.8561
Loss at step 1080: 3.3044
Loss at step 1090: 1.5793
Loss at step 1100: 1.6051
Loss at step 1110: 2.1774
Loss at step 1120: 1.4395
Loss at step 1130: 1.7764
Loss at step 1140: 1.5597
Loss at step 1150: 2.5299
Loss at step 1160: 1.7952
Loss at step 1170: 1.6604
Loss at step 1180: 3.6401
Loss at step 1190: 1.9778
Loss at step 1200: 1.8072
Loss at step 1210: 2.7403
Loss at step 1220: 1.2407
Loss at step 1230: 1.5920
Loss at step 1240: 1.2938
Loss at step 1250: 1.1055
Loss at step 1260: 2.1225
Loss at step 1270: 2.0290
Loss at step 1280: 2.5617
Loss at step 1290: 2.2069
Loss at step 1300: 1.3980
Loss at step 1310: 2.4582
Loss at step 1320: 1.4973
Loss at step 1330: 1.2473
Loss at step 1340: 0.4441
Loss at step 1350: 2.4931
Loss at step 1360: 2.2589
Loss at step 1370: 1.0147
Loss at step 1380: 1.1721
Loss at step 1390: 1.7744
Loss at step 1400: 0.8624
Loss at step 1410: 1.5695
Loss at step 1420: 1.2830
Loss at step 1430: 1.7781
Loss at step 1440: 1.8624
Loss at step 1450: 1.6863
Loss at step 1460: 1.1161
Loss at step 1470: 1.2319
Loss at step 1480: 2.0554
Loss at step 1490: 3.0708
Loss at step 1500: 1.7058
Loss at step 1510: 2.0619
Loss at step 1520: 1.4203
Loss at step 1530: 2.1376
Loss at step 1540: 1.9728
Loss at step 1550: 2.0995
Loss at step 1560: 1.7208
Loss at step 1570: 0.9752
Loss at step 1580: 1.5865
Loss at step 1590: 2.6294
Loss at step 1600: 1.1610
Loss at step 1610: 2.0488
Loss at step 1620: 1.1671
Loss at step 1630: 1.2242
Loss at step 1640: 1.8211
Loss at step 1650: 1.9190
Loss at step 1660: 1.0765
Loss at step 1670: 2.0099
Loss at step 1680: 1.8778
Loss at step 1690: 1.3194
Loss at step 1700: 1.6362
Loss at step 1710: 1.9016
Loss at step 1720: 1.7404
Loss at step 1730: 0.9720
Loss at step 1740: 0.9105
Loss at step 1750: 2.5471
Loss at step 1760: 1.6974
Loss at step 1770: 1.6791
Loss at step 1780: 1.4489
Loss at step 1790: 2.0509
Loss at step 1800: 1.5524
Loss at step 1810: 1.0510
Loss at step 1820: 3.1208
Loss at step 1830: 1.4812
Loss at step 1840: 2.6178
Loss at step 1850: 1.7604
Loss at step 1860: 1.7370
Loss at step 1870: 2.2834
Loss at step 1880: 1.1893
Loss at step 1890: 2.3943
Loss at step 1900: 0.6786
Loss at step 1910: 1.0462
Loss at step 1920: 2.5612
Loss at step 1930: 1.4146
Loss at step 1940: 1.4023
Loss at step 1950: 1.4016
Loss at step 1960: 2.6642
Loss at step 1970: 2.0896
Loss at step 1980: 1.6426
Loss at step 1990: 2.0156
Loss at step 2000: 1.8215
Loss at step 2010: 3.8451
Loss at step 2020: 1.6439
Loss at step 2030: 1.9300
Loss at step 2040: 2.3230
Loss at step 2050: 1.9989
Loss at step 2060: 2.3492
Loss at step 2070: 2.0169
Loss at step 2080: 1.4688
Loss at step 2090: 1.3198
Loss at step 2100: 1.4661
Loss at step 2110: 1.4768
Loss at step 2120: 2.2716
Loss at step 2130: 0.2458
Loss at step 2140: 2.9491
Loss at step 2150: 1.6645
Loss at step 2160: 1.4639
Loss at step 2170: 1.4306
Loss at step 2180: 1.6701
Loss at step 2190: 0.9981
Loss at step 2200: 1.1163
Loss at step 2210: 0.5795
Loss at step 2220: 1.6347
Loss at step 2230: 1.4380
Loss at step 2240: 1.7232
Loss at step 2250: 1.5688
Loss at step 2260: 1.2374
Loss at step 2270: 1.2556
Loss at step 2280: 1.7172
Loss at step 2290: 1.0145
Loss at step 2300: 1.7715
Loss at step 2310: 0.7387
Loss at step 2320: 1.3543
Loss at step 2330: 1.7663
Loss at step 2340: 1.0922
Loss at step 2350: 1.0412
Loss at step 2360: 1.6338
Loss at step 2370: 1.2846
Loss at step 2380: 1.4608
Loss at step 2390: 1.3333
Loss at step 2400: 2.5522
Loss at step 2410: 2.2713
Loss at step 2420: 1.2201
Loss at step 2430: 1.8531
Loss at step 2440: 0.8215
Loss at step 2450: 1.4818
Loss at step 2460: 1.1239
Loss at step 2470: 1.8036
Loss at step 2480: 1.4627
Loss at step 2490: 1.7700
Loss at step 2500: 2.0320
Loss at step 2510: 0.9214
Loss at step 2520: 1.4656
Loss at step 2530: 1.5904
Loss at step 2540: 1.0586
Loss at step 2550: 0.8314
Loss at step 2560: 1.9225
Loss at step 2570: 0.9181
Loss at step 2580: 1.9338
Loss at step 2590: 0.9666
Loss at step 2600: 1.0144
Loss at step 2610: 0.8703
Loss at step 2620: 0.9186
Loss at step 2630: 1.3417
Loss at step 2640: 1.5689
Loss at step 2650: 0.5972
Loss at step 2660: 1.2439
Loss at step 2670: 2.0284
Loss at step 2680: 1.8534
Loss at step 2690: 1.2238
Loss at step 2700: 1.2915
Loss at step 2710: 1.8601
Loss at step 2720: 0.9575
Loss at step 2730: 0.7860
Loss at step 2740: 0.7653
Loss at step 2750: 1.4557
Loss at step 2760: 1.6065
Loss at step 2770: 0.6943
Loss at step 2780: 2.3510
Loss at step 2790: 1.2952
Loss at step 2800: 0.8608
Loss at step 2810: 1.2722
Loss at step 2820: 0.8620
Loss at step 2830: 1.5140
Loss at step 2840: 3.2523
Loss at step 2850: 0.9654
Loss at step 2860: 1.5063
Loss at step 2870: 1.7595
Loss at step 2880: 0.9819
Loss at step 2890: 2.0697
Loss at step 2900: 2.0086
Loss at step 2910: 1.4204
Loss at step 2920: 1.2420
Loss at step 2930: 1.5461
Loss at step 2940: 1.2241
Loss at step 2950: 1.1336
Loss at step 2960: 1.1154
Loss at step 2970: 1.3980
Loss at step 2980: 0.9320
Loss at step 2990: 1.5270
Loss at step 3000: 1.5861
Loss at step 3010: 1.9389
Loss at step 3020: 1.8117
Loss at step 3030: 1.3427
Loss at step 3040: 0.7499
Loss at step 3050: 2.0977
Loss at step 3060: 1.0639
Loss at step 3070: 1.4891
Loss at step 3080: 1.2686
Loss at step 3090: 1.2319
Loss at step 3100: 2.2833
Loss at step 3110: 0.9507
Loss at step 3120: 1.5970
Loss at step 3130: 0.5191
Loss at step 3140: 1.0385
Loss at step 3150: 0.1885
Loss at step 3160: 2.1652
Loss at step 3170: 1.6310
Loss at step 3180: 2.0094
Loss at step 3190: 1.3449
Loss at step 3200: 1.8933
Loss at step 3210: 1.0740
Loss at step 3220: 1.7842
Loss at step 3230: 1.0144
Loss at step 3240: 1.1167
Loss at step 3250: 2.5867
Loss at step 3260: 1.2438
Loss at step 3270: 2.2563
Loss at step 3280: 1.7056
Loss at step 3290: 0.9578
Loss at step 3300: 0.9187
Loss at step 3310: 0.4840
Loss at step 3320: 1.2988
Loss at step 3330: 1.4961
Loss at step 3340: 2.0070
Loss at step 3350: 1.0892
Loss at step 3360: 1.2059
Loss at step 3370: 0.6108
Loss at step 3380: 1.4018
Loss at step 3390: 0.6818
Loss at step 3400: 1.6669
Loss at step 3410: 1.3455
Loss at step 3420: 1.2814
Loss at step 3430: 1.2674
Loss at step 3440: 1.7510
Loss at step 3450: 1.5106
Loss at step 3460: 0.6996
Loss at step 3470: 0.7265
Loss at step 3480: 1.0450
Loss at step 3490: 1.7853
Loss at step 3500: 1.0164
Loss at step 3510: 1.6564
Loss at step 3520: 1.5626
Loss at step 3530: 1.2265
Loss at step 3540: 0.8858
Loss at step 3550: 1.2289
Loss at step 3560: 1.4466
Loss at step 3570: 1.6267
Loss at step 3580: 0.6376
Loss at step 3590: 1.8330
Loss at step 3600: 1.1885
Loss at step 3610: 1.2761
Loss at step 3620: 1.2969
Loss at step 3630: 1.7593
Loss at step 3640: 0.8679
Loss at step 3650: 1.3885
Loss at step 3660: 1.2381
Loss at step 3670: 1.2915
Loss at step 3680: 1.0892
Loss at step 3690: 1.0086
Loss at step 3700: 0.7823
Loss at step 3710: 1.0409
Loss at step 3720: 0.7746
Loss at step 3730: 2.0562
Loss at step 3740: 1.0589
Loss at step 3750: 1.3726
Loss at step 3760: 1.4908
Loss at step 3770: 0.7717
Loss at step 3780: 0.6036
Loss at step 3790: 0.7656
Loss at step 3800: 2.6408
Loss at step 3810: 0.5060
Loss at step 3820: 1.7107
Loss at step 3830: 1.1700
Loss at step 3840: 0.8977
Loss at step 3850: 1.4354
Loss at step 3860: 0.4084
Loss at step 3870: 0.9068
Loss at step 3880: 1.1136
Loss at step 3890: 1.1953
Loss at step 3900: 1.4457
Loss at step 3910: 1.1123
Loss at step 3920: 1.5631
Loss at step 3930: 1.6362
Loss at step 3940: 0.5838
Loss at step 3950: 1.6461
Loss at step 3960: 1.4358
Loss at step 3970: 1.6026
Loss at step 3980: 1.3805
Loss at step 3990: 0.8699
Loss at step 4000: 0.5409
Loss at step 4010: 0.9185
Loss at step 4020: 0.7726
Loss at step 4030: 0.6038
Loss at step 4040: 1.0288
Loss at step 4050: 1.3628
Loss at step 4060: 1.1997
Loss at step 4070: 1.3482
Loss at step 4080: 1.3612
Loss at step 4090: 2.1750
Loss at step 4100: 0.9617
Loss at step 4110: 1.3677
Loss at step 4120: 2.4386
Loss at step 4130: 2.2050
Loss at step 4140: 0.9339
Loss at step 4150: 0.5382
Loss at step 4160: 1.5338
Loss at step 4170: 1.0250
Loss at step 4180: 1.3792
Loss at step 4190: 1.7133
Loss at step 4200: 0.5532
Loss at step 4210: 0.6086
Loss at step 4220: 0.9662
Loss at step 4230: 0.2013
Loss at step 4240: 1.7706
Loss at step 4250: 1.7741
Loss at step 4260: 1.2897
Loss at step 4270: 1.3005
Loss at step 4280: 1.3051
Loss at step 4290: 0.5537
Loss at step 4300: 1.0183
Loss at step 4310: 1.0127
Loss at step 4320: 1.1472
Loss at step 4330: 1.0009
Loss at step 4340: 1.3469
Loss at step 4350: 1.0635
Loss at step 4360: 1.0786
Loss at step 4370: 0.6817
Loss at step 4380: 0.4625
Loss at step 4390: 1.5993
Loss at step 4400: 1.1187
Loss at step 4410: 1.2499
Loss at step 4420: 0.9967
Loss at step 4430: 0.3427
Loss at step 4440: 2.1479
Loss at step 4450: 1.6529
Loss at step 4460: 0.9792
Loss at step 4470: 1.3349
Loss at step 4480: 0.5786
Loss at step 4490: 0.5379
Loss at step 4500: 1.3867
Loss at step 4510: 0.4214
Loss at step 4520: 0.8177
Loss at step 4530: 1.5035
Loss at step 4540: 0.8624
Loss at step 4550: 1.1918
Loss at step 4560: 1.1705
Loss at step 4570: 1.0440
Loss at step 4580: 0.7890
Loss at step 4590: 0.7698
Loss at step 4600: 2.1952
Loss at step 4610: 1.6148
Loss at step 4620: 1.3295
Loss at step 4630: 1.0044
Loss at step 4640: 1.6336
Loss at step 4650: 0.6137
Loss at step 4660: 1.1183
Loss at step 4670: 0.4558
Loss at step 4680: 0.4692
Loss at step 4690: 0.5180
Loss at step 4700: 0.7451
Loss at step 4710: 2.7185
Loss at step 4720: 0.8355
Loss at step 4730: 1.1946
Loss at step 4740: 2.7188
Loss at step 4750: 0.9911
Loss at step 4760: 1.6559
Loss at step 4770: 1.0818
Loss at step 4780: 1.0662
Loss at step 4790: 1.0895
Loss at step 4800: 1.0369
Loss at step 4810: 0.6982
Loss at step 4820: 0.3061
Loss at step 4830: 0.1363
Loss at step 4840: 0.5571
Loss at step 4850: 1.6446
Loss at step 4860: 1.3651
Loss at step 4870: 1.7026
Loss at step 4880: 1.0956
Loss at step 4890: 2.0860
Loss at step 4900: 1.4075
Loss at step 4910: 0.6278
Loss at step 4920: 1.6647
Loss at step 4930: 0.8795
Loss at step 4940: 0.9524
Loss at step 4950: 0.8627
Loss at step 4960: 0.6301
Loss at step 4970: 1.1308
Loss at step 4980: 1.3571
Loss at step 4990: 1.0590
Loss at step 5000: 0.7127
Loss at step 5010: 1.0403
Loss at step 5020: 0.7343
Loss at step 5030: 0.8191
Loss at step 5040: 1.2152
Loss at step 5050: 1.6381
Loss at step 5060: 1.5409
Loss at step 5070: 0.9538
Loss at step 5080: 0.9030
Loss at step 5090: 1.0289
Loss at step 5100: 1.1539
Loss at step 5110: 0.5922
Loss at step 5120: 0.5985
Loss at step 5130: 1.1758
Loss at step 5140: 1.2991
Loss at step 5150: 0.7345
Loss at step 5160: 0.6774
Loss at step 5170: 0.4606
Loss at step 5180: 1.0195
Loss at step 5190: 0.6504
Loss at step 5200: 2.4482
Loss at step 5210: 0.7486
Loss at step 5220: 1.3181
Loss at step 5230: 0.7282
Loss at step 5240: 0.9886
Loss at step 5250: 1.5345
Loss at step 5260: 1.5288
Loss at step 5270: 0.7396
Loss at step 5280: 1.0502
Loss at step 5290: 1.0758
Loss at step 5300: 0.5598
Loss at step 5310: 0.8694
Loss at step 5320: 0.6689
Loss at step 5330: 1.1063
Loss at step 5340: 1.3376
Loss at step 5350: 1.1775
Loss at step 5360: 1.0038
Loss at step 5370: 1.6625
Loss at step 5380: 1.2737
Loss at step 5390: 2.0374
Loss at step 5400: 0.9543
Loss at step 5410: 0.4553
Loss at step 5420: 1.0911
Loss at step 5430: 1.3126
Loss at step 5440: 1.8305
Loss at step 5450: 1.2155
Loss at step 5460: 1.1761
Loss at step 5470: 1.3165
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.704119, 'precision': [0.862241, 0.382448, 0.590515], 'recall': [0.828289, 0.556709, 0.355651], 'f1': [0.844924, 0.453411, 0.443933]}
{'accuracy': 0.86345, 'precision': 0.590515, 'recall': 0.355651, 'f1': 0.443933, 'WordR': 0.130783}
Loss at step 5480: 0.9854
Loss at step 5490: 1.0775
Loss at step 5500: 0.4809
Loss at step 5510: 0.7998
Loss at step 5520: 0.0372
Loss at step 5530: 0.8568
Loss at step 5540: 0.7169
Loss at step 5550: 0.3917
Loss at step 5560: 0.4277
Loss at step 5570: 1.1735
Loss at step 5580: 0.8655
Loss at step 5590: 1.3775
Loss at step 5600: 1.2116
Loss at step 5610: 0.9385
Loss at step 5620: 2.0651
Loss at step 5630: 1.5391
Loss at step 5640: 1.1258
Loss at step 5650: 1.0549
Loss at step 5660: 0.9577
Loss at step 5670: 1.1107
Loss at step 5680: 0.5226
Loss at step 5690: 0.5442
Loss at step 5700: 1.0552
Loss at step 5710: 0.4310
Loss at step 5720: 1.0790
Loss at step 5730: 1.5608
Loss at step 5740: 0.7269
Loss at step 5750: 0.3065
Loss at step 5760: 1.1775
Loss at step 5770: 0.9461
Loss at step 5780: 1.1722
Loss at step 5790: 1.2270
Loss at step 5800: 1.0422
Loss at step 5810: 1.1395
Loss at step 5820: 0.2910
Loss at step 5830: 1.2333
Loss at step 5840: 0.4158
Loss at step 5850: 0.2449
Loss at step 5860: 0.7791
Loss at step 5870: 0.3354
Loss at step 5880: 0.8822
Loss at step 5890: 0.4156
Loss at step 5900: 1.0236
Loss at step 5910: 0.5001
Loss at step 5920: 1.2216
Loss at step 5930: 0.8091
Loss at step 5940: 0.4745
Loss at step 5950: 0.8174
Loss at step 5960: 0.5118
Loss at step 5970: 1.8647
Loss at step 5980: 1.1710
Loss at step 5990: 0.1601
Loss at step 6000: 1.0564
Loss at step 6010: 1.5041
Loss at step 6020: 1.1398
Loss at step 6030: 0.4692
Loss at step 6040: 0.8555
Loss at step 6050: 0.6913
Loss at step 6060: 0.5077
Loss at step 6070: 0.7147
Loss at step 6080: 0.2604
Loss at step 6090: 1.5086
Loss at step 6100: 0.5782
Loss at step 6110: 0.6865
Loss at step 6120: 0.4521
Loss at step 6130: 0.6904
Loss at step 6140: 0.4605
Loss at step 6150: 0.3836
Loss at step 6160: 1.4212
Loss at step 6170: 0.7776
Loss at step 6180: 1.1606
Loss at step 6190: 0.5692
Loss at step 6200: 0.7010
Loss at step 6210: 1.3238
Loss at step 6220: 0.9379
Loss at step 6230: 1.2664
Loss at step 6240: 0.6644
Loss at step 6250: 0.6551
Loss at step 6260: 1.3029
Loss at step 6270: 1.1095
Loss at step 6280: 0.7375
Loss at step 6290: 1.0115
Loss at step 6300: 0.8241
Loss at step 6310: 0.2761
Loss at step 6320: 0.1340
Loss at step 6330: 1.1395
Loss at step 6340: 1.4563
Loss at step 6350: 0.5147
Loss at step 6360: 0.8624
Loss at step 6370: 0.6283
Loss at step 6380: 1.5444
Loss at step 6390: 0.6005
Loss at step 6400: 0.3514
Loss at step 6410: 1.0605
Loss at step 6420: 1.1428
Loss at step 6430: 0.9879
Loss at step 6440: 1.0137
Loss at step 6450: 1.4330
Loss at step 6460: 0.7926
Loss at step 6470: 0.7194
Loss at step 6480: 1.5902
Loss at step 6490: 1.4086
Loss at step 6500: 1.0203
Loss at step 6510: 0.8336
Loss at step 6520: 0.9812
Loss at step 6530: 0.5135
Loss at step 6540: 1.1188
Loss at step 6550: 0.7837
Loss at step 6560: 0.9067
Loss at step 6570: 0.7203
Loss at step 6580: 1.7287
Loss at step 6590: 0.7979
Loss at step 6600: 0.8236
Loss at step 6610: 1.6138
Loss at step 6620: 0.9085
Loss at step 6630: 0.2607
Loss at step 6640: 0.9293
Loss at step 6650: 1.1632
Loss at step 6660: 0.7886
Loss at step 6670: 0.7025
Loss at step 6680: 1.1761
Loss at step 6690: 0.9908
Loss at step 6700: 0.8567
Loss at step 6710: 0.6563
Loss at step 6720: 0.9693
Loss at step 6730: 0.2524
Loss at step 6740: 0.7578
Loss at step 6750: 0.9223
Loss at step 6760: 0.8918
Loss at step 6770: 0.3476
Loss at step 6780: 1.1921
Loss at step 6790: 0.4487
Loss at step 6800: 0.9430
Loss at step 6810: 1.0275
Loss at step 6820: 0.7072
Loss at step 6830: 0.6359
Loss at step 6840: 0.7147
Loss at step 6850: 1.1576
Loss at step 6860: 1.2544
Loss at step 6870: 1.3820
Loss at step 6880: 1.4057
Loss at step 6890: 0.5962
Loss at step 6900: 1.2914
Loss at step 6910: 1.4961
Loss at step 6920: 1.1592
Loss at step 6930: 1.0700
Loss at step 6940: 0.7290
Loss at step 6950: 0.5426
Loss at step 6960: 0.7413
Loss at step 6970: 0.3305
Loss at step 6980: 0.4536
Loss at step 6990: 1.5860
Loss at step 7000: 1.5652
Loss at step 7010: 1.0910
Loss at step 7020: 0.8574
Loss at step 7030: 0.6183
Loss at step 7040: 0.2255
Loss at step 7050: 0.8025
Loss at step 7060: 0.6001
Loss at step 7070: 0.2583
Loss at step 7080: 0.2120
Loss at step 7090: 1.2880
Loss at step 7100: 1.2449
Loss at step 7110: 1.0569
Loss at step 7120: 0.5779
Loss at step 7130: 0.6188
Loss at step 7140: 0.5008
Loss at step 7150: 0.4402
Loss at step 7160: 0.7809
Loss at step 7170: 0.7484
Loss at step 7180: 0.9297
Loss at step 7190: 1.0340
Loss at step 7200: 0.8372
Loss at step 7210: 0.6353
Loss at step 7220: 0.2482
Loss at step 7230: 0.2514
Loss at step 7240: 0.7438
Loss at step 7250: 1.2952
Loss at step 7260: 0.5855
Loss at step 7270: 1.4001
Loss at step 7280: 0.9023
Loss at step 7290: 0.6122
Loss at step 7300: 1.4068
Loss at step 7310: 0.5646
Loss at step 7320: 1.6185
Loss at step 7330: 0.7239
Loss at step 7340: 1.1647
Loss at step 7350: 0.7135
Loss at step 7360: 0.8562
Loss at step 7370: 0.6834
Loss at step 7380: 0.9734
Loss at step 7390: 0.3274
Loss at step 7400: 0.5040
Loss at step 7410: 1.0538
Loss at step 7420: 0.2671
Loss at step 7430: 1.7182
Loss at step 7440: 1.5101
Loss at step 7450: 0.5404
Loss at step 7460: 0.9815
Loss at step 7470: 0.0883
Loss at step 7480: 0.7662
Loss at step 7490: 0.3844
Loss at step 7500: 1.0345
Loss at step 7510: 0.8579
Loss at step 7520: 1.3915
Loss at step 7530: 0.5122
Loss at step 7540: 0.9387
Loss at step 7550: 0.3727
Loss at step 7560: 0.4746
Loss at step 7570: 0.3132
Loss at step 7580: 1.2705
Loss at step 7590: 0.7376
Loss at step 7600: 1.1060
Loss at step 7610: 1.3790
Loss at step 7620: 1.1505
Loss at step 7630: 1.1625
Loss at step 7640: 1.0276
Loss at step 7650: 0.4893
Loss at step 7660: 0.7929
Loss at step 7670: 0.9683
Loss at step 7680: 1.3646
Loss at step 7690: 0.2322
Loss at step 7700: 0.5209
Loss at step 7710: 0.7185
Loss at step 7720: 0.7467
Loss at step 7730: 0.2165
Loss at step 7740: 0.0130
Loss at step 7750: 0.6474
Loss at step 7760: 0.9533
Loss at step 7770: 0.1634
Loss at step 7780: 0.4649
Loss at step 7790: 0.5259
Loss at step 7800: 0.6747
Loss at step 7810: 0.4749
Loss at step 7820: 0.3011
Loss at step 7830: 0.0998
Loss at step 7840: 0.7369
Loss at step 7850: 0.6892
Loss at step 7860: 0.6341
Loss at step 7870: 0.9253
Loss at step 7880: 0.4814
Loss at step 7890: 0.1243
Loss at step 7900: 0.6876
Loss at step 7910: 2.4350
Loss at step 7920: 0.9914
Loss at step 7930: 0.4802
Loss at step 7940: 1.4302
Loss at step 7950: 0.4787
Loss at step 7960: 0.9281
Loss at step 7970: 0.5040
Loss at step 7980: 0.4854
Loss at step 7990: 1.0129
Loss at step 8000: 0.6505
Loss at step 8010: 0.4254
Loss at step 8020: 1.8758
Loss at step 8030: 0.2853
Loss at step 8040: 1.7687
Loss at step 8050: 0.9681
Loss at step 8060: 0.6633
Loss at step 8070: 0.6478
Loss at step 8080: 0.7229
Loss at step 8090: 1.7743
Loss at step 8100: 1.4438
Loss at step 8110: 0.2437
Loss at step 8120: 1.1758
Loss at step 8130: 0.5505
Loss at step 8140: 0.5971
Loss at step 8150: 1.5413
Loss at step 8160: 0.5028
Loss at step 8170: 2.5081
Loss at step 8180: 1.0801
Loss at step 8190: 0.2839
Loss at step 8200: 0.4398
Loss at step 8210: 0.1923
Loss at step 8220: 0.7653
Loss at step 8230: 0.3859
Loss at step 8240: 0.9468
Loss at step 8250: 0.2012
Loss at step 8260: 0.6847
Loss at step 8270: 1.1204
Loss at step 8280: 0.1851
Loss at step 8290: 0.3624
Loss at step 8300: 2.6199
Loss at step 8310: 0.9147
Loss at step 8320: 0.4098
Loss at step 8330: 0.3410
Loss at step 8340: 0.9336
Loss at step 8350: 1.0697
Loss at step 8360: 1.5782
Loss at step 8370: 0.6132
Loss at step 8380: 0.5619
Loss at step 8390: 0.5361
Loss at step 8400: 0.1331
Loss at step 8410: 0.2046
Loss at step 8420: 0.2888
Loss at step 8430: 0.5055
Loss at step 8440: 0.5278
Loss at step 8450: 1.0864
Loss at step 8460: 0.3955
Loss at step 8470: 0.5894
Loss at step 8480: 0.5846
Loss at step 8490: 1.5916
Loss at step 8500: 1.0481
Loss at step 8510: 0.9024
Loss at step 8520: 0.6566
Loss at step 8530: 0.6000
Loss at step 8540: 0.1996
Loss at step 8550: 0.5292
Loss at step 8560: 0.5607
Loss at step 8570: 0.5445
Loss at step 8580: 0.0532
Loss at step 8590: 1.9528
Loss at step 8600: 0.6381
Loss at step 8610: 0.7694
Loss at step 8620: 0.5382
Loss at step 8630: 0.0736
Loss at step 8640: 0.5415
Loss at step 8650: 0.9193
Loss at step 8660: 0.1466
Loss at step 8670: 0.4471
Loss at step 8680: 1.1683
Loss at step 8690: 0.9069
Loss at step 8700: 2.4815
Loss at step 8710: 1.0252
Loss at step 8720: 0.6649
Loss at step 8730: 0.1283
Loss at step 8740: 0.0892
Loss at step 8750: 0.1708
Loss at step 8760: 0.1651
Loss at step 8770: 1.0677
Loss at step 8780: 0.4206
Loss at step 8790: 0.9471
Loss at step 8800: 1.0760
Loss at step 8810: 0.2331
Loss at step 8820: 0.6011
Loss at step 8830: 0.6785
Loss at step 8840: 0.5157
Loss at step 8850: 0.3819
Loss at step 8860: 0.9121
Loss at step 8870: 0.4041
Loss at step 8880: 0.9821
Loss at step 8890: 0.2417
Loss at step 8900: 0.3492
Loss at step 8910: 0.5321
Loss at step 8920: 0.4328
Loss at step 8930: 0.8272
Loss at step 8940: 0.0268
Loss at step 8950: 0.9035
Loss at step 8960: 0.9447
Loss at step 8970: 0.1981
Loss at step 8980: 0.5153
Loss at step 8990: 1.4282
Loss at step 9000: 0.7794
Loss at step 9010: 0.3731
Loss at step 9020: 0.5658
Loss at step 9030: 1.1075
Loss at step 9040: 0.7504
Loss at step 9050: 0.4257
Loss at step 9060: 0.5477
Loss at step 9070: 0.6465
Loss at step 9080: 0.0805
Loss at step 9090: 0.3530
Loss at step 9100: 0.5949
Loss at step 9110: 0.0563
Loss at step 9120: 1.2695
Loss at step 9130: 2.2070
Loss at step 9140: 0.8258
Loss at step 9150: 0.6863
Loss at step 9160: 0.6197
Loss at step 9170: 1.8873
Loss at step 9180: 0.1112
Loss at step 9190: 0.3568
Loss at step 9200: 0.3534
Loss at step 9210: 0.4413
Loss at step 9220: 0.6366
Loss at step 9230: 0.8031
Loss at step 9240: 0.9771
Loss at step 9250: 0.2896
Loss at step 9260: 0.7711
Loss at step 9270: 0.8798
Loss at step 9280: 0.7785
Loss at step 9290: 0.5572
Loss at step 9300: 0.3237
Loss at step 9310: 0.7408
Loss at step 9320: 0.4357
Loss at step 9330: 1.0456
Loss at step 9340: 0.6270
Loss at step 9350: 0.5014
Loss at step 9360: 1.0680
Loss at step 9370: 0.6006
Loss at step 9380: 0.8178
Loss at step 9390: 1.0423
Loss at step 9400: 0.4834
Loss at step 9410: 0.6152
Loss at step 9420: 0.4226
Loss at step 9430: 0.7332
Loss at step 9440: 0.5960
Loss at step 9450: 0.7124
Loss at step 9460: 0.1301
Loss at step 9470: 1.2574
Loss at step 9480: 1.3084
Loss at step 9490: 0.2835
Loss at step 9500: 0.4750
Loss at step 9510: 0.4465
Loss at step 9520: 0.1494
Loss at step 9530: 0.3144
Loss at step 9540: 0.5755
Loss at step 9550: 0.5134
Loss at step 9560: 0.5753
Loss at step 9570: 0.4224
Loss at step 9580: 0.6553
Loss at step 9590: 0.4496
Loss at step 9600: 0.5849
Loss at step 9610: 0.6334
Loss at step 9620: 1.3437
Loss at step 9630: 0.6446
Loss at step 9640: 0.7336
Loss at step 9650: 1.1620
Loss at step 9660: 0.9736
Loss at step 9670: 0.3561
Loss at step 9680: 1.5566
Loss at step 9690: 0.6504
Loss at step 9700: 1.3521
Loss at step 9710: 0.5034
Loss at step 9720: 0.1132
Loss at step 9730: 0.7992
Loss at step 9740: 0.5820
Loss at step 9750: 1.0730
Loss at step 9760: 0.2409
Loss at step 9770: 0.7858
Loss at step 9780: 0.6257
Loss at step 9790: 0.1732
Loss at step 9800: 1.0340
Loss at step 9810: 0.3619
Loss at step 9820: 0.4386
Loss at step 9830: 0.5175
Loss at step 9840: 0.2751
Loss at step 9850: 0.3360
Loss at step 9860: 1.1010
Loss at step 9870: 0.1310
Loss at step 9880: 0.3295
Loss at step 9890: 1.0924
Loss at step 9900: 0.2240
Loss at step 9910: 0.4599
Loss at step 9920: 0.4229
Loss at step 9930: 1.0017
Loss at step 9940: 0.6873
Loss at step 9950: 0.1175
Loss at step 9960: 0.3212
Loss at step 9970: 0.3397
Loss at step 9980: 0.3553
Loss at step 9990: 1.4392
Loss at step 10000: 0.3157
Loss at step 10010: 0.1909
Loss at step 10020: 0.2909
Loss at step 10030: 0.4901
Loss at step 10040: 0.8223
Loss at step 10050: 0.9260
Loss at step 10060: 0.4804
Loss at step 10070: 0.1446
Loss at step 10080: 1.0319
Loss at step 10090: 0.4364
Loss at step 10100: 0.6315
Loss at step 10110: 0.8054
Loss at step 10120: 2.2431
Loss at step 10130: 1.5758
Loss at step 10140: 1.0653
Loss at step 10150: 0.1751
Loss at step 10160: 0.5170
Loss at step 10170: 0.4867
Loss at step 10180: 0.2877
Loss at step 10190: 0.2988
Loss at step 10200: 0.0310
Loss at step 10210: 0.3910
Loss at step 10220: 1.3835
Loss at step 10230: 0.3960
Loss at step 10240: 0.7228
Loss at step 10250: 0.2881
Loss at step 10260: 1.2321
Loss at step 10270: 0.3747
Loss at step 10280: 1.5489
Loss at step 10290: 0.8830
Loss at step 10300: 0.5666
Loss at step 10310: 0.7237
Loss at step 10320: 0.5531
Loss at step 10330: 0.9726
Loss at step 10340: 0.2094
Loss at step 10350: 0.5554
Loss at step 10360: 0.6123
Loss at step 10370: 0.3162
Loss at step 10380: 0.2457
Loss at step 10390: 0.5518
Loss at step 10400: 0.5367
Loss at step 10410: 0.2594
Loss at step 10420: 0.6460
Loss at step 10430: 0.7398
Loss at step 10440: 0.4409
Loss at step 10450: 0.3267
Loss at step 10460: 1.6172
Loss at step 10470: 0.2644
Loss at step 10480: 0.4203
Loss at step 10490: 0.5568
Loss at step 10500: 1.1565
Loss at step 10510: 0.1559
Loss at step 10520: 0.0458
Loss at step 10530: 0.8790
Loss at step 10540: 0.6788
Loss at step 10550: 0.0021
Loss at step 10560: 0.0950
Loss at step 10570: 0.8214
Loss at step 10580: 0.5519
Loss at step 10590: 0.5309
Loss at step 10600: 1.2006
Loss at step 10610: 0.8026
Loss at step 10620: 0.6184
Loss at step 10630: 0.3018
Loss at step 10640: 0.0054
Loss at step 10650: 0.5131
Loss at step 10660: 0.6175
Loss at step 10670: 0.1364
Loss at step 10680: 1.1901
Loss at step 10690: 0.7787
Loss at step 10700: 0.5803
Loss at step 10710: 1.3980
Loss at step 10720: 0.3146
Loss at step 10730: 0.1022
Loss at step 10740: 1.1081
Loss at step 10750: 0.4031
Loss at step 10760: 0.2714
Loss at step 10770: 0.6478
Loss at step 10780: 1.1643
Loss at step 10790: 0.4122
Loss at step 10800: 1.2248
Loss at step 10810: 0.7220
Loss at step 10820: 0.2754
Loss at step 10830: 0.7167
Loss at step 10840: 1.4383
Loss at step 10850: 0.7897
Loss at step 10860: 0.4767
Loss at step 10870: 0.1612
Loss at step 10880: 0.2396
Loss at step 10890: 0.4105
Loss at step 10900: 0.0490
Loss at step 10910: 0.4240
Loss at step 10920: 0.9338
Loss at step 10930: 1.1197
Loss at step 10940: 0.6265
Loss at step 10950: 0.7697
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.730901, 'precision': [0.831732, 0.429651, 0.572664], 'recall': [0.900516, 0.407462, 0.406634], 'f1': [0.864759, 0.418263, 0.475575]}
{'accuracy': 0.862556, 'precision': 0.572664, 'recall': 0.406634, 'f1': 0.475575, 'WordR': 0.130783}
Loss at step 10960: 0.2116
Loss at step 10970: 0.4395
Loss at step 10980: 0.0536
Loss at step 10990: 1.4990
Loss at step 11000: 0.0738
Loss at step 11010: 0.1503
Loss at step 11020: 0.6673
Loss at step 11030: 0.2838
Loss at step 11040: 0.9096
Loss at step 11050: 0.1607
Loss at step 11060: 0.5905
Loss at step 11070: 0.8597
Loss at step 11080: 0.4090
Loss at step 11090: 0.1284
Loss at step 11100: 0.2492
Loss at step 11110: 0.8471
Loss at step 11120: 1.2294
Loss at step 11130: 0.3360
Loss at step 11140: 1.7315
Loss at step 11150: 0.4088
Loss at step 11160: 0.3461
Loss at step 11170: 0.3293
Loss at step 11180: 0.2247
Loss at step 11190: 0.5673
Loss at step 11200: 0.2060
Loss at step 11210: 0.7598
Loss at step 11220: 0.8762
Loss at step 11230: 0.4826
Loss at step 11240: 0.0643
Loss at step 11250: 0.7438
Loss at step 11260: 0.5670
Loss at step 11270: 0.2644
Loss at step 11280: 0.5670
Loss at step 11290: 0.0284
Loss at step 11300: 0.2736
Loss at step 11310: 0.5056
Loss at step 11320: 0.5724
Loss at step 11330: 0.4570
Loss at step 11340: 0.3430
Loss at step 11350: 0.6852
Loss at step 11360: 0.5522
Loss at step 11370: 1.0828
Loss at step 11380: 0.1113
Loss at step 11390: 0.0826
Loss at step 11400: 0.1911
Loss at step 11410: 0.1518
Loss at step 11420: 0.0008
Loss at step 11430: 0.4096
Loss at step 11440: 2.0700
Loss at step 11450: 0.1138
Loss at step 11460: 0.3444
Loss at step 11470: 1.3907
Loss at step 11480: 0.4339
Loss at step 11490: 0.4522
Loss at step 11500: 0.2444
Loss at step 11510: 0.3411
Loss at step 11520: 0.4217
Loss at step 11530: 4.7230
Loss at step 11540: 0.1773
Loss at step 11550: 0.1981
Loss at step 11560: 0.0439
Loss at step 11570: 0.1611
Loss at step 11580: 0.1952
Loss at step 11590: 0.6923
Loss at step 11600: 0.2452
Loss at step 11610: 0.0314
Loss at step 11620: 0.0708
Loss at step 11630: 0.6835
Loss at step 11640: 0.1071
Loss at step 11650: 0.1063
Loss at step 11660: 0.4715
Loss at step 11670: 0.0911
Loss at step 11680: 0.4922
Loss at step 11690: 0.3068
Loss at step 11700: 0.5220
Loss at step 11710: 0.0727
Loss at step 11720: 0.1178
Loss at step 11730: 0.8016
Loss at step 11740: 0.4263
Loss at step 11750: 0.1007
Loss at step 11760: 0.4572
Loss at step 11770: 0.7685
Loss at step 11780: 0.4233
Loss at step 11790: 0.2977
Loss at step 11800: 0.1416
Loss at step 11810: 0.4795
Loss at step 11820: 0.5466
Loss at step 11830: 0.4582
Loss at step 11840: 0.0969
Loss at step 11850: 0.3930
Loss at step 11860: 0.4533
Loss at step 11870: 0.2598
Loss at step 11880: 0.0069
Loss at step 11890: 0.0011
Loss at step 11900: 0.7382
Loss at step 11910: 0.0111
Loss at step 11920: 0.0003
Loss at step 11930: 0.0375
Loss at step 11940: 0.1893
Loss at step 11950: 0.7502
Loss at step 11960: 0.1325
Loss at step 11970: 0.3119
Loss at step 11980: 0.0852
Loss at step 11990: 0.2995
Loss at step 12000: 0.4767
Loss at step 12010: 0.0708
Loss at step 12020: 1.3922
Loss at step 12030: 0.2609
Loss at step 12040: 0.8809
Loss at step 12050: 0.5754
Loss at step 12060: 2.2418
Loss at step 12070: 0.4442
Loss at step 12080: 0.1777
Loss at step 12090: 1.8883
Loss at step 12100: 0.0061
Loss at step 12110: 0.6169
Loss at step 12120: 0.1892
Loss at step 12130: 0.1564
Loss at step 12140: 0.6172
Loss at step 12150: 0.9817
Loss at step 12160: 0.2194
Loss at step 12170: 0.8743
Loss at step 12180: 0.2093
Loss at step 12190: 0.5014
Loss at step 12200: 0.3240
Loss at step 12210: 0.3333
Loss at step 12220: 0.5053
Loss at step 12230: 0.3794
Loss at step 12240: 0.8286
Loss at step 12250: 1.4054
Loss at step 12260: 0.1925
Loss at step 12270: 1.3542
Loss at step 12280: 0.8577
Loss at step 12290: 0.5488
Loss at step 12300: 0.0491
Loss at step 12310: 0.1600
Loss at step 12320: 0.1488
Loss at step 12330: 0.4157
Loss at step 12340: 0.0146
Loss at step 12350: 0.0646
Loss at step 12360: 0.4913
Loss at step 12370: 0.2988
Loss at step 12380: 0.0008
Loss at step 12390: 1.2052
Loss at step 12400: 0.3271
Loss at step 12410: 0.0786
Loss at step 12420: 0.4933
Loss at step 12430: 0.2492
Loss at step 12440: 0.0256
Loss at step 12450: 0.4870
Loss at step 12460: 0.2727
Loss at step 12470: 0.0519
Loss at step 12480: 0.8404
Loss at step 12490: 0.5889
Loss at step 12500: 0.4903
Loss at step 12510: 1.0337
Loss at step 12520: 0.0133
Loss at step 12530: 0.3214
Loss at step 12540: 0.6366
Loss at step 12550: 0.2508
Loss at step 12560: 0.0170
Loss at step 12570: 0.4449
Loss at step 12580: 0.2540
Loss at step 12590: 0.3808
Loss at step 12600: 0.5102
Loss at step 12610: 0.2630
Loss at step 12620: 1.0265
Loss at step 12630: 0.7294
Loss at step 12640: 0.2705
Loss at step 12650: 1.5714
Loss at step 12660: 0.2514
Loss at step 12670: 0.0348
Loss at step 12680: 0.1805
Loss at step 12690: 0.4812
Loss at step 12700: 0.1228
Loss at step 12710: 0.2258
Loss at step 12720: 0.8727
Loss at step 12730: 0.4698
Loss at step 12740: 0.9033
Loss at step 12750: 0.3690
Loss at step 12760: 0.3753
Loss at step 12770: 0.5636
Loss at step 12780: 0.5324
Loss at step 12790: 0.1830
Loss at step 12800: 0.5544
Loss at step 12810: 0.6650
Loss at step 12820: 0.2951
Loss at step 12830: 0.1073
Loss at step 12840: 0.1094
Loss at step 12850: 0.4766
Loss at step 12860: 1.5163
Loss at step 12870: 0.1274
Loss at step 12880: 0.6564
Loss at step 12890: 0.1802
Loss at step 12900: 0.2017
Loss at step 12910: 0.0547
Loss at step 12920: 1.5985
Loss at step 12930: 0.0202
Loss at step 12940: 0.0011
Loss at step 12950: 0.3697
Loss at step 12960: 0.1932
Loss at step 12970: 0.8866
Loss at step 12980: 0.1236
Loss at step 12990: 0.6570
Loss at step 13000: 0.0197
Loss at step 13010: 0.1219
Loss at step 13020: 0.0840
Loss at step 13030: 0.0343
Loss at step 13040: 0.6747
Loss at step 13050: 0.1611
Loss at step 13060: 0.1108
Loss at step 13070: 0.5240
Loss at step 13080: 1.2017
Loss at step 13090: 0.2666
Loss at step 13100: 0.9885
Loss at step 13110: 0.1184
Loss at step 13120: 0.0920
Loss at step 13130: 0.4633
Loss at step 13140: 0.6572
Loss at step 13150: 0.0584
Loss at step 13160: 0.1666
Loss at step 13170: 0.2119
Loss at step 13180: 1.2753
Loss at step 13190: 0.4450
Loss at step 13200: 0.2060
Loss at step 13210: 0.9053
Loss at step 13220: 0.1283
Loss at step 13230: 0.3897
Loss at step 13240: 0.3325
Loss at step 13250: 0.3514
Loss at step 13260: 0.3285
Loss at step 13270: 0.9863
Loss at step 13280: 0.1052
Loss at step 13290: 0.0021
Loss at step 13300: 0.1462
Loss at step 13310: 0.2097
Loss at step 13320: 0.9420
Loss at step 13330: 0.8394
Loss at step 13340: 1.1623
Loss at step 13350: 0.0463
Loss at step 13360: 0.2931
Loss at step 13370: 0.4343
Loss at step 13380: 0.0013
Loss at step 13390: 0.2917
Loss at step 13400: 0.8642
Loss at step 13410: 0.3727
Loss at step 13420: 0.5263
Loss at step 13430: 0.4766
Loss at step 13440: 0.2400
Loss at step 13450: 0.2015
Loss at step 13460: 0.2083
Loss at step 13470: 0.9043
Loss at step 13480: 1.1957
Loss at step 13490: 0.0144
Loss at step 13500: 0.1505
Loss at step 13510: 0.1247
Loss at step 13520: 0.3919
Loss at step 13530: 0.6738
Loss at step 13540: 0.5232
Loss at step 13550: 0.6059
Loss at step 13560: 0.5378
Loss at step 13570: 0.4489
Loss at step 13580: 0.2011
Loss at step 13590: 0.8590
Loss at step 13600: 0.0497
Loss at step 13610: 0.8660
Loss at step 13620: 0.3376
Loss at step 13630: 0.2637
Loss at step 13640: 0.2406
Loss at step 13650: 0.0490
Loss at step 13660: 1.3315
Loss at step 13670: 0.3187
Loss at step 13680: 0.0032
Loss at step 13690: 0.2794
Loss at step 13700: 0.0895
Loss at step 13710: 0.0701
Loss at step 13720: 1.6014
Loss at step 13730: 0.3560
Loss at step 13740: 0.3852
Loss at step 13750: 0.5783
Loss at step 13760: 0.2176
Loss at step 13770: 0.0197
Loss at step 13780: 0.5782
Loss at step 13790: 0.2377
Loss at step 13800: 0.8548
Loss at step 13810: 0.3026
Loss at step 13820: 0.1493
Loss at step 13830: 0.5191
Loss at step 13840: 0.3143
Loss at step 13850: 0.6905
Loss at step 13860: 0.8051
Loss at step 13870: 0.5875
Loss at step 13880: 0.1696
Loss at step 13890: 1.0950
Loss at step 13900: 0.6067
Loss at step 13910: 0.2677
Loss at step 13920: 0.2058
Loss at step 13930: 0.4249
Loss at step 13940: 0.5915
Loss at step 13950: 0.2481
Loss at step 13960: 0.4536
Loss at step 13970: 0.2395
Loss at step 13980: 0.0585
Loss at step 13990: 0.2913
Loss at step 14000: 0.5640
Loss at step 14010: 1.2862
Loss at step 14020: 0.0994
Loss at step 14030: 0.2395
Loss at step 14040: 0.2639
Loss at step 14050: 0.5075
Loss at step 14060: 0.6112
Loss at step 14070: 0.1493
Loss at step 14080: 0.5993
Loss at step 14090: 1.1333
Loss at step 14100: 0.1358
Loss at step 14110: 0.3651
Loss at step 14120: 0.3106
Loss at step 14130: 0.4438
Loss at step 14140: 1.2441
Loss at step 14150: 0.0176
Loss at step 14160: 0.7724
Loss at step 14170: 0.2767
Loss at step 14180: 0.8247
Loss at step 14190: 0.7898
Loss at step 14200: 0.0012
Loss at step 14210: 0.3129
Loss at step 14220: 0.2231
Loss at step 14230: 0.0575
Loss at step 14240: 0.0097
Loss at step 14250: 0.1549
Loss at step 14260: 0.4357
Loss at step 14270: 0.7606
Loss at step 14280: 0.5375
Loss at step 14290: 0.3113
Loss at step 14300: 0.5898
Loss at step 14310: 0.6013
Loss at step 14320: 0.3231
Loss at step 14330: 0.2187
Loss at step 14340: 0.5148
Loss at step 14350: 1.4075
Loss at step 14360: 0.1579
Loss at step 14370: 0.7786
Loss at step 14380: 0.8113
Loss at step 14390: 0.0004
Loss at step 14400: 0.2248
Loss at step 14410: 1.1203
Loss at step 14420: 0.7587
Loss at step 14430: 0.4404
Loss at step 14440: 0.9975
Loss at step 14450: 0.0478
Loss at step 14460: 0.1254
Loss at step 14470: 0.3201
Loss at step 14480: 0.6874
Loss at step 14490: 0.0677
Loss at step 14500: 0.8464
Loss at step 14510: 1.5609
Loss at step 14520: 0.1223
Loss at step 14530: 1.0602
Loss at step 14540: 0.0593
Loss at step 14550: 0.3006
Loss at step 14560: 0.7024
Loss at step 14570: 0.3001
Loss at step 14580: 0.0575
Loss at step 14590: 0.7967
Loss at step 14600: 0.6357
Loss at step 14610: 0.1192
Loss at step 14620: 0.3211
Loss at step 14630: 0.1043
Loss at step 14640: 0.3937
Loss at step 14650: 0.8579
Loss at step 14660: 0.1042
Loss at step 14670: 0.0440
Loss at step 14680: 0.0788
Loss at step 14690: 0.1965
Loss at step 14700: 0.8558
Loss at step 14710: 0.9536
Loss at step 14720: 0.2015
Loss at step 14730: 0.6497
Loss at step 14740: 0.5249
Loss at step 14750: 0.4584
Loss at step 14760: 0.8546
Loss at step 14770: 0.2012
Loss at step 14780: 0.0835
Loss at step 14790: 0.1698
Loss at step 14800: 1.1195
Loss at step 14810: 1.3925
Loss at step 14820: 0.1294
Loss at step 14830: 0.2505
Loss at step 14840: 0.8861
Loss at step 14850: 0.1086
Loss at step 14860: 0.2334
Loss at step 14870: 0.2196
Loss at step 14880: 0.0484
Loss at step 14890: 0.0282
Loss at step 14900: 0.1788
Loss at step 14910: 0.0603
Loss at step 14920: 0.3932
Loss at step 14930: 0.4165
Loss at step 14940: 1.1506
Loss at step 14950: 0.2958
Loss at step 14960: 0.1146
Loss at step 14970: 0.0030
Loss at step 14980: 1.2090
Loss at step 14990: 1.2480
Loss at step 15000: 1.2052
Loss at step 15010: 0.3478
Loss at step 15020: 0.1938
Loss at step 15030: 0.6153
Loss at step 15040: 0.7257
Loss at step 15050: 0.0003
Loss at step 15060: 0.1738
Loss at step 15070: 0.1319
Loss at step 15080: 0.1594
Loss at step 15090: 0.1190
Loss at step 15100: 0.5629
Loss at step 15110: 0.5703
Loss at step 15120: 0.8152
Loss at step 15130: 0.5399
Loss at step 15140: 0.1305
Loss at step 15150: 3.1275
Loss at step 15160: 0.1838
Loss at step 15170: 0.1633
Loss at step 15180: 0.4462
Loss at step 15190: 0.7573
Loss at step 15200: 0.9839
Loss at step 15210: 0.0000
Loss at step 15220: 0.0026
Loss at step 15230: 0.1265
Loss at step 15240: 0.8425
Loss at step 15250: 0.2829
Loss at step 15260: 0.9251
Loss at step 15270: 0.7424
Loss at step 15280: 0.2078
Loss at step 15290: 0.5679
Loss at step 15300: 0.2771
Loss at step 15310: 0.0002
Loss at step 15320: 0.5153
Loss at step 15330: 0.8690
Loss at step 15340: 0.2522
Loss at step 15350: 0.9971
Loss at step 15360: 0.7162
Loss at step 15370: 0.7050
Loss at step 15380: 0.8895
Loss at step 15390: 0.2781
Loss at step 15400: 0.5900
Loss at step 15410: 0.6381
Loss at step 15420: 0.0031
Loss at step 15430: 0.3957
Loss at step 15440: 0.2848
Loss at step 15450: 0.2199
Loss at step 15460: 0.0780
Loss at step 15470: 0.0144
Loss at step 15480: 0.0608
Loss at step 15490: 0.7575
Loss at step 15500: 0.2885
Loss at step 15510: 0.4229
Loss at step 15520: 0.3124
Loss at step 15530: 0.0289
Loss at step 15540: 0.3343
Loss at step 15550: 0.0543
Loss at step 15560: 0.1776
Loss at step 15570: 0.4715
Loss at step 15580: 0.0088
Loss at step 15590: 0.0017
Loss at step 15600: 0.1111
Loss at step 15610: 0.6217
Loss at step 15620: 2.0273
Loss at step 15630: 0.0027
Loss at step 15640: 0.0423
Loss at step 15650: 0.3007
Loss at step 15660: 0.4842
Loss at step 15670: 2.0778
Loss at step 15680: 0.3687
Loss at step 15690: 0.1935
Loss at step 15700: 0.0564
Loss at step 15710: 0.0218
Loss at step 15720: 0.3603
Loss at step 15730: 0.0285
Loss at step 15740: 0.0293
Loss at step 15750: 0.4522
Loss at step 15760: 0.5115
Loss at step 15770: 0.8057
Loss at step 15780: 0.1092
Loss at step 15790: 0.0981
Loss at step 15800: 0.0714
Loss at step 15810: 0.4169
Loss at step 15820: 0.4796
Loss at step 15830: 0.2287
Loss at step 15840: 0.7188
Loss at step 15850: 0.2504
Loss at step 15860: 0.5485
Loss at step 15870: 0.4257
Loss at step 15880: 0.7326
Loss at step 15890: 0.0074
Loss at step 15900: 0.7053
Loss at step 15910: 0.9409
Loss at step 15920: 0.2214
Loss at step 15930: 0.9308
Loss at step 15940: 0.7365
Loss at step 15950: 0.7756
Loss at step 15960: 0.0109
Loss at step 15970: 0.0271
Loss at step 15980: 0.1617
Loss at step 15990: 0.2231
Loss at step 16000: 0.6650
Loss at step 16010: 0.6846
Loss at step 16020: 0.1938
Loss at step 16030: 0.7366
Loss at step 16040: 0.2583
Loss at step 16050: 0.1326
Loss at step 16060: 0.5399
Loss at step 16070: 0.2912
Loss at step 16080: 0.7248
Loss at step 16090: 0.5316
Loss at step 16100: 0.3004
Loss at step 16110: 0.0100
Loss at step 16120: 0.1274
Loss at step 16130: 0.0589
Loss at step 16140: 0.0283
Loss at step 16150: 0.1493
Loss at step 16160: 0.1977
Loss at step 16170: 0.1287
Loss at step 16180: 0.2158
Loss at step 16190: 0.0007
Loss at step 16200: 0.1114
Loss at step 16210: 0.0236
Loss at step 16220: 0.1895
Loss at step 16230: 0.3155
Loss at step 16240: 0.0441
Loss at step 16250: 0.1956
Loss at step 16260: 0.2782
Loss at step 16270: 0.6492
Loss at step 16280: 0.4307
Loss at step 16290: 0.0034
Loss at step 16300: 0.3282
Loss at step 16310: 0.0014
Loss at step 16320: 0.4414
Loss at step 16330: 0.5440
Loss at step 16340: 0.4200
Loss at step 16350: 0.3472
Loss at step 16360: 0.0880
Loss at step 16370: 0.1276
Loss at step 16380: 0.0178
Loss at step 16390: 0.7297
Loss at step 16400: 0.3272
Loss at step 16410: 0.6109
Loss at step 16420: 0.0581
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.70619, 'precision': [0.888784, 0.389379, 0.556567], 'recall': [0.809927, 0.599703, 0.394349], 'f1': [0.847525, 0.472179, 0.461621]}
{'accuracy': 0.859026, 'precision': 0.556567, 'recall': 0.394349, 'f1': 0.461621, 'WordR': 0.130783}
Loss at step 16430: 0.6849
Loss at step 16440: 0.4418
Loss at step 16450: 0.1005
Loss at step 16460: 0.0204
Loss at step 16470: 1.1940
Loss at step 16480: 0.1547
Loss at step 16490: 1.4190
Loss at step 16500: 0.1947
Loss at step 16510: 0.0155
Loss at step 16520: 0.1586
Loss at step 16530: 0.0565
Loss at step 16540: 0.0743
Loss at step 16550: 0.0031
Loss at step 16560: 0.3827
Loss at step 16570: 0.2960
Loss at step 16580: 0.0005
Loss at step 16590: 0.0363
Loss at step 16600: 0.0399
Loss at step 16610: 0.0086
Loss at step 16620: 0.1013
Loss at step 16630: 0.2669
Loss at step 16640: 0.8443
Loss at step 16650: 0.1212
Loss at step 16660: 1.4197
Loss at step 16670: 0.0018
Loss at step 16680: 0.3816
Loss at step 16690: 0.9643
Loss at step 16700: 0.5307
Loss at step 16710: 0.0750
Loss at step 16720: 0.0545
Loss at step 16730: 0.1789
Loss at step 16740: 0.7583
Loss at step 16750: 0.0026
Loss at step 16760: 0.7782
Loss at step 16770: 0.2645
Loss at step 16780: 0.3141
Loss at step 16790: 1.2268
Loss at step 16800: 0.0469
Loss at step 16810: 0.0469
Loss at step 16820: 0.0011
Loss at step 16830: 0.4320
Loss at step 16840: 0.6937
Loss at step 16850: 0.1293
Loss at step 16860: 0.0918
Loss at step 16870: 0.0838
Loss at step 16880: 0.2550
Loss at step 16890: 0.0345
Loss at step 16900: 0.1784
Loss at step 16910: 0.8937
Loss at step 16920: 0.2125
Loss at step 16930: 0.0776
Loss at step 16940: 0.0060
Loss at step 16950: 0.5016
Loss at step 16960: 0.0556
Loss at step 16970: 0.5451
Loss at step 16980: 0.0808
Loss at step 16990: 0.3611
Loss at step 17000: 0.1617
Loss at step 17010: 0.0210
Loss at step 17020: 0.0000
Loss at step 17030: 0.0033
Loss at step 17040: 0.8133
Loss at step 17050: 0.4715
Loss at step 17060: 0.0906
Loss at step 17070: 0.0467
Loss at step 17080: 0.2693
Loss at step 17090: 0.6961
Loss at step 17100: 0.3148
Loss at step 17110: 0.4001
Loss at step 17120: 0.0526
Loss at step 17130: 0.0113
Loss at step 17140: 0.0138
Loss at step 17150: 0.1861
Loss at step 17160: 1.1405
Loss at step 17170: 1.0607
Loss at step 17180: 0.5398
Loss at step 17190: 0.0478
Loss at step 17200: 0.3411
Loss at step 17210: 0.3671
Loss at step 17220: 0.4219
Loss at step 17230: 0.5217
Loss at step 17240: 0.9740
Loss at step 17250: 0.6784
Loss at step 17260: 0.1342
Loss at step 17270: 0.6991
Loss at step 17280: 0.2822
Loss at step 17290: 0.2514
Loss at step 17300: 1.5011
Loss at step 17310: 0.4415
Loss at step 17320: 0.1298
Loss at step 17330: 0.2055
Loss at step 17340: 0.0336
Loss at step 17350: 0.1385
Loss at step 17360: 0.0158
Loss at step 17370: 0.0001
Loss at step 17380: 0.7869
Loss at step 17390: 0.5895
Loss at step 17400: 0.0510
Loss at step 17410: 0.1651
Loss at step 17420: 0.0003
Loss at step 17430: 0.0029
Loss at step 17440: 1.1072
Loss at step 17450: 0.6795
Loss at step 17460: 0.2125
Loss at step 17470: 0.0835
Loss at step 17480: 0.0458
Loss at step 17490: 0.3866
Loss at step 17500: 0.3407
Loss at step 17510: 0.0988
Loss at step 17520: 0.0273
Loss at step 17530: 0.3294
Loss at step 17540: 0.2839
Loss at step 17550: 0.0632
Loss at step 17560: 0.9607
Loss at step 17570: 0.1115
Loss at step 17580: 1.1084
Loss at step 17590: 0.0648
Loss at step 17600: 0.3326
Loss at step 17610: 0.3519
Loss at step 17620: 1.4917
Loss at step 17630: 0.8074
Loss at step 17640: 0.5708
Loss at step 17650: 0.4971
Loss at step 17660: 0.0106
Loss at step 17670: 0.0043
Loss at step 17680: 0.2811
Loss at step 17690: 0.2807
Loss at step 17700: 0.3996
Loss at step 17710: 1.7204
Loss at step 17720: 0.3281
Loss at step 17730: 0.4416
Loss at step 17740: 0.8810
Loss at step 17750: 0.1179
Loss at step 17760: 0.9384
Loss at step 17770: 0.1882
Loss at step 17780: 0.0203
Loss at step 17790: 0.2159
Loss at step 17800: 0.0012
Loss at step 17810: 0.0071
Loss at step 17820: 0.0024
Loss at step 17830: 0.0500
Loss at step 17840: 0.9197
Loss at step 17850: 0.4689
Loss at step 17860: 0.1511
Loss at step 17870: 0.2838
Loss at step 17880: 0.5544
Loss at step 17890: 0.0520
Loss at step 17900: 0.0079
Loss at step 17910: 0.0446
Loss at step 17920: 1.2557
Loss at step 17930: 0.2494
Loss at step 17940: 0.0141
Loss at step 17950: 0.0000
Loss at step 17960: 0.0055
Loss at step 17970: 0.0086
Loss at step 17980: 0.0009
Loss at step 17990: 0.1351
Loss at step 18000: 0.0063
Loss at step 18010: 1.7134
Loss at step 18020: 1.7078
Loss at step 18030: 0.2641
Loss at step 18040: 0.4719
Loss at step 18050: 0.0075
Loss at step 18060: 0.2583
Loss at step 18070: 0.5090
Loss at step 18080: 0.6072
Loss at step 18090: 1.7953
Loss at step 18100: 0.0000
Loss at step 18110: 0.0006
Loss at step 18120: 0.0705
Loss at step 18130: 0.0053
Loss at step 18140: 0.3691
Loss at step 18150: 0.2775
Loss at step 18160: 0.0677
Loss at step 18170: 0.0020
Loss at step 18180: 0.1805
Loss at step 18190: 0.5269
Loss at step 18200: 0.1851
Loss at step 18210: 0.0031
Loss at step 18220: 2.7782
Loss at step 18230: 0.0001
Loss at step 18240: 0.0047
Loss at step 18250: 0.8648
Loss at step 18260: 0.0422
Loss at step 18270: 1.0953
Loss at step 18280: 0.2908
Loss at step 18290: 0.0005
Loss at step 18300: 1.0588
Loss at step 18310: 0.0667
Loss at step 18320: 0.0982
Loss at step 18330: 0.0191
Loss at step 18340: 0.0287
Loss at step 18350: 0.0290
Loss at step 18360: 0.9918
Loss at step 18370: 0.0149
Loss at step 18380: 0.2103
Loss at step 18390: 0.0831
Loss at step 18400: 0.0002
Loss at step 18410: 1.0193
Loss at step 18420: 0.0001
Loss at step 18430: 0.1241
Loss at step 18440: 0.0002
Loss at step 18450: 0.4695
Loss at step 18460: 0.0217
Loss at step 18470: 0.7820
Loss at step 18480: 1.2202
Loss at step 18490: 0.0301
Loss at step 18500: 0.1891
Loss at step 18510: 0.0311
Loss at step 18520: 0.0075
Loss at step 18530: 0.7216
Loss at step 18540: 0.4444
Loss at step 18550: 0.0359
Loss at step 18560: 1.8217
Loss at step 18570: 0.0645
Loss at step 18580: 0.0994
Loss at step 18590: 0.0673
Loss at step 18600: 0.0006
Loss at step 18610: 0.7472
Loss at step 18620: 0.4463
Loss at step 18630: 0.1421
Loss at step 18640: 0.3417
Loss at step 18650: 0.1917
Loss at step 18660: 0.0206
Loss at step 18670: 0.0000
Loss at step 18680: 3.2754
Loss at step 18690: 0.1946
Loss at step 18700: 0.5294
Loss at step 18710: 0.0491
Loss at step 18720: 0.0526
Loss at step 18730: 0.4549
Loss at step 18740: 0.1300
Loss at step 18750: 0.4173
Loss at step 18760: 0.7425
Loss at step 18770: 0.0094
Loss at step 18780: 2.9531
Loss at step 18790: 0.1315
Loss at step 18800: 0.0017
Loss at step 18810: 0.0556
Loss at step 18820: 0.0167
Loss at step 18830: 0.6092
Loss at step 18840: 0.0153
Loss at step 18850: 0.1389
Loss at step 18860: 0.4742
Loss at step 18870: 0.1758
Loss at step 18880: 0.4397
Loss at step 18890: 0.0405
Loss at step 18900: 0.0001
Loss at step 18910: 0.8697
Loss at step 18920: 1.0395
Loss at step 18930: 0.2482
Loss at step 18940: 0.2528
Loss at step 18950: 0.0463
Loss at step 18960: 0.3018
Loss at step 18970: 0.0221
Loss at step 18980: 0.0007
Loss at step 18990: 0.0030
Loss at step 19000: 0.0141
Loss at step 19010: 0.2037
Loss at step 19020: 0.0141
Loss at step 19030: 0.3556
Loss at step 19040: 0.0739
Loss at step 19050: 0.0254
Loss at step 19060: 0.2850
Loss at step 19070: 0.0000
Loss at step 19080: 0.2624
Loss at step 19090: 0.1409
Loss at step 19100: 1.8303
Loss at step 19110: 0.9451
Loss at step 19120: 0.0140
Loss at step 19130: 0.1861
Loss at step 19140: 0.6783
Loss at step 19150: 0.1129
Loss at step 19160: 0.0092
Loss at step 19170: 0.1899
Loss at step 19180: 0.6821
Loss at step 19190: 0.0615
Loss at step 19200: 0.6385
Loss at step 19210: 0.7338
Loss at step 19220: 0.1155
Loss at step 19230: 1.2057
Loss at step 19240: 0.0978
Loss at step 19250: 0.0213
Loss at step 19260: 0.2190
Loss at step 19270: 2.0531
Loss at step 19280: 0.3075
Loss at step 19290: 0.0017
Loss at step 19300: 0.6747
Loss at step 19310: 0.1484
Loss at step 19320: 0.0098
Loss at step 19330: 0.0001
Loss at step 19340: 0.0013
Loss at step 19350: 0.0211
Loss at step 19360: 0.2081
Loss at step 19370: 0.0198
Loss at step 19380: 0.5208
Loss at step 19390: 1.6944
Loss at step 19400: 0.2158
Loss at step 19410: 0.0077
Loss at step 19420: 0.4935
Loss at step 19430: 0.2837
Loss at step 19440: 0.0773
Loss at step 19450: 0.0147
Loss at step 19460: 0.4647
Loss at step 19470: 0.0019
Loss at step 19480: 0.0003
Loss at step 19490: 0.0186
Loss at step 19500: 0.0327
Loss at step 19510: 0.2761
Loss at step 19520: 1.9740
Loss at step 19530: 0.0019
Loss at step 19540: 0.0055
Loss at step 19550: 0.7143
Loss at step 19560: 0.0001
Loss at step 19570: 0.0159
Loss at step 19580: 0.0001
Loss at step 19590: 0.1380
Loss at step 19600: 0.4400
Loss at step 19610: 0.0138
Loss at step 19620: 0.0458
Loss at step 19630: 0.0009
Loss at step 19640: 0.5714
Loss at step 19650: 0.2911
Loss at step 19660: 0.1180
Loss at step 19670: 0.7003
Loss at step 19680: 0.0007
Loss at step 19690: 0.3793
Loss at step 19700: 0.4708
Loss at step 19710: 0.0283
Loss at step 19720: 0.0033
Loss at step 19730: 0.0517
Loss at step 19740: 0.4337
Loss at step 19750: 0.3474
Loss at step 19760: 0.2609
Loss at step 19770: 0.2711
Loss at step 19780: 0.0115
Loss at step 19790: 0.9776
Loss at step 19800: 0.2032
Loss at step 19810: 0.4593
Loss at step 19820: 0.2819
Loss at step 19830: 0.0005
Loss at step 19840: 0.5740
Loss at step 19850: 0.0662
Loss at step 19860: 0.4111
Loss at step 19870: 0.0046
Loss at step 19880: 0.2399
Loss at step 19890: 0.0162
Loss at step 19900: 0.0008
Loss at step 19910: 1.4798
Loss at step 19920: 0.0132
Loss at step 19930: 0.1198
Loss at step 19940: 0.0031
Loss at step 19950: 0.7129
Loss at step 19960: 0.0315
Loss at step 19970: 0.0756
Loss at step 19980: 2.1216
Loss at step 19990: 0.0005
Loss at step 20000: 0.0465
Loss at step 20010: 0.0252
Loss at step 20020: 0.0057
Loss at step 20030: 0.3975
Loss at step 20040: 0.0015
Loss at step 20050: 0.6653
Loss at step 20060: 0.0026
Loss at step 20070: 0.0898
Loss at step 20080: 0.0073
Loss at step 20090: 0.1061
Loss at step 20100: 0.0062
Loss at step 20110: 1.0496
Loss at step 20120: 0.9004
Loss at step 20130: 0.4894
Loss at step 20140: 0.3795
Loss at step 20150: 0.3367
Loss at step 20160: 0.1138
Loss at step 20170: 0.1121
Loss at step 20180: 0.4945
Loss at step 20190: 0.3957
Loss at step 20200: 0.0781
Loss at step 20210: 0.9185
Loss at step 20220: 0.0766
Loss at step 20230: 0.6429
Loss at step 20240: 0.2291
Loss at step 20250: 0.7220
Loss at step 20260: 0.4978
Loss at step 20270: 0.0914
Loss at step 20280: 0.3374
Loss at step 20290: 0.1620
Loss at step 20300: 0.1050
Loss at step 20310: 0.0143
Loss at step 20320: 0.0461
Loss at step 20330: 0.7456
Loss at step 20340: 0.3675
Loss at step 20350: 1.7193
Loss at step 20360: 1.5551
Loss at step 20370: 0.3991
Loss at step 20380: 0.5308
Loss at step 20390: 0.0076
Loss at step 20400: 0.4539
Loss at step 20410: 0.0029
Loss at step 20420: 0.0000
Loss at step 20430: 0.5495
Loss at step 20440: 0.1081
Loss at step 20450: 0.3339
Loss at step 20460: 1.1792
Loss at step 20470: 0.0052
Loss at step 20480: 0.2431
Loss at step 20490: 0.1924
Loss at step 20500: 0.1724
Loss at step 20510: 0.0060
Loss at step 20520: 0.4936
Loss at step 20530: 0.8626
Loss at step 20540: 0.1928
Loss at step 20550: 0.5050
Loss at step 20560: 0.0060
Loss at step 20570: 0.0623
Loss at step 20580: 0.1129
Loss at step 20590: 0.3422
Loss at step 20600: 0.0134
Loss at step 20610: 0.0008
Loss at step 20620: 0.0014
Loss at step 20630: 0.0164
Loss at step 20640: 0.0006
Loss at step 20650: 0.0040
Loss at step 20660: 0.8379
Loss at step 20670: 0.0120
Loss at step 20680: 0.4574
Loss at step 20690: 0.4812
Loss at step 20700: 0.7368
Loss at step 20710: 0.0795
Loss at step 20720: 0.3072
Loss at step 20730: 0.3725
Loss at step 20740: 0.2839
Loss at step 20750: 0.2136
Loss at step 20760: 0.0013
Loss at step 20770: 0.0016
Loss at step 20780: 0.0032
Loss at step 20790: 0.0060
Loss at step 20800: 0.0063
Loss at step 20810: 0.0000
Loss at step 20820: 0.0001
Loss at step 20830: 0.0222
Loss at step 20840: 1.0153
Loss at step 20850: 0.0164
Loss at step 20860: 0.1354
Loss at step 20870: 1.2345
Loss at step 20880: 0.0016
Loss at step 20890: 0.0125
Loss at step 20900: 0.7156
Loss at step 20910: 0.0011
Loss at step 20920: 0.0007
Loss at step 20930: 0.1706
Loss at step 20940: 0.2620
Loss at step 20950: 0.0012
Loss at step 20960: 0.1624
Loss at step 20970: 0.4704
Loss at step 20980: 0.4070
Loss at step 20990: 0.9834
Loss at step 21000: 0.0813
Loss at step 21010: 0.1814
Loss at step 21020: 0.0776
Loss at step 21030: 0.0332
Loss at step 21040: 0.4517
Loss at step 21050: 0.0152
Loss at step 21060: 0.0009
Loss at step 21070: 0.0001
Loss at step 21080: 0.0408
Loss at step 21090: 0.0001
Loss at step 21100: 0.3459
Loss at step 21110: 0.2735
Loss at step 21120: 0.0620
Loss at step 21130: 0.0730
Loss at step 21140: 0.2538
Loss at step 21150: 0.1166
Loss at step 21160: 0.0000
Loss at step 21170: 0.0568
Loss at step 21180: 0.3097
Loss at step 21190: 0.0925
Loss at step 21200: 0.8445
Loss at step 21210: 0.0082
Loss at step 21220: 0.0003
Loss at step 21230: 0.0001
Loss at step 21240: 0.4867
Loss at step 21250: 0.0080
Loss at step 21260: 0.2642
Loss at step 21270: 0.2651
Loss at step 21280: 0.2035
Loss at step 21290: 0.0715
Loss at step 21300: 0.0002
Loss at step 21310: 0.0672
Loss at step 21320: 0.0116
Loss at step 21330: 0.0001
Loss at step 21340: 0.3126
Loss at step 21350: 0.4420
Loss at step 21360: 0.0031
Loss at step 21370: 0.5925
Loss at step 21380: 0.0431
Loss at step 21390: 0.0018
Loss at step 21400: 0.4770
Loss at step 21410: 0.6566
Loss at step 21420: 0.5893
Loss at step 21430: 0.0039
Loss at step 21440: 0.0016
Loss at step 21450: 0.0916
Loss at step 21460: 0.8467
Loss at step 21470: 0.2603
Loss at step 21480: 0.0091
Loss at step 21490: 0.2408
Loss at step 21500: 0.1006
Loss at step 21510: 0.0186
Loss at step 21520: 0.0794
Loss at step 21530: 0.7497
Loss at step 21540: 0.2820
Loss at step 21550: 0.0078
Loss at step 21560: 0.0088
Loss at step 21570: 0.0060
Loss at step 21580: 0.1581
Loss at step 21590: 0.0420
Loss at step 21600: 0.0004
Loss at step 21610: 0.0042
Loss at step 21620: 0.3454
Loss at step 21630: 0.5380
Loss at step 21640: 0.0120
Loss at step 21650: 0.0000
Loss at step 21660: 0.0000
Loss at step 21670: 0.0286
Loss at step 21680: 0.0625
Loss at step 21690: 0.0110
Loss at step 21700: 0.0047
Loss at step 21710: 1.1514
Loss at step 21720: 0.0753
Loss at step 21730: 0.0102
Loss at step 21740: 0.0008
Loss at step 21750: 0.0033
Loss at step 21760: 0.5569
Loss at step 21770: 0.3584
Loss at step 21780: 1.1587
Loss at step 21790: 0.2453
Loss at step 21800: 0.0001
Loss at step 21810: 0.0050
Loss at step 21820: 0.4462
Loss at step 21830: 0.0258
Loss at step 21840: 0.0016
Loss at step 21850: 0.0170
Loss at step 21860: 0.0012
Loss at step 21870: 0.0106
Loss at step 21880: 0.4676
Loss at step 21890: 0.0002
Loss at step 21900: 0.5283
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.728548, 'precision': [0.867502, 0.432629, 0.518061], 'recall': [0.858916, 0.440326, 0.528563], 'f1': [0.863187, 0.436444, 0.523259]}
{'accuracy': 0.852389, 'precision': 0.518061, 'recall': 0.528563, 'f1': 0.523259, 'WordR': 0.130783}
Loss at step 21910: 0.0009
Loss at step 21920: 0.0869
Loss at step 21930: 0.1462
Loss at step 21940: 0.0087
Loss at step 21950: 0.0001
Loss at step 21960: 0.0003
Loss at step 21970: 0.0229
Loss at step 21980: 0.0000
Loss at step 21990: 0.3748
Loss at step 22000: 0.0000
Loss at step 22010: 0.3226
Loss at step 22020: 0.0021
Loss at step 22030: 2.3369
Loss at step 22040: 0.0030
Loss at step 22050: 0.0016
Loss at step 22060: 0.0497
Loss at step 22070: 0.0088
Loss at step 22080: 0.1793
Loss at step 22090: 0.0146
Loss at step 22100: 1.1224
Loss at step 22110: 0.1093
Loss at step 22120: 0.1628
Loss at step 22130: 0.0087
Loss at step 22140: 0.0062
Loss at step 22150: 0.0000
Loss at step 22160: 0.0003
Loss at step 22170: 0.3756
Loss at step 22180: 0.1911
Loss at step 22190: 0.0001
Loss at step 22200: 0.0002
Loss at step 22210: 0.1904
Loss at step 22220: 0.0000
Loss at step 22230: 0.0044
Loss at step 22240: 0.0528
Loss at step 22250: 0.0497
Loss at step 22260: 0.4576
Loss at step 22270: 0.2109
Loss at step 22280: 0.0086
Loss at step 22290: 0.0000
Loss at step 22300: 0.0000
Loss at step 22310: 0.0708
Loss at step 22320: 0.3952
Loss at step 22330: 0.0094
Loss at step 22340: 0.0003
Loss at step 22350: 0.0000
Loss at step 22360: 0.3199
Loss at step 22370: 0.0072
Loss at step 22380: 0.4510
Loss at step 22390: 0.0238
Loss at step 22400: 0.0013
Loss at step 22410: 0.4827
Loss at step 22420: 0.0075
Loss at step 22430: 0.0289
Loss at step 22440: 0.4872
Loss at step 22450: 0.0007
Loss at step 22460: 0.0962
Loss at step 22470: 0.0002
Loss at step 22480: 0.0438
Loss at step 22490: 0.0000
Loss at step 22500: 0.0024
Loss at step 22510: 0.0010
Loss at step 22520: 0.2341
Loss at step 22530: 1.6923
Loss at step 22540: 1.6076
Loss at step 22550: 0.0003
Loss at step 22560: 0.6294
Loss at step 22570: 0.0268
Loss at step 22580: 0.7765
Loss at step 22590: 0.0133
Loss at step 22600: 0.3571
Loss at step 22610: 0.0004
Loss at step 22620: 0.0004
Loss at step 22630: 0.2311
Loss at step 22640: 0.6636
Loss at step 22650: 0.0834
Loss at step 22660: 0.2243
Loss at step 22670: 0.0027
Loss at step 22680: 0.3409
Loss at step 22690: 0.6079
Loss at step 22700: 0.1223
Loss at step 22710: 0.1644
Loss at step 22720: 0.0000
Loss at step 22730: 1.9375
Loss at step 22740: 0.7956
Loss at step 22750: 0.0826
Loss at step 22760: 0.0044
Loss at step 22770: 0.0012
Loss at step 22780: 0.4525
Loss at step 22790: 1.2171
Loss at step 22800: 0.7238
Loss at step 22810: 0.0024
Loss at step 22820: 0.3769
Loss at step 22830: 0.0539
Loss at step 22840: 0.0044
Loss at step 22850: 0.0006
Loss at step 22860: 0.0072
Loss at step 22870: 0.5003
Loss at step 22880: 0.0002
Loss at step 22890: 0.0001
Loss at step 22900: 0.0156
Loss at step 22910: 0.0648
Loss at step 22920: 0.3370
Loss at step 22930: 0.0013
Loss at step 22940: 0.0002
Loss at step 22950: 0.4075
Loss at step 22960: 0.3693
Loss at step 22970: 0.4402
Loss at step 22980: 0.0408
Loss at step 22990: 0.0004
Loss at step 23000: 0.0026
Loss at step 23010: 1.6810
Loss at step 23020: 0.4230
Loss at step 23030: 0.0003
Loss at step 23040: 0.0014
Loss at step 23050: 0.1701
Loss at step 23060: 0.2850
Loss at step 23070: 0.0008
Loss at step 23080: 2.5596
Loss at step 23090: 0.0608
Loss at step 23100: 0.0316
Loss at step 23110: 0.0316
Loss at step 23120: 0.0018
Loss at step 23130: 0.5924
Loss at step 23140: 0.0722
Loss at step 23150: 0.0001
Loss at step 23160: 0.2472
Loss at step 23170: 0.0048
Loss at step 23180: 0.0056
Loss at step 23190: 0.6384
Loss at step 23200: 1.0937
Loss at step 23210: 0.0362
Loss at step 23220: 0.2447
Loss at step 23230: 0.7943
Loss at step 23240: 0.8488
Loss at step 23250: 0.0704
Loss at step 23260: 0.0003
Loss at step 23270: 0.0001
Loss at step 23280: 0.2446
Loss at step 23290: 0.1893
Loss at step 23300: 0.0000
Loss at step 23310: 0.2453
Loss at step 23320: 0.0650
Loss at step 23330: 0.0089
Loss at step 23340: 0.5657
Loss at step 23350: 0.0015
Loss at step 23360: 0.0542
Loss at step 23370: 0.0041
Loss at step 23380: 0.0392
Loss at step 23390: 0.1222
Loss at step 23400: 0.0037
Loss at step 23410: 0.0188
Loss at step 23420: 0.0027
Loss at step 23430: 0.0000
Loss at step 23440: 0.0026
Loss at step 23450: 0.0000
Loss at step 23460: 0.0642
Loss at step 23470: 0.0389
Loss at step 23480: 0.0050
Loss at step 23490: 0.0000
Loss at step 23500: 0.4220
Loss at step 23510: 0.0313
Loss at step 23520: 0.0014
Loss at step 23530: 0.1011
Loss at step 23540: 0.0009
Loss at step 23550: 0.0639
Loss at step 23560: 0.4581
Loss at step 23570: 0.1451
Loss at step 23580: 0.0000
Loss at step 23590: 0.0034
Loss at step 23600: 0.0031
Loss at step 23610: 0.0014
Loss at step 23620: 0.1284
Loss at step 23630: 0.4186
Loss at step 23640: 0.0055
Loss at step 23650: 0.0010
Loss at step 23660: 0.0000
Loss at step 23670: 0.0013
Loss at step 23680: 0.6006
Loss at step 23690: 0.0009
Loss at step 23700: 3.2771
Loss at step 23710: 0.0345
Loss at step 23720: 0.0018
Loss at step 23730: 0.2914
Loss at step 23740: 0.0001
Loss at step 23750: 0.0022
Loss at step 23760: 0.0568
Loss at step 23770: 0.0000
Loss at step 23780: 0.0003
Loss at step 23790: 0.0002
Loss at step 23800: 0.0013
Loss at step 23810: 0.0002
Loss at step 23820: 0.0017
Loss at step 23830: 0.0001
Loss at step 23840: 0.2235
Loss at step 23850: 0.0015
Loss at step 23860: 0.7488
Loss at step 23870: 0.0012
Loss at step 23880: 0.0000
Loss at step 23890: 0.0002
Loss at step 23900: 0.0924
Loss at step 23910: 0.0324
Loss at step 23920: 2.2024
Loss at step 23930: 0.0000
Loss at step 23940: 0.0001
Loss at step 23950: 0.0103
Loss at step 23960: 0.0613
Loss at step 23970: 0.0001
Loss at step 23980: 0.0152
Loss at step 23990: 0.0181
Loss at step 24000: 0.0765
Loss at step 24010: 0.0026
Loss at step 24020: 0.3956
Loss at step 24030: 0.0069
Loss at step 24040: 0.0283
Loss at step 24050: 0.0011
Loss at step 24060: 0.0019
Loss at step 24070: 0.1944
Loss at step 24080: 0.2205
Loss at step 24090: 0.0003
Loss at step 24100: 0.0000
Loss at step 24110: 0.0060
Loss at step 24120: 0.0001
Loss at step 24130: 0.0016
Loss at step 24140: 0.0232
Loss at step 24150: 0.0000
Loss at step 24160: 0.0003
Loss at step 24170: 0.0354
Loss at step 24180: 1.0950
Loss at step 24190: 0.3802
Loss at step 24200: 0.5128
Loss at step 24210: 0.0000
Loss at step 24220: 0.0093
Loss at step 24230: 2.5823
Loss at step 24240: 0.0014
Loss at step 24250: 0.0000
Loss at step 24260: 0.4827
Loss at step 24270: 0.6216
Loss at step 24280: 0.2826
Loss at step 24290: 0.0005
Loss at step 24300: 0.6134
Loss at step 24310: 0.0000
Loss at step 24320: 0.3390
Loss at step 24330: 0.0047
Loss at step 24340: 1.0418
Loss at step 24350: 0.0002
Loss at step 24360: 0.0464
Loss at step 24370: 0.1456
Loss at step 24380: 0.0007
Loss at step 24390: 0.0012
Loss at step 24400: 0.0035
Loss at step 24410: 0.7517
Loss at step 24420: 0.4195
Loss at step 24430: 0.2685
Loss at step 24440: 0.0310
Loss at step 24450: 0.0725
Loss at step 24460: 0.2018
Loss at step 24470: 0.4513
Loss at step 24480: 2.0576
Loss at step 24490: 0.0011
Loss at step 24500: 0.0489
Loss at step 24510: 0.3955
Loss at step 24520: 0.0004
Loss at step 24530: 0.0002
Loss at step 24540: 0.0959
Loss at step 24550: 0.5300
Loss at step 24560: 0.0964
Loss at step 24570: 0.0660
Loss at step 24580: 0.0762
Loss at step 24590: 1.6762
Loss at step 24600: 0.0011
Loss at step 24610: 0.0199
Loss at step 24620: 0.7734
Loss at step 24630: 0.0008
Loss at step 24640: 0.4663
Loss at step 24650: 0.3229
Loss at step 24660: 0.0000
Loss at step 24670: 0.1651
Loss at step 24680: 0.0628
Loss at step 24690: 0.0559
Loss at step 24700: 0.0665
Loss at step 24710: 0.2543
Loss at step 24720: 0.2586
Loss at step 24730: 0.0718
Loss at step 24740: 0.4510
Loss at step 24750: 0.0000
Loss at step 24760: 0.0793
Loss at step 24770: 0.1638
Loss at step 24780: 0.1919
Loss at step 24790: 0.0579
Loss at step 24800: 0.4304
Loss at step 24810: 0.2990
Loss at step 24820: 0.4356
Loss at step 24830: 0.0476
Loss at step 24840: 0.0009
Loss at step 24850: 0.0001
Loss at step 24860: 1.2109
Loss at step 24870: 0.3518
Loss at step 24880: 0.4715
Loss at step 24890: 0.0000
Loss at step 24900: 0.2821
Loss at step 24910: 0.8682
Loss at step 24920: 0.0000
Loss at step 24930: 0.0002
Loss at step 24940: 0.3696
Loss at step 24950: 0.2516
Loss at step 24960: 2.6438
Loss at step 24970: 0.2941
Loss at step 24980: 0.0534
Loss at step 24990: 0.0428
Loss at step 25000: 0.0000
Loss at step 25010: 0.0063
Loss at step 25020: 0.0054
Loss at step 25030: 0.0000
Loss at step 25040: 0.0051
Loss at step 25050: 0.0013
Loss at step 25060: 0.0579
Loss at step 25070: 0.4096
Loss at step 25080: 0.2610
Loss at step 25090: 0.0036
Loss at step 25100: 0.0008
Loss at step 25110: 0.4706
Loss at step 25120: 0.1341
Loss at step 25130: 0.0000
Loss at step 25140: 0.0000
Loss at step 25150: 0.0209
Loss at step 25160: 0.3181
Loss at step 25170: 0.0423
Loss at step 25180: 0.0001
Loss at step 25190: 0.0706
Loss at step 25200: 0.0002
Loss at step 25210: 0.0234
Loss at step 25220: 0.2677
Loss at step 25230: 1.0040
Loss at step 25240: 0.0002
Loss at step 25250: 0.0023
Loss at step 25260: 0.0019
Loss at step 25270: 0.0013
Loss at step 25280: 0.7445
Loss at step 25290: 0.0000
Loss at step 25300: 0.0000
Loss at step 25310: 0.0067
Loss at step 25320: 0.0000
Loss at step 25330: 0.0015
Loss at step 25340: 0.9460
Loss at step 25350: 0.0001
Loss at step 25360: 0.0110
Loss at step 25370: 0.0026
Loss at step 25380: 0.0011
Loss at step 25390: 0.0237
Loss at step 25400: 0.0314
Loss at step 25410: 0.0786
Loss at step 25420: 0.6364
Loss at step 25430: 1.2909
Loss at step 25440: 0.0001
Loss at step 25450: 0.0004
Loss at step 25460: 0.0000
Loss at step 25470: 0.0027
Loss at step 25480: 0.6904
Loss at step 25490: 0.4713
Loss at step 25500: 0.2163
Loss at step 25510: 0.0749
Loss at step 25520: 0.0056
Loss at step 25530: 0.2092
Loss at step 25540: 0.0000
Loss at step 25550: 0.0002
Loss at step 25560: 0.0198
Loss at step 25570: 0.1792
Loss at step 25580: 0.0005
Loss at step 25590: 0.0019
Loss at step 25600: 0.0034
Loss at step 25610: 0.0001
Loss at step 25620: 0.4555
Loss at step 25630: 0.3828
Loss at step 25640: 0.0001
Loss at step 25650: 0.0117
Loss at step 25660: 0.0362
Loss at step 25670: 0.0034
Loss at step 25680: 0.0050
Loss at step 25690: 0.0010
Loss at step 25700: 0.1923
Loss at step 25710: 0.0001
Loss at step 25720: 1.9943
Loss at step 25730: 0.0000
Loss at step 25740: 0.0349
Loss at step 25750: 0.1200
Loss at step 25760: 0.4622
Loss at step 25770: 0.4415
Loss at step 25780: 0.0006
Loss at step 25790: 0.0000
Loss at step 25800: 0.0012
Loss at step 25810: 0.0008
Loss at step 25820: 0.0008
Loss at step 25830: 0.1743
Loss at step 25840: 0.0000
Loss at step 25850: 1.1776
Loss at step 25860: 0.0164
Loss at step 25870: 0.0011
Loss at step 25880: 0.1745
Loss at step 25890: 1.4901
Loss at step 25900: 0.0001
Loss at step 25910: 0.9038
Loss at step 25920: 0.0034
Loss at step 25930: 0.0000
Loss at step 25940: 0.0073
Loss at step 25950: 0.2950
Loss at step 25960: 0.1884
Loss at step 25970: 0.2860
Loss at step 25980: 0.0224
Loss at step 25990: 0.0223
Loss at step 26000: 0.0162
Loss at step 26010: 2.1507
Loss at step 26020: 0.0552
Loss at step 26030: 0.0001
Loss at step 26040: 0.0037
Loss at step 26050: 0.0001
Loss at step 26060: 0.0702
Loss at step 26070: 0.0223
Loss at step 26080: 0.0000
Loss at step 26090: 0.1141
Loss at step 26100: 0.5679
Loss at step 26110: 0.0015
Loss at step 26120: 0.0015
Loss at step 26130: 0.0001
Loss at step 26140: 0.0217
Loss at step 26150: 0.3195
Loss at step 26160: 0.0016
Loss at step 26170: 0.0186
Loss at step 26180: 0.0109
Loss at step 26190: 0.0024
Loss at step 26200: 0.6107
Loss at step 26210: 0.0162
Loss at step 26220: 0.7290
Loss at step 26230: 0.0007
Loss at step 26240: 0.0415
Loss at step 26250: 0.0011
Loss at step 26260: 0.0265
Loss at step 26270: 0.0467
Loss at step 26280: 0.0014
Loss at step 26290: 0.0000
Loss at step 26300: 0.1465
Loss at step 26310: 0.0005
Loss at step 26320: 0.5289
Loss at step 26330: 0.0147
Loss at step 26340: 0.0002
Loss at step 26350: 0.0861
Loss at step 26360: 0.0000
Loss at step 26370: 0.5140
Loss at step 26380: 0.6002
Loss at step 26390: 0.2587
Loss at step 26400: 0.1866
Loss at step 26410: 0.0003
Loss at step 26420: 0.0319
Loss at step 26430: 0.0143
Loss at step 26440: 0.2548
Loss at step 26450: 0.0102
Loss at step 26460: 0.0001
Loss at step 26470: 0.0010
Loss at step 26480: 0.0565
Loss at step 26490: 0.0938
Loss at step 26500: 0.2408
Loss at step 26510: 0.0002
Loss at step 26520: 0.1092
Loss at step 26530: 0.0551
Loss at step 26540: 0.0001
Loss at step 26550: 0.0069
Loss at step 26560: 0.0000
Loss at step 26570: 0.1270
Loss at step 26580: 0.4136
Loss at step 26590: 0.0000
Loss at step 26600: 0.0001
Loss at step 26610: 0.0003
Loss at step 26620: 0.0140
Loss at step 26630: 0.0000
Loss at step 26640: 0.0049
Loss at step 26650: 0.0049
Loss at step 26660: 0.0068
Loss at step 26670: 0.1477
Loss at step 26680: 0.0011
Loss at step 26690: 0.0288
Loss at step 26700: 0.0331
Loss at step 26710: 0.0002
Loss at step 26720: 0.3203
Loss at step 26730: 0.0000
Loss at step 26740: 0.0139
Loss at step 26750: 0.0000
Loss at step 26760: 0.0046
Loss at step 26770: 0.0001
Loss at step 26780: 0.0007
Loss at step 26790: 0.0009
Loss at step 26800: 0.0781
Loss at step 26810: 0.0005
Loss at step 26820: 0.0001
Loss at step 26830: 0.3738
Loss at step 26840: 0.0012
Loss at step 26850: 0.0062
Loss at step 26860: 0.2826
Loss at step 26870: 0.0000
Loss at step 26880: 0.0677
Loss at step 26890: 0.0020
Loss at step 26900: 0.0000
Loss at step 26910: 0.4963
Loss at step 26920: 0.0189
Loss at step 26930: 0.0000
Loss at step 26940: 0.0007
Loss at step 26950: 0.0069
Loss at step 26960: 0.0000
Loss at step 26970: 0.0013
Loss at step 26980: 0.6007
Loss at step 26990: 0.0000
Loss at step 27000: 0.0001
Loss at step 27010: 0.0001
Loss at step 27020: 0.0000
Loss at step 27030: 0.0458
Loss at step 27040: 0.0004
Loss at step 27050: 1.3071
Loss at step 27060: 0.0002
Loss at step 27070: 2.2685
Loss at step 27080: 0.0002
Loss at step 27090: 1.3962
Loss at step 27100: 0.0507
Loss at step 27110: 0.0089
Loss at step 27120: 0.0000
Loss at step 27130: 0.0026
Loss at step 27140: 1.3512
Loss at step 27150: 0.0004
Loss at step 27160: 0.0002
Loss at step 27170: 0.0001
Loss at step 27180: 0.0039
Loss at step 27190: 0.0000
Loss at step 27200: 0.0311
Loss at step 27210: 0.7517
Loss at step 27220: 0.0001
Loss at step 27230: 0.5744
Loss at step 27240: 0.3423
Loss at step 27250: 0.0001
Loss at step 27260: 0.0019
Loss at step 27270: 0.0430
Loss at step 27280: 0.0000
Loss at step 27290: 0.0000
Loss at step 27300: 0.0146
Loss at step 27310: 0.8379
Loss at step 27320: 0.4136
Loss at step 27330: 1.0912
Loss at step 27340: 0.0448
Loss at step 27350: 0.0191
Loss at step 27360: 0.2070
Loss at step 27370: 0.0011
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.721252, 'precision': [0.876239, 0.413793, 0.518889], 'recall': [0.843494, 0.483321, 0.49355], 'f1': [0.859555, 0.445863, 0.505903]}
{'accuracy': 0.852248, 'precision': 0.518889, 'recall': 0.49355, 'f1': 0.505903, 'WordR': 0.130783}
