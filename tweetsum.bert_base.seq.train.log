Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x788db886a700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 2326, 2073, 2027, 2024, 4039, 2000, 3962, 8757, 1998, 2036, 10821, 2008, 2023, 2003, 2025, 2551, 1012, 4005, 25380, 2005, 1996, 4751, 1998, 2356, 2000, 1040, 2213, 2440, 2171, 1998, 3024, 24471, 2140, 1012, 102, 101, 1045, 10587, 2113, 2339, 28517, 17195, 11597, 2003, 1996, 2069, 21144, 2094, 2299, 1045, 2064, 2424, 2006, 1030, 10630, 2620, 2620, 2620, 102, 101, 1030, 4700, 17465, 16932, 4931, 22093, 999, 2064, 2017, 2425, 2149, 2054, 2406, 2115, 4070, 2003, 2275, 2000, 1029, 2057, 1005, 2222, 4638, 2477, 2041, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 2142, 2163, 999, 102, 101, 1030, 4700, 17465, 16932, 3093, 4625, 2057, 1005, 2222, 2022, 2583, 2000, 2031, 2035, 1997, 2037, 2774, 2574, 1010, 2021, 2045, 1005, 1055, 18558, 2055, 3962, 8757, 4180, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1014, 2072, 2620, 21600, 5714, 14066, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 25430, 4402, 15558, 999, 102, 101, 1030, 4700, 17465, 16932, 4658, 13435, 999, 2017, 2113, 2073, 2000, 2424, 2149, 2065, 2045, 1005, 1055, 2505, 2842, 2017, 2342, 2393, 2007, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1057, 19025, 19666, 2595, 2497, 2480, 2078, 100, 1013, 10381, 102, 101, 1030, 3962, 8757, 16302, 2015, 4067, 2017, 999, 100, 102, 101, 1030, 4700, 17465, 16932, 2017, 1005, 2128, 2467, 6160, 999, 100, 1013, 10381, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.1083
Loss at step 20: 1.0932
Loss at step 30: 1.1125
Loss at step 40: 1.0735
Loss at step 50: 1.0145
Loss at step 60: 1.0981
Loss at step 70: 1.0678
Loss at step 80: 1.0661
Loss at step 90: 1.1532
Loss at step 100: 0.9939
Loss at step 110: 1.0614
Loss at step 120: 1.0334
Loss at step 130: 1.0475
Loss at step 140: 1.1298
Loss at step 150: 1.0862
Loss at step 160: 1.0562
Loss at step 170: 1.0590
Loss at step 180: 1.0542
Loss at step 190: 1.1185
Loss at step 200: 1.0759
Loss at step 210: 1.0588
Loss at step 220: 1.0274
Loss at step 230: 1.1433
Loss at step 240: 0.9840
Loss at step 250: 0.9933
Loss at step 260: 1.0575
Loss at step 270: 1.0707
Loss at step 280: 1.0377
Loss at step 290: 1.0394
Loss at step 300: 1.0529
Loss at step 310: 0.9648
Loss at step 320: 1.1274
Loss at step 330: 1.0659
Loss at step 340: 0.9705
Loss at step 350: 1.0992
Loss at step 360: 1.0319
Loss at step 370: 1.1231
Loss at step 380: 1.0133
Loss at step 390: 1.0610
Loss at step 400: 0.9995
Loss at step 410: 1.1584
Loss at step 420: 1.0841
Loss at step 430: 1.0413
Loss at step 440: 0.9647
Loss at step 450: 1.0143
Loss at step 460: 1.1003
Loss at step 470: 1.0699
Loss at step 480: 0.9553
Loss at step 490: 1.0704
Loss at step 500: 1.1905
Loss at step 510: 0.9205
Loss at step 520: 1.0400
Loss at step 530: 1.0591
Loss at step 540: 1.0173
Loss at step 550: 1.0640
Loss at step 560: 0.9399
Loss at step 570: 0.9966
Loss at step 580: 0.9978
Loss at step 590: 1.0387
Loss at step 600: 0.9914
Loss at step 610: 1.1303
Loss at step 620: 1.0338
Loss at step 630: 1.0259
Loss at step 640: 1.0508
Loss at step 650: 1.0372
Loss at step 660: 0.9775
Loss at step 670: 0.8978
Loss at step 680: 0.9257
Loss at step 690: 1.1804
Loss at step 700: 1.0458
Loss at step 710: 1.0440
Loss at step 720: 1.0017
Loss at step 730: 0.9829
Loss at step 740: 0.9201
Loss at step 750: 0.9290
Loss at step 760: 0.8725
Loss at step 770: 0.9571
Loss at step 780: 1.1073
Loss at step 790: 0.9619
Loss at step 800: 1.0013
Loss at step 810: 0.9677
Loss at step 820: 0.9339
Loss at step 830: 0.9173
Loss at step 840: 0.9793
Loss at step 850: 0.9324
Loss at step 860: 0.8693
Loss at step 870: 0.9570
Loss at step 880: 1.0549
Loss at step 890: 1.0584
Loss at step 900: 0.9743
Loss at step 910: 1.0950
Loss at step 920: 0.9906
Loss at step 930: 0.8720
Loss at step 940: 1.0735
Loss at step 950: 1.0296
Loss at step 960: 0.8163
Loss at step 970: 1.2438
Loss at step 980: 0.8362
Loss at step 990: 0.9211
Loss at step 1000: 0.9181
Loss at step 1010: 0.9432
Loss at step 1020: 0.8418
Loss at step 1030: 0.8571
Loss at step 1040: 1.0083
Loss at step 1050: 0.7625
Loss at step 1060: 1.0713
Loss at step 1070: 0.7419
Loss at step 1080: 1.0078
Loss at step 1090: 0.8996
Loss at step 1100: 0.7956
Loss at step 1110: 0.8818
Loss at step 1120: 0.8238
Loss at step 1130: 0.8786
Loss at step 1140: 0.9024
Loss at step 1150: 0.8589
Loss at step 1160: 0.9435
Loss at step 1170: 0.9862
Loss at step 1180: 0.6844
Loss at step 1190: 1.0744
Loss at step 1200: 0.9123
Loss at step 1210: 0.8358
Loss at step 1220: 0.8971
Loss at step 1230: 0.8110
Loss at step 1240: 0.7007
Loss at step 1250: 0.9284
Loss at step 1260: 0.7412
Loss at step 1270: 0.9322
Loss at step 1280: 0.7578
Loss at step 1290: 1.0022
Loss at step 1300: 0.9541
Loss at step 1310: 0.7462
Loss at step 1320: 1.0453
Loss at step 1330: 0.6851
Loss at step 1340: 0.7567
Loss at step 1350: 0.7437
Loss at step 1360: 0.7668
Loss at step 1370: 0.8359
Loss at step 1380: 0.6655
Loss at step 1390: 0.9630
Loss at step 1400: 0.8909
Loss at step 1410: 0.9391
Loss at step 1420: 0.7181
Loss at step 1430: 0.6789
Loss at step 1440: 0.7774
Loss at step 1450: 0.7910
Loss at step 1460: 0.7458
Loss at step 1470: 0.7240
Loss at step 1480: 0.7510
Loss at step 1490: 1.0467
Loss at step 1500: 0.9841
Loss at step 1510: 0.7913
Loss at step 1520: 0.7394
Loss at step 1530: 0.6938
Loss at step 1540: 0.8084
Loss at step 1550: 0.6220
Loss at step 1560: 0.7256
Loss at step 1570: 0.9711
Loss at step 1580: 0.5448
Loss at step 1590: 0.6355
Loss at step 1600: 0.7500
Loss at step 1610: 0.6526
Loss at step 1620: 0.8227
Loss at step 1630: 0.7060
Loss at step 1640: 0.5534
Loss at step 1650: 0.7655
Loss at step 1660: 0.6628
Loss at step 1670: 0.5762
Loss at step 1680: 0.7017
Loss at step 1690: 0.5595
Loss at step 1700: 0.5160
Loss at step 1710: 0.6817
Loss at step 1720: 0.5461
Loss at step 1730: 0.7410
Loss at step 1740: 0.9177
Loss at step 1750: 0.5507
Loss at step 1760: 0.6725
Loss at step 1770: 0.4740
Loss at step 1780: 0.5651
Loss at step 1790: 0.5486
Loss at step 1800: 0.3104
Loss at step 1810: 0.8387
Loss at step 1820: 0.4399
Loss at step 1830: 0.7888
Loss at step 1840: 0.4063
Loss at step 1850: 0.6136
Loss at step 1860: 0.6764
Loss at step 1870: 0.3852
Loss at step 1880: 0.6967
Loss at step 1890: 0.5084
Loss at step 1900: 0.3286
Loss at step 1910: 0.5102
Loss at step 1920: 0.4812
Loss at step 1930: 0.4079
Loss at step 1940: 0.6466
Loss at step 1950: 0.4587
Loss at step 1960: 0.5683
Loss at step 1970: 0.3694
Loss at step 1980: 0.3797
Loss at step 1990: 0.5265
Loss at step 2000: 0.1617
Loss at step 2010: 0.7963
Loss at step 2020: 0.7657
Loss at step 2030: 0.4585
Loss at step 2040: 0.2360
Loss at step 2050: 0.3460
Loss at step 2060: 0.5251
Loss at step 2070: 0.4981
Loss at step 2080: 0.9458
Loss at step 2090: 0.4517
Loss at step 2100: 0.6344
Loss at step 2110: 0.4474
Loss at step 2120: 0.2322
Loss at step 2130: 0.3179
Loss at step 2140: 0.6871
Loss at step 2150: 0.1407
Loss at step 2160: 0.3740
Loss at step 2170: 0.4330
Loss at step 2180: 0.3549
Loss at step 2190: 0.3970
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 8
  Total eval batch size = 8
{'accuracy': 0.541182, 'precision': [0.672368, 0.485664, 0.499247], 'recall': [0.515574, 0.676171, 0.385017], 'f1': [0.583623, 0.565299, 0.434754]}
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 1824 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 3277, 2027, 2024, 2383, 2007, 5647, 2326, 1012, 4005, 5176, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1998, 5176, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1012, 4005, 5176, 2000, 2224, 1996, 4957, 2917, 2000, 4638, 2006, 1996, 19575, 1012, 102, 101, 1030, 10630, 2683, 21057, 2023, 3277, 1045, 1005, 1049, 2383, 2003, 2007, 5647, 2326, 102, 101, 1030, 21417, 2692, 2575, 2054, 1005, 1055, 2183, 2006, 2007, 2115, 5647, 2326, 1029, 2057, 2097, 2022, 3407, 2000, 6509, 1012, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2042, 2667, 2000, 4651, 2326, 2144, 1996, 13293, 1997, 2197, 3204, 1012, 1012, 1012, 2018, 2019, 6098, 2005, 2048, 2420, 3283, 1012, 1012, 1012, 6627, 2196, 1006, 1015, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2152, 2203, 5310, 1012, 1012, 1012, 1996, 2069, 3114, 2026, 2326, 2347, 1005, 1056, 6880, 7237, 2138, 1045, 2001, 2853, 15453, 28518, 2102, 4274, 2085, 1045, 2031, 1006, 1017, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 3662, 1012, 1012, 1012, 2356, 2000, 2022, 4654, 5669, 17572, 2021, 2000, 2053, 24608, 1045, 2145, 2031, 2000, 3524, 2127, 9857, 2009, 3849, 1012, 1012, 1012, 1045, 1005, 1049, 1037, 7858, 2099, 1998, 1037, 1006, 1016, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 8013, 2326, 16360, 2015, 2008, 4297, 25377, 12870, 5897, 2003, 22173, 2033, 2051, 1045, 2342, 2005, 2678, 9260, 1012, 1012, 1012, 2025, 2000, 5254, 10047, 3517, 2000, 1006, 1020, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 6627, 1010, 4738, 1999, 7318, 2770, 2013, 3105, 3650, 1012, 1012, 1012, 2061, 2085, 1045, 1005, 1049, 2302, 2326, 2349, 2000, 2431, 3435, 6627, 2015, 1998, 2431, 3435, 1006, 1019, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1998, 1045, 1005, 1049, 1037, 7378, 27019, 2102, 1006, 1018, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 3477, 2026, 3021, 2006, 2051, 2021, 1045, 2031, 2000, 5138, 2397, 2326, 2013, 1996, 2194, 6135, 21873, 1012, 1012, 1012, 2821, 1998, 2106, 1045, 5254, 1006, 1021, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 1045, 1005, 1049, 1037, 29525, 2008, 2987, 1005, 1056, 4374, 2510, 100, 1006, 1022, 1013, 1022, 1007, 102, 101, 1030, 21417, 2692, 2575, 2024, 2017, 7161, 2000, 12200, 2115, 3177, 1029, 2030, 2017, 2024, 2025, 2893, 1996, 3177, 2115, 2024, 9347, 2098, 2005, 1029, 102, 101, 1030, 21417, 2692, 2575, 2057, 3305, 2115, 5142, 2021, 2070, 3941, 2089, 2031, 2000, 2022, 9725, 5834, 2006, 2115, 5227, 1012, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2008, 2987, 1005, 1056, 8016, 4496, 4863, 2339, 1030, 10630, 2581, 17788, 2987, 1005, 1056, 2031, 1037, 11476, 8244, 19575, 1012, 1012, 1012, 2035, 1045, 1005, 2310, 2042, 3615, 2000, 1006, 1015, 1013, 1016, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 0, 1, 1, 0, 1, 0, 2, 2, 0, 1]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1099
Loss at step 10: 1.0914
Loss at step 20: 1.0812
Loss at step 30: 1.1183
Loss at step 40: 0.9887
Loss at step 50: 0.9798
Loss at step 60: 1.0221
Loss at step 70: 1.1047
Loss at step 80: 1.0524
Loss at step 90: 1.0634
Loss at step 100: 0.9819
Loss at step 110: 1.0695
Loss at step 120: 1.0588
Loss at step 130: 0.9505
Loss at step 140: 1.0545
Loss at step 150: 1.0996
Loss at step 160: 0.9758
Loss at step 170: 1.0214
Loss at step 180: 0.8786
Loss at step 190: 0.9112
Loss at step 200: 0.9086
Loss at step 210: 1.0132
Loss at step 220: 0.8780
Loss at step 230: 0.8928
Loss at step 240: 0.9995
Loss at step 250: 0.9248
Loss at step 260: 1.0312
Loss at step 270: 0.9501
Loss at step 280: 0.8495
Loss at step 290: 0.8080
Loss at step 300: 0.8017
Loss at step 310: 0.7430
Loss at step 320: 0.7976
Loss at step 330: 0.8879
Loss at step 340: 0.7524
Loss at step 350: 0.7531
Loss at step 360: 0.7565
Loss at step 370: 0.7362
Loss at step 380: 0.8703
Loss at step 390: 0.7003
Loss at step 400: 0.8561
Loss at step 410: 0.7853
Loss at step 420: 0.8973
Loss at step 430: 0.8574
Loss at step 440: 0.7167
Loss at step 450: 0.6670
Loss at step 460: 0.7111
Loss at step 470: 0.5354
Loss at step 480: 0.6710
Loss at step 490: 0.8211
Loss at step 500: 0.6269
Loss at step 510: 0.6623
Loss at step 520: 0.6861
Loss at step 530: 0.7089
Loss at step 540: 0.7247
Loss at step 550: 0.6300
Loss at step 560: 0.5546
Loss at step 570: 0.6087
Loss at step 580: 0.5960
Loss at step 590: 0.5774
Loss at step 600: 0.5263
Loss at step 610: 0.5982
Loss at step 620: 0.6016
Loss at step 630: 0.4893
Loss at step 640: 0.5936
Loss at step 650: 0.5284
Loss at step 660: 0.6251
Loss at step 670: 0.5296
Loss at step 680: 0.5870
Loss at step 690: 0.5671
Loss at step 700: 0.5277
Loss at step 710: 0.5791
Loss at step 720: 0.4397
Loss at step 730: 0.4694
Loss at step 740: 0.5209
Loss at step 750: 0.4508
Loss at step 760: 0.5287
Loss at step 770: 0.4558
Loss at step 780: 0.6961
Loss at step 790: 0.5518
Loss at step 800: 0.4759
Loss at step 810: 0.3002
Loss at step 820: 0.3394
Loss at step 830: 0.5345
Loss at step 840: 0.4461
Loss at step 850: 0.2580
Loss at step 860: 0.4706
Loss at step 870: 0.3892
Loss at step 880: 0.4479
Loss at step 890: 0.4771
Loss at step 900: 0.4168
Loss at step 910: 0.4862
Loss at step 920: 0.4493
Loss at step 930: 0.3009
Loss at step 940: 0.4265
Loss at step 950: 0.4280
Loss at step 960: 0.3421
Loss at step 970: 0.4862
Loss at step 980: 0.4091
Loss at step 990: 0.3657
Loss at step 1000: 0.2702
Loss at step 1010: 0.4384
Loss at step 1020: 0.4525
Loss at step 1030: 0.4798
Loss at step 1040: 0.4353
Loss at step 1050: 0.2710
Loss at step 1060: 0.5518
Loss at step 1070: 0.3002
Loss at step 1080: 0.2929
Loss at step 1090: 0.3759
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.568838, 'precision': [0.667462, 0.521246, 0.526223], 'recall': [0.573361, 0.599593, 0.518583], 'f1': [0.616843, 0.557681, 0.522375]}
{'accuracy': 0.753211, 'precision': 0.526223, 'recall': 0.518583, 'f1': 0.522375, 'WordR': 0.33577}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x73fa2c58da60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7341a078aa60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x756d02988a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1099
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 1824 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 3277, 2027, 2024, 2383, 2007, 5647, 2326, 1012, 4005, 5176, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1998, 5176, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1012, 4005, 5176, 2000, 2224, 1996, 4957, 2917, 2000, 4638, 2006, 1996, 19575, 1012, 102, 101, 1030, 10630, 2683, 21057, 2023, 3277, 1045, 1005, 1049, 2383, 2003, 2007, 5647, 2326, 102, 101, 1030, 21417, 2692, 2575, 2054, 1005, 1055, 2183, 2006, 2007, 2115, 5647, 2326, 1029, 2057, 2097, 2022, 3407, 2000, 6509, 1012, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2042, 2667, 2000, 4651, 2326, 2144, 1996, 13293, 1997, 2197, 3204, 1012, 1012, 1012, 2018, 2019, 6098, 2005, 2048, 2420, 3283, 1012, 1012, 1012, 6627, 2196, 1006, 1015, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2152, 2203, 5310, 1012, 1012, 1012, 1996, 2069, 3114, 2026, 2326, 2347, 1005, 1056, 6880, 7237, 2138, 1045, 2001, 2853, 15453, 28518, 2102, 4274, 2085, 1045, 2031, 1006, 1017, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 3662, 1012, 1012, 1012, 2356, 2000, 2022, 4654, 5669, 17572, 2021, 2000, 2053, 24608, 1045, 2145, 2031, 2000, 3524, 2127, 9857, 2009, 3849, 1012, 1012, 1012, 1045, 1005, 1049, 1037, 7858, 2099, 1998, 1037, 1006, 1016, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 8013, 2326, 16360, 2015, 2008, 4297, 25377, 12870, 5897, 2003, 22173, 2033, 2051, 1045, 2342, 2005, 2678, 9260, 1012, 1012, 1012, 2025, 2000, 5254, 10047, 3517, 2000, 1006, 1020, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 6627, 1010, 4738, 1999, 7318, 2770, 2013, 3105, 3650, 1012, 1012, 1012, 2061, 2085, 1045, 1005, 1049, 2302, 2326, 2349, 2000, 2431, 3435, 6627, 2015, 1998, 2431, 3435, 1006, 1019, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2000, 3524, 2006, 1037, 6627, 1998, 3477, 1037, 7408, 2043, 2035, 1996, 14666, 1998, 3941, 2005, 8272, 2003, 2182, 1998, 1045, 1005, 1049, 1037, 7378, 27019, 2102, 1006, 1018, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 3477, 2026, 3021, 2006, 2051, 2021, 1045, 2031, 2000, 5138, 2397, 2326, 2013, 1996, 2194, 6135, 21873, 1012, 1012, 1012, 2821, 1998, 2106, 1045, 5254, 1006, 1021, 1013, 1022, 1007, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 1045, 1005, 1049, 1037, 29525, 2008, 2987, 1005, 1056, 4374, 2510, 100, 1006, 1022, 1013, 1022, 1007, 102, 101, 1030, 21417, 2692, 2575, 2024, 2017, 7161, 2000, 12200, 2115, 3177, 1029, 2030, 2017, 2024, 2025, 2893, 1996, 3177, 2115, 2024, 9347, 2098, 2005, 1029, 102, 101, 1030, 21417, 2692, 2575, 2057, 3305, 2115, 5142, 2021, 2070, 3941, 2089, 2031, 2000, 2022, 9725, 5834, 2006, 2115, 5227, 1012, 102, 101, 1030, 2310, 21885, 5644, 6279, 6442, 2008, 2987, 1005, 1056, 8016, 4496, 4863, 2339, 1030, 10630, 2581, 17788, 2987, 1005, 1056, 2031, 1037, 11476, 8244, 19575, 1012, 1012, 1012, 2035, 1045, 1005, 2310, 2042, 3615, 2000, 1006, 1015, 1013, 1016, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 0, 1, 1, 0, 1, 0, 2, 2, 0, 1]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1099
Loss at step 10: 1.0914
Loss at step 20: 1.0812
Loss at step 30: 1.1182
Loss at step 40: 0.9885
Loss at step 50: 0.9796
Loss at step 60: 1.0215
Loss at step 70: 1.1042
Loss at step 80: 1.0520
Loss at step 90: 1.0650
Loss at step 100: 0.9845
Loss at step 110: 1.0673
Loss at step 120: 1.0553
Loss at step 130: 0.9616
Loss at step 140: 1.0475
Loss at step 150: 1.0999
Loss at step 160: 0.9694
Loss at step 170: 1.0042
Loss at step 180: 0.8673
Loss at step 190: 0.9309
Loss at step 200: 0.9086
Loss at step 210: 1.0055
Loss at step 220: 0.8817
Loss at step 230: 0.8404
Loss at step 240: 1.0288
Loss at step 250: 0.9440
Loss at step 260: 1.0688
Loss at step 270: 0.9204
Loss at step 280: 0.8821
Loss at step 290: 0.7807
Loss at step 300: 0.8100
Loss at step 310: 0.7864
Loss at step 320: 0.7999
Loss at step 330: 0.8807
Loss at step 340: 0.7805
Loss at step 350: 0.7506
Loss at step 360: 0.7441
Loss at step 370: 0.7338
Loss at step 380: 0.9126
Loss at step 390: 0.7134
Loss at step 400: 0.8235
Loss at step 410: 0.8196
Loss at step 420: 0.7840
Loss at step 430: 0.8558
Loss at step 440: 0.7009
Loss at step 450: 0.7183
Loss at step 460: 0.6880
Loss at step 470: 0.5313
Loss at step 480: 0.6772
Loss at step 490: 0.8039
Loss at step 500: 0.6243
Loss at step 510: 0.6221
Loss at step 520: 0.6292
Loss at step 530: 0.7313
Loss at step 540: 0.7387
Loss at step 550: 0.6211
Loss at step 560: 0.5511
Loss at step 570: 0.5959
Loss at step 580: 0.5227
Loss at step 590: 0.5286
Loss at step 600: 0.5651
Loss at step 610: 0.6449
Loss at step 620: 0.6227
Loss at step 630: 0.5280
Loss at step 640: 0.5353
Loss at step 650: 0.4978
Loss at step 660: 0.6695
Loss at step 670: 0.5044
Loss at step 680: 0.6110
Loss at step 690: 0.6641
Loss at step 700: 0.5566
Loss at step 710: 0.5766
Loss at step 720: 0.4599
Loss at step 730: 0.4835
Loss at step 740: 0.5412
Loss at step 750: 0.5043
Loss at step 760: 0.5002
Loss at step 770: 0.4420
Loss at step 780: 0.7257
Loss at step 790: 0.5590
Loss at step 800: 0.4699
Loss at step 810: 0.3569
Loss at step 820: 0.3848
Loss at step 830: 0.5312
Loss at step 840: 0.3654
Loss at step 850: 0.2943
Loss at step 860: 0.5807
Loss at step 870: 0.3692
Loss at step 880: 0.4758
Loss at step 890: 0.4125
Loss at step 900: 0.4560
Loss at step 910: 0.5166
Loss at step 920: 0.4965
Loss at step 930: 0.3518
Loss at step 940: 0.4121
Loss at step 950: 0.5172
Loss at step 960: 0.3270
Loss at step 970: 0.5384
Loss at step 980: 0.4824
Loss at step 990: 0.3447
Loss at step 1000: 0.2738
Loss at step 1010: 0.4581
Loss at step 1020: 0.4813
Loss at step 1030: 0.4732
Loss at step 1040: 0.4132
Loss at step 1050: 0.3320
Loss at step 1060: 0.5826
Loss at step 1070: 0.2933
Loss at step 1080: 0.2868
Loss at step 1090: 0.2987
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.577452, 'precision': [0.668216, 0.531206, 0.538128], 'recall': [0.589344, 0.617108, 0.504065], 'f1': [0.626307, 0.570944, 0.52054]}
{'accuracy': 0.75835, 'precision': 0.538128, 'recall': 0.504065, 'f1': 0.52054, 'WordR': 0.176186}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7103be189700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x78716038c700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7009f2988700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x70e1e758b790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x75e5c5789700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x75ff9c188700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 400: 0.7902
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Loss at step 410: 0.7585
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 420: 0.6466
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 430: 0.6236
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 440: 0.5841
Loss at step 60: 1.0698
Loss at step 450: 0.6412
Loss at step 70: 1.0458
Loss at step 460: 0.6479
Loss at step 80: 1.0910
Loss at step 470: 0.6365
Loss at step 90: 1.0312
Loss at step 480: 0.5898
Loss at step 100: 1.0636
Loss at step 490: 0.6583
Loss at step 110: 1.0206
Loss at step 500: 0.5881
Loss at step 120: 1.0514
Loss at step 510: 0.5413
Loss at step 130: 1.0237
Loss at step 520: 0.6415
Loss at step 140: 1.0486
Loss at step 530: 0.5342
Loss at step 150: 1.0081
Loss at step 540: 0.6536
Loss at step 160: 1.0254
Loss at step 550: 0.4453
Loss at step 170: 0.9624
Loss at step 180: 0.9511
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
Loss at step 190: 0.9652
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
Loss at step 200: 0.9351
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7e898738c700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
num_proc must be <= 110. Reducing num_proc to 110 for dataset of size 110.
Sample 8346 of the training set: {'input_ids': [101, 8013, 2003, 17949, 2055, 1996, 8536, 1997, 1996, 3345, 2073, 2027, 2024, 4039, 2000, 25416, 8630, 2005, 1017, 1012, 4005, 2163, 2008, 2023, 2097, 2022, 2741, 2068, 1012, 102, 101, 2178, 2654, 2781, 13842, 2006, 1996, 3042, 2000, 1030, 9733, 16001, 2361, 1012, 1022, 2041, 1997, 2184, 4449, 2023, 3204, 2025, 2357, 2039, 2279, 2154, 1012, 2296, 2051, 1523, 2057, 2097, 2191, 2469, 2009, 2987, 1521, 1056, 4148, 2153, 1524, 1012, 2092, 2025, 4902, 2009, 2180, 1521, 1056, 2004, 2025, 2183, 2000, 20687, 1012, 18313, 2026, 2051, 1004, 23713, 1025, 2769, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1049, 3374, 2005, 1996, 3532, 3325, 999, 2000, 12210, 1010, 2031, 2057, 4771, 1996, 6959, 5246, 3491, 2012, 4638, 5833, 1998, 4484, 3081, 1041, 1011, 5653, 1029, 2043, 2017, 3764, 2007, 2149, 1010, 2054, 7047, 2030, 12369, 2020, 2057, 2583, 2000, 3749, 1029, 1034, 24829, 102, 101, 1030, 9733, 16001, 2361, 2748, 1045, 1521, 1049, 1037, 3539, 2266, 7079, 2005, 1996, 2326, 1998, 1996, 6959, 4062, 2467, 3658, 1998, 2758, 6959, 4692, 1012, 2002, 2758, 2009, 1521, 1055, 2138, 9733, 2986, 2032, 2005, 12771, 2397, 2061, 2009, 1521, 1055, 2488, 2005, 2032, 2000, 2360, 2002, 2699, 2000, 8116, 2043, 2002, 2064, 1521, 1056, 2131, 2461, 2438, 11596, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 5705, 2620, 2094, 2243, 3501, 3501, 2692, 2615, 102, 101, 1030, 9733, 16001, 2361, 1045, 2031, 2018, 2619, 1999, 2026, 2160, 2035, 2154, 1012, 2053, 6959, 4692, 1012, 1045, 2342, 10760, 6959, 2005, 4826, 1998, 2085, 2009, 1521, 1055, 2025, 2182, 1998, 9733, 1013, 2037, 6959, 4256, 2003, 10882, 23200, 2055, 2667, 2000, 8116, 1012, 2023, 2038, 3047, 1022, 2335, 1012, 2017, 3253, 27813, 4923, 1010, 2049, 3465, 2033, 2062, 2084, 2008, 1999, 3042, 4455, 1012, 102, 101, 1030, 24568, 17788, 2575, 1045, 1005, 1040, 2066, 1037, 2266, 1997, 2256, 2136, 2000, 2298, 2046, 2023, 2007, 2017, 1012, 3531, 6039, 1999, 1996, 4751, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 18816, 2546, 2620, 11253, 2226, 2581, 2243, 3501, 1998, 2057, 2097, 2022, 1999, 3543, 1012, 1034, 11047, 102, 101, 1030, 9733, 16001, 2361, 2589, 1012, 2123, 1521, 1056, 2215, 2000, 2022, 2409, 1523, 2057, 2097, 2079, 2673, 2000, 2191, 2009, 2025, 4148, 2153, 1524, 2004, 2008, 2038, 2042, 10003, 2000, 2022, 29132, 102, 101, 1030, 9733, 16001, 2361, 1998, 2145, 3403, 2750, 2009, 3038, 2260, 2847, 3433, 2051, 1998, 8110, 2009, 1999, 2322, 8093, 2015, 3283, 1012, 2013, 2919, 2000, 4788, 1012, 1030, 12963, 24434, 2509, 102, 101, 1030, 24568, 17788, 2575, 7632, 1010, 1045, 2064, 12210, 2057, 2031, 2363, 2115, 4751, 1998, 2097, 3967, 2017, 3859, 1012, 1034, 1038, 2480, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 0, 2, 1, 2, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 550
Loss at step 10: 1.0742
Loss at step 20: 1.0981
Loss at step 30: 1.0846
Loss at step 40: 1.0846
Loss at step 50: 1.0673
Loss at step 60: 1.0698
Loss at step 70: 1.0458
Loss at step 80: 1.0910
Loss at step 90: 1.0312
Loss at step 100: 1.0636
Loss at step 110: 1.0206
Loss at step 120: 1.0514
Loss at step 130: 1.0237
Loss at step 140: 1.0486
Loss at step 150: 1.0081
Loss at step 160: 1.0254
Loss at step 170: 0.9624
Loss at step 180: 0.9511
Loss at step 190: 0.9652
Loss at step 200: 0.9351
Loss at step 210: 0.9453
Loss at step 220: 0.8748
Loss at step 230: 0.8442
Loss at step 240: 0.8123
Loss at step 250: 0.8946
Loss at step 260: 0.8643
Loss at step 270: 0.8444
Loss at step 280: 0.8338
Loss at step 290: 0.8017
Loss at step 300: 0.7160
Loss at step 310: 0.8146
Loss at step 320: 0.7445
Loss at step 330: 0.7865
Loss at step 340: 0.7488
Loss at step 350: 0.7065
Loss at step 360: 0.6773
Loss at step 370: 0.7429
Loss at step 380: 0.7720
Loss at step 390: 0.8036
Loss at step 400: 0.7902
Loss at step 410: 0.7585
Loss at step 420: 0.6466
Loss at step 430: 0.6236
Loss at step 440: 0.5841
Loss at step 450: 0.6412
Loss at step 460: 0.6479
Loss at step 470: 0.6365
Loss at step 480: 0.5898
Loss at step 490: 0.6583
Loss at step 500: 0.5881
Loss at step 510: 0.5413
Loss at step 520: 0.6415
Loss at step 530: 0.5342
Loss at step 540: 0.6536
Loss at step 550: 0.4453
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 32
  Total eval batch size = 32
{'accuracy': 0.568384, 'precision': [0.705945, 0.516805, 0.508102], 'recall': [0.545082, 0.632587, 0.509872], 'f1': [0.615171, 0.568864, 0.508986]}
{'accuracy': 0.743993, 'precision': 0.508102, 'recall': 0.509872, 'f1': 0.508986, 'WordR': 0.018182}
