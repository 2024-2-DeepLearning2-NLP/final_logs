Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 107997
Loss at step 10: 0.6331
Loss at step 20: 0.6378
Loss at step 30: 0.5645
Loss at step 40: 0.6584
Loss at step 50: 0.3537
Loss at step 60: 0.6158
Loss at step 70: 0.3305
Loss at step 80: 0.4975
Loss at step 90: 0.8160
Loss at step 100: 0.5706
Loss at step 110: 0.7857
Loss at step 120: 0.2467
Loss at step 130: 0.5443
Loss at step 140: 0.7278
Loss at step 150: 0.5303
Loss at step 160: 0.3747
Loss at step 170: 1.0521
Loss at step 180: 1.4030
Loss at step 190: 0.4431
Loss at step 200: 0.6521
Loss at step 210: 0.4507
Loss at step 220: 0.4618
Loss at step 230: 0.3170
Loss at step 240: 0.2985
Loss at step 250: 0.8836
Loss at step 260: 0.3540
Loss at step 270: 0.3407
Loss at step 280: 0.2992
Loss at step 290: 0.5603
Loss at step 300: 0.7712
Loss at step 310: 0.3802
Loss at step 320: 0.5924
Loss at step 330: 0.3580
Loss at step 340: 0.3232
Loss at step 350: 0.4644
Loss at step 360: 0.4555
Loss at step 370: 0.5716
Loss at step 380: 0.3766
Loss at step 390: 0.5764
Loss at step 400: 0.7534
Loss at step 410: 0.7180
Loss at step 420: 0.4636
Loss at step 430: 0.1887
Loss at step 440: 0.3135
Loss at step 450: 0.4033
Loss at step 460: 0.4662
Loss at step 470: 0.2624
Loss at step 480: 0.5996
Loss at step 490: 0.4919
Loss at step 500: 0.5695
Loss at step 510: 0.4580
Loss at step 520: 0.3932
Loss at step 530: 0.8038
Loss at step 540: 0.8029
Loss at step 550: 0.4467
Loss at step 560: 0.6183
Loss at step 570: 0.6059
Loss at step 580: 0.7677
Loss at step 590: 0.5172
Loss at step 600: 0.2261
Loss at step 610: 0.4722
Loss at step 620: 0.3687
Loss at step 630: 0.1759
Loss at step 640: 0.6288
Loss at step 650: 0.4572
Loss at step 660: 0.3724
Loss at step 670: 0.8187
Loss at step 680: 0.4601
Loss at step 690: 0.3738
Loss at step 700: 0.3721
Loss at step 710: 0.5181
Loss at step 720: 0.5963
Loss at step 730: 0.3596
Loss at step 740: 0.1320
Loss at step 750: 0.5401
Loss at step 760: 0.4429
Loss at step 770: 0.3184
Loss at step 780: 0.3729
Loss at step 790: 0.4004
Loss at step 800: 0.5468
Loss at step 810: 0.6106
Loss at step 820: 0.3147
Loss at step 830: 0.3489
Loss at step 840: 0.8929
Loss at step 850: 0.4916
Loss at step 860: 0.4001
Loss at step 870: 0.2822
Loss at step 880: 0.6228
Loss at step 890: 0.2898
Loss at step 900: 0.3415
Loss at step 910: 0.3086
Loss at step 920: 0.4107
Loss at step 930: 0.1643
Loss at step 940: 0.4958
Loss at step 950: 0.4872
Loss at step 960: 0.2005
Loss at step 970: 0.4060
Loss at step 980: 0.5352
Loss at step 990: 0.4236
Loss at step 1000: 0.4190
Loss at step 1010: 0.4972
Loss at step 1020: 0.6461
Loss at step 1030: 0.3603
Loss at step 1040: 0.2734
Loss at step 1050: 0.4315
Loss at step 1060: 0.4245
Loss at step 1070: 0.5037
Loss at step 1080: 0.4160
Loss at step 1090: 0.3985
Loss at step 1100: 0.4319
Loss at step 1110: 0.1297
Loss at step 1120: 0.2579
Loss at step 1130: 0.1275
Loss at step 1140: 0.3484
Loss at step 1150: 0.2686
Loss at step 1160: 0.2221
Loss at step 1170: 0.2451
Loss at step 1180: 0.2412
Loss at step 1190: 0.1171
Loss at step 1200: 0.3768
Loss at step 1210: 0.6752
Loss at step 1220: 0.2635
Loss at step 1230: 0.4822
Loss at step 1240: 0.3639
Loss at step 1250: 0.4751
Loss at step 1260: 0.2550
Loss at step 1270: 0.4451
Loss at step 1280: 0.5886
Loss at step 1290: 0.2150
Loss at step 1300: 0.4472
Loss at step 1310: 0.4047
Loss at step 1320: 0.3394
Loss at step 1330: 0.4356
Loss at step 1340: 0.7764
Loss at step 1350: 0.7463
Loss at step 1360: 0.2266
Loss at step 1370: 0.1766
Loss at step 1380: 0.5142
Loss at step 1390: 0.3649
Loss at step 1400: 0.6089
Loss at step 1410: 0.2498
Loss at step 1420: 0.3200
Loss at step 1430: 0.5153
Loss at step 1440: 0.3597
Loss at step 1450: 0.4704
Loss at step 1460: 0.8216
Loss at step 1470: 0.4856
Loss at step 1480: 0.5098
Loss at step 1490: 0.7359
Loss at step 1500: 0.4211
Loss at step 1510: 0.2970
Loss at step 1520: 0.2057
Loss at step 1530: 0.0993
Loss at step 1540: 0.2277
Loss at step 1550: 0.8121
Loss at step 1560: 0.3003
Loss at step 1570: 0.5055
Loss at step 1580: 0.6812
Loss at step 1590: 0.2198
Loss at step 1600: 0.8978
Loss at step 1610: 0.3196
Loss at step 1620: 0.3604
Loss at step 1630: 0.3763
Loss at step 1640: 0.3523
Loss at step 1650: 0.5596
Loss at step 1660: 0.4586
Loss at step 1670: 0.4052
Loss at step 1680: 0.3860
Loss at step 1690: 0.8062
Loss at step 1700: 0.1343
Loss at step 1710: 0.1276
Loss at step 1720: 0.5706
Loss at step 1730: 0.6086
Loss at step 1740: 0.6717
Loss at step 1750: 0.5213
Loss at step 1760: 0.6494
Loss at step 1770: 0.2750
Loss at step 1780: 0.5040
Loss at step 1790: 1.0718
Loss at step 1800: 0.5302
Loss at step 1810: 0.3763
Loss at step 1820: 0.4757
Loss at step 1830: 0.3693
Loss at step 1840: 0.1402
Loss at step 1850: 0.4937
Loss at step 1860: 0.5620
Loss at step 1870: 0.6437
Loss at step 1880: 0.3323
Loss at step 1890: 0.0981
Loss at step 1900: 0.4804
Loss at step 1910: 0.5460
Loss at step 1920: 0.8167
Loss at step 1930: 0.6923
Loss at step 1940: 0.1716
Loss at step 1950: 0.3015
Loss at step 1960: 0.5007
Loss at step 1970: 0.2653
Loss at step 1980: 0.2751
Loss at step 1990: 0.3337
Loss at step 2000: 0.6581
Loss at step 2010: 0.2271
Loss at step 2020: 0.4531
Loss at step 2030: 0.5432
Loss at step 2040: 0.2655
Loss at step 2050: 0.5906
Loss at step 2060: 0.7554
Loss at step 2070: 1.0333
Loss at step 2080: 0.4201
Loss at step 2090: 0.3278
Loss at step 2100: 0.3862
Loss at step 2110: 0.3217
Loss at step 2120: 0.2202
Loss at step 2130: 0.4427
Loss at step 2140: 0.5847
Loss at step 2150: 0.7791
Loss at step 2160: 0.7995
Loss at step 2170: 0.2628
Loss at step 2180: 0.2815
Loss at step 2190: 0.6027
Loss at step 2200: 0.8068
Loss at step 2210: 0.4149
Loss at step 2220: 0.3928
Loss at step 2230: 0.6693
Loss at step 2240: 0.3567
Loss at step 2250: 0.4437
Loss at step 2260: 0.5533
Loss at step 2270: 0.5959
Loss at step 2280: 0.2143
Loss at step 2290: 0.0915
Loss at step 2300: 0.4448
Loss at step 2310: 0.2604
Loss at step 2320: 0.4464
Loss at step 2330: 0.4886
Loss at step 2340: 0.0430
Loss at step 2350: 0.2730
Loss at step 2360: 0.1709
Loss at step 2370: 0.1353
Loss at step 2380: 0.3848
Loss at step 2390: 0.4519
Loss at step 2400: 0.3708
Loss at step 2410: 0.2624
Loss at step 2420: 0.6224
Loss at step 2430: 0.5680
Loss at step 2440: 0.4184
Loss at step 2450: 0.6478
Loss at step 2460: 0.4287
Loss at step 2470: 0.7816
Loss at step 2480: 1.0821
Loss at step 2490: 0.4411
Loss at step 2500: 0.3568
Loss at step 2510: 0.3227
Loss at step 2520: 0.3971
Loss at step 2530: 0.2843
Loss at step 2540: 0.4526
Loss at step 2550: 0.8498
Loss at step 2560: 0.2954
Loss at step 2570: 0.1978
Loss at step 2580: 0.7892
Loss at step 2590: 0.2059
Loss at step 2600: 0.7219
Loss at step 2610: 0.3043
Loss at step 2620: 0.5088
Loss at step 2630: 0.5243
Loss at step 2640: 0.4229
Loss at step 2650: 0.5990
Loss at step 2660: 0.4831
Loss at step 2670: 0.5632
Loss at step 2680: 0.1267
Loss at step 2690: 0.1583
Loss at step 2700: 0.3537
Loss at step 2710: 0.4161
Loss at step 2720: 0.4161
Loss at step 2730: 0.3519
Loss at step 2740: 0.6762
Loss at step 2750: 0.3587
Loss at step 2760: 0.4454
Loss at step 2770: 0.1555
Loss at step 2780: 0.3734
Loss at step 2790: 0.4074
Loss at step 2800: 0.4482
Loss at step 2810: 0.1470
Loss at step 2820: 0.2006
Loss at step 2830: 0.2511
Loss at step 2840: 0.1868
Loss at step 2850: 0.2584
Loss at step 2860: 0.3022
Loss at step 2870: 0.3703
Loss at step 2880: 0.5037
Loss at step 2890: 0.4442
Loss at step 2900: 0.3401
Loss at step 2910: 0.3038
Loss at step 2920: 0.4923
Loss at step 2930: 0.2605
Loss at step 2940: 0.3760
Loss at step 2950: 0.2997
Loss at step 2960: 0.3768
Loss at step 2970: 0.4090
Loss at step 2980: 0.5429
Loss at step 2990: 0.9249
Loss at step 3000: 0.4530
Loss at step 3010: 0.2104
Loss at step 3020: 0.4320
Loss at step 3030: 0.2845
Loss at step 3040: 0.4358
Loss at step 3050: 0.0731
Loss at step 3060: 0.6197
Loss at step 3070: 0.5614
Loss at step 3080: 0.4781
Loss at step 3090: 0.4313
Loss at step 3100: 0.6786
Loss at step 3110: 0.4286
Loss at step 3120: 1.1553
Loss at step 3130: 0.9310
Loss at step 3140: 0.3727
Loss at step 3150: 0.5681
Loss at step 3160: 0.3059
Loss at step 3170: 0.6902
Loss at step 3180: 0.3078
Loss at step 3190: 0.1728
Loss at step 3200: 0.3694
Loss at step 3210: 0.2540
Loss at step 3220: 0.5189
Loss at step 3230: 0.8748
Loss at step 3240: 0.9219
Loss at step 3250: 0.4520
Loss at step 3260: 0.8005
Loss at step 3270: 0.8532
Loss at step 3280: 0.2669
Loss at step 3290: 0.5219
Loss at step 3300: 0.3340
Loss at step 3310: 0.2046
Loss at step 3320: 0.2435
Loss at step 3330: 0.4935
Loss at step 3340: 0.4310
Loss at step 3350: 0.3153
Loss at step 3360: 0.5555
Loss at step 3370: 0.5080
Loss at step 3380: 0.0801
Loss at step 3390: 0.6069
Loss at step 3400: 0.3428
Loss at step 3410: 0.4088
Loss at step 3420: 0.5538
Loss at step 3430: 0.5615
Loss at step 3440: 0.8281
Loss at step 3450: 0.2820
Loss at step 3460: 0.8319
Loss at step 3470: 0.3468
Loss at step 3480: 0.7284
Loss at step 3490: 0.3650
Loss at step 3500: 0.4751
Loss at step 3510: 0.3072
Loss at step 3520: 0.2389
Loss at step 3530: 0.2250
Loss at step 3540: 0.2629
Loss at step 3550: 0.2706
Loss at step 3560: 0.4623
Loss at step 3570: 0.2663
Loss at step 3580: 0.4928
Loss at step 3590: 0.2378
Loss at step 3600: 0.8776
Loss at step 3610: 0.3692
Loss at step 3620: 0.4242
Loss at step 3630: 0.6673
Loss at step 3640: 0.1548
Loss at step 3650: 0.1437
Loss at step 3660: 0.3679
Loss at step 3670: 0.0605
Loss at step 3680: 0.2456
Loss at step 3690: 0.4637
Loss at step 3700: 0.4576
Loss at step 3710: 0.3477
Loss at step 3720: 0.2638
Loss at step 3730: 0.2217
Loss at step 3740: 0.3072
Loss at step 3750: 0.5315
Loss at step 3760: 0.6737
Loss at step 3770: 0.3048
Loss at step 3780: 0.3251
Loss at step 3790: 0.2104
Loss at step 3800: 0.4189
Loss at step 3810: 0.8097
Loss at step 3820: 0.5435
Loss at step 3830: 0.1484
Loss at step 3840: 0.4302
Loss at step 3850: 0.4725
Loss at step 3860: 0.1578
Loss at step 3870: 0.3461
Loss at step 3880: 0.8283
Loss at step 3890: 0.6629
Loss at step 3900: 0.2441
Loss at step 3910: 0.0369
Loss at step 3920: 0.6176
Loss at step 3930: 0.4883
Loss at step 3940: 0.2453
Loss at step 3950: 0.2623
Loss at step 3960: 0.2999
Loss at step 3970: 0.5296
Loss at step 3980: 0.2682
Loss at step 3990: 0.8044
Loss at step 4000: 0.1928
Loss at step 4010: 0.4716
Loss at step 4020: 0.2124
Loss at step 4030: 0.7238
Loss at step 4040: 0.2323
Loss at step 4050: 0.4333
Loss at step 4060: 0.1823
Loss at step 4070: 0.2230
Loss at step 4080: 0.2430
Loss at step 4090: 0.2625
Loss at step 4100: 0.3905
Loss at step 4110: 0.7025
Loss at step 4120: 0.8550
Loss at step 4130: 0.5314
Loss at step 4140: 0.2157
Loss at step 4150: 0.2117
Loss at step 4160: 0.3916
Loss at step 4170: 0.4210
Loss at step 4180: 0.4230
Loss at step 4190: 0.7121
Loss at step 4200: 0.3329
Loss at step 4210: 0.5811
Loss at step 4220: 0.4414
Loss at step 4230: 0.3169
Loss at step 4240: 0.2660
Loss at step 4250: 0.3322
Loss at step 4260: 0.7281
Loss at step 4270: 0.6639
Loss at step 4280: 0.2962
Loss at step 4290: 0.2816
Loss at step 4300: 0.2975
Loss at step 4310: 0.7959
Loss at step 4320: 0.2746
Loss at step 4330: 0.5640
Loss at step 4340: 0.2439
Loss at step 4350: 0.5887
Loss at step 4360: 0.5900
Loss at step 4370: 0.4829
Loss at step 4380: 0.2664
Loss at step 4390: 0.3558
Loss at step 4400: 0.3450
Loss at step 4410: 0.5691
Loss at step 4420: 0.7986
Loss at step 4430: 0.7641
Loss at step 4440: 0.3308
Loss at step 4450: 0.5113
Loss at step 4460: 0.5257
Loss at step 4470: 0.2386
Loss at step 4480: 0.5698
Loss at step 4490: 0.3378
Loss at step 4500: 0.4212
Loss at step 4510: 0.3768
Loss at step 4520: 0.3088
Loss at step 4530: 0.4160
Loss at step 4540: 0.3742
Loss at step 4550: 0.4960
Loss at step 4560: 0.4220
Loss at step 4570: 0.3775
Loss at step 4580: 0.7349
Loss at step 4590: 0.4504
Loss at step 4600: 0.3678
Loss at step 4610: 0.1875
Loss at step 4620: 0.5742
Loss at step 4630: 0.3753
Loss at step 4640: 1.1880
Loss at step 4650: 0.4049
Loss at step 4660: 0.3458
Loss at step 4670: 0.2268
Loss at step 4680: 0.4939
Loss at step 4690: 0.5674
Loss at step 4700: 0.3664
Loss at step 4710: 0.2904
Loss at step 4720: 0.1903
Loss at step 4730: 0.5866
Loss at step 4740: 0.8207
Loss at step 4750: 0.6586
Loss at step 4760: 1.3364
Loss at step 4770: 0.2088
Loss at step 4780: 0.6653
Loss at step 4790: 0.2971
Loss at step 4800: 0.2831
Loss at step 4810: 0.2109
Loss at step 4820: 0.7449
Loss at step 4830: 0.6412
Loss at step 4840: 0.4076
Loss at step 4850: 0.3892
Loss at step 4860: 0.1725
Loss at step 4870: 0.7202
Loss at step 4880: 0.7761
Loss at step 4890: 0.6235
Loss at step 4900: 0.2569
Loss at step 4910: 0.3672
Loss at step 4920: 0.2445
Loss at step 4930: 0.4879
Loss at step 4940: 0.6428
Loss at step 4950: 0.3986
Loss at step 4960: 0.3325
Loss at step 4970: 0.6060
Loss at step 4980: 0.1482
Loss at step 4990: 0.3875
Loss at step 5000: 0.5565
Loss at step 5010: 0.4593
Loss at step 5020: 0.3148
Loss at step 5030: 0.2171
Loss at step 5040: 0.2909
Loss at step 5050: 0.2705
Loss at step 5060: 0.2264
Loss at step 5070: 0.5175
Loss at step 5080: 0.2768
Loss at step 5090: 0.4735
Loss at step 5100: 0.5119
Loss at step 5110: 0.5244
Loss at step 5120: 0.5588
Loss at step 5130: 0.2494
Loss at step 5140: 0.4492
Loss at step 5150: 0.6687
Loss at step 5160: 0.3099
Loss at step 5170: 0.3214
Loss at step 5180: 0.2856
Loss at step 5190: 0.4122
Loss at step 5200: 0.3393
Loss at step 5210: 0.4671
Loss at step 5220: 0.2649
Loss at step 5230: 0.2368
Loss at step 5240: 0.3676
Loss at step 5250: 0.7490
Loss at step 5260: 0.3683
Loss at step 5270: 0.3971
Loss at step 5280: 0.2451
Loss at step 5290: 0.2961
Loss at step 5300: 0.3253
Loss at step 5310: 0.4308
Loss at step 5320: 0.4170
Loss at step 5330: 0.1778
Loss at step 5340: 0.3596
Loss at step 5350: 0.9993
Loss at step 5360: 0.3330
Loss at step 5370: 0.3938
Loss at step 5380: 0.3263
Loss at step 5390: 0.1936
Loss at step 5400: 0.7429
Loss at step 5410: 0.3679
Loss at step 5420: 0.0664
Loss at step 5430: 0.3025
Loss at step 5440: 0.6368
Loss at step 5450: 0.2939
Loss at step 5460: 0.4040
Loss at step 5470: 0.2919
Loss at step 5480: 0.3650
Loss at step 5490: 0.3283
Loss at step 5500: 0.7254
Loss at step 5510: 0.0811
Loss at step 5520: 0.1433
Loss at step 5530: 0.3512
Loss at step 5540: 0.4916
Loss at step 5550: 0.2823
Loss at step 5560: 0.2464
Loss at step 5570: 0.5454
Loss at step 5580: 0.4039
Loss at step 5590: 0.1751
Loss at step 5600: 0.3874
Loss at step 5610: 0.7752
Loss at step 5620: 0.3189
Loss at step 5630: 0.2654
Loss at step 5640: 0.2622
Loss at step 5650: 0.4457
Loss at step 5660: 0.3285
Loss at step 5670: 0.1914
Loss at step 5680: 0.3054
Loss at step 5690: 0.2015
Loss at step 5700: 0.8475
Loss at step 5710: 0.3808
Loss at step 5720: 0.4567
Loss at step 5730: 0.0202
Loss at step 5740: 0.4944
Loss at step 5750: 0.1009
Loss at step 5760: 0.2118
Loss at step 5770: 0.7346
Loss at step 5780: 0.1134
Loss at step 5790: 0.0976
Loss at step 5800: 0.0908
Loss at step 5810: 0.6601
Loss at step 5820: 0.3434
Loss at step 5830: 0.4870
Loss at step 5840: 0.2639
Loss at step 5850: 0.3526
Loss at step 5860: 0.6138
Loss at step 5870: 0.4576
Loss at step 5880: 0.1766
Loss at step 5890: 0.3377
Loss at step 5900: 0.8002
Loss at step 5910: 0.3798
Loss at step 5920: 0.1555
Loss at step 5930: 0.0993
Loss at step 5940: 0.3431
Loss at step 5950: 0.5138
Loss at step 5960: 0.2659
Loss at step 5970: 0.3461
Loss at step 5980: 0.2709
Loss at step 5990: 0.7137
Loss at step 6000: 0.3939
Loss at step 6010: 0.8131
Loss at step 6020: 0.2958
Loss at step 6030: 0.3638
Loss at step 6040: 0.1160
Loss at step 6050: 0.4848
Loss at step 6060: 0.4279
Loss at step 6070: 0.3531
Loss at step 6080: 0.4646
Loss at step 6090: 0.3770
Loss at step 6100: 0.3153
Loss at step 6110: 0.5722
Loss at step 6120: 0.6114
Loss at step 6130: 0.3148
Loss at step 6140: 0.5972
Loss at step 6150: 0.7252
Loss at step 6160: 1.0491
Loss at step 6170: 0.1026
Loss at step 6180: 0.3219
Loss at step 6190: 0.4360
Loss at step 6200: 0.3829
Loss at step 6210: 0.3815
Loss at step 6220: 0.5922
Loss at step 6230: 0.3402
Loss at step 6240: 0.5915
Loss at step 6250: 0.6754
Loss at step 6260: 0.2178
Loss at step 6270: 0.1939
Loss at step 6280: 0.1139
Loss at step 6290: 0.2299
Loss at step 6300: 0.5402
Loss at step 6310: 0.4732
Loss at step 6320: 0.3084
Loss at step 6330: 0.4388
Loss at step 6340: 0.5150
Loss at step 6350: 0.1553
Loss at step 6360: 0.2697
Loss at step 6370: 0.3914
Loss at step 6380: 0.3333
Loss at step 6390: 0.2359
Loss at step 6400: 0.4068
Loss at step 6410: 0.3031
Loss at step 6420: 0.4572
Loss at step 6430: 0.8352
Loss at step 6440: 0.2744
Loss at step 6450: 0.1945
Loss at step 6460: 0.4613
Loss at step 6470: 0.2500
Loss at step 6480: 0.5223
Loss at step 6490: 0.6287
Loss at step 6500: 0.6206
Loss at step 6510: 0.5647
Loss at step 6520: 0.4066
Loss at step 6530: 0.6529
Loss at step 6540: 0.3709
Loss at step 6550: 0.2485
Loss at step 6560: 0.4185
Loss at step 6570: 0.3899
Loss at step 6580: 0.3564
Loss at step 6590: 0.2098
Loss at step 6600: 0.3807
Loss at step 6610: 0.7556
Loss at step 6620: 0.2278
Loss at step 6630: 0.1001
Loss at step 6640: 0.5077
Loss at step 6650: 0.3994
Loss at step 6660: 0.1422
Loss at step 6670: 0.3134
Loss at step 6680: 0.3260
Loss at step 6690: 0.2369
Loss at step 6700: 0.1695
Loss at step 6710: 0.2099
Loss at step 6720: 0.5655
Loss at step 6730: 0.8424
Loss at step 6740: 0.3339
Loss at step 6750: 0.1935
Loss at step 6760: 0.2086
Loss at step 6770: 0.3817
Loss at step 6780: 0.2448
Loss at step 6790: 0.2879
Loss at step 6800: 0.4139
Loss at step 6810: 0.3577
Loss at step 6820: 0.5950
Loss at step 6830: 0.4182
Loss at step 6840: 0.3070
Loss at step 6850: 0.2118
Loss at step 6860: 0.3702
Loss at step 6870: 0.2597
Loss at step 6880: 0.1213
Loss at step 6890: 0.2876
Loss at step 6900: 0.4237
Loss at step 6910: 0.4214
Loss at step 6920: 0.5831
Loss at step 6930: 0.2349
Loss at step 6940: 0.4166
Loss at step 6950: 0.2794
Loss at step 6960: 0.3488
Loss at step 6970: 0.3373
Loss at step 6980: 0.2134
Loss at step 6990: 0.3925
Loss at step 7000: 0.1127
Loss at step 7010: 0.3625
Loss at step 7020: 0.4225
Loss at step 7030: 0.4030
Loss at step 7040: 0.1643
Loss at step 7050: 0.4104
Loss at step 7060: 0.0672
Loss at step 7070: 0.5414
Loss at step 7080: 0.2650
Loss at step 7090: 0.5159
Loss at step 7100: 0.4561
Loss at step 7110: 0.6365
Loss at step 7120: 0.2095
Loss at step 7130: 0.4598
Loss at step 7140: 0.1003
Loss at step 7150: 0.3379
Loss at step 7160: 0.2015
Loss at step 7170: 0.9157
Loss at step 7180: 0.2552
Loss at step 7190: 0.2144
Loss at step 7200: 0.5338
Loss at step 7210: 0.5292
Loss at step 7220: 0.2570
Loss at step 7230: 0.4865
Loss at step 7240: 1.0473
Loss at step 7250: 0.1923
Loss at step 7260: 0.8191
Loss at step 7270: 0.2698
Loss at step 7280: 0.4093
Loss at step 7290: 0.0328
Loss at step 7300: 0.3867
Loss at step 7310: 0.5122
Loss at step 7320: 0.3926
Loss at step 7330: 0.3872
Loss at step 7340: 0.4729
Loss at step 7350: 0.5294
Loss at step 7360: 0.1961
Loss at step 7370: 0.2515
Loss at step 7380: 0.4257
Loss at step 7390: 0.0642
Loss at step 7400: 0.2342
Loss at step 7410: 0.4685
Loss at step 7420: 0.0385
Loss at step 7430: 0.7257
Loss at step 7440: 0.2740
Loss at step 7450: 0.3368
Loss at step 7460: 0.3970
Loss at step 7470: 0.1305
Loss at step 7480: 0.4283
Loss at step 7490: 0.5831
Loss at step 7500: 0.3587
Loss at step 7510: 0.5201
Loss at step 7520: 0.6060
Loss at step 7530: 0.2725
Loss at step 7540: 0.4421
Loss at step 7550: 0.4891
Loss at step 7560: 0.2494
Loss at step 7570: 0.1802
Loss at step 7580: 0.2330
Loss at step 7590: 0.1796
Loss at step 7600: 0.2923
Loss at step 7610: 0.4195
Loss at step 7620: 0.0839
Loss at step 7630: 0.6531
Loss at step 7640: 0.1849
Loss at step 7650: 0.2286
Loss at step 7660: 0.5353
Loss at step 7670: 0.1250
Loss at step 7680: 0.4149
Loss at step 7690: 0.2068
Loss at step 7700: 0.3152
Loss at step 7710: 0.4755
Loss at step 7720: 0.1812
Loss at step 7730: 0.0702
Loss at step 7740: 0.5575
Loss at step 7750: 0.1951
Loss at step 7760: 0.7606
Loss at step 7770: 0.3432
Loss at step 7780: 0.2894
Loss at step 7790: 0.0506
Loss at step 7800: 0.3878
Loss at step 7810: 0.0819
Loss at step 7820: 0.3338
Loss at step 7830: 0.2880
Loss at step 7840: 0.6516
Loss at step 7850: 0.4367
Loss at step 7860: 0.4023
Loss at step 7870: 0.3450
Loss at step 7880: 0.8539
Loss at step 7890: 0.4142
Loss at step 7900: 0.6785
Loss at step 7910: 0.4928
Loss at step 7920: 0.2453
Loss at step 7930: 0.6130
Loss at step 7940: 0.4697
Loss at step 7950: 0.0557
Loss at step 7960: 0.1161
Loss at step 7970: 0.2223
Loss at step 7980: 0.5394
Loss at step 7990: 0.4456
Loss at step 8000: 0.3820
Loss at step 8010: 0.5635
Loss at step 8020: 0.5825
Loss at step 8030: 0.2089
Loss at step 8040: 0.1976
Loss at step 8050: 0.2119
Loss at step 8060: 0.4239
Loss at step 8070: 0.4426
Loss at step 8080: 0.4025
Loss at step 8090: 0.7188
Loss at step 8100: 0.2966
Loss at step 8110: 0.4540
Loss at step 8120: 0.3991
Loss at step 8130: 0.1822
Loss at step 8140: 0.4088
Loss at step 8150: 0.1431
Loss at step 8160: 0.1537
Loss at step 8170: 0.3079
Loss at step 8180: 0.1703
Loss at step 8190: 0.7235
Loss at step 8200: 0.4047
Loss at step 8210: 0.2729
Loss at step 8220: 0.3072
Loss at step 8230: 0.3975
Loss at step 8240: 0.6954
Loss at step 8250: 0.4573
Loss at step 8260: 0.3270
Loss at step 8270: 0.6361
Loss at step 8280: 0.4096
Loss at step 8290: 0.4654
Loss at step 8300: 0.2950
Loss at step 8310: 0.5696
Loss at step 8320: 0.0292
Loss at step 8330: 0.0074
Loss at step 8340: 0.1869
Loss at step 8350: 0.1114
Loss at step 8360: 0.4602
Loss at step 8370: 0.1900
Loss at step 8380: 0.2063
Loss at step 8390: 0.2679
Loss at step 8400: 0.2468
Loss at step 8410: 0.2857
Loss at step 8420: 0.2044
Loss at step 8430: 0.3099
Loss at step 8440: 0.3684
Loss at step 8450: 0.1170
Loss at step 8460: 0.1536
Loss at step 8470: 0.6364
Loss at step 8480: 0.3947
Loss at step 8490: 0.4653
Loss at step 8500: 0.1893
Loss at step 8510: 0.4882
Loss at step 8520: 0.1967
Loss at step 8530: 0.2920
Loss at step 8540: 0.8431
Loss at step 8550: 0.1526
Loss at step 8560: 0.4819
Loss at step 8570: 0.2556
Loss at step 8580: 0.3942
Loss at step 8590: 0.1018
Loss at step 8600: 0.2067
Loss at step 8610: 0.2557
Loss at step 8620: 0.4332
Loss at step 8630: 0.5344
Loss at step 8640: 0.5012
Loss at step 8650: 0.0434
Loss at step 8660: 0.1231
Loss at step 8670: 0.4787
Loss at step 8680: 0.0804
Loss at step 8690: 0.2900
Loss at step 8700: 0.7459
Loss at step 8710: 0.2927
Loss at step 8720: 0.5816
Loss at step 8730: 0.1618
Loss at step 8740: 0.2763
Loss at step 8750: 0.5587
Loss at step 8760: 0.4202
Loss at step 8770: 0.4949
Loss at step 8780: 0.3614
Loss at step 8790: 1.2953
Loss at step 8800: 0.4709
Loss at step 8810: 0.3891
Loss at step 8820: 0.3025
Loss at step 8830: 0.1439
Loss at step 8840: 0.1789
Loss at step 8850: 0.1183
Loss at step 8860: 0.1678
Loss at step 8870: 0.2910
Loss at step 8880: 0.1214
Loss at step 8890: 0.3891
Loss at step 8900: 0.2870
Loss at step 8910: 0.4187
Loss at step 8920: 0.2649
Loss at step 8930: 0.8548
Loss at step 8940: 1.2405
Loss at step 8950: 0.5183
Loss at step 8960: 0.0590
Loss at step 8970: 0.0479
Loss at step 8980: 0.8505
Loss at step 8990: 0.4965
Loss at step 9000: 0.0778
Loss at step 9010: 0.3194
Loss at step 9020: 0.7967
Loss at step 9030: 0.1403
Loss at step 9040: 0.2459
Loss at step 9050: 0.2607
Loss at step 9060: 0.3746
Loss at step 9070: 0.3977
Loss at step 9080: 0.1854
Loss at step 9090: 0.1399
Loss at step 9100: 0.1667
Loss at step 9110: 0.2811
Loss at step 9120: 0.3846
Loss at step 9130: 0.4166
Loss at step 9140: 0.2810
Loss at step 9150: 0.2774
Loss at step 9160: 0.3771
Loss at step 9170: 0.3801
Loss at step 9180: 0.3416
Loss at step 9190: 0.1935
Loss at step 9200: 0.2406
Loss at step 9210: 1.0255
Loss at step 9220: 0.3516
Loss at step 9230: 0.4405
Loss at step 9240: 0.5744
Loss at step 9250: 0.4640
Loss at step 9260: 0.5819
Loss at step 9270: 0.4443
Loss at step 9280: 0.3046
Loss at step 9290: 0.3412
Loss at step 9300: 0.3055
Loss at step 9310: 0.5095
Loss at step 9320: 0.1757
Loss at step 9330: 0.4732
Loss at step 9340: 0.2709
Loss at step 9350: 0.5084
Loss at step 9360: 0.2337
Loss at step 9370: 0.4044
Loss at step 9380: 0.2327
Loss at step 9390: 0.6066
Loss at step 9400: 0.0862
Loss at step 9410: 0.0367
Loss at step 9420: 0.4661
Loss at step 9430: 0.5758
Loss at step 9440: 0.0857
Loss at step 9450: 0.8637
Loss at step 9460: 0.1779
Loss at step 9470: 0.2329
Loss at step 9480: 0.1166
Loss at step 9490: 0.5581
Loss at step 9500: 0.3386
Loss at step 9510: 0.1797
Loss at step 9520: 0.2777
Loss at step 9530: 0.1962
Loss at step 9540: 0.1371
Loss at step 9550: 0.3266
Loss at step 9560: 0.1913
Loss at step 9570: 0.4611
Loss at step 9580: 0.2231
Loss at step 9590: 0.5254
Loss at step 9600: 0.4814
Loss at step 9610: 0.4466
Loss at step 9620: 0.2220
Loss at step 9630: 0.4325
Loss at step 9640: 0.5067
Loss at step 9650: 0.2533
Loss at step 9660: 0.7374
Loss at step 9670: 0.2798
Loss at step 9680: 0.3262
Loss at step 9690: 0.3949
Loss at step 9700: 0.3983
Loss at step 9710: 0.4097
Loss at step 9720: 0.1422
Loss at step 9730: 0.5327
Loss at step 9740: 0.4854
Loss at step 9750: 0.0579
Loss at step 9760: 0.1588
Loss at step 9770: 0.4940
Loss at step 9780: 0.4700
Loss at step 9790: 0.0617
Loss at step 9800: 0.2362
Loss at step 9810: 0.1964
Loss at step 9820: 0.1218
Loss at step 9830: 0.4602
Loss at step 9840: 0.4195
Loss at step 9850: 0.5127
Loss at step 9860: 0.1353
Loss at step 9870: 0.0728
Loss at step 9880: 0.4125
Loss at step 9890: 0.0650
Loss at step 9900: 0.4245
Loss at step 9910: 0.3558
Loss at step 9920: 0.2511
Loss at step 9930: 0.1338
Loss at step 9940: 0.0257
Loss at step 9950: 0.1593
Loss at step 9960: 0.5031
Loss at step 9970: 0.5657
Loss at step 9980: 0.4650
Loss at step 9990: 0.4382
Loss at step 10000: 0.4142
Loss at step 10010: 0.1803
Loss at step 10020: 0.0567
Loss at step 10030: 0.4582
Loss at step 10040: 0.2303
Loss at step 10050: 0.4182
Loss at step 10060: 0.7432
Loss at step 10070: 0.2336
Loss at step 10080: 0.1321
Loss at step 10090: 0.5071
Loss at step 10100: 0.3636
Loss at step 10110: 0.7525
Loss at step 10120: 0.2328
Loss at step 10130: 0.5550
Loss at step 10140: 0.3509
Loss at step 10150: 0.5427
Loss at step 10160: 0.5871
Loss at step 10170: 0.4172
Loss at step 10180: 0.4926
Loss at step 10190: 0.3148
Loss at step 10200: 0.1951
Loss at step 10210: 0.4335
Loss at step 10220: 0.7243
Loss at step 10230: 0.1910
Loss at step 10240: 0.4153
Loss at step 10250: 0.3739
Loss at step 10260: 0.2925
Loss at step 10270: 0.1841
Loss at step 10280: 0.3362
Loss at step 10290: 0.4156
Loss at step 10300: 0.7015
Loss at step 10310: 0.3333
Loss at step 10320: 0.2487
Loss at step 10330: 0.1701
Loss at step 10340: 0.4188
Loss at step 10350: 0.1058
Loss at step 10360: 0.4005
Loss at step 10370: 0.8844
Loss at step 10380: 0.3169
Loss at step 10390: 0.3220
Loss at step 10400: 0.3696
Loss at step 10410: 0.0769
Loss at step 10420: 0.5879
Loss at step 10430: 0.3792
Loss at step 10440: 0.1955
Loss at step 10450: 0.3679
Loss at step 10460: 0.1807
Loss at step 10470: 0.2117
Loss at step 10480: 0.1755
Loss at step 10490: 0.2915
Loss at step 10500: 0.5662
Loss at step 10510: 0.3923
Loss at step 10520: 0.2475
Loss at step 10530: 0.2340
Loss at step 10540: 0.0827
Loss at step 10550: 0.5029
Loss at step 10560: 0.3020
Loss at step 10570: 0.1341
Loss at step 10580: 0.5825
Loss at step 10590: 0.2682
Loss at step 10600: 0.4484
Loss at step 10610: 0.4366
Loss at step 10620: 0.1090
Loss at step 10630: 0.4017
Loss at step 10640: 0.7244
Loss at step 10650: 0.2406
Loss at step 10660: 0.1334
Loss at step 10670: 0.2460
Loss at step 10680: 0.1232
Loss at step 10690: 0.2661
Loss at step 10700: 0.1625
Loss at step 10710: 0.4070
Loss at step 10720: 0.1973
Loss at step 10730: 0.1537
Loss at step 10740: 0.8022
Loss at step 10750: 0.2064
Loss at step 10760: 0.3162
Loss at step 10770: 0.5318
Loss at step 10780: 0.3965
Loss at step 10790: 0.0871
Loss at step 10800: 0.2945
Loss at step 10810: 0.2544
Loss at step 10820: 0.1702
Loss at step 10830: 0.5691
Loss at step 10840: 0.3214
Loss at step 10850: 0.4686
Loss at step 10860: 0.4344
Loss at step 10870: 0.2807
Loss at step 10880: 0.3833
Loss at step 10890: 0.6288
Loss at step 10900: 0.1390
Loss at step 10910: 0.2286
Loss at step 10920: 0.1081
Loss at step 10930: 0.3833
Loss at step 10940: 0.1138
Loss at step 10950: 0.7436
Loss at step 10960: 0.3500
Loss at step 10970: 0.2671
Loss at step 10980: 0.1545
Loss at step 10990: 0.1213
Loss at step 11000: 0.0855
Loss at step 11010: 0.4775
Loss at step 11020: 0.4780
Loss at step 11030: 0.2740
Loss at step 11040: 0.1262
Loss at step 11050: 0.2934
Loss at step 11060: 0.2872
Loss at step 11070: 0.2729
Loss at step 11080: 0.1243
Loss at step 11090: 0.1543
Loss at step 11100: 0.6456
Loss at step 11110: 0.3472
Loss at step 11120: 0.2388
Loss at step 11130: 0.5993
Loss at step 11140: 0.3597
Loss at step 11150: 0.4731
Loss at step 11160: 0.0885
Loss at step 11170: 0.5476
Loss at step 11180: 0.3099
Loss at step 11190: 0.2965
Loss at step 11200: 0.0738
Loss at step 11210: 0.4062
Loss at step 11220: 0.2451
Loss at step 11230: 0.4368
Loss at step 11240: 0.2280
Loss at step 11250: 0.3539
Loss at step 11260: 0.2806
Loss at step 11270: 0.0878
Loss at step 11280: 0.1474
Loss at step 11290: 0.2244
Loss at step 11300: 0.3499
Loss at step 11310: 0.2982
Loss at step 11320: 0.2717
Loss at step 11330: 0.3202
Loss at step 11340: 0.3383
Loss at step 11350: 0.1789
Loss at step 11360: 0.0526
Loss at step 11370: 0.5101
Loss at step 11380: 0.4084
Loss at step 11390: 0.0968
Loss at step 11400: 0.6354
Loss at step 11410: 0.5296
Loss at step 11420: 0.5428
Loss at step 11430: 0.1114
Loss at step 11440: 0.3324
Loss at step 11450: 0.0911
Loss at step 11460: 0.5758
Loss at step 11470: 0.0652
Loss at step 11480: 0.2236
Loss at step 11490: 0.1570
Loss at step 11500: 0.1988
Loss at step 11510: 0.4921
Loss at step 11520: 0.2066
Loss at step 11530: 0.1727
Loss at step 11540: 0.3502
Loss at step 11550: 0.3557
Loss at step 11560: 0.1700
Loss at step 11570: 0.3623
Loss at step 11580: 0.2546
Loss at step 11590: 0.2762
Loss at step 11600: 0.3835
Loss at step 11610: 0.1918
Loss at step 11620: 0.1869
Loss at step 11630: 0.4966
Loss at step 11640: 0.3842
Loss at step 11650: 0.5928
Loss at step 11660: 0.1358
Loss at step 11670: 0.2721
Loss at step 11680: 0.4883
Loss at step 11690: 0.4146
Loss at step 11700: 0.2847
Loss at step 11710: 0.5884
Loss at step 11720: 0.4875
Loss at step 11730: 0.0918
Loss at step 11740: 0.2954
Loss at step 11750: 0.2182
Loss at step 11760: 0.1516
Loss at step 11770: 0.3334
Loss at step 11780: 0.4317
Loss at step 11790: 0.3931
Loss at step 11800: 0.2293
Loss at step 11810: 0.6200
Loss at step 11820: 0.4095
Loss at step 11830: 0.3504
Loss at step 11840: 0.4697
Loss at step 11850: 0.1960
Loss at step 11860: 0.1990
Loss at step 11870: 0.4569
Loss at step 11880: 0.5583
Loss at step 11890: 0.1824
Loss at step 11900: 0.0244
Loss at step 11910: 0.3159
Loss at step 11920: 0.4284
Loss at step 11930: 0.1404
Loss at step 11940: 0.3066
Loss at step 11950: 0.1095
Loss at step 11960: 0.1044
Loss at step 11970: 0.3941
Loss at step 11980: 0.2598
Loss at step 11990: 0.0953
Loss at step 12000: 0.1983
Loss at step 12010: 0.2038
Loss at step 12020: 0.1323
Loss at step 12030: 0.2478
Loss at step 12040: 0.2922
Loss at step 12050: 0.5996
Loss at step 12060: 0.6185
Loss at step 12070: 0.2022
Loss at step 12080: 0.3889
Loss at step 12090: 0.0654
Loss at step 12100: 0.5229
Loss at step 12110: 0.2919
Loss at step 12120: 0.3498
Loss at step 12130: 0.4035
Loss at step 12140: 0.6203
Loss at step 12150: 0.5633
Loss at step 12160: 0.4345
Loss at step 12170: 0.3954
Loss at step 12180: 0.5502
Loss at step 12190: 0.1328
Loss at step 12200: 0.7707
Loss at step 12210: 0.1532
Loss at step 12220: 0.5357
Loss at step 12230: 0.2733
Loss at step 12240: 0.5406
Loss at step 12250: 0.2822
Loss at step 12260: 0.4741
Loss at step 12270: 0.1629
Loss at step 12280: 0.2671
Loss at step 12290: 0.0536
Loss at step 12300: 0.1003
Loss at step 12310: 0.4583
Loss at step 12320: 0.1365
Loss at step 12330: 0.5930
Loss at step 12340: 0.1645
Loss at step 12350: 0.2679
Loss at step 12360: 0.3135
Loss at step 12370: 0.5097
Loss at step 12380: 0.7437
Loss at step 12390: 0.3055
Loss at step 12400: 0.1321
Loss at step 12410: 0.1896
Loss at step 12420: 0.7553
Loss at step 12430: 0.4924
Loss at step 12440: 0.1177
Loss at step 12450: 0.1058
Loss at step 12460: 0.4061
Loss at step 12470: 0.3046
Loss at step 12480: 0.1241
Loss at step 12490: 0.0998
Loss at step 12500: 0.2537
Loss at step 12510: 0.0810
Loss at step 12520: 0.6198
Loss at step 12530: 0.1907
Loss at step 12540: 0.0934
Loss at step 12550: 0.3836
Loss at step 12560: 0.1990
Loss at step 12570: 0.3106
Loss at step 12580: 0.2789
Loss at step 12590: 0.4703
Loss at step 12600: 0.4007
Loss at step 12610: 0.1086
Loss at step 12620: 0.4351
Loss at step 12630: 0.4747
Loss at step 12640: 0.5250
Loss at step 12650: 0.3193
Loss at step 12660: 0.4030
Loss at step 12670: 0.2563
Loss at step 12680: 0.2036
Loss at step 12690: 0.6433
Loss at step 12700: 0.3022
Loss at step 12710: 0.1881
Loss at step 12720: 0.1839
Loss at step 12730: 0.3759
Loss at step 12740: 0.4066
Loss at step 12750: 0.6049
Loss at step 12760: 0.1525
Loss at step 12770: 0.1308
Loss at step 12780: 0.1761
Loss at step 12790: 0.0826
Loss at step 12800: 0.4597
Loss at step 12810: 0.0978
Loss at step 12820: 0.3344
Loss at step 12830: 0.3414
Loss at step 12840: 0.5032
Loss at step 12850: 0.2256
Loss at step 12860: 0.5203
Loss at step 12870: 0.2441
Loss at step 12880: 0.0560
Loss at step 12890: 0.3931
Loss at step 12900: 0.1660
Loss at step 12910: 0.4249
Loss at step 12920: 0.3632
Loss at step 12930: 0.1854
Loss at step 12940: 0.2384
Loss at step 12950: 0.3223
Loss at step 12960: 0.7531
Loss at step 12970: 0.0954
Loss at step 12980: 0.1829
Loss at step 12990: 0.3997
Loss at step 13000: 0.1718
Loss at step 13010: 0.1961
Loss at step 13020: 0.2432
Loss at step 13030: 0.0908
Loss at step 13040: 0.3530
Loss at step 13050: 0.3216
Loss at step 13060: 0.2417
Loss at step 13070: 0.1570
Loss at step 13080: 0.3520
Loss at step 13090: 0.4171
Loss at step 13100: 0.3817
Loss at step 13110: 0.4483
Loss at step 13120: 0.1536
Loss at step 13130: 0.5944
Loss at step 13140: 0.2462
Loss at step 13150: 0.2587
Loss at step 13160: 0.3687
Loss at step 13170: 0.1255
Loss at step 13180: 0.1912
Loss at step 13190: 0.1577
Loss at step 13200: 0.1107
Loss at step 13210: 0.0510
Loss at step 13220: 0.1608
Loss at step 13230: 0.2385
Loss at step 13240: 0.3163
Loss at step 13250: 0.0415
Loss at step 13260: 0.3225
Loss at step 13270: 0.1591
Loss at step 13280: 0.7561
Loss at step 13290: 0.3136
Loss at step 13300: 0.1543
Loss at step 13310: 0.2492
Loss at step 13320: 0.2294
Loss at step 13330: 0.1174
Loss at step 13340: 0.1488
Loss at step 13350: 0.4729
Loss at step 13360: 0.4782
Loss at step 13370: 0.2232
Loss at step 13380: 0.5466
Loss at step 13390: 0.3367
Loss at step 13400: 0.1445
Loss at step 13410: 0.3951
Loss at step 13420: 0.2506
Loss at step 13430: 0.1198
Loss at step 13440: 0.1859
Loss at step 13450: 0.4784
Loss at step 13460: 0.1311
Loss at step 13470: 0.1635
Loss at step 13480: 0.1416
Loss at step 13490: 0.4620
Loss at step 13500: 0.6789
Loss at step 13510: 0.1198
Loss at step 13520: 0.8787
Loss at step 13530: 0.1225
Loss at step 13540: 0.2236
Loss at step 13550: 0.1784
Loss at step 13560: 0.4361
Loss at step 13570: 0.2858
Loss at step 13580: 0.1787
Loss at step 13590: 0.2661
Loss at step 13600: 0.4865
Loss at step 13610: 0.1060
Loss at step 13620: 0.3504
Loss at step 13630: 0.2157
Loss at step 13640: 0.5869
Loss at step 13650: 0.2526
Loss at step 13660: 0.1489
Loss at step 13670: 0.3388
Loss at step 13680: 0.3700
Loss at step 13690: 0.6316
Loss at step 13700: 0.1054
Loss at step 13710: 0.3206
Loss at step 13720: 0.2162
Loss at step 13730: 0.0809
Loss at step 13740: 0.1334
Loss at step 13750: 0.4388
Loss at step 13760: 0.1656
Loss at step 13770: 0.2546
Loss at step 13780: 0.2983
Loss at step 13790: 0.3667
Loss at step 13800: 0.1714
Loss at step 13810: 0.1043
Loss at step 13820: 0.1913
Loss at step 13830: 0.2324
Loss at step 13840: 0.6249
Loss at step 13850: 0.4320
Loss at step 13860: 0.3653
Loss at step 13870: 0.3337
Loss at step 13880: 0.3319
Loss at step 13890: 0.4046
Loss at step 13900: 0.1908
Loss at step 13910: 0.5670
Loss at step 13920: 0.2931
Loss at step 13930: 0.2056
Loss at step 13940: 0.2180
Loss at step 13950: 0.0726
Loss at step 13960: 0.5722
Loss at step 13970: 0.3337
Loss at step 13980: 0.3489
Loss at step 13990: 0.2694
Loss at step 14000: 0.1533
Loss at step 14010: 0.2340
Loss at step 14020: 0.1621
Loss at step 14030: 0.6893
Loss at step 14040: 0.1473
Loss at step 14050: 0.0603
Loss at step 14060: 0.3161
Loss at step 14070: 0.5232
Loss at step 14080: 0.1628
Loss at step 14090: 0.3305
Loss at step 14100: 0.1947
Loss at step 14110: 0.2198
Loss at step 14120: 0.1242
Loss at step 14130: 0.2646
Loss at step 14140: 0.1341
Loss at step 14150: 0.6581
Loss at step 14160: 0.5728
Loss at step 14170: 0.0517
Loss at step 14180: 0.1823
Loss at step 14190: 0.7790
Loss at step 14200: 0.3829
Loss at step 14210: 0.1357
Loss at step 14220: 0.4706
Loss at step 14230: 0.4939
Loss at step 14240: 0.1875
Loss at step 14250: 0.9552
Loss at step 14260: 0.2547
Loss at step 14270: 0.1379
Loss at step 14280: 0.0853
Loss at step 14290: 0.4573
Loss at step 14300: 0.4277
Loss at step 14310: 0.2655
Loss at step 14320: 0.3443
Loss at step 14330: 0.4246
Loss at step 14340: 0.3775
Loss at step 14350: 0.2551
Loss at step 14360: 0.4127
Loss at step 14370: 0.6359
Loss at step 14380: 0.0578
Loss at step 14390: 0.4578
Loss at step 14400: 0.1918
Loss at step 14410: 0.4705
Loss at step 14420: 0.1213
Loss at step 14430: 0.4736
Loss at step 14440: 0.3081
Loss at step 14450: 0.1949
Loss at step 14460: 0.0764
Loss at step 14470: 0.2682
Loss at step 14480: 0.2690
Loss at step 14490: 0.0955
Loss at step 14500: 0.1089
Loss at step 14510: 0.2635
Loss at step 14520: 0.4356
Loss at step 14530: 0.5200
Loss at step 14540: 0.5690
Loss at step 14550: 0.2651
Loss at step 14560: 0.6317
Loss at step 14570: 0.6725
Loss at step 14580: 0.3767
Loss at step 14590: 0.1843
Loss at step 14600: 0.6178
Loss at step 14610: 0.4345
Loss at step 14620: 0.1725
Loss at step 14630: 0.4664
Loss at step 14640: 0.1537
Loss at step 14650: 0.5352
Loss at step 14660: 0.3796
Loss at step 14670: 0.2879
Loss at step 14680: 0.4101
Loss at step 14690: 0.3642
Loss at step 14700: 0.2696
Loss at step 14710: 0.5224
Loss at step 14720: 0.3011
Loss at step 14730: 0.3018
Loss at step 14740: 0.2586
Loss at step 14750: 0.1496
Loss at step 14760: 0.3190
Loss at step 14770: 0.1432
Loss at step 14780: 0.3545
Loss at step 14790: 0.1560
Loss at step 14800: 0.1675
Loss at step 14810: 0.1590
Loss at step 14820: 0.2553
Loss at step 14830: 0.2054
Loss at step 14840: 0.2676
Loss at step 14850: 0.0438
Loss at step 14860: 0.1741
Loss at step 14870: 0.7796
Loss at step 14880: 0.1205
Loss at step 14890: 0.1531
Loss at step 14900: 0.3062
Loss at step 14910: 0.0931
Loss at step 14920: 0.3906
Loss at step 14930: 0.0716
Loss at step 14940: 0.0656
Loss at step 14950: 0.0322
Loss at step 14960: 0.0319
Loss at step 14970: 0.3245
Loss at step 14980: 0.3828
Loss at step 14990: 0.4559
Loss at step 15000: 0.2291
Loss at step 15010: 0.5826
Loss at step 15020: 0.2238
Loss at step 15030: 0.1794
Loss at step 15040: 0.2690
Loss at step 15050: 0.5037
Loss at step 15060: 0.5786
Loss at step 15070: 0.4990
Loss at step 15080: 0.2274
Loss at step 15090: 0.2768
Loss at step 15100: 0.1550
Loss at step 15110: 0.4189
Loss at step 15120: 0.2177
Loss at step 15130: 0.4679
Loss at step 15140: 0.1896
Loss at step 15150: 0.1497
Loss at step 15160: 0.2020
Loss at step 15170: 0.1415
Loss at step 15180: 0.2447
Loss at step 15190: 0.6842
Loss at step 15200: 0.2360
Loss at step 15210: 0.3248
Loss at step 15220: 0.1177
Loss at step 15230: 0.0144
Loss at step 15240: 0.1435
Loss at step 15250: 0.1865
Loss at step 15260: 0.6587
Loss at step 15270: 0.1594
Loss at step 15280: 0.3851
Loss at step 15290: 0.1250
Loss at step 15300: 0.3033
Loss at step 15310: 0.2775
Loss at step 15320: 0.4827
Loss at step 15330: 0.0642
Loss at step 15340: 0.1583
Loss at step 15350: 0.3259
Loss at step 15360: 0.4190
Loss at step 15370: 0.2808
Loss at step 15380: 0.4107
Loss at step 15390: 0.0612
Loss at step 15400: 0.0567
Loss at step 15410: 0.4716
Loss at step 15420: 0.2507
Loss at step 15430: 0.2246
Loss at step 15440: 0.2820
Loss at step 15450: 0.3759
Loss at step 15460: 0.3786
Loss at step 15470: 0.1616
Loss at step 15480: 0.2089
Loss at step 15490: 0.5634
Loss at step 15500: 0.4553
Loss at step 15510: 0.1646
Loss at step 15520: 0.1552
Loss at step 15530: 0.1050
Loss at step 15540: 0.2900
Loss at step 15550: 0.4304
Loss at step 15560: 0.1738
Loss at step 15570: 0.1926
Loss at step 15580: 0.1348
Loss at step 15590: 0.5380
Loss at step 15600: 0.5222
Loss at step 15610: 0.4859
Loss at step 15620: 0.1106
Loss at step 15630: 0.1367
Loss at step 15640: 0.1779
Loss at step 15650: 0.3125
Loss at step 15660: 0.0700
Loss at step 15670: 0.7867
Loss at step 15680: 0.2708
Loss at step 15690: 0.3121
Loss at step 15700: 0.0615
Loss at step 15710: 0.2328
Loss at step 15720: 0.5996
Loss at step 15730: 0.1966
Loss at step 15740: 0.0721
Loss at step 15750: 0.1560
Loss at step 15760: 0.1511
Loss at step 15770: 0.2194
Loss at step 15780: 0.0809
Loss at step 15790: 0.1961
Loss at step 15800: 0.1901
Loss at step 15810: 0.2416
Loss at step 15820: 0.0055
Loss at step 15830: 0.3055
Loss at step 15840: 0.5680
Loss at step 15850: 0.1370
Loss at step 15860: 0.3088
Loss at step 15870: 0.1992
Loss at step 15880: 0.5019
Loss at step 15890: 0.6421
Loss at step 15900: 0.1802
Loss at step 15910: 0.1485
Loss at step 15920: 0.6013
Loss at step 15930: 0.3052
Loss at step 15940: 0.7333
Loss at step 15950: 0.3637
Loss at step 15960: 0.3287
Loss at step 15970: 0.5191
Loss at step 15980: 0.2905
Loss at step 15990: 0.5391
Loss at step 16000: 0.2179
Loss at step 16010: 0.1868
Loss at step 16020: 0.2036
Loss at step 16030: 0.5263
Loss at step 16040: 0.2530
Loss at step 16050: 0.2219
Loss at step 16060: 0.2207
Loss at step 16070: 0.0619
Loss at step 16080: 0.1919
Loss at step 16090: 0.4946
Loss at step 16100: 0.1512
Loss at step 16110: 0.1298
Loss at step 16120: 0.2857
Loss at step 16130: 0.1322
Loss at step 16140: 0.1349
Loss at step 16150: 0.3248
Loss at step 16160: 0.9227
Loss at step 16170: 0.2064
Loss at step 16180: 0.1432
Loss at step 16190: 0.0989
Loss at step 16200: 0.0460
Loss at step 16210: 0.0234
Loss at step 16220: 0.1940
Loss at step 16230: 0.4338
Loss at step 16240: 0.1423
Loss at step 16250: 0.2674
Loss at step 16260: 0.0957
Loss at step 16270: 0.2553
Loss at step 16280: 0.4934
Loss at step 16290: 0.3251
Loss at step 16300: 0.2942
Loss at step 16310: 0.6185
Loss at step 16320: 0.1168
Loss at step 16330: 0.4860
Loss at step 16340: 0.3388
Loss at step 16350: 0.2591
Loss at step 16360: 0.1533
Loss at step 16370: 0.5232
Loss at step 16380: 0.2126
Loss at step 16390: 0.1098
Loss at step 16400: 0.7149
Loss at step 16410: 0.4399
Loss at step 16420: 0.2183
Loss at step 16430: 0.3219
Loss at step 16440: 0.1971
Loss at step 16450: 0.1936
Loss at step 16460: 0.0462
Loss at step 16470: 0.4050
Loss at step 16480: 0.4665
Loss at step 16490: 0.3406
Loss at step 16500: 0.4146
Loss at step 16510: 0.1086
Loss at step 16520: 0.4339
Loss at step 16530: 0.3522
Loss at step 16540: 0.4534
Loss at step 16550: 0.4374
Loss at step 16560: 0.1603
Loss at step 16570: 0.3785
Loss at step 16580: 0.2282
Loss at step 16590: 0.3163
Loss at step 16600: 0.3374
Loss at step 16610: 0.2715
Loss at step 16620: 0.3466
Loss at step 16630: 0.4777
Loss at step 16640: 0.3599
Loss at step 16650: 0.1317
Loss at step 16660: 0.2440
Loss at step 16670: 0.3677
Loss at step 16680: 0.3572
Loss at step 16690: 0.2865
Loss at step 16700: 0.2887
Loss at step 16710: 0.7695
Loss at step 16720: 0.2777
Loss at step 16730: 0.2040
Loss at step 16740: 0.4233
Loss at step 16750: 0.2950
Loss at step 16760: 0.4664
Loss at step 16770: 0.3383
Loss at step 16780: 0.1879
Loss at step 16790: 0.1006
Loss at step 16800: 0.2243
Loss at step 16810: 0.2816
Loss at step 16820: 0.3383
Loss at step 16830: 0.4360
Loss at step 16840: 0.1740
Loss at step 16850: 0.1201
Loss at step 16860: 0.0291
Loss at step 16870: 0.2358
Loss at step 16880: 0.1679
Loss at step 16890: 0.0872
Loss at step 16900: 0.3888
Loss at step 16910: 0.5690
Loss at step 16920: 0.3391
Loss at step 16930: 0.2794
Loss at step 16940: 0.3597
Loss at step 16950: 0.1916
Loss at step 16960: 0.1177
Loss at step 16970: 0.1513
Loss at step 16980: 0.1150
Loss at step 16990: 0.2561
Loss at step 17000: 0.1531
Loss at step 17010: 0.0861
Loss at step 17020: 0.3219
Loss at step 17030: 0.9144
Loss at step 17040: 0.0433
Loss at step 17050: 0.3192
Loss at step 17060: 0.4018
Loss at step 17070: 0.0551
Loss at step 17080: 0.1808
Loss at step 17090: 0.5250
Loss at step 17100: 0.2667
Loss at step 17110: 0.3239
Loss at step 17120: 0.0852
Loss at step 17130: 0.4107
Loss at step 17140: 0.0759
Loss at step 17150: 0.3060
Loss at step 17160: 0.2514
Loss at step 17170: 0.4464
Loss at step 17180: 0.0699
Loss at step 17190: 0.4003
Loss at step 17200: 0.1083
Loss at step 17210: 0.4468
Loss at step 17220: 0.0816
Loss at step 17230: 0.3299
Loss at step 17240: 0.1691
Loss at step 17250: 0.0642
Loss at step 17260: 0.0293
Loss at step 17270: 0.1502
Loss at step 17280: 0.2366
Loss at step 17290: 0.2379
Loss at step 17300: 0.3001
Loss at step 17310: 0.1484
Loss at step 17320: 0.1360
Loss at step 17330: 0.1273
Loss at step 17340: 0.7060
Loss at step 17350: 0.1784
Loss at step 17360: 0.1883
Loss at step 17370: 0.1401
Loss at step 17380: 0.1668
Loss at step 17390: 0.7629
Loss at step 17400: 0.2245
Loss at step 17410: 0.0752
Loss at step 17420: 0.0868
Loss at step 17430: 0.2116
Loss at step 17440: 0.7292
Loss at step 17450: 0.6696
Loss at step 17460: 0.5282
Loss at step 17470: 0.2285
Loss at step 17480: 0.3361
Loss at step 17490: 0.2781
Loss at step 17500: 0.2217
Loss at step 17510: 0.1224
Loss at step 17520: 0.5547
Loss at step 17530: 0.0780
Loss at step 17540: 0.2143
Loss at step 17550: 0.3240
Loss at step 17560: 0.7406
Loss at step 17570: 0.0316
Loss at step 17580: 0.1610
Loss at step 17590: 0.3104
Loss at step 17600: 0.0898
Loss at step 17610: 0.1932
Loss at step 17620: 0.1934
Loss at step 17630: 0.3656
Loss at step 17640: 0.2989
Loss at step 17650: 0.4445
Loss at step 17660: 0.2757
Loss at step 17670: 0.0274
Loss at step 17680: 0.0411
Loss at step 17690: 0.2873
Loss at step 17700: 0.0172
Loss at step 17710: 0.2309
Loss at step 17720: 0.3718
Loss at step 17730: 0.1847
Loss at step 17740: 0.2152
Loss at step 17750: 0.3184
Loss at step 17760: 0.3927
Loss at step 17770: 0.5732
Loss at step 17780: 0.5923
Loss at step 17790: 0.2450
Loss at step 17800: 0.4575
Loss at step 17810: 0.2104
Loss at step 17820: 0.4600
Loss at step 17830: 0.1958
Loss at step 17840: 0.2600
Loss at step 17850: 0.1350
Loss at step 17860: 0.0192
Loss at step 17870: 0.1553
Loss at step 17880: 0.3439
Loss at step 17890: 0.1875
Loss at step 17900: 0.1279
Loss at step 17910: 0.4748
Loss at step 17920: 0.3919
Loss at step 17930: 0.6752
Loss at step 17940: 0.1404
Loss at step 17950: 0.3886
Loss at step 17960: 0.4115
Loss at step 17970: 0.1762
Loss at step 17980: 0.2067
Loss at step 17990: 0.5374
Loss at step 18000: 0.3427
Loss at step 18010: 0.4508
Loss at step 18020: 0.1633
Loss at step 18030: 0.5674
Loss at step 18040: 0.1766
Loss at step 18050: 0.2348
Loss at step 18060: 0.3095
Loss at step 18070: 0.3566
Loss at step 18080: 0.4983
Loss at step 18090: 0.1753
Loss at step 18100: 0.2537
Loss at step 18110: 0.1179
Loss at step 18120: 0.3697
Loss at step 18130: 0.1185
Loss at step 18140: 0.1080
Loss at step 18150: 0.2269
Loss at step 18160: 0.1454
Loss at step 18170: 0.6449
Loss at step 18180: 0.2949
Loss at step 18190: 0.6942
Loss at step 18200: 0.6380
Loss at step 18210: 0.1707
Loss at step 18220: 0.1413
Loss at step 18230: 0.3393
Loss at step 18240: 0.0845
Loss at step 18250: 0.0804
Loss at step 18260: 0.0967
Loss at step 18270: 0.4932
Loss at step 18280: 0.3157
Loss at step 18290: 0.1591
Loss at step 18300: 0.3221
Loss at step 18310: 0.4472
Loss at step 18320: 0.2851
Loss at step 18330: 0.5757
Loss at step 18340: 0.0187
Loss at step 18350: 0.1427
Loss at step 18360: 0.3693
Loss at step 18370: 0.3290
Loss at step 18380: 0.1529
Loss at step 18390: 0.1853
Loss at step 18400: 0.0303
Loss at step 18410: 0.1496
Loss at step 18420: 0.0649
Loss at step 18430: 0.2504
Loss at step 18440: 0.0687
Loss at step 18450: 0.0798
Loss at step 18460: 0.0907
Loss at step 18470: 0.0882
Loss at step 18480: 0.2306
Loss at step 18490: 0.7122
Loss at step 18500: 0.6444
Loss at step 18510: 0.2036
Loss at step 18520: 0.2709
Loss at step 18530: 0.3669
Loss at step 18540: 0.4856
Loss at step 18550: 0.0504
Loss at step 18560: 0.3618
Loss at step 18570: 0.0762
Loss at step 18580: 0.3956
Loss at step 18590: 0.3305
Loss at step 18600: 0.2414
Loss at step 18610: 0.2752
Loss at step 18620: 0.4601
Loss at step 18630: 0.2076
Loss at step 18640: 0.2670
Loss at step 18650: 0.1563
Loss at step 18660: 0.2921
Loss at step 18670: 0.1803
Loss at step 18680: 0.2858
Loss at step 18690: 0.0385
Loss at step 18700: 0.7278
Loss at step 18710: 0.1865
Loss at step 18720: 0.3107
Loss at step 18730: 0.6485
Loss at step 18740: 0.2472
Loss at step 18750: 0.2006
Loss at step 18760: 0.3885
Loss at step 18770: 0.1113
Loss at step 18780: 0.1186
Loss at step 18790: 0.1493
Loss at step 18800: 0.0799
Loss at step 18810: 0.1035
Loss at step 18820: 0.6279
Loss at step 18830: 0.3284
Loss at step 18840: 0.2969
Loss at step 18850: 0.0783
Loss at step 18860: 0.0215
Loss at step 18870: 0.1248
Loss at step 18880: 0.2080
Loss at step 18890: 0.0441
Loss at step 18900: 0.2526
Loss at step 18910: 0.0828
Loss at step 18920: 0.0266
Loss at step 18930: 0.3760
Loss at step 18940: 0.3305
Loss at step 18950: 0.0859
Loss at step 18960: 0.2277
Loss at step 18970: 0.0294
Loss at step 18980: 0.3034
Loss at step 18990: 0.3208
Loss at step 19000: 0.1674
Loss at step 19010: 0.4204
Loss at step 19020: 0.2892
Loss at step 19030: 0.1653
Loss at step 19040: 0.0845
Loss at step 19050: 0.1015
Loss at step 19060: 0.0532
Loss at step 19070: 0.3639
Loss at step 19080: 0.2770
Loss at step 19090: 0.1716
Loss at step 19100: 0.5071
Loss at step 19110: 0.1862
Loss at step 19120: 0.2852
Loss at step 19130: 0.3451
Loss at step 19140: 0.2401
Loss at step 19150: 0.1387
Loss at step 19160: 0.5545
Loss at step 19170: 0.4838
Loss at step 19180: 0.4122
Loss at step 19190: 0.5284
Loss at step 19200: 0.4958
Loss at step 19210: 0.4512
Loss at step 19220: 0.1476
Loss at step 19230: 0.1599
Loss at step 19240: 0.0371
Loss at step 19250: 0.3080
Loss at step 19260: 0.2753
Loss at step 19270: 0.2505
Loss at step 19280: 0.2809
Loss at step 19290: 0.1862
Loss at step 19300: 0.4774
Loss at step 19310: 0.2248
Loss at step 19320: 0.1164
Loss at step 19330: 0.1137
Loss at step 19340: 0.0869
Loss at step 19350: 0.1730
Loss at step 19360: 0.4423
Loss at step 19370: 0.2602
Loss at step 19380: 0.0416
Loss at step 19390: 0.0902
Loss at step 19400: 0.2075
Loss at step 19410: 0.2420
Loss at step 19420: 0.2712
Loss at step 19430: 0.1394
Loss at step 19440: 0.3965
Loss at step 19450: 0.0668
Loss at step 19460: 0.1664
Loss at step 19470: 0.2245
Loss at step 19480: 0.1967
Loss at step 19490: 0.3434
Loss at step 19500: 0.0331
Loss at step 19510: 0.1936
Loss at step 19520: 0.1927
Loss at step 19530: 0.2372
Loss at step 19540: 0.0794
Loss at step 19550: 0.3834
Loss at step 19560: 0.7533
Loss at step 19570: 0.1064
Loss at step 19580: 0.4439
Loss at step 19590: 0.6995
Loss at step 19600: 0.1569
Loss at step 19610: 0.2431
Loss at step 19620: 0.4585
Loss at step 19630: 0.1171
Loss at step 19640: 0.0184
Loss at step 19650: 0.1476
Loss at step 19660: 0.2387
Loss at step 19670: 0.2092
Loss at step 19680: 0.1901
Loss at step 19690: 0.2710
Loss at step 19700: 0.4445
Loss at step 19710: 0.2710
Loss at step 19720: 0.0431
Loss at step 19730: 0.5592
Loss at step 19740: 0.0923
Loss at step 19750: 0.3819
Loss at step 19760: 0.2238
Loss at step 19770: 0.0772
Loss at step 19780: 0.1947
Loss at step 19790: 0.3492
Loss at step 19800: 0.0135
Loss at step 19810: 0.1551
Loss at step 19820: 0.1568
Loss at step 19830: 0.0879
Loss at step 19840: 0.2371
Loss at step 19850: 0.2785
Loss at step 19860: 0.2027
Loss at step 19870: 0.4012
Loss at step 19880: 0.2822
Loss at step 19890: 0.1474
Loss at step 19900: 0.1528
Loss at step 19910: 0.1662
Loss at step 19920: 0.2678
Loss at step 19930: 0.0445
Loss at step 19940: 0.1523
Loss at step 19950: 0.0040
Loss at step 19960: 0.0964
Loss at step 19970: 0.1340
Loss at step 19980: 0.3634
Loss at step 19990: 0.1300
Loss at step 20000: 0.0514
Loss at step 20010: 0.2538
Loss at step 20020: 0.3665
Loss at step 20030: 0.0975
Loss at step 20040: 0.4418
Loss at step 20050: 0.0810
Loss at step 20060: 0.2979
Loss at step 20070: 0.1833
Loss at step 20080: 0.2041
Loss at step 20090: 0.0704
Loss at step 20100: 0.5042
Loss at step 20110: 0.2021
Loss at step 20120: 0.0051
Loss at step 20130: 0.6091
Loss at step 20140: 0.4581
Loss at step 20150: 0.4549
Loss at step 20160: 0.1179
Loss at step 20170: 0.4920
Loss at step 20180: 0.4403
Loss at step 20190: 0.1721
Loss at step 20200: 0.0899
Loss at step 20210: 0.1870
Loss at step 20220: 0.3635
Loss at step 20230: 0.2639
Loss at step 20240: 0.0796
Loss at step 20250: 0.0334
Loss at step 20260: 0.2606
Loss at step 20270: 0.4224
Loss at step 20280: 0.5389
Loss at step 20290: 0.2535
Loss at step 20300: 0.2903
Loss at step 20310: 0.4582
Loss at step 20320: 0.1514
Loss at step 20330: 0.5461
Loss at step 20340: 0.2550
Loss at step 20350: 0.3673
Loss at step 20360: 0.4370
Loss at step 20370: 0.2054
Loss at step 20380: 0.0626
Loss at step 20390: 0.3923
Loss at step 20400: 0.0715
Loss at step 20410: 0.1635
Loss at step 20420: 0.1630
Loss at step 20430: 0.2818
Loss at step 20440: 0.4247
Loss at step 20450: 0.5806
Loss at step 20460: 0.1399
Loss at step 20470: 0.5671
Loss at step 20480: 0.6410
Loss at step 20490: 0.2002
Loss at step 20500: 0.4865
Loss at step 20510: 0.2766
Loss at step 20520: 0.2760
Loss at step 20530: 0.2251
Loss at step 20540: 0.2616
Loss at step 20550: 0.3607
Loss at step 20560: 0.2628
Loss at step 20570: 0.1923
Loss at step 20580: 0.2464
Loss at step 20590: 0.2928
Loss at step 20600: 0.1707
Loss at step 20610: 0.4018
Loss at step 20620: 0.3913
Loss at step 20630: 0.2520
Loss at step 20640: 0.7595
Loss at step 20650: 0.2305
Loss at step 20660: 0.5751
Loss at step 20670: 0.1472
Loss at step 20680: 0.3293
Loss at step 20690: 0.1630
Loss at step 20700: 0.2387
Loss at step 20710: 0.6397
Loss at step 20720: 0.0284
Loss at step 20730: 0.0611
Loss at step 20740: 0.2449
Loss at step 20750: 0.1697
Loss at step 20760: 0.2339
Loss at step 20770: 0.2297
Loss at step 20780: 0.0434
Loss at step 20790: 0.2846
Loss at step 20800: 0.1111
Loss at step 20810: 0.7148
Loss at step 20820: 0.0884
Loss at step 20830: 0.0779
Loss at step 20840: 0.3951
Loss at step 20850: 0.6703
Loss at step 20860: 0.1563
Loss at step 20870: 0.2613
Loss at step 20880: 0.5417
Loss at step 20890: 0.0488
Loss at step 20900: 0.3619
Loss at step 20910: 0.5409
Loss at step 20920: 0.4203
Loss at step 20930: 0.5323
Loss at step 20940: 0.0485
Loss at step 20950: 0.3333
Loss at step 20960: 0.0444
Loss at step 20970: 0.0601
Loss at step 20980: 0.3333
Loss at step 20990: 0.1889
Loss at step 21000: 0.0996
Loss at step 21010: 0.2888
Loss at step 21020: 0.1375
Loss at step 21030: 0.3575
Loss at step 21040: 0.3131
Loss at step 21050: 0.4601
Loss at step 21060: 0.1521
Loss at step 21070: 0.0469
Loss at step 21080: 0.4753
Loss at step 21090: 0.1867
Loss at step 21100: 0.7359
Loss at step 21110: 0.2586
Loss at step 21120: 0.3176
Loss at step 21130: 0.3645
Loss at step 21140: 0.0583
Loss at step 21150: 0.2188
Loss at step 21160: 0.2508
Loss at step 21170: 0.1006
Loss at step 21180: 0.3541
Loss at step 21190: 0.3774
Loss at step 21200: 0.2730
Loss at step 21210: 0.3504
Loss at step 21220: 0.3146
Loss at step 21230: 0.5423
Loss at step 21240: 0.0129
Loss at step 21250: 0.0533
Loss at step 21260: 0.2420
Loss at step 21270: 0.1987
Loss at step 21280: 0.7422
Loss at step 21290: 0.1226
Loss at step 21300: 0.0877
Loss at step 21310: 0.1530
Loss at step 21320: 0.5242
Loss at step 21330: 0.0591
Loss at step 21340: 0.0973
Loss at step 21350: 0.0914
Loss at step 21360: 0.5269
Loss at step 21370: 0.1574
Loss at step 21380: 0.2251
Loss at step 21390: 0.0309
Loss at step 21400: 0.1731
Loss at step 21410: 0.0118
Loss at step 21420: 0.4528
Loss at step 21430: 0.1352
Loss at step 21440: 0.1404
Loss at step 21450: 0.3130
Loss at step 21460: 0.2700
Loss at step 21470: 0.6726
Loss at step 21480: 0.1142
Loss at step 21490: 0.0301
Loss at step 21500: 0.0708
Loss at step 21510: 0.0163
Loss at step 21520: 0.2058
Loss at step 21530: 0.7235
Loss at step 21540: 0.3787
Loss at step 21550: 0.1342
Loss at step 21560: 0.4416
Loss at step 21570: 0.3446
Loss at step 21580: 0.3918
Loss at step 21590: 0.0830
Loss at step 21600: 0.1382
Loss at step 21610: 0.0295
Loss at step 21620: 0.1836
Loss at step 21630: 0.4150
Loss at step 21640: 0.4321
Loss at step 21650: 0.3043
Loss at step 21660: 0.9037
Loss at step 21670: 0.3770
Loss at step 21680: 0.1199
Loss at step 21690: 0.0492
Loss at step 21700: 0.1614
Loss at step 21710: 0.2756
Loss at step 21720: 0.4064
Loss at step 21730: 0.2503
Loss at step 21740: 0.3033
Loss at step 21750: 0.4583
Loss at step 21760: 0.4010
Loss at step 21770: 0.3041
Loss at step 21780: 0.1818
Loss at step 21790: 0.0236
Loss at step 21800: 0.1567
Loss at step 21810: 0.2909
Loss at step 21820: 0.1419
Loss at step 21830: 0.0532
Loss at step 21840: 0.0383
Loss at step 21850: 0.3007
Loss at step 21860: 0.2020
Loss at step 21870: 0.0766
Loss at step 21880: 0.4830
Loss at step 21890: 0.0878
Loss at step 21900: 0.1331
Loss at step 21910: 0.3030
Loss at step 21920: 0.3609
Loss at step 21930: 0.1377
Loss at step 21940: 0.2092
Loss at step 21950: 0.0686
Loss at step 21960: 0.2820
Loss at step 21970: 0.9243
Loss at step 21980: 0.1775
Loss at step 21990: 0.0461
Loss at step 22000: 0.5599
Loss at step 22010: 0.4405
Loss at step 22020: 0.0306
Loss at step 22030: 0.2867
Loss at step 22040: 0.0624
Loss at step 22050: 0.0979
Loss at step 22060: 0.1994
Loss at step 22070: 0.2934
Loss at step 22080: 0.1805
Loss at step 22090: 0.2335
Loss at step 22100: 0.2054
Loss at step 22110: 0.1281
Loss at step 22120: 0.2845
Loss at step 22130: 0.1117
Loss at step 22140: 0.3174
Loss at step 22150: 0.3609
Loss at step 22160: 0.2912
Loss at step 22170: 0.5381
Loss at step 22180: 0.2958
Loss at step 22190: 0.1056
Loss at step 22200: 0.1861
Loss at step 22210: 0.3761
Loss at step 22220: 0.0573
Loss at step 22230: 0.4471
Loss at step 22240: 0.0484
Loss at step 22250: 0.1853
Loss at step 22260: 0.3756
Loss at step 22270: 0.2828
Loss at step 22280: 0.1257
Loss at step 22290: 0.3957
Loss at step 22300: 0.2453
Loss at step 22310: 0.4300
Loss at step 22320: 0.0471
Loss at step 22330: 0.2038
Loss at step 22340: 0.1721
Loss at step 22350: 0.5364
Loss at step 22360: 0.0486
Loss at step 22370: 0.0155
Loss at step 22380: 0.1268
Loss at step 22390: 0.2834
Loss at step 22400: 0.0517
Loss at step 22410: 0.7526
Loss at step 22420: 0.1936
Loss at step 22430: 0.4663
Loss at step 22440: 0.1203
Loss at step 22450: 0.1826
Loss at step 22460: 0.0615
Loss at step 22470: 0.0484
Loss at step 22480: 0.2963
Loss at step 22490: 0.2751
Loss at step 22500: 0.1867
Loss at step 22510: 0.6047
Loss at step 22520: 0.8103
Loss at step 22530: 0.0414
Loss at step 22540: 0.0772
Loss at step 22550: 0.6458
Loss at step 22560: 0.4350
Loss at step 22570: 0.5764
Loss at step 22580: 0.4695
Loss at step 22590: 0.0504
Loss at step 22600: 0.1321
Loss at step 22610: 0.1371
Loss at step 22620: 0.2830
Loss at step 22630: 0.4058
Loss at step 22640: 0.1519
Loss at step 22650: 0.4040
Loss at step 22660: 0.1074
Loss at step 22670: 0.4975
Loss at step 22680: 0.1811
Loss at step 22690: 0.2239
Loss at step 22700: 0.6459
Loss at step 22710: 0.0231
Loss at step 22720: 0.1811
Loss at step 22730: 0.4803
Loss at step 22740: 0.6730
Loss at step 22750: 0.0326
Loss at step 22760: 0.4099
Loss at step 22770: 0.2722
Loss at step 22780: 0.0130
Loss at step 22790: 0.0961
Loss at step 22800: 0.1494
Loss at step 22810: 0.1262
Loss at step 22820: 0.1959
Loss at step 22830: 0.2137
Loss at step 22840: 0.2898
Loss at step 22850: 0.1797
Loss at step 22860: 0.1656
Loss at step 22870: 0.0637
Loss at step 22880: 0.3864
Loss at step 22890: 0.0594
Loss at step 22900: 0.3837
Loss at step 22910: 0.2184
Loss at step 22920: 0.2842
Loss at step 22930: 0.5068
Loss at step 22940: 0.0378
Loss at step 22950: 0.4858
Loss at step 22960: 0.0438
Loss at step 22970: 0.1336
Loss at step 22980: 0.2570
Loss at step 22990: 0.2511
Loss at step 23000: 0.0578
Loss at step 23010: 0.2968
Loss at step 23020: 0.2829
Loss at step 23030: 0.2755
Loss at step 23040: 0.0332
Loss at step 23050: 0.1739
Loss at step 23060: 0.2712
Loss at step 23070: 0.0709
Loss at step 23080: 0.2835
Loss at step 23090: 0.5095
Loss at step 23100: 0.5497
Loss at step 23110: 0.4553
Loss at step 23120: 0.3742
Loss at step 23130: 0.2886
Loss at step 23140: 0.1372
Loss at step 23150: 0.0908
Loss at step 23160: 0.0674
Loss at step 23170: 0.3182
Loss at step 23180: 0.0733
Loss at step 23190: 0.0257
Loss at step 23200: 0.3655
Loss at step 23210: 0.1023
Loss at step 23220: 0.2286
Loss at step 23230: 0.4267
Loss at step 23240: 0.0424
Loss at step 23250: 0.0908
Loss at step 23260: 0.2737
Loss at step 23270: 0.4391
Loss at step 23280: 0.8397
Loss at step 23290: 0.8858
Loss at step 23300: 0.0559
Loss at step 23310: 0.4723
Loss at step 23320: 0.0985
Loss at step 23330: 0.2451
Loss at step 23340: 0.0718
Loss at step 23350: 0.4578
Loss at step 23360: 0.0410
Loss at step 23370: 0.1662
Loss at step 23380: 0.0816
Loss at step 23390: 0.2312
Loss at step 23400: 0.3007
Loss at step 23410: 0.3392
Loss at step 23420: 0.3120
Loss at step 23430: 0.1431
Loss at step 23440: 0.0824
Loss at step 23450: 0.2419
Loss at step 23460: 0.0835
Loss at step 23470: 0.2463
Loss at step 23480: 0.0971
Loss at step 23490: 0.1286
Loss at step 23500: 0.0780
Loss at step 23510: 0.1890
Loss at step 23520: 0.5373
Loss at step 23530: 0.1030
Loss at step 23540: 0.1856
Loss at step 23550: 0.0578
Loss at step 23560: 0.4248
Loss at step 23570: 0.4726
Loss at step 23580: 0.0336
Loss at step 23590: 0.1524
Loss at step 23600: 0.2253
Loss at step 23610: 0.5707
Loss at step 23620: 0.2122
Loss at step 23630: 0.4179
Loss at step 23640: 0.5138
Loss at step 23650: 0.1963
Loss at step 23660: 0.0884
Loss at step 23670: 0.5124
Loss at step 23680: 0.4280
Loss at step 23690: 0.0555
Loss at step 23700: 0.0393
Loss at step 23710: 0.0667
Loss at step 23720: 0.3975
Loss at step 23730: 0.6649
Loss at step 23740: 0.0211
Loss at step 23750: 0.0499
Loss at step 23760: 0.3776
Loss at step 23770: 0.0929
Loss at step 23780: 0.0932
Loss at step 23790: 0.4505
Loss at step 23800: 0.0447
Loss at step 23810: 0.0473
Loss at step 23820: 0.5010
Loss at step 23830: 0.0522
Loss at step 23840: 0.2198
Loss at step 23850: 0.0192
Loss at step 23860: 0.0838
Loss at step 23870: 0.0047
Loss at step 23880: 0.4897
Loss at step 23890: 0.3101
Loss at step 23900: 0.0849
Loss at step 23910: 0.1682
Loss at step 23920: 0.5425
Loss at step 23930: 0.0244
Loss at step 23940: 0.2436
Loss at step 23950: 0.5040
Loss at step 23960: 0.0403
Loss at step 23970: 0.2623
Loss at step 23980: 0.4485
Loss at step 23990: 0.3711
Loss at step 24000: 0.3129
Loss at step 24010: 0.5420
Loss at step 24020: 0.1037
Loss at step 24030: 0.4430
Loss at step 24040: 0.6933
Loss at step 24050: 0.0269
Loss at step 24060: 0.0186
Loss at step 24070: 0.6527
Loss at step 24080: 0.2219
Loss at step 24090: 0.1798
Loss at step 24100: 0.0618
Loss at step 24110: 0.0232
Loss at step 24120: 0.4258
Loss at step 24130: 0.6917
Loss at step 24140: 0.0135
Loss at step 24150: 0.3145
Loss at step 24160: 0.1735
Loss at step 24170: 0.0198
Loss at step 24180: 0.3417
Loss at step 24190: 0.0272
Loss at step 24200: 0.2083
Loss at step 24210: 0.2187
Loss at step 24220: 0.1584
Loss at step 24230: 0.9393
Loss at step 24240: 0.0354
Loss at step 24250: 0.5343
Loss at step 24260: 0.0972
Loss at step 24270: 0.3907
Loss at step 24280: 0.1177
Loss at step 24290: 0.0450
Loss at step 24300: 0.1743
Loss at step 24310: 0.4370
Loss at step 24320: 0.4180
Loss at step 24330: 0.1438
Loss at step 24340: 0.2757
Loss at step 24350: 0.1478
Loss at step 24360: 0.1092
Loss at step 24370: 0.1490
Loss at step 24380: 0.1614
Loss at step 24390: 0.3696
Loss at step 24400: 0.1143
Loss at step 24410: 0.0161
Loss at step 24420: 0.0594
Loss at step 24430: 0.1109
Loss at step 24440: 0.0113
Loss at step 24450: 0.1336
Loss at step 24460: 0.2306
Loss at step 24470: 0.3365
Loss at step 24480: 0.1302
Loss at step 24490: 0.0960
Loss at step 24500: 0.0540
Loss at step 24510: 0.0300
Loss at step 24520: 0.0619
Loss at step 24530: 0.0158
Loss at step 24540: 0.6375
Loss at step 24550: 0.3513
Loss at step 24560: 0.2618
Loss at step 24570: 0.5226
Loss at step 24580: 0.0271
Loss at step 24590: 0.0658
Loss at step 24600: 0.0989
Loss at step 24610: 0.3830
Loss at step 24620: 0.0679
Loss at step 24630: 0.1303
Loss at step 24640: 0.0364
Loss at step 24650: 0.3026
Loss at step 24660: 0.2309
Loss at step 24670: 0.1828
Loss at step 24680: 0.4162
Loss at step 24690: 0.0302
Loss at step 24700: 0.6705
Loss at step 24710: 0.4917
Loss at step 24720: 0.3560
Loss at step 24730: 0.0571
Loss at step 24740: 0.4080
Loss at step 24750: 0.5751
Loss at step 24760: 0.4105
Loss at step 24770: 0.2168
Loss at step 24780: 0.0401
Loss at step 24790: 0.0471
Loss at step 24800: 0.1436
Loss at step 24810: 0.3347
Loss at step 24820: 0.2911
Loss at step 24830: 0.2521
Loss at step 24840: 0.3596
Loss at step 24850: 0.3454
Loss at step 24860: 0.2641
Loss at step 24870: 0.2078
Loss at step 24880: 0.4467
Loss at step 24890: 0.1690
Loss at step 24900: 0.1433
Loss at step 24910: 0.0622
Loss at step 24920: 0.2212
Loss at step 24930: 0.4710
Loss at step 24940: 0.0633
Loss at step 24950: 0.2707
Loss at step 24960: 0.3750
Loss at step 24970: 0.3140
Loss at step 24980: 0.1043
Loss at step 24990: 0.0431
Loss at step 25000: 0.5209
Loss at step 25010: 0.1001
Loss at step 25020: 0.0855
Loss at step 25030: 0.1684
Loss at step 25040: 0.2791
Loss at step 25050: 0.3872
Loss at step 25060: 0.2681
Loss at step 25070: 0.0938
Loss at step 25080: 0.2148
Loss at step 25090: 0.0483
Loss at step 25100: 0.0239
Loss at step 25110: 0.6132
Loss at step 25120: 0.2718
Loss at step 25130: 0.2637
Loss at step 25140: 0.2523
Loss at step 25150: 0.3043
Loss at step 25160: 0.4435
Loss at step 25170: 0.4373
Loss at step 25180: 0.3687
Loss at step 25190: 0.0095
Loss at step 25200: 0.0354
Loss at step 25210: 0.0282
Loss at step 25220: 0.0229
Loss at step 25230: 0.0462
Loss at step 25240: 0.3135
Loss at step 25250: 0.1222
Loss at step 25260: 0.4641
Loss at step 25270: 0.1497
Loss at step 25280: 0.0578
Loss at step 25290: 0.1270
Loss at step 25300: 0.1506
Loss at step 25310: 0.3568
Loss at step 25320: 0.1954
Loss at step 25330: 0.2484
Loss at step 25340: 0.3149
Loss at step 25350: 0.1398
Loss at step 25360: 0.6267
Loss at step 25370: 0.0569
Loss at step 25380: 0.3149
Loss at step 25390: 0.5056
Loss at step 25400: 0.0581
Loss at step 25410: 0.1563
Loss at step 25420: 0.1369
Loss at step 25430: 0.2477
Loss at step 25440: 0.1434
Loss at step 25450: 0.3344
Loss at step 25460: 0.1054
Loss at step 25470: 0.1621
Loss at step 25480: 0.0345
Loss at step 25490: 0.6395
Loss at step 25500: 0.1436
Loss at step 25510: 0.0997
Loss at step 25520: 0.2058
Loss at step 25530: 0.1718
Loss at step 25540: 0.5584
Loss at step 25550: 0.3159
Loss at step 25560: 0.1120
Loss at step 25570: 0.2977
Loss at step 25580: 0.0559
Loss at step 25590: 0.2236
Loss at step 25600: 0.4323
Loss at step 25610: 0.0221
Loss at step 25620: 0.4417
Loss at step 25630: 0.0421
Loss at step 25640: 0.1136
Loss at step 25650: 0.1278
Loss at step 25660: 0.2874
Loss at step 25670: 0.0800
Loss at step 25680: 0.2357
Loss at step 25690: 0.1680
Loss at step 25700: 0.0777
Loss at step 25710: 0.0523
Loss at step 25720: 0.3747
Loss at step 25730: 0.2692
Loss at step 25740: 0.1699
Loss at step 25750: 0.3305
Loss at step 25760: 0.1169
Loss at step 25770: 0.0153
Loss at step 25780: 0.0508
Loss at step 25790: 0.0183
Loss at step 25800: 0.0152
Loss at step 25810: 0.0274
Loss at step 25820: 0.0107
Loss at step 25830: 0.2125
Loss at step 25840: 0.0127
Loss at step 25850: 0.4690
Loss at step 25860: 0.1166
Loss at step 25870: 0.0950
Loss at step 25880: 0.2032
Loss at step 25890: 0.6294
Loss at step 25900: 0.0655
Loss at step 25910: 0.2644
Loss at step 25920: 0.0478
Loss at step 25930: 0.2054
Loss at step 25940: 0.4377
Loss at step 25950: 0.1385
Loss at step 25960: 0.3577
Loss at step 25970: 0.1300
Loss at step 25980: 0.1004
Loss at step 25990: 0.1042
Loss at step 26000: 0.3996
Loss at step 26010: 0.4111
Loss at step 26020: 0.0643
Loss at step 26030: 0.1407
Loss at step 26040: 0.2807
Loss at step 26050: 0.3320
Loss at step 26060: 0.7807
Loss at step 26070: 0.2889
Loss at step 26080: 0.0376
Loss at step 26090: 0.2875
Loss at step 26100: 0.0063
Loss at step 26110: 0.2848
Loss at step 26120: 0.1915
Loss at step 26130: 0.0706
Loss at step 26140: 0.0413
Loss at step 26150: 0.1499
Loss at step 26160: 0.5419
Loss at step 26170: 0.5682
Loss at step 26180: 0.2300
Loss at step 26190: 0.0760
Loss at step 26200: 0.1985
Loss at step 26210: 0.1239
Loss at step 26220: 0.3255
Loss at step 26230: 0.1599
Loss at step 26240: 0.2459
Loss at step 26250: 0.0841
Loss at step 26260: 0.0210
Loss at step 26270: 0.0322
Loss at step 26280: 0.0197
Loss at step 26290: 0.8273
Loss at step 26300: 0.0475
Loss at step 26310: 0.6782
Loss at step 26320: 0.1025
Loss at step 26330: 0.1318
Loss at step 26340: 0.3538
Loss at step 26350: 0.2318
Loss at step 26360: 0.0634
Loss at step 26370: 0.1437
Loss at step 26380: 0.0983
Loss at step 26390: 0.1456
Loss at step 26400: 0.1101
Loss at step 26410: 0.0510
Loss at step 26420: 0.0339
Loss at step 26430: 0.3140
Loss at step 26440: 0.4927
Loss at step 26450: 0.0781
Loss at step 26460: 0.2781
Loss at step 26470: 0.0381
Loss at step 26480: 0.2062
Loss at step 26490: 0.3158
Loss at step 26500: 0.1971
Loss at step 26510: 0.2832
Loss at step 26520: 0.3570
Loss at step 26530: 0.2449
Loss at step 26540: 0.1596
Loss at step 26550: 0.3121
Loss at step 26560: 0.4549
Loss at step 26570: 0.0167
Loss at step 26580: 0.2092
Loss at step 26590: 0.2771
Loss at step 26600: 0.0369
Loss at step 26610: 0.2376
Loss at step 26620: 0.0773
Loss at step 26630: 0.0472
Loss at step 26640: 0.3089
Loss at step 26650: 0.4159
Loss at step 26660: 0.4839
Loss at step 26670: 0.5244
Loss at step 26680: 0.1762
Loss at step 26690: 0.4873
Loss at step 26700: 0.4309
Loss at step 26710: 0.4090
Loss at step 26720: 0.2689
Loss at step 26730: 0.0276
Loss at step 26740: 0.0354
Loss at step 26750: 0.1680
Loss at step 26760: 0.2772
Loss at step 26770: 0.1460
Loss at step 26780: 0.0431
Loss at step 26790: 0.2931
Loss at step 26800: 0.0393
Loss at step 26810: 0.3792
Loss at step 26820: 0.2252
Loss at step 26830: 0.2422
Loss at step 26840: 0.0279
Loss at step 26850: 0.5344
Loss at step 26860: 0.1600
Loss at step 26870: 0.0385
Loss at step 26880: 0.2101
Loss at step 26890: 0.0358
Loss at step 26900: 0.0305
Loss at step 26910: 0.3423
Loss at step 26920: 0.3472
Loss at step 26930: 0.1470
Loss at step 26940: 0.0162
Loss at step 26950: 0.2478
Loss at step 26960: 0.3983
Loss at step 26970: 0.2280
Loss at step 26980: 0.0112
Loss at step 26990: 0.0686
Loss at step 27000: 0.2839
Loss at step 27010: 0.2703
Loss at step 27020: 0.6045
Loss at step 27030: 0.2139
Loss at step 27040: 0.0542
Loss at step 27050: 0.3511
Loss at step 27060: 0.2804
Loss at step 27070: 0.0373
Loss at step 27080: 0.0445
Loss at step 27090: 0.2438
Loss at step 27100: 0.0763
Loss at step 27110: 0.0489
Loss at step 27120: 0.4071
Loss at step 27130: 0.2021
Loss at step 27140: 0.0438
Loss at step 27150: 0.2139
Loss at step 27160: 0.5320
Loss at step 27170: 0.3128
Loss at step 27180: 0.1475
Loss at step 27190: 0.4040
Loss at step 27200: 0.1292
Loss at step 27210: 0.1673
Loss at step 27220: 0.0197
Loss at step 27230: 0.1362
Loss at step 27240: 0.1343
Loss at step 27250: 0.1337
Loss at step 27260: 0.3163
Loss at step 27270: 0.0849
Loss at step 27280: 0.0105
Loss at step 27290: 0.0539
Loss at step 27300: 0.0652
Loss at step 27310: 0.2068
Loss at step 27320: 0.1339
Loss at step 27330: 0.2771
Loss at step 27340: 0.4307
Loss at step 27350: 0.0418
Loss at step 27360: 0.4729
Loss at step 27370: 0.1743
Loss at step 27380: 0.0302
Loss at step 27390: 0.3561
Loss at step 27400: 0.1001
Loss at step 27410: 0.0290
Loss at step 27420: 0.6660
Loss at step 27430: 0.0320
Loss at step 27440: 0.0147
Loss at step 27450: 0.0930
Loss at step 27460: 0.1575
Loss at step 27470: 0.3245
Loss at step 27480: 0.0124
Loss at step 27490: 0.1417
Loss at step 27500: 0.1886
Loss at step 27510: 0.0231
Loss at step 27520: 0.6180
Loss at step 27530: 0.1809
Loss at step 27540: 0.1544
Loss at step 27550: 0.3402
Loss at step 27560: 0.0921
Loss at step 27570: 0.1380
Loss at step 27580: 0.7377
Loss at step 27590: 0.3152
Loss at step 27600: 0.1241
Loss at step 27610: 0.5871
Loss at step 27620: 0.5003
Loss at step 27630: 0.0526
Loss at step 27640: 0.2423
Loss at step 27650: 0.4835
Loss at step 27660: 0.3757
Loss at step 27670: 0.0553
Loss at step 27680: 0.2578
Loss at step 27690: 0.2152
Loss at step 27700: 0.5614
Loss at step 27710: 0.3494
Loss at step 27720: 0.0493
Loss at step 27730: 0.5586
Loss at step 27740: 0.1830
Loss at step 27750: 0.4221
Loss at step 27760: 0.3684
Loss at step 27770: 0.0927
Loss at step 27780: 0.1695
Loss at step 27790: 0.3396
Loss at step 27800: 0.4525
Loss at step 27810: 0.0126
Loss at step 27820: 0.1106
Loss at step 27830: 0.0146
Loss at step 27840: 0.1791
Loss at step 27850: 0.2344
Loss at step 27860: 0.1123
Loss at step 27870: 0.4579
Loss at step 27880: 0.2606
Loss at step 27890: 0.0916
Loss at step 27900: 0.0049
Loss at step 27910: 0.3209
Loss at step 27920: 0.4080
Loss at step 27930: 0.1163
Loss at step 27940: 0.4345
Loss at step 27950: 0.4509
Loss at step 27960: 0.4272
Loss at step 27970: 0.0350
Loss at step 27980: 0.1185
Loss at step 27990: 0.1193
Loss at step 28000: 0.0109
Loss at step 28010: 0.4078
Loss at step 28020: 0.4745
Loss at step 28030: 0.5678
Loss at step 28040: 0.2649
Loss at step 28050: 0.3506
Loss at step 28060: 0.4016
Loss at step 28070: 0.2143
Loss at step 28080: 0.1776
Loss at step 28090: 0.3668
Loss at step 28100: 0.1355
Loss at step 28110: 0.6893
Loss at step 28120: 0.6175
Loss at step 28130: 0.3589
Loss at step 28140: 0.4285
Loss at step 28150: 0.1252
Loss at step 28160: 0.0791
Loss at step 28170: 0.0416
Loss at step 28180: 0.0072
Loss at step 28190: 0.8423
Loss at step 28200: 0.0704
Loss at step 28210: 0.0287
Loss at step 28220: 0.0190
Loss at step 28230: 0.2598
Loss at step 28240: 0.3255
Loss at step 28250: 0.0941
Loss at step 28260: 0.2017
Loss at step 28270: 0.2333
Loss at step 28280: 0.1400
Loss at step 28290: 0.0292
Loss at step 28300: 0.6667
Loss at step 28310: 0.4673
Loss at step 28320: 0.3838
Loss at step 28330: 0.2708
Loss at step 28340: 0.0569
Loss at step 28350: 0.0267
Loss at step 28360: 0.0466
Loss at step 28370: 0.6115
Loss at step 28380: 0.0715
Loss at step 28390: 0.1943
Loss at step 28400: 0.3778
Loss at step 28410: 0.3766
Loss at step 28420: 0.3732
Loss at step 28430: 0.0312
Loss at step 28440: 0.0903
Loss at step 28450: 0.2905
Loss at step 28460: 0.0676
Loss at step 28470: 0.0399
Loss at step 28480: 0.0262
Loss at step 28490: 0.5618
Loss at step 28500: 0.0423
Loss at step 28510: 0.1883
Loss at step 28520: 0.4929
Loss at step 28530: 0.1187
Loss at step 28540: 0.0193
Loss at step 28550: 0.2126
Loss at step 28560: 0.3335
Loss at step 28570: 0.1411
Loss at step 28580: 0.0454
Loss at step 28590: 0.0710
Loss at step 28600: 0.1769
Loss at step 28610: 0.2552
Loss at step 28620: 0.0613
Loss at step 28630: 0.1214
Loss at step 28640: 0.2323
Loss at step 28650: 0.0465
Loss at step 28660: 0.5276
Loss at step 28670: 0.2586
Loss at step 28680: 0.0659
Loss at step 28690: 0.2284
Loss at step 28700: 0.1990
Loss at step 28710: 0.0839
Loss at step 28720: 0.0465
Loss at step 28730: 0.0268
Loss at step 28740: 0.0461
Loss at step 28750: 0.0698
Loss at step 28760: 0.3845
Loss at step 28770: 0.2108
Loss at step 28780: 0.3590
Loss at step 28790: 0.1865
Loss at step 28800: 0.0330
Loss at step 28810: 0.0209
Loss at step 28820: 0.0657
Loss at step 28830: 0.2807
Loss at step 28840: 0.2849
Loss at step 28850: 0.4874
Loss at step 28860: 0.8115
Loss at step 28870: 0.1988
Loss at step 28880: 0.0140
Loss at step 28890: 0.0765
Loss at step 28900: 0.3988
Loss at step 28910: 0.4111
Loss at step 28920: 0.2339
Loss at step 28930: 0.2612
Loss at step 28940: 0.0200
Loss at step 28950: 0.1074
Loss at step 28960: 0.0378
Loss at step 28970: 0.0102
Loss at step 28980: 0.1578
Loss at step 28990: 0.0474
Loss at step 29000: 0.0762
Loss at step 29010: 0.0737
Loss at step 29020: 0.2916
Loss at step 29030: 0.0495
Loss at step 29040: 0.3365
Loss at step 29050: 0.2726
Loss at step 29060: 0.2301
Loss at step 29070: 0.6692
Loss at step 29080: 0.2560
Loss at step 29090: 0.4430
Loss at step 29100: 0.3348
Loss at step 29110: 0.0846
Loss at step 29120: 0.3937
Loss at step 29130: 0.1888
Loss at step 29140: 0.1176
Loss at step 29150: 0.0394
Loss at step 29160: 0.0309
Loss at step 29170: 0.1928
Loss at step 29180: 0.0066
Loss at step 29190: 0.6773
Loss at step 29200: 0.2541
Loss at step 29210: 0.0187
Loss at step 29220: 0.2331
Loss at step 29230: 0.1165
Loss at step 29240: 0.2489
Loss at step 29250: 0.1589
Loss at step 29260: 0.2447
Loss at step 29270: 0.0371
Loss at step 29280: 0.6040
Loss at step 29290: 0.1616
Loss at step 29300: 0.0529
Loss at step 29310: 0.0448
Loss at step 29320: 0.0319
Loss at step 29330: 0.1714
Loss at step 29340: 0.0206
Loss at step 29350: 0.2313
Loss at step 29360: 0.3804
Loss at step 29370: 0.0175
Loss at step 29380: 0.0703
Loss at step 29390: 0.0786
Loss at step 29400: 0.1284
Loss at step 29410: 0.0077
Loss at step 29420: 0.1787
Loss at step 29430: 0.0328
Loss at step 29440: 0.2984
Loss at step 29450: 0.0107
Loss at step 29460: 0.5405
Loss at step 29470: 0.1621
Loss at step 29480: 0.0460
Loss at step 29490: 0.2600
Loss at step 29500: 0.2602
Loss at step 29510: 0.0222
Loss at step 29520: 0.3894
Loss at step 29530: 0.0532
Loss at step 29540: 0.7576
Loss at step 29550: 0.0148
Loss at step 29560: 0.1787
Loss at step 29570: 0.0384
Loss at step 29580: 0.0136
Loss at step 29590: 0.0753
Loss at step 29600: 0.2295
Loss at step 29610: 0.1849
Loss at step 29620: 0.2639
Loss at step 29630: 0.6143
Loss at step 29640: 0.0402
Loss at step 29650: 0.1597
Loss at step 29660: 0.1069
Loss at step 29670: 0.2182
Loss at step 29680: 0.3445
Loss at step 29690: 0.1858
Loss at step 29700: 0.1734
Loss at step 29710: 0.1562
Loss at step 29720: 0.1126
Loss at step 29730: 0.2475
Loss at step 29740: 0.6166
Loss at step 29750: 0.2481
Loss at step 29760: 0.2212
Loss at step 29770: 0.6253
Loss at step 29780: 0.0273
Loss at step 29790: 0.1925
Loss at step 29800: 0.1546
Loss at step 29810: 0.0562
Loss at step 29820: 0.0378
Loss at step 29830: 0.0237
Loss at step 29840: 0.3569
Loss at step 29850: 0.1224
Loss at step 29860: 0.0505
Loss at step 29870: 0.3271
Loss at step 29880: 0.2927
Loss at step 29890: 0.5904
Loss at step 29900: 0.0389
Loss at step 29910: 0.3218
Loss at step 29920: 0.3819
Loss at step 29930: 0.3588
Loss at step 29940: 0.1509
Loss at step 29950: 0.0236
Loss at step 29960: 0.3542
Loss at step 29970: 0.6116
Loss at step 29980: 0.0154
Loss at step 29990: 0.0354
Loss at step 30000: 0.1252
Loss at step 30010: 0.1242
Loss at step 30020: 0.0218
Loss at step 30030: 0.2835
Loss at step 30040: 0.1221
Loss at step 30050: 0.2277
Loss at step 30060: 0.2410
Loss at step 30070: 0.6154
Loss at step 30080: 0.3439
Loss at step 30090: 0.0443
Loss at step 30100: 0.4121
Loss at step 30110: 0.2293
Loss at step 30120: 0.2326
Loss at step 30130: 0.0490
Loss at step 30140: 0.0191
Loss at step 30150: 0.4074
Loss at step 30160: 0.2123
Loss at step 30170: 0.3767
Loss at step 30180: 0.2193
Loss at step 30190: 0.0455
Loss at step 30200: 0.0184
Loss at step 30210: 0.1123
Loss at step 30220: 0.2245
Loss at step 30230: 0.1751
Loss at step 30240: 0.7210
Loss at step 30250: 0.0776
Loss at step 30260: 0.0176
Loss at step 30270: 0.1832
Loss at step 30280: 0.0148
Loss at step 30290: 0.0430
Loss at step 30300: 0.0123
Loss at step 30310: 0.2460
Loss at step 30320: 0.2036
Loss at step 30330: 0.0543
Loss at step 30340: 0.1937
Loss at step 30350: 0.2740
Loss at step 30360: 0.1437
Loss at step 30370: 0.0707
Loss at step 30380: 0.0170
Loss at step 30390: 0.4325
Loss at step 30400: 0.1524
Loss at step 30410: 0.0983
Loss at step 30420: 0.1651
Loss at step 30430: 0.0091
Loss at step 30440: 0.6332
Loss at step 30450: 0.2267
Loss at step 30460: 0.0814
Loss at step 30470: 0.2872
Loss at step 30480: 0.1309
Loss at step 30490: 0.1943
Loss at step 30500: 0.0904
Loss at step 30510: 0.3426
Loss at step 30520: 0.2373
Loss at step 30530: 0.0655
Loss at step 30540: 0.3310
Loss at step 30550: 0.1152
Loss at step 30560: 0.3192
Loss at step 30570: 0.4181
Loss at step 30580: 0.0342
Loss at step 30590: 0.0663
Loss at step 30600: 0.0403
Loss at step 30610: 0.4961
Loss at step 30620: 0.2778
Loss at step 30630: 0.2220
Loss at step 30640: 0.0977
Loss at step 30650: 0.1013
Loss at step 30660: 0.2748
Loss at step 30670: 0.2422
Loss at step 30680: 0.3330
Loss at step 30690: 0.1695
Loss at step 30700: 0.0290
Loss at step 30710: 0.1530
Loss at step 30720: 0.4862
Loss at step 30730: 0.0120
Loss at step 30740: 0.3980
Loss at step 30750: 0.1714
Loss at step 30760: 0.3900
Loss at step 30770: 0.6083
Loss at step 30780: 0.3072
Loss at step 30790: 0.0455
Loss at step 30800: 0.1621
Loss at step 30810: 0.1164
Loss at step 30820: 0.0381
Loss at step 30830: 0.1493
Loss at step 30840: 0.0226
Loss at step 30850: 0.1650
Loss at step 30860: 0.3264
Loss at step 30870: 0.0165
Loss at step 30880: 0.2191
Loss at step 30890: 0.1838
Loss at step 30900: 0.1324
Loss at step 30910: 0.0171
Loss at step 30920: 0.0894
Loss at step 30930: 0.0815
Loss at step 30940: 0.0277
Loss at step 30950: 0.7980
Loss at step 30960: 0.1036
Loss at step 30970: 0.0955
Loss at step 30980: 0.1587
Loss at step 30990: 0.4178
Loss at step 31000: 0.0425
Loss at step 31010: 0.2002
Loss at step 31020: 0.3766
Loss at step 31030: 0.4643
Loss at step 31040: 0.0517
Loss at step 31050: 0.0259
Loss at step 31060: 0.0125
Loss at step 31070: 0.0258
Loss at step 31080: 0.0485
Loss at step 31090: 0.5113
Loss at step 31100: 0.0344
Loss at step 31110: 0.1944
Loss at step 31120: 0.1955
Loss at step 31130: 0.3114
Loss at step 31140: 0.1886
Loss at step 31150: 0.6643
Loss at step 31160: 0.0180
Loss at step 31170: 0.1485
Loss at step 31180: 0.0166
Loss at step 31190: 0.4785
Loss at step 31200: 0.1982
Loss at step 31210: 0.0757
Loss at step 31220: 0.1373
Loss at step 31230: 0.0336
Loss at step 31240: 0.3707
Loss at step 31250: 0.2330
Loss at step 31260: 0.1831
Loss at step 31270: 0.0282
Loss at step 31280: 0.1028
Loss at step 31290: 0.1031
Loss at step 31300: 0.0249
Loss at step 31310: 0.2300
Loss at step 31320: 0.3766
Loss at step 31330: 0.0262
Loss at step 31340: 0.2545
Loss at step 31350: 0.2303
Loss at step 31360: 0.5226
Loss at step 31370: 0.0178
Loss at step 31380: 0.1941
Loss at step 31390: 0.1815
Loss at step 31400: 0.1524
Loss at step 31410: 0.3690
Loss at step 31420: 0.2466
Loss at step 31430: 0.5436
Loss at step 31440: 0.0271
Loss at step 31450: 0.3198
Loss at step 31460: 0.8310
Loss at step 31470: 0.0211
Loss at step 31480: 0.0197
Loss at step 31490: 0.3639
Loss at step 31500: 0.0756
Loss at step 31510: 0.2217
Loss at step 31520: 0.3894
Loss at step 31530: 0.2843
Loss at step 31540: 0.2930
Loss at step 31550: 0.0160
Loss at step 31560: 0.0933
Loss at step 31570: 0.0637
Loss at step 31580: 0.3109
Loss at step 31590: 0.0086
Loss at step 31600: 0.3871
Loss at step 31610: 0.2501
Loss at step 31620: 0.1921
Loss at step 31630: 0.0185
Loss at step 31640: 0.1155
Loss at step 31650: 0.0168
Loss at step 31660: 0.7088
Loss at step 31670: 0.2307
Loss at step 31680: 0.6453
Loss at step 31690: 0.3347
Loss at step 31700: 0.0175
Loss at step 31710: 0.2310
Loss at step 31720: 0.5937
Loss at step 31730: 0.2870
Loss at step 31740: 0.0277
Loss at step 31750: 0.3256
Loss at step 31760: 0.1999
Loss at step 31770: 0.2629
Loss at step 31780: 0.3134
Loss at step 31790: 0.0139
Loss at step 31800: 0.0231
Loss at step 31810: 0.3391
Loss at step 31820: 0.0235
Loss at step 31830: 0.0217
Loss at step 31840: 0.2416
Loss at step 31850: 0.0297
Loss at step 31860: 0.0181
Loss at step 31870: 0.0144
Loss at step 31880: 0.4356
Loss at step 31890: 0.4524
Loss at step 31900: 0.5797
Loss at step 31910: 0.1636
Loss at step 31920: 0.5934
Loss at step 31930: 0.3241
Loss at step 31940: 0.2991
Loss at step 31950: 0.0333
Loss at step 31960: 0.0367
Loss at step 31970: 0.0029
Loss at step 31980: 0.0149
Loss at step 31990: 0.0885
Loss at step 32000: 0.5515
Loss at step 32010: 0.2157
Loss at step 32020: 0.2351
Loss at step 32030: 0.0618
Loss at step 32040: 0.1906
Loss at step 32050: 0.2609
Loss at step 32060: 0.1061
Loss at step 32070: 0.6395
Loss at step 32080: 0.6665
Loss at step 32090: 0.1299
Loss at step 32100: 0.1768
Loss at step 32110: 0.3420
Loss at step 32120: 0.2157
Loss at step 32130: 0.4155
Loss at step 32140: 0.1629
Loss at step 32150: 0.1047
Loss at step 32160: 0.0768
Loss at step 32170: 0.3713
Loss at step 32180: 0.0344
Loss at step 32190: 0.3817
Loss at step 32200: 0.2514
Loss at step 32210: 0.2319
Loss at step 32220: 0.0388
Loss at step 32230: 0.2240
Loss at step 32240: 0.0193
Loss at step 32250: 0.4270
Loss at step 32260: 0.1143
Loss at step 32270: 0.2880
Loss at step 32280: 0.1799
Loss at step 32290: 0.0151
Loss at step 32300: 0.1943
Loss at step 32310: 0.1597
Loss at step 32320: 0.0603
Loss at step 32330: 0.1039
Loss at step 32340: 0.3176
Loss at step 32350: 0.3423
Loss at step 32360: 0.2719
Loss at step 32370: 0.6002
Loss at step 32380: 0.4589
Loss at step 32390: 0.0158
Loss at step 32400: 0.6013
Loss at step 32410: 0.0083
Loss at step 32420: 0.0207
Loss at step 32430: 0.2763
Loss at step 32440: 0.0355
Loss at step 32450: 0.0538
Loss at step 32460: 0.0237
Loss at step 32470: 0.1921
Loss at step 32480: 0.1300
Loss at step 32490: 0.3182
Loss at step 32500: 0.6141
Loss at step 32510: 0.1259
Loss at step 32520: 0.1084
Loss at step 32530: 0.0515
Loss at step 32540: 0.4279
Loss at step 32550: 0.3909
Loss at step 32560: 0.0955
Loss at step 32570: 0.3945
Loss at step 32580: 0.2317
Loss at step 32590: 0.2548
Loss at step 32600: 0.2866
Loss at step 32610: 0.1393
Loss at step 32620: 0.3188
Loss at step 32630: 0.0246
Loss at step 32640: 0.0856
Loss at step 32650: 0.3792
Loss at step 32660: 0.0307
Loss at step 32670: 0.1543
Loss at step 32680: 0.3038
Loss at step 32690: 0.5562
Loss at step 32700: 0.5518
Loss at step 32710: 0.0989
Loss at step 32720: 0.1142
Loss at step 32730: 0.0849
Loss at step 32740: 0.4435
Loss at step 32750: 0.0225
Loss at step 32760: 0.0138
Loss at step 32770: 0.0413
Loss at step 32780: 0.2110
Loss at step 32790: 0.2588
Loss at step 32800: 0.1146
Loss at step 32810: 0.0735
Loss at step 32820: 0.2902
Loss at step 32830: 0.4086
Loss at step 32840: 0.0318
Loss at step 32850: 0.0543
Loss at step 32860: 0.2182
Loss at step 32870: 0.0465
Loss at step 32880: 0.1220
Loss at step 32890: 0.0196
Loss at step 32900: 0.0401
Loss at step 32910: 0.1270
Loss at step 32920: 0.0207
Loss at step 32930: 0.0168
Loss at step 32940: 0.0485
Loss at step 32950: 0.2032
Loss at step 32960: 0.2557
Loss at step 32970: 0.2194
Loss at step 32980: 0.2133
Loss at step 32990: 0.0858
Loss at step 33000: 0.2781
Loss at step 33010: 0.0352
Loss at step 33020: 0.0518
Loss at step 33030: 0.3081
Loss at step 33040: 0.2216
Loss at step 33050: 0.4170
Loss at step 33060: 0.0038
Loss at step 33070: 0.3140
Loss at step 33080: 0.2316
Loss at step 33090: 0.4214
Loss at step 33100: 0.0396
Loss at step 33110: 0.2532
Loss at step 33120: 0.3815
Loss at step 33130: 0.0232
Loss at step 33140: 0.2169
Loss at step 33150: 0.4966
Loss at step 33160: 0.3383
Loss at step 33170: 0.1809
Loss at step 33180: 0.3129
Loss at step 33190: 0.2775
Loss at step 33200: 0.0409
Loss at step 33210: 0.0264
Loss at step 33220: 0.1252
Loss at step 33230: 0.3288
Loss at step 33240: 0.1629
Loss at step 33250: 0.0625
Loss at step 33260: 0.1125
Loss at step 33270: 0.0820
Loss at step 33280: 0.5981
Loss at step 33290: 0.2502
Loss at step 33300: 0.4254
Loss at step 33310: 0.5132
Loss at step 33320: 0.0262
Loss at step 33330: 0.2787
Loss at step 33340: 0.2757
Loss at step 33350: 0.1668
Loss at step 33360: 0.0495
Loss at step 33370: 0.0316
Loss at step 33380: 0.0193
Loss at step 33390: 0.5844
Loss at step 33400: 0.0130
Loss at step 33410: 0.0174
Loss at step 33420: 0.0525
Loss at step 33430: 0.2734
Loss at step 33440: 0.0331
Loss at step 33450: 0.0292
Loss at step 33460: 0.4043
Loss at step 33470: 0.0856
Loss at step 33480: 0.2208
Loss at step 33490: 0.8068
Loss at step 33500: 0.3077
Loss at step 33510: 0.2587
Loss at step 33520: 0.4445
Loss at step 33530: 0.0572
Loss at step 33540: 0.2043
Loss at step 33550: 0.2193
Loss at step 33560: 0.0734
Loss at step 33570: 0.0262
Loss at step 33580: 0.2695
Loss at step 33590: 0.0208
Loss at step 33600: 0.3359
Loss at step 33610: 0.2537
Loss at step 33620: 0.3898
Loss at step 33630: 0.2557
Loss at step 33640: 0.1588
Loss at step 33650: 0.2073
Loss at step 33660: 0.3291
Loss at step 33670: 0.3591
Loss at step 33680: 0.2031
Loss at step 33690: 0.2152
Loss at step 33700: 0.0108
Loss at step 33710: 0.0711
Loss at step 33720: 0.8688
Loss at step 33730: 0.0409
Loss at step 33740: 0.0331
Loss at step 33750: 0.0089
Loss at step 33760: 0.4226
Loss at step 33770: 0.4038
Loss at step 33780: 0.0226
Loss at step 33790: 0.5600
Loss at step 33800: 0.5500
Loss at step 33810: 0.2249
Loss at step 33820: 0.0948
Loss at step 33830: 0.0771
Loss at step 33840: 0.0188
Loss at step 33850: 0.4496
Loss at step 33860: 0.4132
Loss at step 33870: 0.3023
Loss at step 33880: 0.0259
Loss at step 33890: 0.3912
Loss at step 33900: 0.4016
Loss at step 33910: 0.1186
Loss at step 33920: 0.0245
Loss at step 33930: 0.4206
Loss at step 33940: 0.5880
Loss at step 33950: 0.0040
Loss at step 33960: 0.0244
Loss at step 33970: 0.1526
Loss at step 33980: 0.1866
Loss at step 33990: 0.2850
Loss at step 34000: 0.3531
Loss at step 34010: 0.1350
Loss at step 34020: 0.2234
Loss at step 34030: 0.2725
Loss at step 34040: 0.1651
Loss at step 34050: 0.3520
Loss at step 34060: 0.2259
Loss at step 34070: 0.2881
Loss at step 34080: 0.3801
Loss at step 34090: 0.0104
Loss at step 34100: 0.2876
Loss at step 34110: 0.1502
Loss at step 34120: 0.4280
Loss at step 34130: 0.4921
Loss at step 34140: 0.0602
Loss at step 34150: 0.2292
Loss at step 34160: 0.4260
Loss at step 34170: 0.0242
Loss at step 34180: 0.1835
Loss at step 34190: 0.2572
Loss at step 34200: 0.1039
Loss at step 34210: 0.2327
Loss at step 34220: 0.2243
Loss at step 34230: 0.0133
Loss at step 34240: 0.2505
Loss at step 34250: 0.1950
Loss at step 34260: 0.1722
Loss at step 34270: 0.4970
Loss at step 34280: 0.7155
Loss at step 34290: 0.1163
Loss at step 34300: 0.1164
Loss at step 34310: 0.0180
Loss at step 34320: 0.3046
Loss at step 34330: 0.1046
Loss at step 34340: 0.0148
Loss at step 34350: 0.2892
Loss at step 34360: 0.2582
Loss at step 34370: 0.3344
Loss at step 34380: 0.0206
Loss at step 34390: 0.0407
Loss at step 34400: 0.4935
Loss at step 34410: 0.4500
Loss at step 34420: 0.0108
Loss at step 34430: 0.0062
Loss at step 34440: 0.3993
Loss at step 34450: 0.2023
Loss at step 34460: 0.1145
Loss at step 34470: 0.0272
Loss at step 34480: 0.1933
Loss at step 34490: 0.0967
Loss at step 34500: 0.2182
Loss at step 34510: 0.3080
Loss at step 34520: 0.1969
Loss at step 34530: 0.2968
Loss at step 34540: 0.0225
Loss at step 34550: 0.3768
Loss at step 34560: 0.0647
Loss at step 34570: 0.2969
Loss at step 34580: 0.3290
Loss at step 34590: 0.2177
Loss at step 34600: 0.2552
Loss at step 34610: 0.0302
Loss at step 34620: 0.0412
Loss at step 34630: 0.2455
Loss at step 34640: 0.0196
Loss at step 34650: 0.0097
Loss at step 34660: 0.2286
Loss at step 34670: 0.0292
Loss at step 34680: 0.1061
Loss at step 34690: 0.3732
Loss at step 34700: 0.0364
Loss at step 34710: 0.4664
Loss at step 34720: 0.0686
Loss at step 34730: 0.2562
Loss at step 34740: 0.2275
Loss at step 34750: 0.1477
Loss at step 34760: 0.1692
Loss at step 34770: 0.4758
Loss at step 34780: 0.5416
Loss at step 34790: 0.0636
Loss at step 34800: 0.4702
Loss at step 34810: 0.3554
Loss at step 34820: 0.1017
Loss at step 34830: 0.3898
Loss at step 34840: 0.5711
Loss at step 34850: 0.1178
Loss at step 34860: 0.3229
Loss at step 34870: 0.0066
Loss at step 34880: 0.0053
Loss at step 34890: 0.3202
Loss at step 34900: 0.3423
Loss at step 34910: 0.4208
Loss at step 34920: 0.2752
Loss at step 34930: 0.0286
Loss at step 34940: 0.0528
Loss at step 34950: 0.2398
Loss at step 34960: 0.0381
Loss at step 34970: 0.0101
Loss at step 34980: 0.0614
Loss at step 34990: 0.0280
Loss at step 35000: 0.8329
Loss at step 35010: 0.6897
Loss at step 35020: 0.0332
Loss at step 35030: 0.2201
Loss at step 35040: 0.1739
Loss at step 35050: 0.1021
Loss at step 35060: 0.0322
Loss at step 35070: 0.0136
Loss at step 35080: 0.0426
Loss at step 35090: 0.1408
Loss at step 35100: 0.0100
Loss at step 35110: 0.0302
Loss at step 35120: 0.2812
Loss at step 35130: 0.2157
Loss at step 35140: 0.3019
Loss at step 35150: 0.3463
Loss at step 35160: 0.0116
Loss at step 35170: 0.2860
Loss at step 35180: 0.1357
Loss at step 35190: 0.1754
Loss at step 35200: 0.8518
Loss at step 35210: 0.0483
Loss at step 35220: 0.6549
Loss at step 35230: 0.4910
Loss at step 35240: 0.2963
Loss at step 35250: 0.1933
Loss at step 35260: 0.3857
Loss at step 35270: 0.4378
Loss at step 35280: 0.2664
Loss at step 35290: 0.0216
Loss at step 35300: 0.0457
Loss at step 35310: 0.1467
Loss at step 35320: 0.0165
Loss at step 35330: 0.0916
Loss at step 35340: 0.0230
Loss at step 35350: 0.2002
Loss at step 35360: 0.2091
Loss at step 35370: 0.0932
Loss at step 35380: 0.1610
Loss at step 35390: 0.0271
Loss at step 35400: 0.0206
Loss at step 35410: 0.3756
Loss at step 35420: 0.5298
Loss at step 35430: 0.2147
Loss at step 35440: 0.2642
Loss at step 35450: 0.2132
Loss at step 35460: 0.1568
Loss at step 35470: 0.0351
Loss at step 35480: 0.1731
Loss at step 35490: 0.2289
Loss at step 35500: 0.1061
Loss at step 35510: 0.0136
Loss at step 35520: 0.1260
Loss at step 35530: 0.0300
Loss at step 35540: 0.0360
Loss at step 35550: 0.1313
Loss at step 35560: 0.2464
Loss at step 35570: 0.1117
Loss at step 35580: 0.0228
Loss at step 35590: 0.0484
Loss at step 35600: 0.3609
Loss at step 35610: 0.1872
Loss at step 35620: 0.6223
Loss at step 35630: 0.0294
Loss at step 35640: 0.1419
Loss at step 35650: 0.2359
Loss at step 35660: 0.1040
Loss at step 35670: 0.0136
Loss at step 35680: 0.0170
Loss at step 35690: 0.2720
Loss at step 35700: 0.2341
Loss at step 35710: 0.0154
Loss at step 35720: 0.0576
Loss at step 35730: 0.1550
Loss at step 35740: 0.1026
Loss at step 35750: 0.2090
Loss at step 35760: 0.1570
Loss at step 35770: 0.1812
Loss at step 35780: 0.0076
Loss at step 35790: 0.1209
Loss at step 35800: 0.0529
Loss at step 35810: 0.4852
Loss at step 35820: 0.1440
Loss at step 35830: 0.0128
Loss at step 35840: 0.2967
Loss at step 35850: 0.2040
Loss at step 35860: 0.0349
Loss at step 35870: 0.2585
Loss at step 35880: 0.1034
Loss at step 35890: 0.0160
Loss at step 35900: 0.3053
Loss at step 35910: 0.0288
Loss at step 35920: 0.4301
Loss at step 35930: 0.2435
Loss at step 35940: 0.1786
Loss at step 35950: 0.5836
Loss at step 35960: 0.1702
Loss at step 35970: 0.0541
Loss at step 35980: 0.0236
Loss at step 35990: 0.0409
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 82080
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.813755, 'precision': [0.894064, 0.374376, 0.366039], 'recall': [0.930744, 0.311639, 0.294839], 'f1': [0.912035, 0.340138, 0.326604]}
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7a9a3b1aeca0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 352944 of the training set: {'input_ids': [101, 3919, 5859, 1024, 1063, 2955, 28819, 1065, 2040, 14977, 1010, 2157, 1012, 102, 1996, 8312, 2006, 6177, 1010, 9874, 3666, 1010, 1998, 8278, 8417, 2001, 3591, 2011, 1996, 2622, 3208, 1012, 1996, 2622, 3208, 2787, 2000, 2079, 1996, 6556, 2491, 8474, 2007, 1037, 3543, 1011, 3898, 2008, 2442, 2022, 1037, 2553, 2105, 2009, 2061, 7910, 7910, 7910, 7910, 2004, 3338, 3085, 1012, 1996, 2622, 3208, 2787, 2000, 2079, 1996, 7910, 4487, 22747, 10665, 2121, 2007, 1037, 3543, 1011, 3898, 2008, 2442, 2022, 1037, 2553, 2105, 2009, 2061, 7910, 2009, 2180, 1005, 1056, 2022, 7910, 7910, 2004, 3338, 3085, 1012, 1996, 2622, 3208, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 107997
Loss at step 10: 0.4708
Loss at step 20: 0.9274
Loss at step 30: 0.3882
Loss at step 40: 0.0916
Loss at step 50: 0.6859
Loss at step 60: 0.5392
Loss at step 70: 0.3208
Loss at step 80: 0.8917
Loss at step 90: 0.1732
Loss at step 100: 0.9071
Loss at step 110: 0.3198
Loss at step 120: 0.9296
Loss at step 130: 0.3423
Loss at step 140: 0.4172
Loss at step 150: 0.5264
Loss at step 160: 0.3834
Loss at step 170: 0.3809
Loss at step 180: 0.6230
Loss at step 190: 0.3054
Loss at step 200: 0.4937
Loss at step 210: 0.9517
Loss at step 220: 0.6029
Loss at step 230: 0.8119
Loss at step 240: 0.2793
Loss at step 250: 0.1814
Loss at step 260: 0.3296
Loss at step 270: 0.6186
Loss at step 280: 0.5216
Loss at step 290: 0.5538
Loss at step 300: 1.0769
Loss at step 310: 0.5435
Loss at step 320: 0.6236
Loss at step 330: 0.1533
Loss at step 340: 0.3501
Loss at step 350: 0.6210
Loss at step 360: 0.5344
Loss at step 370: 0.2078
Loss at step 380: 0.1531
Loss at step 390: 0.3371
Loss at step 400: 0.4544
Loss at step 410: 0.3298
Loss at step 420: 0.7488
Loss at step 430: 0.4009
Loss at step 440: 0.5472
Loss at step 450: 0.4454
Loss at step 460: 0.0336
Loss at step 470: 0.3618
Loss at step 480: 0.2992
Loss at step 490: 0.4420
Loss at step 500: 0.5294
Loss at step 510: 0.3617
Loss at step 520: 0.7099
Loss at step 530: 0.4820
Loss at step 540: 0.2528
Loss at step 550: 0.5901
Loss at step 560: 0.6646
Loss at step 570: 0.6387
Loss at step 580: 0.6187
Loss at step 590: 0.5530
Loss at step 600: 0.6737
Loss at step 610: 0.5094
Loss at step 620: 0.9562
Loss at step 630: 0.2659
Loss at step 640: 0.3258
Loss at step 650: 0.5766
Loss at step 660: 0.2586
Loss at step 670: 0.1404
Loss at step 680: 0.6617
Loss at step 690: 0.1903
Loss at step 700: 0.5761
Loss at step 710: 1.0481
Loss at step 720: 0.5299
Loss at step 730: 0.4653
Loss at step 740: 0.2894
Loss at step 750: 0.1784
Loss at step 760: 0.2925
Loss at step 770: 0.4783
Loss at step 780: 0.3085
Loss at step 790: 0.1700
Loss at step 800: 0.2051
Loss at step 810: 0.6372
Loss at step 820: 0.5501
Loss at step 830: 0.2460
Loss at step 840: 0.3429
Loss at step 850: 0.3265
Loss at step 860: 0.9663
Loss at step 870: 0.4418
Loss at step 880: 0.7188
Loss at step 890: 0.1938
Loss at step 900: 0.5219
Loss at step 910: 0.2854
Loss at step 920: 0.2144
Loss at step 930: 0.6904
Loss at step 940: 0.4043
Loss at step 950: 0.4839
Loss at step 960: 0.4199
Loss at step 970: 0.3589
Loss at step 980: 0.3749
Loss at step 990: 0.5725
Loss at step 1000: 0.6596
Loss at step 1010: 0.4757
Loss at step 1020: 0.4593
Loss at step 1030: 0.5812
Loss at step 1040: 0.0973
Loss at step 1050: 0.3194
Loss at step 1060: 0.7942
Loss at step 1070: 0.1358
Loss at step 1080: 0.6925
Loss at step 1090: 0.3591
Loss at step 1100: 0.2565
Loss at step 1110: 0.4923
Loss at step 1120: 0.2809
Loss at step 1130: 0.3781
Loss at step 1140: 0.3662
Loss at step 1150: 0.0530
Loss at step 1160: 0.2209
Loss at step 1170: 0.4996
Loss at step 1180: 0.4069
Loss at step 1190: 0.4669
Loss at step 1200: 0.5673
Loss at step 1210: 0.3513
Loss at step 1220: 0.5721
Loss at step 1230: 0.2441
Loss at step 1240: 0.2941
Loss at step 1250: 0.3879
Loss at step 1260: 0.4838
Loss at step 1270: 0.4551
Loss at step 1280: 0.5732
Loss at step 1290: 0.3601
Loss at step 1300: 0.4341
Loss at step 1310: 0.3342
Loss at step 1320: 0.4559
Loss at step 1330: 0.2862
Loss at step 1340: 0.1618
Loss at step 1350: 0.4584
Loss at step 1360: 0.4077
Loss at step 1370: 0.0841
Loss at step 1380: 0.2702
Loss at step 1390: 0.2208
Loss at step 1400: 0.7027
Loss at step 1410: 0.7129
Loss at step 1420: 0.4542
Loss at step 1430: 0.4615
Loss at step 1440: 0.3629
Loss at step 1450: 0.6861
Loss at step 1460: 0.3611
Loss at step 1470: 0.4017
Loss at step 1480: 0.2267
Loss at step 1490: 0.9833
Loss at step 1500: 0.5224
Loss at step 1510: 0.3195
Loss at step 1520: 0.5055
Loss at step 1530: 0.1344
Loss at step 1540: 0.6180
Loss at step 1550: 0.3843
Loss at step 1560: 0.3305
Loss at step 1570: 0.4185
Loss at step 1580: 0.6992
Loss at step 1590: 0.3544
Loss at step 1600: 0.5353
Loss at step 1610: 0.2645
Loss at step 1620: 0.5695
Loss at step 1630: 0.1425
Loss at step 1640: 0.5914
Loss at step 1650: 0.3439
Loss at step 1660: 0.1684
Loss at step 1670: 0.5894
Loss at step 1680: 0.6606
Loss at step 1690: 0.2047
Loss at step 1700: 0.2118
Loss at step 1710: 0.3911
Loss at step 1720: 0.5698
Loss at step 1730: 0.1210
Loss at step 1740: 0.7013
Loss at step 1750: 0.2654
Loss at step 1760: 0.5614
Loss at step 1770: 0.0530
Loss at step 1780: 0.4181
Loss at step 1790: 0.5477
Loss at step 1800: 0.6233
Loss at step 1810: 0.3281
Loss at step 1820: 0.2501
Loss at step 1830: 0.5915
Loss at step 1840: 0.6224
Loss at step 1850: 0.2856
Loss at step 1860: 0.6121
Loss at step 1870: 0.4242
Loss at step 1880: 0.2704
Loss at step 1890: 0.4049
Loss at step 1900: 0.5172
Loss at step 1910: 0.5274
Loss at step 1920: 0.3440
Loss at step 1930: 0.5836
Loss at step 1940: 0.4301
Loss at step 1950: 0.3972
Loss at step 1960: 0.3399
Loss at step 1970: 0.5181
Loss at step 1980: 0.8865
Loss at step 1990: 1.2696
Loss at step 2000: 0.2084
Loss at step 2010: 0.4763
Loss at step 2020: 0.1349
Loss at step 2030: 0.3895
Loss at step 2040: 0.2652
Loss at step 2050: 0.4594
Loss at step 2060: 0.2174
Loss at step 2070: 0.7099
Loss at step 2080: 0.3150
Loss at step 2090: 0.2262
Loss at step 2100: 0.8422
Loss at step 2110: 0.3456
Loss at step 2120: 0.4562
Loss at step 2130: 0.3509
Loss at step 2140: 0.6666
Loss at step 2150: 0.2113
Loss at step 2160: 0.4966
Loss at step 2170: 0.3513
Loss at step 2180: 0.7300
Loss at step 2190: 0.5465
Loss at step 2200: 0.3664
Loss at step 2210: 0.5464
Loss at step 2220: 0.3026
Loss at step 2230: 0.5416
Loss at step 2240: 0.4115
Loss at step 2250: 0.9609
Loss at step 2260: 0.2977
Loss at step 2270: 0.1866
Loss at step 2280: 0.4125
Loss at step 2290: 0.2433
Loss at step 2300: 0.2629
Loss at step 2310: 0.5117
Loss at step 2320: 0.4855
Loss at step 2330: 0.6949
Loss at step 2340: 0.1817
Loss at step 2350: 0.5024
Loss at step 2360: 0.6013
Loss at step 2370: 0.5851
Loss at step 2380: 0.1873
Loss at step 2390: 0.5925
Loss at step 2400: 0.3447
Loss at step 2410: 0.4419
Loss at step 2420: 0.5341
Loss at step 2430: 0.7208
Loss at step 2440: 0.5969
Loss at step 2450: 0.2606
Loss at step 2460: 0.8160
Loss at step 2470: 0.4990
Loss at step 2480: 0.3544
Loss at step 2490: 0.2707
Loss at step 2500: 0.1770
Loss at step 2510: 0.1324
Loss at step 2520: 0.5938
Loss at step 2530: 0.3152
Loss at step 2540: 0.3985
Loss at step 2550: 0.5690
Loss at step 2560: 0.4873
Loss at step 2570: 0.3073
Loss at step 2580: 0.4079
Loss at step 2590: 0.3848
Loss at step 2600: 0.8417
Loss at step 2610: 0.5173
Loss at step 2620: 1.0288
Loss at step 2630: 0.5839
Loss at step 2640: 0.1805
Loss at step 2650: 0.6963
Loss at step 2660: 0.4293
Loss at step 2670: 0.2540
Loss at step 2680: 0.2005
Loss at step 2690: 0.2947
Loss at step 2700: 0.3823
Loss at step 2710: 0.2337
Loss at step 2720: 0.4589
Loss at step 2730: 0.9439
Loss at step 2740: 0.2811
Loss at step 2750: 0.2594
Loss at step 2760: 0.2690
Loss at step 2770: 0.8093
Loss at step 2780: 0.2730
Loss at step 2790: 0.5734
Loss at step 2800: 0.4085
Loss at step 2810: 0.1522
Loss at step 2820: 1.0420
Loss at step 2830: 0.5341
Loss at step 2840: 0.5949
Loss at step 2850: 0.4447
Loss at step 2860: 0.6156
Loss at step 2870: 0.3669
Loss at step 2880: 0.1936
Loss at step 2890: 0.2798
Loss at step 2900: 0.3179
Loss at step 2910: 0.3577
Loss at step 2920: 0.3450
Loss at step 2930: 0.3507
Loss at step 2940: 0.4748
Loss at step 2950: 0.5389
Loss at step 2960: 0.6946
Loss at step 2970: 0.0846
Loss at step 2980: 0.6226
Loss at step 2990: 0.2255
Loss at step 3000: 0.5087
Loss at step 3010: 0.5209
Loss at step 3020: 0.3129
Loss at step 3030: 0.6863
Loss at step 3040: 0.4301
Loss at step 3050: 0.2485
Loss at step 3060: 0.4338
Loss at step 3070: 0.5492
Loss at step 3080: 0.2291
Loss at step 3090: 0.1758
Loss at step 3100: 0.5454
Loss at step 3110: 0.5679
Loss at step 3120: 0.4632
Loss at step 3130: 0.2721
Loss at step 3140: 0.3837
Loss at step 3150: 0.9989
Loss at step 3160: 0.4376
Loss at step 3170: 0.2523
Loss at step 3180: 0.2052
Loss at step 3190: 0.4715
Loss at step 3200: 0.8054
Loss at step 3210: 0.3928
Loss at step 3220: 0.5823
Loss at step 3230: 0.1879
Loss at step 3240: 0.2099
Loss at step 3250: 0.6042
Loss at step 3260: 0.5215
Loss at step 3270: 0.6605
Loss at step 3280: 0.4685
Loss at step 3290: 0.0701
Loss at step 3300: 0.6747
Loss at step 3310: 0.2822
Loss at step 3320: 0.3669
Loss at step 3330: 0.2474
Loss at step 3340: 0.5742
Loss at step 3350: 0.2184
Loss at step 3360: 0.3712
Loss at step 3370: 0.7849
Loss at step 3380: 0.2964
Loss at step 3390: 0.1354
Loss at step 3400: 0.1698
Loss at step 3410: 0.4431
Loss at step 3420: 0.4759
Loss at step 3430: 0.6241
Loss at step 3440: 0.5381
Loss at step 3450: 0.7157
Loss at step 3460: 0.3018
Loss at step 3470: 0.2652
Loss at step 3480: 0.2378
Loss at step 3490: 0.2607
Loss at step 3500: 0.1327
Loss at step 3510: 1.1929
Loss at step 3520: 0.5886
Loss at step 3530: 0.5351
Loss at step 3540: 0.2540
Loss at step 3550: 0.2975
Loss at step 3560: 0.5406
Loss at step 3570: 0.4581
Loss at step 3580: 0.1925
Loss at step 3590: 0.3457
Loss at step 3600: 0.2598
Loss at step 3610: 0.3374
Loss at step 3620: 0.3958
Loss at step 3630: 0.2537
Loss at step 3640: 0.3193
Loss at step 3650: 0.4948
Loss at step 3660: 0.8480
Loss at step 3670: 0.2781
Loss at step 3680: 0.5995
Loss at step 3690: 0.4541
Loss at step 3700: 0.7275
Loss at step 3710: 0.2345
Loss at step 3720: 0.4293
Loss at step 3730: 0.7752
Loss at step 3740: 0.3444
Loss at step 3750: 0.6965
Loss at step 3760: 0.8413
Loss at step 3770: 0.4375
Loss at step 3780: 0.2803
Loss at step 3790: 0.6713
Loss at step 3800: 0.6186
Loss at step 3810: 0.1439
Loss at step 3820: 0.1209
Loss at step 3830: 0.1890
Loss at step 3840: 0.4906
Loss at step 3850: 0.4556
Loss at step 3860: 0.2668
Loss at step 3870: 0.4677
Loss at step 3880: 0.3064
Loss at step 3890: 0.6940
Loss at step 3900: 0.6496
Loss at step 3910: 0.3272
Loss at step 3920: 0.6877
Loss at step 3930: 0.3610
Loss at step 3940: 0.4676
Loss at step 3950: 0.3608
Loss at step 3960: 0.6841
Loss at step 3970: 0.3478
Loss at step 3980: 0.4015
Loss at step 3990: 0.2902
Loss at step 4000: 0.2105
Loss at step 4010: 0.4443
Loss at step 4020: 0.6191
Loss at step 4030: 0.5436
Loss at step 4040: 1.0350
Loss at step 4050: 0.2017
Loss at step 4060: 0.4132
Loss at step 4070: 0.5260
Loss at step 4080: 0.1619
Loss at step 4090: 0.4160
Loss at step 4100: 0.5409
Loss at step 4110: 0.5333
Loss at step 4120: 0.4752
Loss at step 4130: 0.4796
Loss at step 4140: 0.7569
Loss at step 4150: 0.6998
Loss at step 4160: 0.2988
Loss at step 4170: 0.4951
Loss at step 4180: 0.3984
Loss at step 4190: 0.9652
Loss at step 4200: 0.6898
Loss at step 4210: 0.1079
Loss at step 4220: 0.4215
Loss at step 4230: 0.8592
Loss at step 4240: 0.5844
Loss at step 4250: 0.5782
Loss at step 4260: 0.2820
Loss at step 4270: 0.4127
Loss at step 4280: 0.3542
Loss at step 4290: 0.2470
Loss at step 4300: 0.4303
Loss at step 4310: 0.5696
Loss at step 4320: 0.5084
Loss at step 4330: 0.2307
Loss at step 4340: 0.0889
Loss at step 4350: 0.4456
Loss at step 4360: 0.3977
Loss at step 4370: 0.5687
Loss at step 4380: 0.6287
Loss at step 4390: 0.5689
Loss at step 4400: 0.3218
Loss at step 4410: 0.2167
Loss at step 4420: 0.2716
Loss at step 4430: 0.2907
Loss at step 4440: 0.4644
Loss at step 4450: 0.2739
Loss at step 4460: 0.1954
Loss at step 4470: 0.7421
Loss at step 4480: 0.4831
Loss at step 4490: 0.5456
Loss at step 4500: 0.6994
Loss at step 4510: 0.4667
Loss at step 4520: 0.3804
Loss at step 4530: 0.3369
Loss at step 4540: 0.4468
Loss at step 4550: 0.4341
Loss at step 4560: 0.2803
Loss at step 4570: 0.2505
Loss at step 4580: 0.2272
Loss at step 4590: 0.2610
Loss at step 4600: 0.6633
Loss at step 4610: 0.4690
Loss at step 4620: 0.2905
Loss at step 4630: 0.4137
Loss at step 4640: 0.4667
Loss at step 4650: 0.5701
Loss at step 4660: 0.4784
Loss at step 4670: 0.1743
Loss at step 4680: 0.3491
Loss at step 4690: 0.4225
Loss at step 4700: 0.7632
Loss at step 4710: 0.4704
Loss at step 4720: 0.1967
Loss at step 4730: 0.2300
Loss at step 4740: 0.3339
Loss at step 4750: 0.1776
Loss at step 4760: 0.6139
Loss at step 4770: 0.2269
Loss at step 4780: 0.3716
Loss at step 4790: 0.3358
Loss at step 4800: 0.1633
Loss at step 4810: 0.2888
Loss at step 4820: 0.0998
Loss at step 4830: 0.2980
Loss at step 4840: 0.2930
Loss at step 4850: 0.1019
Loss at step 4860: 0.4108
Loss at step 4870: 0.4898
Loss at step 4880: 0.2894
Loss at step 4890: 0.3416
Loss at step 4900: 0.3368
Loss at step 4910: 0.1463
Loss at step 4920: 0.7649
Loss at step 4930: 0.4967
Loss at step 4940: 0.2959
Loss at step 4950: 0.4391
Loss at step 4960: 0.2733
Loss at step 4970: 0.3940
Loss at step 4980: 0.4325
Loss at step 4990: 0.5342
Loss at step 5000: 0.4269
Loss at step 5010: 0.5136
Loss at step 5020: 0.4824
Loss at step 5030: 0.7808
Loss at step 5040: 0.3233
Loss at step 5050: 0.4681
Loss at step 5060: 0.3866
Loss at step 5070: 0.2961
Loss at step 5080: 0.3233
Loss at step 5090: 0.1915
Loss at step 5100: 0.5245
Loss at step 5110: 0.1373
Loss at step 5120: 0.3869
Loss at step 5130: 0.2246
Loss at step 5140: 0.4175
Loss at step 5150: 0.8322
Loss at step 5160: 0.2328
Loss at step 5170: 0.2800
Loss at step 5180: 0.3404
Loss at step 5190: 0.1481
Loss at step 5200: 0.2896
Loss at step 5210: 0.2288
Loss at step 5220: 0.3646
Loss at step 5230: 0.7008
Loss at step 5240: 0.2435
Loss at step 5250: 0.2250
Loss at step 5260: 0.2353
Loss at step 5270: 0.3318
Loss at step 5280: 0.4961
Loss at step 5290: 0.4458
Loss at step 5300: 0.3111
Loss at step 5310: 0.4848
Loss at step 5320: 0.2661
Loss at step 5330: 0.4430
Loss at step 5340: 0.2022
Loss at step 5350: 0.3142
Loss at step 5360: 0.2393
Loss at step 5370: 0.2521
Loss at step 5380: 0.5691
Loss at step 5390: 0.4819
Loss at step 5400: 0.1880
Loss at step 5410: 0.2570
Loss at step 5420: 0.6484
Loss at step 5430: 0.6047
Loss at step 5440: 0.2008
Loss at step 5450: 0.4599
Loss at step 5460: 0.3844
Loss at step 5470: 0.5327
Loss at step 5480: 0.5538
Loss at step 5490: 0.4992
Loss at step 5500: 0.3344
Loss at step 5510: 0.5422
Loss at step 5520: 0.5902
Loss at step 5530: 0.4067
Loss at step 5540: 0.3774
Loss at step 5550: 0.8609
Loss at step 5560: 0.4263
Loss at step 5570: 0.2376
Loss at step 5580: 0.4002
Loss at step 5590: 0.4307
Loss at step 5600: 0.3576
Loss at step 5610: 0.2525
Loss at step 5620: 0.3166
Loss at step 5630: 0.5143
Loss at step 5640: 0.4059
Loss at step 5650: 0.1657
Loss at step 5660: 0.6449
Loss at step 5670: 0.2974
Loss at step 5680: 0.3223
Loss at step 5690: 0.7047
Loss at step 5700: 0.2441
Loss at step 5710: 0.1979
Loss at step 5720: 0.1938
Loss at step 5730: 0.1646
Loss at step 5740: 0.2694
Loss at step 5750: 0.2818
Loss at step 5760: 0.4240
Loss at step 5770: 0.7766
Loss at step 5780: 0.3641
Loss at step 5790: 0.1015
Loss at step 5800: 0.5810
Loss at step 5810: 0.2514
Loss at step 5820: 0.4433
Loss at step 5830: 0.4643
Loss at step 5840: 0.4961
Loss at step 5850: 0.2057
Loss at step 5860: 0.4200
Loss at step 5870: 0.4593
Loss at step 5880: 0.4257
Loss at step 5890: 0.4557
Loss at step 5900: 0.4910
Loss at step 5910: 0.1085
Loss at step 5920: 0.4069
Loss at step 5930: 0.5731
Loss at step 5940: 0.4263
Loss at step 5950: 0.7764
Loss at step 5960: 0.4994
Loss at step 5970: 0.4216
Loss at step 5980: 0.1607
Loss at step 5990: 0.4907
Loss at step 6000: 0.4220
Loss at step 6010: 0.4870
Loss at step 6020: 0.3127
Loss at step 6030: 0.7440
Loss at step 6040: 0.5257
Loss at step 6050: 0.4142
Loss at step 6060: 0.4088
Loss at step 6070: 0.3759
Loss at step 6080: 0.1393
Loss at step 6090: 0.4314
Loss at step 6100: 0.2759
Loss at step 6110: 0.6270
Loss at step 6120: 0.3094
Loss at step 6130: 0.2500
Loss at step 6140: 0.4062
Loss at step 6150: 0.1705
Loss at step 6160: 0.1592
Loss at step 6170: 0.3671
Loss at step 6180: 0.2408
Loss at step 6190: 0.3260
Loss at step 6200: 0.5606
Loss at step 6210: 0.0480
Loss at step 6220: 0.2673
Loss at step 6230: 0.4189
Loss at step 6240: 0.4772
Loss at step 6250: 0.3201
Loss at step 6260: 0.7801
Loss at step 6270: 0.4979
Loss at step 6280: 0.1289
Loss at step 6290: 0.3303
Loss at step 6300: 0.2253
Loss at step 6310: 0.4721
Loss at step 6320: 0.7484
Loss at step 6330: 0.4161
Loss at step 6340: 0.2921
Loss at step 6350: 0.1954
Loss at step 6360: 0.3803
Loss at step 6370: 0.2863
Loss at step 6380: 0.1380
Loss at step 6390: 0.3221
Loss at step 6400: 0.1178
Loss at step 6410: 0.4813
Loss at step 6420: 0.7002
Loss at step 6430: 0.3456
Loss at step 6440: 0.4552
Loss at step 6450: 0.4462
Loss at step 6460: 0.3254
Loss at step 6470: 0.2205
Loss at step 6480: 0.3679
Loss at step 6490: 0.5215
Loss at step 6500: 1.1562
Loss at step 6510: 0.2257
Loss at step 6520: 0.5742
Loss at step 6530: 0.2940
Loss at step 6540: 0.3770
Loss at step 6550: 0.1204
Loss at step 6560: 0.5027
Loss at step 6570: 0.7486
Loss at step 6580: 0.1906
Loss at step 6590: 0.3805
Loss at step 6600: 0.1970
Loss at step 6610: 0.4632
Loss at step 6620: 0.4669
Loss at step 6630: 0.4904
Loss at step 6640: 0.3190
Loss at step 6650: 0.3819
Loss at step 6660: 0.2054
Loss at step 6670: 0.1442
Loss at step 6680: 0.6393
Loss at step 6690: 0.1916
Loss at step 6700: 0.5197
Loss at step 6710: 0.3434
Loss at step 6720: 0.2422
Loss at step 6730: 0.3109
Loss at step 6740: 0.2811
Loss at step 6750: 0.5229
Loss at step 6760: 0.3250
Loss at step 6770: 0.4566
Loss at step 6780: 0.4678
Loss at step 6790: 0.7193
Loss at step 6800: 0.4108
Loss at step 6810: 0.2928
Loss at step 6820: 0.3683
Loss at step 6830: 0.5616
Loss at step 6840: 0.3886
Loss at step 6850: 0.2551
Loss at step 6860: 0.2279
Loss at step 6870: 0.4936
Loss at step 6880: 0.5363
Loss at step 6890: 0.3910
Loss at step 6900: 0.0296
Loss at step 6910: 0.3360
Loss at step 6920: 0.2422
Loss at step 6930: 1.0121
Loss at step 6940: 0.3190
Loss at step 6950: 0.2198
Loss at step 6960: 0.3999
Loss at step 6970: 0.4697
Loss at step 6980: 0.3244
Loss at step 6990: 0.1591
Loss at step 7000: 0.1696
Loss at step 7010: 0.3039
Loss at step 7020: 0.5212
Loss at step 7030: 0.4157
Loss at step 7040: 0.5080
Loss at step 7050: 0.3370
Loss at step 7060: 0.4415
Loss at step 7070: 0.2836
Loss at step 7080: 0.1545
Loss at step 7090: 0.0356
Loss at step 7100: 0.1383
Loss at step 7110: 0.6817
Loss at step 7120: 0.5274
Loss at step 7130: 0.4019
Loss at step 7140: 0.2795
Loss at step 7150: 0.2275
Loss at step 7160: 0.2689
Loss at step 7170: 0.0980
Loss at step 7180: 0.4347
Loss at step 7190: 0.2380
Loss at step 7200: 0.1368
Loss at step 7210: 0.9074
Loss at step 7220: 0.2243
Loss at step 7230: 0.6932
Loss at step 7240: 0.5210
Loss at step 7250: 0.4108
Loss at step 7260: 0.2318
Loss at step 7270: 0.1825
Loss at step 7280: 0.3571
Loss at step 7290: 0.6480
Loss at step 7300: 0.3515
Loss at step 7310: 0.1237
Loss at step 7320: 0.3811
Loss at step 7330: 0.3022
Loss at step 7340: 0.6236
Loss at step 7350: 0.3489
Loss at step 7360: 0.3440
Loss at step 7370: 0.4797
Loss at step 7380: 0.2630
Loss at step 7390: 0.2542
Loss at step 7400: 0.2066
Loss at step 7410: 0.3470
Loss at step 7420: 0.2775
Loss at step 7430: 0.5905
Loss at step 7440: 0.1085
Loss at step 7450: 0.2566
Loss at step 7460: 0.4134
Loss at step 7470: 0.5991
Loss at step 7480: 0.1938
Loss at step 7490: 0.0632
Loss at step 7500: 0.1103
Loss at step 7510: 0.2240
Loss at step 7520: 0.3086
Loss at step 7530: 0.2130
Loss at step 7540: 0.3565
Loss at step 7550: 1.0285
Loss at step 7560: 0.4760
Loss at step 7570: 0.4048
Loss at step 7580: 0.6710
Loss at step 7590: 0.4619
Loss at step 7600: 0.0635
Loss at step 7610: 0.4456
Loss at step 7620: 0.8663
Loss at step 7630: 0.2557
Loss at step 7640: 0.2337
Loss at step 7650: 0.7643
Loss at step 7660: 0.5962
Loss at step 7670: 0.1633
Loss at step 7680: 1.0194
Loss at step 7690: 0.3993
Loss at step 7700: 0.3845
Loss at step 7710: 0.4050
Loss at step 7720: 0.1555
Loss at step 7730: 0.1221
Loss at step 7740: 0.3156
Loss at step 7750: 0.2827
Loss at step 7760: 0.3989
Loss at step 7770: 0.7359
Loss at step 7780: 0.0365
Loss at step 7790: 0.3366
Loss at step 7800: 0.3431
Loss at step 7810: 0.7850
Loss at step 7820: 0.5911
Loss at step 7830: 0.1402
Loss at step 7840: 0.5230
Loss at step 7850: 0.3699
Loss at step 7860: 0.3107
Loss at step 7870: 0.1753
Loss at step 7880: 0.3392
Loss at step 7890: 0.3765
Loss at step 7900: 0.3237
Loss at step 7910: 0.3679
Loss at step 7920: 1.1468
Loss at step 7930: 0.3005
Loss at step 7940: 0.7751
Loss at step 7950: 0.4346
Loss at step 7960: 0.3690
Loss at step 7970: 0.3710
Loss at step 7980: 0.8393
Loss at step 7990: 0.5050
Loss at step 8000: 0.1846
Loss at step 8010: 0.6133
Loss at step 8020: 0.4488
Loss at step 8030: 0.1919
Loss at step 8040: 0.2483
Loss at step 8050: 0.0813
Loss at step 8060: 0.4854
Loss at step 8070: 0.2576
Loss at step 8080: 0.1801
Loss at step 8090: 0.2770
Loss at step 8100: 0.2698
Loss at step 8110: 0.7005
Loss at step 8120: 0.4941
Loss at step 8130: 0.3207
Loss at step 8140: 0.6125
Loss at step 8150: 0.2921
Loss at step 8160: 0.2619
Loss at step 8170: 0.3557
Loss at step 8180: 0.6167
Loss at step 8190: 0.4956
Loss at step 8200: 0.2830
Loss at step 8210: 0.2303
Loss at step 8220: 0.2130
Loss at step 8230: 0.2686
Loss at step 8240: 0.5340
Loss at step 8250: 0.6692
Loss at step 8260: 0.4224
Loss at step 8270: 0.3153
Loss at step 8280: 0.5173
Loss at step 8290: 0.1507
Loss at step 8300: 0.2678
Loss at step 8310: 0.2929
Loss at step 8320: 0.1460
Loss at step 8330: 0.2135
Loss at step 8340: 0.2388
Loss at step 8350: 0.0833
Loss at step 8360: 0.1877
Loss at step 8370: 0.4870
Loss at step 8380: 0.1236
Loss at step 8390: 0.0711
Loss at step 8400: 0.7558
Loss at step 8410: 0.6818
Loss at step 8420: 0.2690
Loss at step 8430: 0.1791
Loss at step 8440: 0.3275
Loss at step 8450: 0.2669
Loss at step 8460: 0.7889
Loss at step 8470: 0.2282
Loss at step 8480: 0.5807
Loss at step 8490: 0.3511
Loss at step 8500: 0.5677
Loss at step 8510: 0.2117
Loss at step 8520: 0.3432
Loss at step 8530: 0.5068
Loss at step 8540: 0.2613
Loss at step 8550: 0.3733
Loss at step 8560: 0.2126
Loss at step 8570: 0.5257
Loss at step 8580: 0.3799
Loss at step 8590: 0.4853
Loss at step 8600: 0.2955
Loss at step 8610: 0.3151
Loss at step 8620: 0.1645
Loss at step 8630: 0.1464
Loss at step 8640: 0.0492
Loss at step 8650: 0.2896
Loss at step 8660: 0.3798
Loss at step 8670: 0.4090
Loss at step 8680: 0.3480
Loss at step 8690: 0.3043
Loss at step 8700: 0.4912
Loss at step 8710: 0.2969
Loss at step 8720: 0.2750
Loss at step 8730: 0.9164
Loss at step 8740: 0.6217
Loss at step 8750: 0.1895
Loss at step 8760: 0.5700
Loss at step 8770: 0.2407
Loss at step 8780: 0.5325
Loss at step 8790: 0.4863
Loss at step 8800: 0.1610
Loss at step 8810: 0.5837
Loss at step 8820: 0.3075
Loss at step 8830: 0.2534
Loss at step 8840: 0.0506
Loss at step 8850: 0.2657
Loss at step 8860: 0.4404
Loss at step 8870: 0.5918
Loss at step 8880: 0.6139
Loss at step 8890: 0.5334
Loss at step 8900: 0.6963
Loss at step 8910: 0.0154
Loss at step 8920: 0.2675
Loss at step 8930: 0.2184
Loss at step 8940: 0.9281
Loss at step 8950: 0.5095
Loss at step 8960: 0.7307
Loss at step 8970: 0.3708
Loss at step 8980: 0.4202
Loss at step 8990: 0.2781
Loss at step 9000: 0.3215
Loss at step 9010: 0.9560
Loss at step 9020: 0.1937
Loss at step 9030: 0.1599
Loss at step 9040: 0.6576
Loss at step 9050: 0.3344
Loss at step 9060: 0.3245
Loss at step 9070: 0.2335
Loss at step 9080: 0.2085
Loss at step 9090: 0.4189
Loss at step 9100: 0.4501
Loss at step 9110: 0.1774
Loss at step 9120: 0.3461
Loss at step 9130: 0.4334
Loss at step 9140: 0.3209
Loss at step 9150: 0.4933
Loss at step 9160: 0.2417
Loss at step 9170: 0.1458
Loss at step 9180: 0.4768
Loss at step 9190: 0.0814
Loss at step 9200: 0.2447
Loss at step 9210: 0.3177
Loss at step 9220: 0.1699
Loss at step 9230: 0.2117
Loss at step 9240: 0.2838
Loss at step 9250: 0.3823
Loss at step 9260: 0.1975
Loss at step 9270: 0.3820
Loss at step 9280: 0.2978
Loss at step 9290: 0.4454
Loss at step 9300: 0.2475
Loss at step 9310: 0.4842
Loss at step 9320: 0.2844
Loss at step 9330: 0.0318
Loss at step 9340: 0.6052
Loss at step 9350: 0.4201
Loss at step 9360: 0.5379
Loss at step 9370: 0.2633
Loss at step 9380: 0.3002
Loss at step 9390: 0.2041
Loss at step 9400: 0.1557
Loss at step 9410: 0.5232
Loss at step 9420: 0.3924
Loss at step 9430: 0.2467
Loss at step 9440: 0.2169
Loss at step 9450: 1.0075
Loss at step 9460: 0.2452
Loss at step 9470: 0.7827
Loss at step 9480: 0.4675
Loss at step 9490: 0.1160
Loss at step 9500: 0.2733
Loss at step 9510: 0.2782
Loss at step 9520: 0.1204
Loss at step 9530: 0.1236
Loss at step 9540: 0.2864
Loss at step 9550: 0.2807
Loss at step 9560: 0.4009
Loss at step 9570: 0.4149
Loss at step 9580: 0.6049
Loss at step 9590: 0.0556
Loss at step 9600: 0.2637
Loss at step 9610: 0.1717
Loss at step 9620: 0.4935
Loss at step 9630: 0.6365
Loss at step 9640: 0.2806
Loss at step 9650: 0.4540
Loss at step 9660: 0.4464
Loss at step 9670: 0.3964
Loss at step 9680: 0.3487
Loss at step 9690: 0.4137
Loss at step 9700: 0.5863
Loss at step 9710: 0.5577
Loss at step 9720: 0.3952
Loss at step 9730: 0.3911
Loss at step 9740: 0.5562
Loss at step 9750: 0.1272
Loss at step 9760: 0.7141
Loss at step 9770: 0.5160
Loss at step 9780: 0.2725
Loss at step 9790: 0.7097
Loss at step 9800: 0.3785
Loss at step 9810: 0.6013
Loss at step 9820: 0.1252
Loss at step 9830: 0.1675
Loss at step 9840: 0.1873
Loss at step 9850: 0.8931
Loss at step 9860: 0.4706
Loss at step 9870: 0.6930
Loss at step 9880: 0.2895
Loss at step 9890: 0.6404
Loss at step 9900: 0.3025
Loss at step 9910: 0.4449
Loss at step 9920: 0.7548
Loss at step 9930: 0.1184
Loss at step 9940: 0.3925
Loss at step 9950: 0.4192
Loss at step 9960: 0.2297
Loss at step 9970: 0.2182
Loss at step 9980: 0.1445
Loss at step 9990: 0.8210
Loss at step 10000: 0.0264
Loss at step 10010: 0.3146
Loss at step 10020: 0.4588
Loss at step 10030: 0.3991
Loss at step 10040: 0.2379
Loss at step 10050: 0.1022
Loss at step 10060: 0.1079
Loss at step 10070: 0.3778
Loss at step 10080: 0.5042
Loss at step 10090: 0.3798
Loss at step 10100: 0.5333
Loss at step 10110: 0.1688
Loss at step 10120: 0.2057
Loss at step 10130: 0.2789
Loss at step 10140: 0.1824
Loss at step 10150: 0.2627
Loss at step 10160: 0.0462
Loss at step 10170: 0.3614
Loss at step 10180: 0.4059
Loss at step 10190: 0.2431
Loss at step 10200: 0.1597
Loss at step 10210: 0.0815
Loss at step 10220: 0.6545
Loss at step 10230: 0.3942
Loss at step 10240: 0.3698
Loss at step 10250: 0.5808
Loss at step 10260: 0.0154
Loss at step 10270: 0.3664
Loss at step 10280: 0.2708
Loss at step 10290: 0.2486
Loss at step 10300: 0.7372
Loss at step 10310: 0.2721
Loss at step 10320: 0.3409
Loss at step 10330: 0.2578
Loss at step 10340: 0.6384
Loss at step 10350: 0.2995
Loss at step 10360: 0.2406
Loss at step 10370: 0.5655
Loss at step 10380: 0.3223
Loss at step 10390: 0.4086
Loss at step 10400: 0.3574
Loss at step 10410: 0.3048
Loss at step 10420: 0.4700
Loss at step 10430: 0.3563
Loss at step 10440: 0.2849
Loss at step 10450: 0.3263
Loss at step 10460: 0.1150
Loss at step 10470: 0.4511
Loss at step 10480: 0.3538
Loss at step 10490: 0.3215
Loss at step 10500: 0.7292
Loss at step 10510: 0.2817
Loss at step 10520: 0.6985
Loss at step 10530: 0.5049
Loss at step 10540: 0.4572
Loss at step 10550: 0.1314
Loss at step 10560: 0.2500
Loss at step 10570: 0.1521
Loss at step 10580: 0.2115
Loss at step 10590: 0.5758
Loss at step 10600: 0.3560
Loss at step 10610: 0.6093
Loss at step 10620: 0.2122
Loss at step 10630: 0.1582
Loss at step 10640: 0.8614
Loss at step 10650: 0.1903
Loss at step 10660: 0.1787
Loss at step 10670: 0.4508
Loss at step 10680: 0.6532
Loss at step 10690: 0.1672
Loss at step 10700: 0.2415
Loss at step 10710: 0.2304
Loss at step 10720: 0.4117
Loss at step 10730: 0.4210
Loss at step 10740: 0.1192
Loss at step 10750: 0.5712
Loss at step 10760: 0.4246
Loss at step 10770: 0.2644
Loss at step 10780: 0.4907
Loss at step 10790: 0.8476
Loss at step 10800: 0.4365
Loss at step 10810: 0.2875
Loss at step 10820: 1.1548
Loss at step 10830: 0.1017
Loss at step 10840: 0.2685
Loss at step 10850: 0.2389
Loss at step 10860: 0.2322
Loss at step 10870: 0.4827
Loss at step 10880: 0.4730
Loss at step 10890: 0.6209
Loss at step 10900: 0.2095
Loss at step 10910: 0.3240
Loss at step 10920: 0.3195
Loss at step 10930: 0.2397
Loss at step 10940: 0.6206
Loss at step 10950: 0.2309
Loss at step 10960: 0.3959
Loss at step 10970: 0.3363
Loss at step 10980: 0.4469
Loss at step 10990: 0.4001
Loss at step 11000: 0.1595
Loss at step 11010: 0.3774
Loss at step 11020: 0.5534
Loss at step 11030: 0.1758
Loss at step 11040: 0.2387
Loss at step 11050: 0.3947
Loss at step 11060: 0.2127
Loss at step 11070: 0.2737
Loss at step 11080: 0.2175
Loss at step 11090: 0.2065
Loss at step 11100: 0.1857
Loss at step 11110: 0.0804
Loss at step 11120: 0.3123
Loss at step 11130: 0.3622
Loss at step 11140: 0.2644
Loss at step 11150: 0.3473
Loss at step 11160: 0.3012
Loss at step 11170: 0.0933
Loss at step 11180: 0.7510
Loss at step 11190: 0.2699
Loss at step 11200: 0.4134
Loss at step 11210: 0.2247
Loss at step 11220: 0.0276
Loss at step 11230: 0.2517
Loss at step 11240: 0.3679
Loss at step 11250: 0.3460
Loss at step 11260: 0.1185
Loss at step 11270: 0.4670
Loss at step 11280: 0.3276
Loss at step 11290: 0.1933
Loss at step 11300: 0.3698
Loss at step 11310: 0.1564
Loss at step 11320: 0.3244
Loss at step 11330: 0.1662
Loss at step 11340: 0.5589
Loss at step 11350: 0.3116
Loss at step 11360: 0.0606
Loss at step 11370: 0.3327
Loss at step 11380: 0.3833
Loss at step 11390: 0.6358
Loss at step 11400: 0.1980
Loss at step 11410: 0.5609
Loss at step 11420: 0.3885
Loss at step 11430: 0.4532
Loss at step 11440: 0.7956
Loss at step 11450: 1.0233
Loss at step 11460: 0.3767
Loss at step 11470: 0.3794
Loss at step 11480: 0.0287
Loss at step 11490: 0.2836
Loss at step 11500: 0.2286
Loss at step 11510: 0.1908
Loss at step 11520: 0.0744
Loss at step 11530: 0.1100
Loss at step 11540: 0.3832
Loss at step 11550: 0.3693
Loss at step 11560: 0.1523
Loss at step 11570: 0.1241
Loss at step 11580: 0.8785
Loss at step 11590: 0.1566
Loss at step 11600: 0.6246
Loss at step 11610: 0.1378
Loss at step 11620: 0.3322
Loss at step 11630: 0.6795
Loss at step 11640: 0.3477
Loss at step 11650: 0.2079
Loss at step 11660: 0.1605
Loss at step 11670: 0.3963
Loss at step 11680: 0.1299
Loss at step 11690: 0.4549
Loss at step 11700: 0.4313
Loss at step 11710: 0.6092
Loss at step 11720: 0.2502
Loss at step 11730: 0.4965
Loss at step 11740: 0.1535
Loss at step 11750: 0.0829
Loss at step 11760: 0.2545
Loss at step 11770: 0.2956
Loss at step 11780: 0.1130
Loss at step 11790: 0.0934
Loss at step 11800: 0.2406
Loss at step 11810: 0.3992
Loss at step 11820: 0.0646
Loss at step 11830: 0.4865
Loss at step 11840: 0.8882
Loss at step 11850: 0.5584
Loss at step 11860: 0.3736
Loss at step 11870: 0.2667
Loss at step 11880: 0.5051
Loss at step 11890: 0.2614
Loss at step 11900: 0.1649
Loss at step 11910: 0.2306
Loss at step 11920: 0.3120
Loss at step 11930: 0.0525
Loss at step 11940: 0.6072
Loss at step 11950: 0.3327
Loss at step 11960: 0.2679
Loss at step 11970: 0.6640
Loss at step 11980: 0.2346
Loss at step 11990: 0.2406
Loss at step 12000: 0.5743
Loss at step 12010: 0.2828
Loss at step 12020: 0.3184
Loss at step 12030: 0.3468
Loss at step 12040: 0.1768
Loss at step 12050: 0.3213
Loss at step 12060: 0.1765
Loss at step 12070: 0.1854
Loss at step 12080: 0.3094
Loss at step 12090: 0.1615
Loss at step 12100: 0.2267
Loss at step 12110: 0.4615
Loss at step 12120: 0.3882
Loss at step 12130: 0.2820
Loss at step 12140: 0.1554
Loss at step 12150: 0.1078
Loss at step 12160: 0.0952
Loss at step 12170: 0.3174
Loss at step 12180: 0.4284
Loss at step 12190: 0.2905
Loss at step 12200: 0.3937
Loss at step 12210: 0.2982
Loss at step 12220: 0.2356
Loss at step 12230: 0.2102
Loss at step 12240: 0.3537
Loss at step 12250: 0.2867
Loss at step 12260: 0.7039
Loss at step 12270: 0.5487
Loss at step 12280: 0.1016
Loss at step 12290: 0.2401
Loss at step 12300: 0.0971
Loss at step 12310: 0.2590
Loss at step 12320: 0.1186
Loss at step 12330: 0.0509
Loss at step 12340: 0.2871
Loss at step 12350: 0.2176
Loss at step 12360: 0.3801
Loss at step 12370: 0.1540
Loss at step 12380: 0.3168
Loss at step 12390: 0.5649
Loss at step 12400: 0.3989
Loss at step 12410: 0.4769
Loss at step 12420: 0.9149
Loss at step 12430: 0.3298
Loss at step 12440: 0.3826
Loss at step 12450: 0.6095
Loss at step 12460: 0.3963
Loss at step 12470: 0.4432
Loss at step 12480: 0.5195
Loss at step 12490: 0.1958
Loss at step 12500: 0.0294
Loss at step 12510: 0.4060
Loss at step 12520: 0.1724
Loss at step 12530: 0.3123
Loss at step 12540: 0.4503
Loss at step 12550: 0.3865
Loss at step 12560: 0.3545
Loss at step 12570: 0.1503
Loss at step 12580: 0.7613
Loss at step 12590: 0.1177
Loss at step 12600: 0.3448
Loss at step 12610: 0.3037
Loss at step 12620: 0.3791
Loss at step 12630: 0.4646
Loss at step 12640: 0.2641
Loss at step 12650: 0.2145
Loss at step 12660: 0.8158
Loss at step 12670: 0.0728
Loss at step 12680: 0.2587
Loss at step 12690: 0.6839
Loss at step 12700: 0.0572
Loss at step 12710: 0.5259
Loss at step 12720: 0.3904
Loss at step 12730: 0.2744
Loss at step 12740: 0.3685
Loss at step 12750: 0.6677
Loss at step 12760: 0.1491
Loss at step 12770: 0.9166
Loss at step 12780: 0.2781
Loss at step 12790: 0.4071
Loss at step 12800: 0.2266
Loss at step 12810: 0.3692
Loss at step 12820: 0.5424
Loss at step 12830: 0.7432
Loss at step 12840: 0.0754
Loss at step 12850: 0.4052
Loss at step 12860: 0.4309
Loss at step 12870: 0.2302
Loss at step 12880: 0.2787
Loss at step 12890: 0.0792
Loss at step 12900: 0.0413
Loss at step 12910: 0.2365
Loss at step 12920: 0.5952
Loss at step 12930: 0.3719
Loss at step 12940: 0.5757
Loss at step 12950: 0.4111
Loss at step 12960: 0.2923
Loss at step 12970: 0.5662
Loss at step 12980: 0.2895
Loss at step 12990: 0.2526
Loss at step 13000: 0.5682
Loss at step 13010: 0.5069
Loss at step 13020: 0.3370
Loss at step 13030: 0.3081
Loss at step 13040: 0.2594
Loss at step 13050: 0.2536
Loss at step 13060: 0.1324
Loss at step 13070: 0.6416
Loss at step 13080: 0.3225
Loss at step 13090: 0.5479
Loss at step 13100: 0.6465
Loss at step 13110: 0.6482
Loss at step 13120: 0.2467
Loss at step 13130: 0.3281
Loss at step 13140: 0.0247
Loss at step 13150: 0.4299
Loss at step 13160: 0.0862
Loss at step 13170: 0.6274
Loss at step 13180: 0.5720
Loss at step 13190: 0.3592
Loss at step 13200: 0.2996
Loss at step 13210: 0.4301
Loss at step 13220: 0.1250
Loss at step 13230: 0.2430
Loss at step 13240: 0.0977
Loss at step 13250: 0.2752
Loss at step 13260: 0.2376
Loss at step 13270: 0.1613
Loss at step 13280: 0.2129
Loss at step 13290: 0.2491
Loss at step 13300: 0.2465
Loss at step 13310: 0.4521
Loss at step 13320: 0.2068
Loss at step 13330: 0.0965
Loss at step 13340: 0.3626
Loss at step 13350: 0.1790
Loss at step 13360: 0.1912
Loss at step 13370: 0.7324
Loss at step 13380: 0.1080
Loss at step 13390: 0.4070
Loss at step 13400: 0.2020
Loss at step 13410: 0.2312
Loss at step 13420: 0.4357
Loss at step 13430: 0.0832
Loss at step 13440: 0.1580
Loss at step 13450: 0.2287
Loss at step 13460: 0.2194
Loss at step 13470: 0.3287
Loss at step 13480: 0.3881
Loss at step 13490: 0.0729
Loss at step 13500: 0.2208
Loss at step 13510: 0.2632
Loss at step 13520: 0.1824
Loss at step 13530: 0.5319
Loss at step 13540: 0.2526
Loss at step 13550: 0.2385
Loss at step 13560: 0.1372
Loss at step 13570: 0.1479
Loss at step 13580: 0.2956
Loss at step 13590: 0.4195
Loss at step 13600: 0.1133
Loss at step 13610: 0.5839
Loss at step 13620: 0.5582
Loss at step 13630: 0.4331
Loss at step 13640: 0.4044
Loss at step 13650: 0.2076
Loss at step 13660: 0.1793
Loss at step 13670: 0.5958
Loss at step 13680: 0.2538
Loss at step 13690: 0.3143
Loss at step 13700: 0.2053
Loss at step 13710: 0.2626
Loss at step 13720: 0.1837
Loss at step 13730: 0.4936
Loss at step 13740: 0.2009
Loss at step 13750: 0.1817
Loss at step 13760: 0.1001
Loss at step 13770: 0.4667
Loss at step 13780: 0.2012
Loss at step 13790: 0.2836
Loss at step 13800: 0.0780
Loss at step 13810: 0.3311
Loss at step 13820: 0.2899
Loss at step 13830: 0.4573
Loss at step 13840: 0.0259
Loss at step 13850: 0.3301
Loss at step 13860: 0.4212
Loss at step 13870: 0.2645
Loss at step 13880: 0.2174
Loss at step 13890: 0.1241
Loss at step 13900: 0.0830
Loss at step 13910: 0.3638
Loss at step 13920: 0.5027
Loss at step 13930: 0.2627
Loss at step 13940: 0.5222
Loss at step 13950: 0.2099
Loss at step 13960: 0.6618
Loss at step 13970: 0.1790
Loss at step 13980: 0.4447
Loss at step 13990: 0.2387
Loss at step 14000: 0.0376
Loss at step 14010: 0.6538
Loss at step 14020: 0.0379
Loss at step 14030: 0.0907
Loss at step 14040: 0.7101
Loss at step 14050: 0.1643
Loss at step 14060: 0.4284
Loss at step 14070: 0.3484
Loss at step 14080: 0.2241
Loss at step 14090: 0.4511
Loss at step 14100: 0.7023
Loss at step 14110: 0.5147
Loss at step 14120: 0.2930
Loss at step 14130: 0.1665
Loss at step 14140: 0.1726
Loss at step 14150: 0.1001
Loss at step 14160: 0.1059
Loss at step 14170: 0.4437
Loss at step 14180: 0.3917
Loss at step 14190: 0.5212
Loss at step 14200: 0.5137
Loss at step 14210: 0.2578
Loss at step 14220: 0.4868
Loss at step 14230: 0.3527
Loss at step 14240: 0.1076
Loss at step 14250: 0.3361
Loss at step 14260: 0.3625
Loss at step 14270: 0.5342
Loss at step 14280: 0.0280
Loss at step 14290: 0.1583
Loss at step 14300: 0.1592
Loss at step 14310: 0.1481
Loss at step 14320: 0.0174
Loss at step 14330: 0.4433
Loss at step 14340: 0.3621
Loss at step 14350: 0.2296
Loss at step 14360: 0.2063
Loss at step 14370: 0.3539
Loss at step 14380: 0.3630
Loss at step 14390: 0.2774
Loss at step 14400: 0.0858
Loss at step 14410: 0.4335
Loss at step 14420: 0.6641
Loss at step 14430: 0.3040
Loss at step 14440: 0.8601
Loss at step 14450: 0.2836
Loss at step 14460: 0.2820
Loss at step 14470: 0.5013
Loss at step 14480: 0.1748
Loss at step 14490: 0.4794
Loss at step 14500: 0.3607
Loss at step 14510: 0.2145
Loss at step 14520: 0.3598
Loss at step 14530: 0.3865
Loss at step 14540: 0.6107
Loss at step 14550: 0.4706
Loss at step 14560: 0.5866
Loss at step 14570: 0.1038
Loss at step 14580: 0.5123
Loss at step 14590: 0.3296
Loss at step 14600: 0.2936
Loss at step 14610: 0.1691
Loss at step 14620: 0.5875
Loss at step 14630: 0.0315
Loss at step 14640: 0.2421
Loss at step 14650: 0.5325
Loss at step 14660: 0.2359
Loss at step 14670: 0.2029
Loss at step 14680: 0.1059
Loss at step 14690: 0.4574
Loss at step 14700: 0.2031
Loss at step 14710: 0.1103
Loss at step 14720: 0.1650
Loss at step 14730: 0.7984
Loss at step 14740: 0.6469
Loss at step 14750: 0.3296
Loss at step 14760: 0.2411
Loss at step 14770: 0.4883
Loss at step 14780: 0.2151
Loss at step 14790: 0.3999
Loss at step 14800: 0.2574
Loss at step 14810: 0.4505
Loss at step 14820: 0.0987
Loss at step 14830: 0.1606
Loss at step 14840: 0.0328
Loss at step 14850: 0.1400
Loss at step 14860: 0.3603
Loss at step 14870: 0.2133
Loss at step 14880: 0.1151
Loss at step 14890: 0.4409
Loss at step 14900: 0.1760
Loss at step 14910: 0.1494
Loss at step 14920: 0.1265
Loss at step 14930: 0.3792
Loss at step 14940: 0.5599
Loss at step 14950: 0.8115
Loss at step 14960: 0.3212
Loss at step 14970: 0.2560
Loss at step 14980: 0.6592
Loss at step 14990: 0.4383
Loss at step 15000: 0.1525
Loss at step 15010: 0.2786
Loss at step 15020: 0.3180
Loss at step 15030: 0.3641
Loss at step 15040: 0.0749
Loss at step 15050: 0.4437
Loss at step 15060: 0.0526
Loss at step 15070: 0.2030
Loss at step 15080: 0.1513
Loss at step 15090: 0.5214
Loss at step 15100: 0.1099
Loss at step 15110: 0.3940
Loss at step 15120: 0.3044
Loss at step 15130: 0.1264
Loss at step 15140: 0.2996
Loss at step 15150: 0.0789
Loss at step 15160: 0.4294
Loss at step 15170: 0.1112
Loss at step 15180: 0.5651
Loss at step 15190: 0.1580
Loss at step 15200: 0.2897
Loss at step 15210: 0.4688
Loss at step 15220: 0.2620
Loss at step 15230: 0.2635
Loss at step 15240: 0.5875
Loss at step 15250: 0.1926
Loss at step 15260: 0.4675
Loss at step 15270: 0.2270
Loss at step 15280: 0.4652
Loss at step 15290: 0.0836
Loss at step 15300: 0.0719
Loss at step 15310: 0.3822
Loss at step 15320: 0.1505
Loss at step 15330: 0.1067
Loss at step 15340: 0.2429
Loss at step 15350: 0.3346
Loss at step 15360: 0.4472
Loss at step 15370: 0.3766
Loss at step 15380: 0.2717
Loss at step 15390: 0.5860
Loss at step 15400: 0.2386
Loss at step 15410: 0.3135
Loss at step 15420: 0.4548
Loss at step 15430: 0.3604
Loss at step 15440: 0.1149
Loss at step 15450: 0.1801
Loss at step 15460: 0.5739
Loss at step 15470: 0.1526
Loss at step 15480: 0.1871
Loss at step 15490: 0.3391
Loss at step 15500: 0.1431
Loss at step 15510: 0.1082
Loss at step 15520: 0.0315
Loss at step 15530: 0.1784
Loss at step 15540: 0.3060
Loss at step 15550: 0.2910
Loss at step 15560: 0.0570
Loss at step 15570: 0.0828
Loss at step 15580: 0.2546
Loss at step 15590: 0.4099
Loss at step 15600: 0.2358
Loss at step 15610: 0.6398
Loss at step 15620: 0.0939
Loss at step 15630: 0.2462
Loss at step 15640: 0.4652
Loss at step 15650: 0.2543
Loss at step 15660: 0.6411
Loss at step 15670: 0.0643
Loss at step 15680: 0.2649
Loss at step 15690: 0.1676
Loss at step 15700: 0.0534
Loss at step 15710: 0.2647
Loss at step 15720: 0.2335
Loss at step 15730: 0.4285
Loss at step 15740: 0.1793
Loss at step 15750: 0.9092
Loss at step 15760: 0.0162
Loss at step 15770: 0.3810
Loss at step 15780: 0.0089
Loss at step 15790: 0.2581
Loss at step 15800: 0.0131
Loss at step 15810: 0.2706
Loss at step 15820: 0.2547
Loss at step 15830: 0.3094
Loss at step 15840: 0.1328
Loss at step 15850: 0.2159
Loss at step 15860: 0.2500
Loss at step 15870: 0.0733
Loss at step 15880: 0.1083
Loss at step 15890: 0.3349
Loss at step 15900: 0.2278
Loss at step 15910: 0.2599
Loss at step 15920: 0.4171
Loss at step 15930: 0.2640
Loss at step 15940: 0.3334
Loss at step 15950: 0.0850
Loss at step 15960: 0.3489
Loss at step 15970: 0.2740
Loss at step 15980: 0.1227
Loss at step 15990: 0.3114
Loss at step 16000: 0.3042
Loss at step 16010: 0.2381
Loss at step 16020: 0.1262
Loss at step 16030: 0.0596
Loss at step 16040: 0.6065
Loss at step 16050: 0.2560
Loss at step 16060: 0.0685
Loss at step 16070: 0.0712
Loss at step 16080: 0.6447
Loss at step 16090: 0.0568
Loss at step 16100: 0.3261
Loss at step 16110: 0.1772
Loss at step 16120: 0.1591
Loss at step 16130: 0.1000
Loss at step 16140: 0.3209
Loss at step 16150: 0.1885
Loss at step 16160: 0.3680
Loss at step 16170: 0.6359
Loss at step 16180: 0.2857
Loss at step 16190: 0.0530
Loss at step 16200: 0.2763
Loss at step 16210: 0.6450
Loss at step 16220: 0.1839
Loss at step 16230: 0.4482
Loss at step 16240: 0.5972
Loss at step 16250: 0.5034
Loss at step 16260: 0.6089
Loss at step 16270: 0.0991
Loss at step 16280: 0.0658
Loss at step 16290: 0.1735
Loss at step 16300: 0.5977
Loss at step 16310: 0.4712
Loss at step 16320: 0.0706
Loss at step 16330: 0.1949
Loss at step 16340: 0.6339
Loss at step 16350: 0.2595
Loss at step 16360: 0.2375
Loss at step 16370: 0.1954
Loss at step 16380: 0.2744
Loss at step 16390: 0.4466
Loss at step 16400: 0.4037
Loss at step 16410: 0.2196
Loss at step 16420: 0.5424
Loss at step 16430: 0.1461
Loss at step 16440: 0.1936
Loss at step 16450: 0.2376
Loss at step 16460: 0.1897
Loss at step 16470: 0.1253
Loss at step 16480: 0.0556
Loss at step 16490: 0.1742
Loss at step 16500: 0.1542
Loss at step 16510: 0.2935
Loss at step 16520: 0.2347
Loss at step 16530: 0.5496
Loss at step 16540: 0.3018
Loss at step 16550: 0.4372
Loss at step 16560: 0.1017
Loss at step 16570: 0.3161
Loss at step 16580: 0.1654
Loss at step 16590: 0.3017
Loss at step 16600: 0.5685
Loss at step 16610: 0.0741
Loss at step 16620: 0.1315
Loss at step 16630: 0.4043
Loss at step 16640: 0.3678
Loss at step 16650: 0.5047
Loss at step 16660: 0.3929
Loss at step 16670: 0.2289
Loss at step 16680: 0.1501
Loss at step 16690: 0.0993
Loss at step 16700: 0.7621
Loss at step 16710: 0.5249
Loss at step 16720: 0.0780
Loss at step 16730: 0.4060
Loss at step 16740: 0.1119
Loss at step 16750: 0.0808
Loss at step 16760: 0.1373
Loss at step 16770: 0.8064
Loss at step 16780: 0.5852
Loss at step 16790: 0.1801
Loss at step 16800: 0.3518
Loss at step 16810: 0.3924
Loss at step 16820: 0.3443
Loss at step 16830: 0.0321
Loss at step 16840: 0.2020
Loss at step 16850: 0.1694
Loss at step 16860: 0.0397
Loss at step 16870: 0.5621
Loss at step 16880: 0.4424
Loss at step 16890: 0.1850
Loss at step 16900: 0.1795
Loss at step 16910: 0.3969
Loss at step 16920: 0.4081
Loss at step 16930: 0.1644
Loss at step 16940: 0.0731
Loss at step 16950: 0.2020
Loss at step 16960: 0.1673
Loss at step 16970: 0.2300
Loss at step 16980: 0.2226
Loss at step 16990: 0.2929
Loss at step 17000: 0.3002
Loss at step 17010: 0.2663
Loss at step 17020: 0.4063
Loss at step 17030: 0.3637
Loss at step 17040: 0.3781
Loss at step 17050: 0.3365
Loss at step 17060: 0.1561
Loss at step 17070: 0.6069
Loss at step 17080: 0.5979
Loss at step 17090: 0.1949
Loss at step 17100: 0.1496
Loss at step 17110: 0.1501
Loss at step 17120: 0.4080
Loss at step 17130: 0.4488
Loss at step 17140: 0.0378
Loss at step 17150: 0.3238
Loss at step 17160: 0.3437
Loss at step 17170: 0.5053
Loss at step 17180: 0.3864
Loss at step 17190: 0.2592
Loss at step 17200: 0.1520
Loss at step 17210: 0.3156
Loss at step 17220: 0.1778
Loss at step 17230: 0.0406
Loss at step 17240: 0.3416
Loss at step 17250: 0.3493
Loss at step 17260: 0.2490
Loss at step 17270: 0.0420
Loss at step 17280: 0.2445
Loss at step 17290: 0.2907
Loss at step 17300: 0.2054
Loss at step 17310: 0.4560
Loss at step 17320: 0.0795
Loss at step 17330: 0.3683
Loss at step 17340: 0.2849
Loss at step 17350: 0.5054
Loss at step 17360: 0.3206
Loss at step 17370: 0.5159
Loss at step 17380: 0.0615
Loss at step 17390: 0.1815
Loss at step 17400: 0.1233
Loss at step 17410: 0.2906
Loss at step 17420: 0.5705
Loss at step 17430: 0.2754
Loss at step 17440: 0.1819
Loss at step 17450: 0.7212
Loss at step 17460: 0.2765
Loss at step 17470: 0.1627
Loss at step 17480: 0.5481
Loss at step 17490: 0.1227
Loss at step 17500: 0.2093
Loss at step 17510: 0.2658
Loss at step 17520: 0.0620
Loss at step 17530: 0.2600
Loss at step 17540: 0.0839
Loss at step 17550: 0.1852
Loss at step 17560: 0.4232
Loss at step 17570: 0.2973
Loss at step 17580: 0.3516
Loss at step 17590: 0.2279
Loss at step 17600: 0.4266
Loss at step 17610: 0.0211
Loss at step 17620: 0.1998
Loss at step 17630: 0.1637
Loss at step 17640: 0.6382
Loss at step 17650: 0.2009
Loss at step 17660: 0.0470
Loss at step 17670: 0.5559
Loss at step 17680: 0.3884
Loss at step 17690: 0.2667
Loss at step 17700: 0.5233
Loss at step 17710: 0.1457
Loss at step 17720: 0.2219
Loss at step 17730: 0.1839
Loss at step 17740: 0.1200
Loss at step 17750: 0.2935
Loss at step 17760: 0.3725
Loss at step 17770: 0.1299
Loss at step 17780: 0.2417
Loss at step 17790: 0.0642
Loss at step 17800: 0.4030
Loss at step 17810: 0.1919
Loss at step 17820: 0.7362
Loss at step 17830: 0.1915
Loss at step 17840: 0.1994
Loss at step 17850: 0.1044
Loss at step 17860: 0.2750
Loss at step 17870: 0.2822
Loss at step 17880: 0.5988
Loss at step 17890: 0.1329
Loss at step 17900: 0.2535
Loss at step 17910: 0.5844
Loss at step 17920: 0.1941
Loss at step 17930: 0.1126
Loss at step 17940: 0.2652
Loss at step 17950: 0.3606
Loss at step 17960: 1.2054
Loss at step 17970: 0.7860
Loss at step 17980: 0.3645
Loss at step 17990: 0.2274
Loss at step 18000: 0.1866
Loss at step 18010: 0.1708
Loss at step 18020: 0.0159
Loss at step 18030: 0.4444
Loss at step 18040: 0.5909
Loss at step 18050: 0.1964
Loss at step 18060: 0.2984
Loss at step 18070: 0.0887
Loss at step 18080: 0.3826
Loss at step 18090: 0.3763
Loss at step 18100: 0.5033
Loss at step 18110: 0.0282
Loss at step 18120: 0.2798
Loss at step 18130: 0.3073
Loss at step 18140: 0.1603
Loss at step 18150: 0.1335
Loss at step 18160: 0.2366
Loss at step 18170: 0.0908
Loss at step 18180: 0.3939
Loss at step 18190: 0.1484
Loss at step 18200: 0.3265
Loss at step 18210: 0.4308
Loss at step 18220: 0.4219
Loss at step 18230: 0.3474
Loss at step 18240: 0.3354
Loss at step 18250: 0.0613
Loss at step 18260: 0.8449
Loss at step 18270: 0.2386
Loss at step 18280: 0.7899
Loss at step 18290: 0.0541
Loss at step 18300: 0.1802
Loss at step 18310: 0.0197
Loss at step 18320: 0.3913
Loss at step 18330: 0.7544
Loss at step 18340: 0.2737
Loss at step 18350: 0.3392
Loss at step 18360: 0.2405
Loss at step 18370: 0.2710
Loss at step 18380: 0.6527
Loss at step 18390: 0.2604
Loss at step 18400: 0.1482
Loss at step 18410: 0.8738
Loss at step 18420: 0.4487
Loss at step 18430: 0.0095
Loss at step 18440: 0.7488
Loss at step 18450: 0.3123
Loss at step 18460: 0.1754
Loss at step 18470: 0.6748
Loss at step 18480: 0.4357
Loss at step 18490: 0.5455
Loss at step 18500: 0.4502
Loss at step 18510: 0.2256
Loss at step 18520: 0.1720
Loss at step 18530: 0.4630
Loss at step 18540: 0.6123
Loss at step 18550: 0.1848
Loss at step 18560: 0.1814
Loss at step 18570: 0.1840
Loss at step 18580: 0.1146
Loss at step 18590: 0.4408
Loss at step 18600: 0.0875
Loss at step 18610: 0.1959
Loss at step 18620: 0.2824
Loss at step 18630: 0.1584
Loss at step 18640: 0.0788
Loss at step 18650: 0.1496
Loss at step 18660: 0.2334
Loss at step 18670: 0.2689
Loss at step 18680: 0.1767
Loss at step 18690: 0.3957
Loss at step 18700: 0.3073
Loss at step 18710: 0.0575
Loss at step 18720: 0.1615
Loss at step 18730: 0.2781
Loss at step 18740: 0.2551
Loss at step 18750: 0.0615
Loss at step 18760: 0.0268
Loss at step 18770: 0.2283
Loss at step 18780: 0.5031
Loss at step 18790: 0.0667
Loss at step 18800: 0.2677
Loss at step 18810: 0.3813
Loss at step 18820: 0.1904
Loss at step 18830: 0.2940
Loss at step 18840: 0.1290
Loss at step 18850: 0.6405
Loss at step 18860: 0.3234
Loss at step 18870: 0.2836
Loss at step 18880: 0.2771
Loss at step 18890: 0.2109
Loss at step 18900: 0.0566
Loss at step 18910: 0.0664
Loss at step 18920: 0.3813
Loss at step 18930: 0.4710
Loss at step 18940: 0.2794
Loss at step 18950: 0.1650
Loss at step 18960: 0.4401
Loss at step 18970: 0.5015
Loss at step 18980: 0.0664
Loss at step 18990: 0.4741
Loss at step 19000: 0.3310
Loss at step 19010: 0.3072
Loss at step 19020: 0.4459
Loss at step 19030: 0.3148
Loss at step 19040: 0.1721
Loss at step 19050: 0.1734
Loss at step 19060: 0.2637
Loss at step 19070: 0.4503
Loss at step 19080: 0.1457
Loss at step 19090: 0.1617
Loss at step 19100: 0.0239
Loss at step 19110: 0.4484
Loss at step 19120: 0.0794
Loss at step 19130: 0.2158
Loss at step 19140: 0.1960
Loss at step 19150: 0.2970
Loss at step 19160: 0.3569
Loss at step 19170: 0.2202
Loss at step 19180: 0.1702
Loss at step 19190: 0.2588
Loss at step 19200: 0.6918
Loss at step 19210: 0.5931
Loss at step 19220: 0.1632
Loss at step 19230: 0.3642
Loss at step 19240: 0.1958
Loss at step 19250: 0.1110
Loss at step 19260: 0.2928
Loss at step 19270: 0.1938
Loss at step 19280: 0.2808
Loss at step 19290: 0.6477
Loss at step 19300: 0.1349
Loss at step 19310: 0.1042
Loss at step 19320: 0.4595
Loss at step 19330: 0.0162
Loss at step 19340: 0.5239
Loss at step 19350: 0.1566
Loss at step 19360: 0.3784
Loss at step 19370: 0.3775
Loss at step 19380: 0.3602
Loss at step 19390: 0.0900
Loss at step 19400: 0.5649
Loss at step 19410: 0.1497
Loss at step 19420: 0.4052
Loss at step 19430: 0.3720
Loss at step 19440: 0.4414
Loss at step 19450: 0.3928
Loss at step 19460: 0.1307
Loss at step 19470: 0.0704
Loss at step 19480: 0.1109
Loss at step 19490: 0.4545
Loss at step 19500: 0.2347
Loss at step 19510: 0.3490
Loss at step 19520: 0.1691
Loss at step 19530: 0.0555
Loss at step 19540: 0.0872
Loss at step 19550: 0.2736
Loss at step 19560: 0.1511
Loss at step 19570: 0.3675
Loss at step 19580: 0.3815
Loss at step 19590: 0.2728
Loss at step 19600: 0.0986
Loss at step 19610: 0.0895
Loss at step 19620: 0.3976
Loss at step 19630: 0.5782
Loss at step 19640: 0.2333
Loss at step 19650: 0.1910
Loss at step 19660: 0.1774
Loss at step 19670: 0.4640
Loss at step 19680: 0.0516
Loss at step 19690: 0.2651
Loss at step 19700: 0.1821
Loss at step 19710: 0.3404
Loss at step 19720: 0.1704
Loss at step 19730: 0.3511
Loss at step 19740: 0.0943
Loss at step 19750: 0.1854
Loss at step 19760: 0.3403
Loss at step 19770: 0.1619
Loss at step 19780: 0.4066
Loss at step 19790: 0.2631
Loss at step 19800: 0.6051
Loss at step 19810: 0.5946
Loss at step 19820: 0.3799
Loss at step 19830: 0.4739
Loss at step 19840: 0.6097
Loss at step 19850: 0.1836
Loss at step 19860: 0.3096
Loss at step 19870: 0.4485
Loss at step 19880: 0.4047
Loss at step 19890: 0.2737
Loss at step 19900: 0.1445
Loss at step 19910: 0.0566
Loss at step 19920: 0.1865
Loss at step 19930: 0.0375
Loss at step 19940: 0.3496
Loss at step 19950: 0.3090
Loss at step 19960: 0.3859
Loss at step 19970: 0.0331
Loss at step 19980: 0.2540
Loss at step 19990: 0.4523
Loss at step 20000: 0.2473
Loss at step 20010: 0.2581
Loss at step 20020: 0.2950
Loss at step 20030: 0.1572
Loss at step 20040: 0.2999
Loss at step 20050: 0.1639
Loss at step 20060: 0.4481
Loss at step 20070: 0.2069
Loss at step 20080: 0.2931
Loss at step 20090: 0.0420
Loss at step 20100: 0.3964
Loss at step 20110: 0.2964
Loss at step 20120: 0.3100
Loss at step 20130: 0.4640
Loss at step 20140: 0.1921
Loss at step 20150: 0.1975
Loss at step 20160: 0.3789
Loss at step 20170: 0.1112
Loss at step 20180: 0.1527
Loss at step 20190: 0.1527
Loss at step 20200: 0.0801
Loss at step 20210: 0.6275
Loss at step 20220: 0.0582
Loss at step 20230: 0.6764
Loss at step 20240: 0.1007
Loss at step 20250: 0.7607
Loss at step 20260: 0.1391
Loss at step 20270: 0.1506
Loss at step 20280: 0.1749
Loss at step 20290: 0.1125
Loss at step 20300: 0.8213
Loss at step 20310: 0.2974
Loss at step 20320: 0.5792
Loss at step 20330: 0.0243
Loss at step 20340: 1.2495
Loss at step 20350: 0.2201
Loss at step 20360: 0.4527
Loss at step 20370: 0.3249
Loss at step 20380: 0.1790
Loss at step 20390: 0.3367
Loss at step 20400: 0.1893
Loss at step 20410: 0.0505
Loss at step 20420: 0.0423
Loss at step 20430: 0.2435
Loss at step 20440: 0.3420
Loss at step 20450: 0.2667
Loss at step 20460: 0.4501
Loss at step 20470: 0.1405
Loss at step 20480: 0.0660
Loss at step 20490: 0.1189
Loss at step 20500: 0.0563
Loss at step 20510: 0.0363
Loss at step 20520: 0.0315
Loss at step 20530: 0.1627
Loss at step 20540: 0.0462
Loss at step 20550: 0.4410
Loss at step 20560: 0.4332
Loss at step 20570: 0.1507
Loss at step 20580: 0.1377
Loss at step 20590: 0.0482
Loss at step 20600: 0.2036
Loss at step 20610: 0.2675
Loss at step 20620: 0.2223
Loss at step 20630: 0.9280
Loss at step 20640: 0.3291
Loss at step 20650: 0.0354
Loss at step 20660: 0.5971
Loss at step 20670: 0.5137
Loss at step 20680: 0.3419
Loss at step 20690: 0.0227
Loss at step 20700: 0.1452
Loss at step 20710: 0.3022
Loss at step 20720: 0.1616
Loss at step 20730: 0.1364
Loss at step 20740: 0.0964
Loss at step 20750: 0.0897
Loss at step 20760: 0.5295
Loss at step 20770: 0.5408
Loss at step 20780: 0.2528
Loss at step 20790: 0.6694
Loss at step 20800: 0.3097
Loss at step 20810: 0.2409
Loss at step 20820: 0.2235
Loss at step 20830: 0.3131
Loss at step 20840: 0.3962
Loss at step 20850: 0.4041
Loss at step 20860: 0.1309
Loss at step 20870: 0.5526
Loss at step 20880: 0.1508
Loss at step 20890: 0.0302
Loss at step 20900: 0.2171
Loss at step 20910: 0.2533
Loss at step 20920: 0.0304
Loss at step 20930: 0.3754
Loss at step 20940: 0.0633
Loss at step 20950: 0.2910
Loss at step 20960: 0.1773
Loss at step 20970: 0.2724
Loss at step 20980: 0.2900
Loss at step 20990: 0.0136
Loss at step 21000: 0.4766
Loss at step 21010: 0.2129
Loss at step 21020: 0.2202
Loss at step 21030: 0.0437
Loss at step 21040: 0.1444
Loss at step 21050: 0.2033
Loss at step 21060: 0.2143
Loss at step 21070: 0.2712
Loss at step 21080: 0.2201
Loss at step 21090: 0.1878
Loss at step 21100: 0.2333
Loss at step 21110: 0.0610
Loss at step 21120: 0.1283
Loss at step 21130: 0.1130
Loss at step 21140: 0.1374
Loss at step 21150: 0.3182
Loss at step 21160: 0.2334
Loss at step 21170: 0.0889
Loss at step 21180: 0.1835
Loss at step 21190: 0.4280
Loss at step 21200: 0.3756
Loss at step 21210: 0.1872
Loss at step 21220: 0.5150
Loss at step 21230: 0.6607
Loss at step 21240: 0.1781
Loss at step 21250: 0.1913
Loss at step 21260: 0.1284
Loss at step 21270: 0.0946
Loss at step 21280: 0.0922
Loss at step 21290: 0.1467
Loss at step 21300: 0.2089
Loss at step 21310: 0.5186
Loss at step 21320: 0.3161
Loss at step 21330: 0.3765
Loss at step 21340: 0.2601
Loss at step 21350: 0.4590
Loss at step 21360: 0.2535
Loss at step 21370: 0.1572
Loss at step 21380: 0.0670
Loss at step 21390: 0.0255
Loss at step 21400: 0.8918
Loss at step 21410: 0.1496
Loss at step 21420: 0.0433
Loss at step 21430: 0.3138
Loss at step 21440: 0.4091
Loss at step 21450: 0.2154
Loss at step 21460: 0.3416
Loss at step 21470: 0.0472
Loss at step 21480: 0.0215
Loss at step 21490: 0.1805
Loss at step 21500: 0.6252
Loss at step 21510: 0.4203
Loss at step 21520: 0.1062
Loss at step 21530: 0.4782
Loss at step 21540: 0.0602
Loss at step 21550: 0.0427
Loss at step 21560: 0.3849
Loss at step 21570: 0.2539
Loss at step 21580: 0.1590
Loss at step 21590: 0.0528
Loss at step 21600: 0.0968
Loss at step 21610: 0.5220
Loss at step 21620: 0.6843
Loss at step 21630: 0.2045
Loss at step 21640: 0.2383
Loss at step 21650: 0.4227
Loss at step 21660: 0.3514
Loss at step 21670: 0.0771
Loss at step 21680: 0.2215
Loss at step 21690: 0.4267
Loss at step 21700: 0.1475
Loss at step 21710: 0.2083
Loss at step 21720: 0.0764
Loss at step 21730: 0.1038
Loss at step 21740: 0.0073
Loss at step 21750: 0.0127
Loss at step 21760: 0.3282
Loss at step 21770: 0.4127
Loss at step 21780: 0.0366
Loss at step 21790: 0.2127
Loss at step 21800: 0.1862
Loss at step 21810: 0.1450
Loss at step 21820: 0.3391
Loss at step 21830: 0.3734
Loss at step 21840: 0.2284
Loss at step 21850: 0.5106
Loss at step 21860: 0.0943
Loss at step 21870: 0.2762
Loss at step 21880: 0.1677
Loss at step 21890: 0.2750
Loss at step 21900: 0.0716
Loss at step 21910: 0.2909
Loss at step 21920: 0.0580
Loss at step 21930: 0.2199
Loss at step 21940: 0.3295
Loss at step 21950: 0.2913
Loss at step 21960: 0.1509
Loss at step 21970: 0.4492
Loss at step 21980: 0.6239
Loss at step 21990: 0.2987
Loss at step 22000: 0.3144
Loss at step 22010: 0.2079
Loss at step 22020: 0.3297
Loss at step 22030: 0.0696
Loss at step 22040: 0.0355
Loss at step 22050: 0.7270
Loss at step 22060: 0.0811
Loss at step 22070: 0.6609
Loss at step 22080: 0.1122
Loss at step 22090: 0.8887
Loss at step 22100: 0.1518
Loss at step 22110: 0.1071
Loss at step 22120: 0.0402
Loss at step 22130: 0.1322
Loss at step 22140: 0.6712
Loss at step 22150: 0.5749
Loss at step 22160: 0.2333
Loss at step 22170: 0.2556
Loss at step 22180: 0.1691
Loss at step 22190: 0.1496
Loss at step 22200: 0.2260
Loss at step 22210: 0.0671
Loss at step 22220: 0.3165
Loss at step 22230: 0.1814
Loss at step 22240: 0.7400
Loss at step 22250: 0.1136
Loss at step 22260: 0.2133
Loss at step 22270: 0.0350
Loss at step 22280: 0.4862
Loss at step 22290: 0.5896
Loss at step 22300: 0.5413
Loss at step 22310: 0.0237
Loss at step 22320: 0.3805
Loss at step 22330: 0.4112
Loss at step 22340: 0.1944
Loss at step 22350: 0.2623
Loss at step 22360: 0.1026
Loss at step 22370: 0.0850
Loss at step 22380: 0.0382
Loss at step 22390: 0.2722
Loss at step 22400: 0.4392
Loss at step 22410: 0.3303
Loss at step 22420: 0.3384
Loss at step 22430: 0.1114
Loss at step 22440: 0.1840
Loss at step 22450: 0.3961
Loss at step 22460: 0.3188
Loss at step 22470: 0.2825
Loss at step 22480: 0.0577
Loss at step 22490: 0.5824
Loss at step 22500: 0.8111
Loss at step 22510: 0.1371
Loss at step 22520: 0.1893
Loss at step 22530: 1.0214
Loss at step 22540: 0.2103
Loss at step 22550: 0.3518
Loss at step 22560: 0.6245
Loss at step 22570: 0.0369
Loss at step 22580: 0.4086
Loss at step 22590: 0.0394
Loss at step 22600: 0.4011
Loss at step 22610: 0.1945
Loss at step 22620: 0.2078
Loss at step 22630: 0.1355
Loss at step 22640: 0.3110
Loss at step 22650: 0.0410
Loss at step 22660: 0.0962
Loss at step 22670: 0.0600
Loss at step 22680: 0.6299
Loss at step 22690: 0.0608
Loss at step 22700: 0.1890
Loss at step 22710: 0.1879
Loss at step 22720: 0.2692
Loss at step 22730: 0.1698
Loss at step 22740: 0.2269
Loss at step 22750: 0.6439
Loss at step 22760: 0.3947
Loss at step 22770: 0.1740
Loss at step 22780: 0.0969
Loss at step 22790: 0.1315
Loss at step 22800: 0.1184
Loss at step 22810: 0.0770
Loss at step 22820: 0.2452
Loss at step 22830: 0.0274
Loss at step 22840: 0.0136
Loss at step 22850: 0.2030
Loss at step 22860: 0.0948
Loss at step 22870: 0.3505
Loss at step 22880: 0.1301
Loss at step 22890: 0.2860
Loss at step 22900: 0.1220
Loss at step 22910: 0.0492
Loss at step 22920: 0.3759
Loss at step 22930: 0.4766
Loss at step 22940: 0.2270
Loss at step 22950: 0.3069
Loss at step 22960: 0.4621
Loss at step 22970: 0.4496
Loss at step 22980: 0.6161
Loss at step 22990: 0.4655
Loss at step 23000: 0.7957
Loss at step 23010: 0.1848
Loss at step 23020: 0.1895
Loss at step 23030: 0.1123
Loss at step 23040: 0.0778
Loss at step 23050: 0.1294
Loss at step 23060: 0.1510
Loss at step 23070: 0.2899
Loss at step 23080: 0.4484
Loss at step 23090: 0.0868
Loss at step 23100: 0.1191
Loss at step 23110: 0.5440
Loss at step 23120: 0.0969
Loss at step 23130: 0.1404
Loss at step 23140: 0.4217
Loss at step 23150: 0.1238
Loss at step 23160: 0.3611
Loss at step 23170: 0.0103
Loss at step 23180: 0.1924
Loss at step 23190: 0.1066
Loss at step 23200: 0.2418
Loss at step 23210: 0.2813
Loss at step 23220: 0.3913
Loss at step 23230: 0.4467
Loss at step 23240: 0.4511
Loss at step 23250: 0.0285
Loss at step 23260: 0.3295
Loss at step 23270: 0.5719
Loss at step 23280: 0.1927
Loss at step 23290: 0.5908
Loss at step 23300: 0.2744
Loss at step 23310: 0.2826
Loss at step 23320: 0.3125
Loss at step 23330: 0.3417
Loss at step 23340: 0.0428
Loss at step 23350: 0.3516
Loss at step 23360: 0.1873
Loss at step 23370: 0.5886
Loss at step 23380: 0.1947
Loss at step 23390: 0.0657
Loss at step 23400: 0.3243
Loss at step 23410: 0.4153
Loss at step 23420: 0.2997
Loss at step 23430: 0.1265
Loss at step 23440: 0.0085
Loss at step 23450: 0.0302
Loss at step 23460: 0.5964
Loss at step 23470: 0.0220
Loss at step 23480: 0.1659
Loss at step 23490: 0.0949
Loss at step 23500: 0.0120
Loss at step 23510: 0.0638
Loss at step 23520: 0.8176
Loss at step 23530: 0.2726
Loss at step 23540: 0.0311
Loss at step 23550: 0.2774
Loss at step 23560: 0.3423
Loss at step 23570: 0.2368
Loss at step 23580: 0.2265
Loss at step 23590: 0.1715
Loss at step 23600: 0.0286
Loss at step 23610: 0.5775
Loss at step 23620: 0.1892
Loss at step 23630: 0.2927
Loss at step 23640: 0.0388
Loss at step 23650: 0.2227
Loss at step 23660: 0.1174
Loss at step 23670: 0.1565
Loss at step 23680: 0.4888
Loss at step 23690: 0.2549
Loss at step 23700: 0.2816
Loss at step 23710: 0.2855
Loss at step 23720: 0.0190
Loss at step 23730: 0.0405
Loss at step 23740: 0.7237
Loss at step 23750: 0.0191
Loss at step 23760: 0.9828
Loss at step 23770: 0.2080
Loss at step 23780: 0.0846
Loss at step 23790: 0.2446
Loss at step 23800: 0.1699
Loss at step 23810: 0.1243
Loss at step 23820: 0.4609
Loss at step 23830: 0.0931
Loss at step 23840: 0.3585
Loss at step 23850: 0.7129
Loss at step 23860: 0.2854
Loss at step 23870: 0.1850
Loss at step 23880: 0.1433
Loss at step 23890: 0.0087
Loss at step 23900: 0.1329
Loss at step 23910: 0.0847
Loss at step 23920: 0.1472
Loss at step 23930: 0.0648
Loss at step 23940: 0.0186
Loss at step 23950: 0.3131
Loss at step 23960: 0.2292
Loss at step 23970: 0.0130
Loss at step 23980: 0.2080
Loss at step 23990: 0.0316
Loss at step 24000: 0.1196
Loss at step 24010: 0.1772
Loss at step 24020: 0.5191
Loss at step 24030: 0.1565
Loss at step 24040: 0.0395
Loss at step 24050: 0.2366
Loss at step 24060: 0.2792
Loss at step 24070: 0.2462
Loss at step 24080: 0.1364
Loss at step 24090: 0.0979
Loss at step 24100: 0.1471
Loss at step 24110: 0.4973
Loss at step 24120: 0.3114
Loss at step 24130: 0.0629
Loss at step 24140: 0.4019
Loss at step 24150: 0.4771
Loss at step 24160: 0.0391
Loss at step 24170: 0.1591
Loss at step 24180: 0.0630
Loss at step 24190: 0.1027
Loss at step 24200: 0.1127
Loss at step 24210: 0.3485
Loss at step 24220: 0.6355
Loss at step 24230: 0.4029
Loss at step 24240: 0.3133
Loss at step 24250: 0.0472
Loss at step 24260: 0.3383
Loss at step 24270: 0.0623
Loss at step 24280: 0.0695
Loss at step 24290: 0.3569
Loss at step 24300: 0.2036
Loss at step 24310: 0.3581
Loss at step 24320: 0.0943
Loss at step 24330: 0.3483
Loss at step 24340: 0.0575
Loss at step 24350: 0.4925
Loss at step 24360: 0.8241
Loss at step 24370: 0.4684
Loss at step 24380: 0.1768
Loss at step 24390: 0.1303
Loss at step 24400: 0.1065
Loss at step 24410: 0.7408
Loss at step 24420: 0.3080
Loss at step 24430: 0.3351
Loss at step 24440: 0.0346
Loss at step 24450: 0.3055
Loss at step 24460: 0.3634
Loss at step 24470: 0.3043
Loss at step 24480: 0.1243
Loss at step 24490: 0.2068
Loss at step 24500: 0.3231
Loss at step 24510: 0.3157
Loss at step 24520: 0.2260
Loss at step 24530: 0.4558
Loss at step 24540: 0.1865
Loss at step 24550: 0.1285
Loss at step 24560: 0.0096
Loss at step 24570: 0.3365
Loss at step 24580: 0.2097
Loss at step 24590: 0.5426
Loss at step 24600: 0.0618
Loss at step 24610: 0.1924
Loss at step 24620: 0.1745
Loss at step 24630: 0.1239
Loss at step 24640: 0.0237
Loss at step 24650: 0.4253
Loss at step 24660: 0.4794
Loss at step 24670: 0.9541
Loss at step 24680: 0.6312
Loss at step 24690: 0.1412
Loss at step 24700: 0.2124
Loss at step 24710: 0.0330
Loss at step 24720: 0.1057
Loss at step 24730: 0.2378
Loss at step 24740: 0.2137
Loss at step 24750: 0.1214
Loss at step 24760: 0.0267
Loss at step 24770: 0.0390
Loss at step 24780: 0.0181
Loss at step 24790: 0.4969
Loss at step 24800: 0.4044
Loss at step 24810: 0.0538
Loss at step 24820: 0.3183
Loss at step 24830: 0.4827
Loss at step 24840: 0.6626
Loss at step 24850: 0.4705
Loss at step 24860: 0.1026
Loss at step 24870: 0.1642
Loss at step 24880: 0.1673
Loss at step 24890: 0.3163
Loss at step 24900: 0.2270
Loss at step 24910: 0.1426
Loss at step 24920: 0.2642
Loss at step 24930: 0.2607
Loss at step 24940: 0.7775
Loss at step 24950: 0.3878
Loss at step 24960: 0.2318
Loss at step 24970: 0.0753
Loss at step 24980: 0.3195
Loss at step 24990: 0.1629
Loss at step 25000: 0.2302
Loss at step 25010: 0.3174
Loss at step 25020: 0.4767
Loss at step 25030: 0.2121
Loss at step 25040: 0.0456
Loss at step 25050: 0.1628
Loss at step 25060: 0.3044
Loss at step 25070: 0.3951
Loss at step 25080: 0.3982
Loss at step 25090: 0.1895
Loss at step 25100: 0.6218
Loss at step 25110: 0.0741
Loss at step 25120: 0.3828
Loss at step 25130: 0.5043
Loss at step 25140: 0.2021
Loss at step 25150: 0.1614
Loss at step 25160: 0.3374
Loss at step 25170: 0.1931
Loss at step 25180: 0.3503
Loss at step 25190: 0.3993
Loss at step 25200: 0.2277
Loss at step 25210: 0.0657
Loss at step 25220: 0.2881
Loss at step 25230: 0.0522
Loss at step 25240: 0.3515
Loss at step 25250: 0.0716
Loss at step 25260: 0.1826
Loss at step 25270: 0.1627
Loss at step 25280: 0.6343
Loss at step 25290: 0.2203
Loss at step 25300: 0.0888
Loss at step 25310: 0.2754
Loss at step 25320: 0.0974
Loss at step 25330: 0.0668
Loss at step 25340: 0.0157
Loss at step 25350: 0.0618
Loss at step 25360: 0.0799
Loss at step 25370: 0.0217
Loss at step 25380: 0.6485
Loss at step 25390: 0.2105
Loss at step 25400: 0.4936
Loss at step 25410: 0.2008
Loss at step 25420: 0.0793
Loss at step 25430: 0.8583
Loss at step 25440: 0.0780
Loss at step 25450: 0.3645
Loss at step 25460: 0.2392
Loss at step 25470: 0.2258
Loss at step 25480: 0.0597
Loss at step 25490: 0.1177
Loss at step 25500: 0.0568
Loss at step 25510: 0.6051
Loss at step 25520: 0.4018
Loss at step 25530: 0.0784
Loss at step 25540: 0.3005
Loss at step 25550: 0.2295
Loss at step 25560: 0.2647
Loss at step 25570: 0.0247
Loss at step 25580: 0.1648
Loss at step 25590: 0.4477
Loss at step 25600: 0.2296
Loss at step 25610: 0.1449
Loss at step 25620: 0.2102
Loss at step 25630: 0.1822
Loss at step 25640: 0.1045
Loss at step 25650: 0.1607
Loss at step 25660: 0.1241
Loss at step 25670: 0.5439
Loss at step 25680: 0.7648
Loss at step 25690: 0.7269
Loss at step 25700: 0.4692
Loss at step 25710: 0.0535
Loss at step 25720: 0.4653
Loss at step 25730: 0.3323
Loss at step 25740: 0.5350
Loss at step 25750: 0.1514
Loss at step 25760: 0.1837
Loss at step 25770: 0.2144
Loss at step 25780: 0.2358
Loss at step 25790: 0.0990
Loss at step 25800: 0.0349
Loss at step 25810: 0.2276
Loss at step 25820: 0.3790
Loss at step 25830: 0.1795
Loss at step 25840: 0.1635
Loss at step 25850: 0.3900
Loss at step 25860: 0.4945
Loss at step 25870: 0.3539
Loss at step 25880: 0.1612
Loss at step 25890: 0.5831
Loss at step 25900: 0.1439
Loss at step 25910: 0.6574
Loss at step 25920: 0.0260
Loss at step 25930: 0.0152
Loss at step 25940: 0.3183
Loss at step 25950: 0.3360
Loss at step 25960: 0.1147
Loss at step 25970: 0.3321
Loss at step 25980: 0.0782
Loss at step 25990: 0.5690
Loss at step 26000: 0.5178
Loss at step 26010: 0.2456
Loss at step 26020: 0.3723
Loss at step 26030: 0.1455
Loss at step 26040: 0.1983
Loss at step 26050: 0.0388
Loss at step 26060: 0.0443
Loss at step 26070: 0.2079
Loss at step 26080: 0.7632
Loss at step 26090: 0.0739
Loss at step 26100: 0.2180
Loss at step 26110: 0.4805
Loss at step 26120: 0.2596
Loss at step 26130: 0.5237
Loss at step 26140: 0.0125
Loss at step 26150: 0.3381
Loss at step 26160: 0.4455
Loss at step 26170: 0.6061
Loss at step 26180: 0.4724
Loss at step 26190: 0.1887
Loss at step 26200: 0.3354
Loss at step 26210: 0.4793
Loss at step 26220: 0.1514
Loss at step 26230: 0.1700
Loss at step 26240: 0.1227
Loss at step 26250: 0.4379
Loss at step 26260: 0.0413
Loss at step 26270: 0.1706
Loss at step 26280: 0.2413
Loss at step 26290: 0.1403
Loss at step 26300: 0.2738
Loss at step 26310: 0.4191
Loss at step 26320: 0.6120
Loss at step 26330: 0.1710
Loss at step 26340: 0.0045
Loss at step 26350: 0.2245
Loss at step 26360: 0.4153
Loss at step 26370: 0.1763
Loss at step 26380: 0.2872
Loss at step 26390: 0.7873
Loss at step 26400: 0.1907
Loss at step 26410: 0.1666
Loss at step 26420: 0.5827
Loss at step 26430: 0.0408
Loss at step 26440: 0.2921
Loss at step 26450: 0.1493
Loss at step 26460: 0.1343
Loss at step 26470: 0.0915
Loss at step 26480: 0.1173
Loss at step 26490: 0.3383
Loss at step 26500: 0.2753
Loss at step 26510: 0.1759
Loss at step 26520: 0.2320
Loss at step 26530: 0.6938
Loss at step 26540: 0.0588
Loss at step 26550: 0.0185
Loss at step 26560: 0.3417
Loss at step 26570: 0.3158
Loss at step 26580: 0.5561
Loss at step 26590: 0.7428
Loss at step 26600: 0.2859
Loss at step 26610: 0.0191
Loss at step 26620: 0.2672
Loss at step 26630: 0.2045
Loss at step 26640: 0.2178
Loss at step 26650: 0.6175
Loss at step 26660: 0.0645
Loss at step 26670: 0.0137
Loss at step 26680: 0.0391
Loss at step 26690: 0.5279
Loss at step 26700: 0.0850
Loss at step 26710: 0.1972
Loss at step 26720: 0.1812
Loss at step 26730: 0.1004
Loss at step 26740: 0.5776
Loss at step 26750: 0.1762
Loss at step 26760: 0.2283
Loss at step 26770: 0.0193
Loss at step 26780: 0.3653
Loss at step 26790: 0.0886
Loss at step 26800: 0.0190
Loss at step 26810: 0.3526
Loss at step 26820: 0.1941
Loss at step 26830: 0.1762
Loss at step 26840: 0.3207
Loss at step 26850: 0.3149
Loss at step 26860: 0.4476
Loss at step 26870: 0.0889
Loss at step 26880: 0.0931
Loss at step 26890: 0.2795
Loss at step 26900: 0.1227
Loss at step 26910: 0.6973
Loss at step 26920: 0.5621
Loss at step 26930: 0.4601
Loss at step 26940: 0.0175
Loss at step 26950: 0.0222
Loss at step 26960: 0.3525
Loss at step 26970: 0.1292
Loss at step 26980: 0.7208
Loss at step 26990: 0.0262
Loss at step 27000: 0.4299
Loss at step 27010: 0.1450
Loss at step 27020: 0.5996
Loss at step 27030: 0.1516
Loss at step 27040: 0.2112
Loss at step 27050: 0.4046
Loss at step 27060: 0.0692
Loss at step 27070: 0.1852
Loss at step 27080: 0.4516
Loss at step 27090: 0.0513
Loss at step 27100: 0.1679
Loss at step 27110: 0.0342
Loss at step 27120: 0.0774
Loss at step 27130: 0.5411
Loss at step 27140: 0.2730
Loss at step 27150: 0.3698
Loss at step 27160: 0.3350
Loss at step 27170: 0.2076
Loss at step 27180: 0.6094
Loss at step 27190: 0.8918
Loss at step 27200: 0.2446
Loss at step 27210: 0.0569
Loss at step 27220: 0.1093
Loss at step 27230: 0.0133
Loss at step 27240: 0.4502
Loss at step 27250: 0.5828
Loss at step 27260: 0.3098
Loss at step 27270: 0.0270
Loss at step 27280: 0.1944
Loss at step 27290: 0.1583
Loss at step 27300: 0.5431
Loss at step 27310: 0.1473
Loss at step 27320: 0.0511
Loss at step 27330: 0.4598
Loss at step 27340: 0.2038
Loss at step 27350: 0.4951
Loss at step 27360: 0.4091
Loss at step 27370: 0.0612
Loss at step 27380: 0.2527
Loss at step 27390: 0.1556
Loss at step 27400: 0.3198
Loss at step 27410: 0.5185
Loss at step 27420: 0.1311
Loss at step 27430: 0.3620
Loss at step 27440: 0.3588
Loss at step 27450: 0.0338
Loss at step 27460: 0.2467
Loss at step 27470: 0.2734
Loss at step 27480: 0.0955
Loss at step 27490: 0.0458
Loss at step 27500: 0.1509
Loss at step 27510: 0.4250
Loss at step 27520: 0.4540
Loss at step 27530: 0.4893
Loss at step 27540: 0.3027
Loss at step 27550: 0.0732
Loss at step 27560: 0.1480
Loss at step 27570: 0.3090
Loss at step 27580: 0.4062
Loss at step 27590: 0.1516
Loss at step 27600: 0.3345
Loss at step 27610: 0.0488
Loss at step 27620: 0.0488
Loss at step 27630: 0.0222
Loss at step 27640: 0.3519
Loss at step 27650: 0.0253
Loss at step 27660: 0.3711
Loss at step 27670: 0.0961
Loss at step 27680: 0.1932
Loss at step 27690: 0.2012
Loss at step 27700: 0.1520
Loss at step 27710: 0.0261
Loss at step 27720: 0.1905
Loss at step 27730: 0.3712
Loss at step 27740: 0.4079
Loss at step 27750: 0.3316
Loss at step 27760: 0.2522
Loss at step 27770: 0.4171
Loss at step 27780: 0.2826
Loss at step 27790: 0.1528
Loss at step 27800: 0.4741
Loss at step 27810: 0.1089
Loss at step 27820: 0.5789
Loss at step 27830: 0.1074
Loss at step 27840: 0.2326
Loss at step 27850: 0.0411
Loss at step 27860: 0.1624
Loss at step 27870: 0.0731
Loss at step 27880: 0.1684
Loss at step 27890: 0.0422
Loss at step 27900: 0.6387
Loss at step 27910: 0.3768
Loss at step 27920: 0.3288
Loss at step 27930: 0.0906
Loss at step 27940: 0.1130
Loss at step 27950: 0.1709
Loss at step 27960: 0.4655
Loss at step 27970: 0.2409
Loss at step 27980: 0.3747
Loss at step 27990: 0.0220
Loss at step 28000: 0.1270
Loss at step 28010: 0.4262
Loss at step 28020: 0.0290
Loss at step 28030: 0.0759
Loss at step 28040: 0.2697
Loss at step 28050: 0.0337
Loss at step 28060: 0.1944
Loss at step 28070: 0.1896
Loss at step 28080: 0.0590
Loss at step 28090: 0.5990
Loss at step 28100: 0.0701
Loss at step 28110: 0.2533
Loss at step 28120: 0.0452
Loss at step 28130: 0.1697
Loss at step 28140: 0.7072
Loss at step 28150: 0.0379
Loss at step 28160: 0.1833
Loss at step 28170: 0.2607
Loss at step 28180: 0.4493
Loss at step 28190: 0.8413
Loss at step 28200: 0.1068
Loss at step 28210: 0.0474
Loss at step 28220: 0.0608
Loss at step 28230: 0.1764
Loss at step 28240: 0.3353
Loss at step 28250: 0.5693
Loss at step 28260: 0.6155
Loss at step 28270: 0.3122
Loss at step 28280: 0.2243
Loss at step 28290: 0.1883
Loss at step 28300: 0.2596
Loss at step 28310: 0.2912
Loss at step 28320: 0.0926
Loss at step 28330: 0.2787
Loss at step 28340: 0.3301
Loss at step 28350: 0.3208
Loss at step 28360: 0.1219
Loss at step 28370: 0.0549
Loss at step 28380: 0.2774
Loss at step 28390: 0.0545
Loss at step 28400: 0.2043
Loss at step 28410: 0.0261
Loss at step 28420: 0.4314
Loss at step 28430: 0.3535
Loss at step 28440: 0.0393
Loss at step 28450: 0.0297
Loss at step 28460: 0.1516
Loss at step 28470: 0.5187
Loss at step 28480: 0.1450
Loss at step 28490: 0.1968
Loss at step 28500: 0.3512
Loss at step 28510: 0.2442
Loss at step 28520: 0.2244
Loss at step 28530: 0.3314
Loss at step 28540: 0.0732
Loss at step 28550: 0.0086
Loss at step 28560: 0.2694
Loss at step 28570: 0.1831
Loss at step 28580: 0.2317
Loss at step 28590: 0.4260
Loss at step 28600: 0.2711
Loss at step 28610: 0.0152
Loss at step 28620: 0.4772
Loss at step 28630: 0.2142
Loss at step 28640: 0.0036
Loss at step 28650: 0.0287
Loss at step 28660: 0.2414
Loss at step 28670: 0.0167
Loss at step 28680: 0.1202
Loss at step 28690: 0.3675
Loss at step 28700: 0.1944
Loss at step 28710: 0.1990
Loss at step 28720: 0.3596
Loss at step 28730: 0.1638
Loss at step 28740: 0.4340
Loss at step 28750: 0.0514
Loss at step 28760: 0.0374
Loss at step 28770: 0.1928
Loss at step 28780: 0.0321
Loss at step 28790: 0.3451
Loss at step 28800: 0.5216
Loss at step 28810: 0.6072
Loss at step 28820: 0.0350
Loss at step 28830: 0.1433
Loss at step 28840: 0.2843
Loss at step 28850: 0.1332
Loss at step 28860: 0.4076
Loss at step 28870: 0.4923
Loss at step 28880: 0.1509
Loss at step 28890: 0.3752
Loss at step 28900: 0.3915
Loss at step 28910: 0.5259
Loss at step 28920: 0.1271
Loss at step 28930: 0.4883
Loss at step 28940: 0.0281
Loss at step 28950: 0.3796
Loss at step 28960: 0.5684
Loss at step 28970: 0.0144
Loss at step 28980: 0.3164
Loss at step 28990: 0.0372
Loss at step 29000: 0.0198
Loss at step 29010: 0.7170
Loss at step 29020: 0.2104
Loss at step 29030: 0.3790
Loss at step 29040: 0.0297
Loss at step 29050: 0.0141
Loss at step 29060: 0.1471
Loss at step 29070: 0.4682
Loss at step 29080: 0.3013
Loss at step 29090: 0.4686
Loss at step 29100: 0.2009
Loss at step 29110: 0.1020
Loss at step 29120: 0.4655
Loss at step 29130: 0.1174
Loss at step 29140: 0.4295
Loss at step 29150: 0.2795
Loss at step 29160: 0.3951
Loss at step 29170: 0.2111
Loss at step 29180: 0.6970
Loss at step 29190: 0.8299
Loss at step 29200: 0.2267
Loss at step 29210: 0.1450
Loss at step 29220: 0.0411
Loss at step 29230: 0.2010
Loss at step 29240: 0.0045
Loss at step 29250: 0.4402
Loss at step 29260: 0.0633
Loss at step 29270: 0.1639
Loss at step 29280: 0.2183
Loss at step 29290: 0.0380
Loss at step 29300: 0.1250
Loss at step 29310: 0.5781
Loss at step 29320: 0.0848
Loss at step 29330: 0.4288
Loss at step 29340: 0.0526
Loss at step 29350: 0.1817
Loss at step 29360: 0.1888
Loss at step 29370: 0.2416
Loss at step 29380: 0.8345
Loss at step 29390: 0.1900
Loss at step 29400: 0.3776
Loss at step 29410: 0.0518
Loss at step 29420: 0.1946
Loss at step 29430: 0.1262
Loss at step 29440: 0.2285
Loss at step 29450: 0.3033
Loss at step 29460: 0.0964
Loss at step 29470: 0.3291
Loss at step 29480: 0.4016
Loss at step 29490: 0.0830
Loss at step 29500: 0.0581
Loss at step 29510: 0.1754
Loss at step 29520: 0.0221
Loss at step 29530: 0.5181
Loss at step 29540: 0.2078
Loss at step 29550: 0.3323
Loss at step 29560: 0.1048
Loss at step 29570: 0.5200
Loss at step 29580: 0.1017
Loss at step 29590: 0.1578
Loss at step 29600: 0.0291
Loss at step 29610: 0.4767
Loss at step 29620: 0.1138
Loss at step 29630: 0.1426
Loss at step 29640: 0.4976
Loss at step 29650: 0.2893
Loss at step 29660: 0.0748
Loss at step 29670: 0.4872
Loss at step 29680: 0.2369
Loss at step 29690: 0.4535
Loss at step 29700: 0.2594
Loss at step 29710: 0.0492
Loss at step 29720: 0.2265
Loss at step 29730: 0.1337
Loss at step 29740: 0.3409
Loss at step 29750: 0.0515
Loss at step 29760: 0.4614
Loss at step 29770: 0.2011
Loss at step 29780: 0.3297
Loss at step 29790: 0.1855
Loss at step 29800: 0.4800
Loss at step 29810: 0.2722
Loss at step 29820: 0.3761
Loss at step 29830: 0.3333
Loss at step 29840: 0.3298
Loss at step 29850: 0.0717
Loss at step 29860: 0.1943
Loss at step 29870: 0.3161
Loss at step 29880: 0.1272
Loss at step 29890: 0.0675
Loss at step 29900: 0.3697
Loss at step 29910: 0.5784
Loss at step 29920: 0.3628
Loss at step 29930: 0.4013
Loss at step 29940: 0.1115
Loss at step 29950: 0.2098
Loss at step 29960: 0.5143
Loss at step 29970: 0.2378
Loss at step 29980: 0.4282
Loss at step 29990: 0.0070
Loss at step 30000: 0.3944
Loss at step 30010: 0.3810
Loss at step 30020: 0.2284
Loss at step 30030: 0.4854
Loss at step 30040: 0.0286
Loss at step 30050: 0.0208
Loss at step 30060: 0.0891
Loss at step 30070: 0.0652
Loss at step 30080: 0.1361
Loss at step 30090: 0.3825
Loss at step 30100: 0.0940
Loss at step 30110: 0.1739
Loss at step 30120: 0.3909
Loss at step 30130: 0.3801
Loss at step 30140: 0.0385
Loss at step 30150: 0.2890
Loss at step 30160: 0.0594
Loss at step 30170: 0.1838
Loss at step 30180: 1.1919
Loss at step 30190: 0.0858
Loss at step 30200: 0.1470
Loss at step 30210: 0.1696
Loss at step 30220: 0.1894
Loss at step 30230: 0.0904
Loss at step 30240: 0.1667
Loss at step 30250: 0.4836
Loss at step 30260: 0.1626
Loss at step 30270: 0.2383
Loss at step 30280: 0.0392
Loss at step 30290: 0.0916
Loss at step 30300: 0.1556
Loss at step 30310: 0.5898
Loss at step 30320: 0.0831
Loss at step 30330: 0.3168
Loss at step 30340: 0.0356
Loss at step 30350: 0.4976
Loss at step 30360: 0.1508
Loss at step 30370: 0.5822
Loss at step 30380: 0.0372
Loss at step 30390: 0.3717
Loss at step 30400: 0.1613
Loss at step 30410: 0.4510
Loss at step 30420: 0.2311
Loss at step 30430: 0.2913
Loss at step 30440: 0.2543
Loss at step 30450: 0.6977
Loss at step 30460: 0.4454
Loss at step 30470: 0.0351
Loss at step 30480: 0.1879
Loss at step 30490: 0.2471
Loss at step 30500: 0.0251
Loss at step 30510: 0.2110
Loss at step 30520: 0.2141
Loss at step 30530: 0.1162
Loss at step 30540: 0.0470
Loss at step 30550: 0.0135
Loss at step 30560: 0.0223
Loss at step 30570: 0.0213
Loss at step 30580: 0.4050
Loss at step 30590: 0.3116
Loss at step 30600: 0.3809
Loss at step 30610: 0.1314
Loss at step 30620: 0.0777
Loss at step 30630: 0.4231
Loss at step 30640: 0.2206
Loss at step 30650: 0.0483
Loss at step 30660: 0.3552
Loss at step 30670: 0.0298
Loss at step 30680: 0.1029
Loss at step 30690: 0.3821
Loss at step 30700: 0.2233
Loss at step 30710: 0.1797
Loss at step 30720: 0.3474
Loss at step 30730: 0.3903
Loss at step 30740: 0.0912
Loss at step 30750: 0.5215
Loss at step 30760: 0.1106
Loss at step 30770: 0.0630
Loss at step 30780: 0.2076
Loss at step 30790: 0.2491
Loss at step 30800: 0.3802
Loss at step 30810: 0.2073
Loss at step 30820: 0.0091
Loss at step 30830: 0.2573
Loss at step 30840: 0.2553
Loss at step 30850: 0.0431
Loss at step 30860: 0.0976
Loss at step 30870: 0.6987
Loss at step 30880: 0.1882
Loss at step 30890: 0.2299
Loss at step 30900: 0.0374
Loss at step 30910: 0.2937
Loss at step 30920: 0.6676
Loss at step 30930: 0.2088
Loss at step 30940: 0.6004
Loss at step 30950: 0.3411
Loss at step 30960: 0.0091
Loss at step 30970: 0.3740
Loss at step 30980: 0.0358
Loss at step 30990: 0.1501
Loss at step 31000: 0.1483
Loss at step 31010: 0.0276
Loss at step 31020: 0.2079
Loss at step 31030: 0.0140
Loss at step 31040: 0.6266
Loss at step 31050: 0.2147
Loss at step 31060: 0.3108
Loss at step 31070: 0.3825
Loss at step 31080: 0.2168
Loss at step 31090: 0.3787
Loss at step 31100: 0.6534
Loss at step 31110: 0.0440
Loss at step 31120: 0.3865
Loss at step 31130: 0.0212
Loss at step 31140: 0.0902
Loss at step 31150: 0.0148
Loss at step 31160: 0.1976
Loss at step 31170: 0.2909
Loss at step 31180: 0.0203
Loss at step 31190: 0.0538
Loss at step 31200: 0.2411
Loss at step 31210: 0.2952
Loss at step 31220: 0.0458
Loss at step 31230: 0.0187
Loss at step 31240: 0.2287
Loss at step 31250: 0.0203
Loss at step 31260: 0.0236
Loss at step 31270: 0.0546
Loss at step 31280: 0.5176
Loss at step 31290: 0.1450
Loss at step 31300: 0.2830
Loss at step 31310: 0.2697
Loss at step 31320: 0.4060
Loss at step 31330: 0.5598
Loss at step 31340: 0.1256
Loss at step 31350: 0.1276
Loss at step 31360: 0.0295
Loss at step 31370: 0.0514
Loss at step 31380: 0.1702
Loss at step 31390: 0.0642
Loss at step 31400: 0.5134
Loss at step 31410: 0.2444
Loss at step 31420: 0.1669
Loss at step 31430: 0.2577
Loss at step 31440: 0.6555
Loss at step 31450: 0.2443
Loss at step 31460: 0.2965
Loss at step 31470: 0.2354
Loss at step 31480: 0.3618
Loss at step 31490: 0.1417
Loss at step 31500: 0.3149
Loss at step 31510: 0.1390
Loss at step 31520: 0.0889
Loss at step 31530: 0.1984
Loss at step 31540: 0.0332
Loss at step 31550: 0.0351
Loss at step 31560: 0.2010
Loss at step 31570: 0.1192
Loss at step 31580: 0.2167
Loss at step 31590: 0.1384
Loss at step 31600: 0.0339
Loss at step 31610: 0.3090
Loss at step 31620: 0.0261
Loss at step 31630: 0.3028
Loss at step 31640: 0.0445
Loss at step 31650: 0.4411
Loss at step 31660: 0.0192
Loss at step 31670: 0.2949
Loss at step 31680: 0.0082
Loss at step 31690: 0.3756
Loss at step 31700: 0.0137
Loss at step 31710: 0.3724
Loss at step 31720: 0.1638
Loss at step 31730: 0.0686
Loss at step 31740: 0.1707
Loss at step 31750: 0.2934
Loss at step 31760: 0.5394
Loss at step 31770: 0.0773
Loss at step 31780: 0.2807
Loss at step 31790: 0.0291
Loss at step 31800: 0.4659
Loss at step 31810: 0.1977
Loss at step 31820: 0.3283
Loss at step 31830: 0.0286
Loss at step 31840: 0.6388
Loss at step 31850: 0.2111
Loss at step 31860: 0.5711
Loss at step 31870: 0.2748
Loss at step 31880: 0.0602
Loss at step 31890: 0.1429
Loss at step 31900: 0.4094
Loss at step 31910: 0.2600
Loss at step 31920: 0.2641
Loss at step 31930: 0.1681
Loss at step 31940: 0.3818
Loss at step 31950: 0.0409
Loss at step 31960: 0.0560
Loss at step 31970: 0.5710
Loss at step 31980: 0.0528
Loss at step 31990: 0.2873
Loss at step 32000: 0.4701
Loss at step 32010: 0.4396
Loss at step 32020: 0.5711
Loss at step 32030: 0.7675
Loss at step 32040: 0.1319
Loss at step 32050: 0.3911
Loss at step 32060: 0.4406
Loss at step 32070: 0.4524
Loss at step 32080: 0.0387
Loss at step 32090: 0.2514
Loss at step 32100: 0.0487
Loss at step 32110: 0.2438
Loss at step 32120: 0.2937
Loss at step 32130: 0.2507
Loss at step 32140: 0.5290
Loss at step 32150: 0.3707
Loss at step 32160: 0.1540
Loss at step 32170: 0.5265
Loss at step 32180: 0.1184
Loss at step 32190: 0.3740
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 107997
Loss at step 10: 0.6331
Loss at step 20: 0.6378
Loss at step 30: 0.5645
Loss at step 40: 0.6579
Loss at step 50: 0.3540
Loss at step 60: 0.6192
Loss at step 70: 0.3272
Loss at step 80: 0.4869
Loss at step 90: 0.8105
Loss at step 100: 0.5712
Loss at step 110: 0.7714
Loss at step 120: 0.2553
Loss at step 130: 0.5448
Loss at step 140: 0.7050
Loss at step 150: 0.5018
Loss at step 160: 0.3763
Loss at step 170: 1.0432
Loss at step 180: 1.4105
Loss at step 190: 0.4406
Loss at step 200: 0.6246
Loss at step 210: 0.4564
Loss at step 220: 0.5262
Loss at step 230: 0.3355
Loss at step 240: 0.2923
Loss at step 250: 0.8549
Loss at step 260: 0.3955
Loss at step 270: 0.3327
Loss at step 280: 0.3014
Loss at step 290: 0.5315
Loss at step 300: 0.6777
Loss at step 310: 0.3720
Loss at step 320: 0.6120
Loss at step 330: 0.3000
Loss at step 340: 0.3715
Loss at step 350: 0.4565
Loss at step 360: 0.4476
Loss at step 370: 0.5592
Loss at step 380: 0.3576
Loss at step 390: 0.5339
Loss at step 400: 0.6917
Loss at step 410: 0.6689
Loss at step 420: 0.4637
Loss at step 430: 0.1810
Loss at step 440: 0.3212
Loss at step 450: 0.3567
Loss at step 460: 0.4918
Loss at step 470: 0.2736
Loss at step 480: 0.5783
Loss at step 490: 0.4521
Loss at step 500: 0.6163
Loss at step 510: 0.4722
Loss at step 520: 0.3540
Loss at step 530: 0.7878
Loss at step 540: 0.8527
Loss at step 550: 0.4634
Loss at step 560: 0.6342
Loss at step 570: 0.6331
Loss at step 580: 0.7291
Loss at step 590: 0.4924
Loss at step 600: 0.2476
Loss at step 610: 0.4323
Loss at step 620: 0.3917
Loss at step 630: 0.1898
Loss at step 640: 0.5947
Loss at step 650: 0.4238
Loss at step 660: 0.3587
Loss at step 670: 0.8995
Loss at step 680: 0.5386
Loss at step 690: 0.3659
Loss at step 700: 0.3451
Loss at step 710: 0.4856
Loss at step 720: 0.5388
Loss at step 730: 0.3913
Loss at step 740: 0.1362
Loss at step 750: 0.5852
Loss at step 760: 0.4279
Loss at step 770: 0.3524
Loss at step 780: 0.3814
Loss at step 790: 0.4256
Loss at step 800: 0.5969
Loss at step 810: 0.6283
Loss at step 820: 0.3234
Loss at step 830: 0.3823
Loss at step 840: 0.8230
Loss at step 850: 0.4186
Loss at step 860: 0.4564
Loss at step 870: 0.3178
Loss at step 880: 0.6491
Loss at step 890: 0.2656
Loss at step 900: 0.3210
Loss at step 910: 0.2693
Loss at step 920: 0.4386
Loss at step 930: 0.1922
Loss at step 940: 0.4771
Loss at step 950: 0.4792
Loss at step 960: 0.2028
Loss at step 970: 0.4182
Loss at step 980: 0.4817
Loss at step 990: 0.4044
Loss at step 1000: 0.4238
Loss at step 1010: 0.4914
Loss at step 1020: 0.6137
Loss at step 1030: 0.3249
Loss at step 1040: 0.2600
Loss at step 1050: 0.5624
Loss at step 1060: 0.3985
Loss at step 1070: 0.4806
Loss at step 1080: 0.4813
Loss at step 1090: 0.4842
Loss at step 1100: 0.4345
Loss at step 1110: 0.1390
Loss at step 1120: 0.2859
Loss at step 1130: 0.1298
Loss at step 1140: 0.3501
Loss at step 1150: 0.2349
Loss at step 1160: 0.1919
Loss at step 1170: 0.2795
Loss at step 1180: 0.2362
Loss at step 1190: 0.0881
Loss at step 1200: 0.3744
Loss at step 1210: 0.6240
Loss at step 1220: 0.1941
Loss at step 1230: 0.4597
Loss at step 1240: 0.3418
Loss at step 1250: 0.5068
Loss at step 1260: 0.2877
Loss at step 1270: 0.4385
Loss at step 1280: 0.5374
Loss at step 1290: 0.2044
Loss at step 1300: 0.4532
Loss at step 1310: 0.3795
Loss at step 1320: 0.3693
Loss at step 1330: 0.4613
Loss at step 1340: 0.9057
Loss at step 1350: 0.7712
Loss at step 1360: 0.2636
Loss at step 1370: 0.1484
Loss at step 1380: 0.5220
Loss at step 1390: 0.3592
Loss at step 1400: 0.5471
Loss at step 1410: 0.2267
Loss at step 1420: 0.3018
Loss at step 1430: 0.4616
Loss at step 1440: 0.3467
Loss at step 1450: 0.4113
Loss at step 1460: 0.8225
Loss at step 1470: 0.4825
Loss at step 1480: 0.4875
Loss at step 1490: 0.7491
Loss at step 1500: 0.4804
Loss at step 1510: 0.2874
Loss at step 1520: 0.2552
Loss at step 1530: 0.1206
Loss at step 1540: 0.2126
Loss at step 1550: 0.7910
Loss at step 1560: 0.3197
Loss at step 1570: 0.5164
Loss at step 1580: 0.6615
Loss at step 1590: 0.2310
Loss at step 1600: 0.9293
Loss at step 1610: 0.3417
Loss at step 1620: 0.3579
Loss at step 1630: 0.3744
Loss at step 1640: 0.3088
Loss at step 1650: 0.5291
Loss at step 1660: 0.4544
Loss at step 1670: 0.3982
Loss at step 1680: 0.3662
Loss at step 1690: 0.7495
Loss at step 1700: 0.1551
Loss at step 1710: 0.1202
Loss at step 1720: 0.6087
Loss at step 1730: 0.5525
Loss at step 1740: 0.6814
Loss at step 1750: 0.5357
Loss at step 1760: 0.7323
Loss at step 1770: 0.2893
Loss at step 1780: 0.5445
Loss at step 1790: 1.0823
Loss at step 1800: 0.5050
Loss at step 1810: 0.3797
Loss at step 1820: 0.4664
Loss at step 1830: 0.3867
Loss at step 1840: 0.1180
Loss at step 1850: 0.4857
Loss at step 1860: 0.5462
Loss at step 1870: 0.6501
Loss at step 1880: 0.3523
Loss at step 1890: 0.0929
Loss at step 1900: 0.5152
Loss at step 1910: 0.5306
Loss at step 1920: 0.7729
Loss at step 1930: 0.7479
Loss at step 1940: 0.1879
Loss at step 1950: 0.3421
Loss at step 1960: 0.4783
Loss at step 1970: 0.2611
Loss at step 1980: 0.2169
Loss at step 1990: 0.3005
Loss at step 2000: 0.7064
Loss at step 2010: 0.2256
Loss at step 2020: 0.4609
Loss at step 2030: 0.5443
Loss at step 2040: 0.1844
Loss at step 2050: 0.5765
Loss at step 2060: 0.6432
Loss at step 2070: 0.9759
Loss at step 2080: 0.3816
Loss at step 2090: 0.3311
Loss at step 2100: 0.4069
Loss at step 2110: 0.3270
Loss at step 2120: 0.2356
Loss at step 2130: 0.4220
Loss at step 2140: 0.6102
Loss at step 2150: 0.7685
Loss at step 2160: 0.8057
Loss at step 2170: 0.3695
Loss at step 2180: 0.2592
Loss at step 2190: 0.5811
Loss at step 2200: 0.6263
Loss at step 2210: 0.3775
Loss at step 2220: 0.4012
Loss at step 2230: 0.6051
Loss at step 2240: 0.3856
Loss at step 2250: 0.4525
Loss at step 2260: 0.4351
Loss at step 2270: 0.6383
Loss at step 2280: 0.2867
Loss at step 2290: 0.0796
Loss at step 2300: 0.4272
Loss at step 2310: 0.2761
Loss at step 2320: 0.4788
Loss at step 2330: 0.4798
Loss at step 2340: 0.0347
Loss at step 2350: 0.2534
Loss at step 2360: 0.1896
Loss at step 2370: 0.0764
Loss at step 2380: 0.4451
Loss at step 2390: 0.4555
Loss at step 2400: 0.4040
Loss at step 2410: 0.2463
Loss at step 2420: 0.6439
Loss at step 2430: 0.4559
Loss at step 2440: 0.4533
Loss at step 2450: 0.6021
Loss at step 2460: 0.4431
Loss at step 2470: 0.8408
Loss at step 2480: 1.1772
Loss at step 2490: 0.4084
Loss at step 2500: 0.4067
Loss at step 2510: 0.3039
Loss at step 2520: 0.3817
Loss at step 2530: 0.3038
Loss at step 2540: 0.4767
Loss at step 2550: 0.6784
Loss at step 2560: 0.5220
Loss at step 2570: 0.1593
Loss at step 2580: 0.7260
Loss at step 2590: 0.1721
Loss at step 2600: 0.7534
Loss at step 2610: 0.3097
Loss at step 2620: 0.4677
Loss at step 2630: 0.5321
Loss at step 2640: 0.4408
Loss at step 2650: 0.5432
Loss at step 2660: 0.4318
Loss at step 2670: 0.5855
Loss at step 2680: 0.1034
Loss at step 2690: 0.1594
Loss at step 2700: 0.3836
Loss at step 2710: 0.3852
Loss at step 2720: 0.3702
Loss at step 2730: 0.3756
Loss at step 2740: 0.7082
Loss at step 2750: 0.3852
Loss at step 2760: 0.4360
Loss at step 2770: 0.1493
Loss at step 2780: 0.3454
Loss at step 2790: 0.3804
Loss at step 2800: 0.4169
Loss at step 2810: 0.1298
Loss at step 2820: 0.1662
Loss at step 2830: 0.2382
Loss at step 2840: 0.2163
Loss at step 2850: 0.3555
Loss at step 2860: 0.2328
Loss at step 2870: 0.4001
Loss at step 2880: 0.4043
Loss at step 2890: 0.3528
Loss at step 2900: 0.3623
Loss at step 2910: 0.2873
Loss at step 2920: 0.4147
Loss at step 2930: 0.2677
Loss at step 2940: 0.3765
Loss at step 2950: 0.3705
Loss at step 2960: 0.3365
Loss at step 2970: 0.3871
Loss at step 2980: 0.4546
Loss at step 2990: 0.7674
Loss at step 3000: 0.4817
Loss at step 3010: 0.2245
Loss at step 3020: 0.3920
Loss at step 3030: 0.3344
Loss at step 3040: 0.4164
Loss at step 3050: 0.0783
Loss at step 3060: 0.6206
Loss at step 3070: 0.5355
Loss at step 3080: 0.5511
Loss at step 3090: 0.3940
Loss at step 3100: 0.6758
Loss at step 3110: 0.5091
Loss at step 3120: 1.0903
Loss at step 3130: 0.9580
Loss at step 3140: 0.3841
Loss at step 3150: 0.5801
Loss at step 3160: 0.3669
Loss at step 3170: 0.6830
Loss at step 3180: 0.3319
Loss at step 3190: 0.1483
Loss at step 3200: 0.3488
Loss at step 3210: 0.2290
Loss at step 3220: 0.4527
Loss at step 3230: 0.8153
Loss at step 3240: 0.8281
Loss at step 3250: 0.4504
Loss at step 3260: 0.8639
Loss at step 3270: 0.7050
Loss at step 3280: 0.2857
Loss at step 3290: 0.5743
Loss at step 3300: 0.4164
Loss at step 3310: 0.2242
Loss at step 3320: 0.2496
Loss at step 3330: 0.4739
Loss at step 3340: 0.4997
Loss at step 3350: 0.3034
Loss at step 3360: 0.5666
Loss at step 3370: 0.4473
Loss at step 3380: 0.0645
Loss at step 3390: 0.6073
Loss at step 3400: 0.3477
Loss at step 3410: 0.3468
Loss at step 3420: 0.5735
Loss at step 3430: 0.5318
Loss at step 3440: 0.8140
Loss at step 3450: 0.2441
Loss at step 3460: 0.7927
Loss at step 3470: 0.3495
Loss at step 3480: 0.8039
Loss at step 3490: 0.3233
Loss at step 3500: 0.4810
Loss at step 3510: 0.3142
Loss at step 3520: 0.2713
Loss at step 3530: 0.2659
Loss at step 3540: 0.3664
Loss at step 3550: 0.2744
Loss at step 3560: 0.2727
Loss at step 3570: 0.1789
Loss at step 3580: 0.5156
Loss at step 3590: 0.2788
Loss at step 3600: 0.7947
Loss at step 3610: 0.3129
Loss at step 3620: 0.4409
Loss at step 3630: 0.5192
Loss at step 3640: 0.1237
Loss at step 3650: 0.1441
Loss at step 3660: 0.3360
Loss at step 3670: 0.0498
Loss at step 3680: 0.2696
Loss at step 3690: 0.4212
Loss at step 3700: 0.4477
Loss at step 3710: 0.3759
Loss at step 3720: 0.2996
Loss at step 3730: 0.2134
Loss at step 3740: 0.3467
Loss at step 3750: 0.5321
Loss at step 3760: 0.7534
Loss at step 3770: 0.2981
Loss at step 3780: 0.3219
Loss at step 3790: 0.1973
Loss at step 3800: 0.3509
Loss at step 3810: 0.7255
Loss at step 3820: 0.5269
Loss at step 3830: 0.1290
Loss at step 3840: 0.4252
Loss at step 3850: 0.4749
Loss at step 3860: 0.1552
Loss at step 3870: 0.3679
Loss at step 3880: 0.8250
Loss at step 3890: 0.5401
Loss at step 3900: 0.2401
Loss at step 3910: 0.0279
Loss at step 3920: 0.5783
Loss at step 3930: 0.3964
Loss at step 3940: 0.2638
Loss at step 3950: 0.2440
Loss at step 3960: 0.2646
Loss at step 3970: 0.4163
Loss at step 3980: 0.2722
Loss at step 3990: 0.7977
Loss at step 4000: 0.1873
Loss at step 4010: 0.5345
Loss at step 4020: 0.2445
Loss at step 4030: 0.8304
Loss at step 4040: 0.2696
Loss at step 4050: 0.2885
Loss at step 4060: 0.1819
Loss at step 4070: 0.1680
Loss at step 4080: 0.2487
Loss at step 4090: 0.1835
Loss at step 4100: 0.3144
Loss at step 4110: 0.7103
Loss at step 4120: 0.8891
Loss at step 4130: 0.5867
Loss at step 4140: 0.2240
Loss at step 4150: 0.1712
Loss at step 4160: 0.4000
Loss at step 4170: 0.4259
Loss at step 4180: 0.3641
Loss at step 4190: 0.7089
Loss at step 4200: 0.3337
Loss at step 4210: 0.5455
Loss at step 4220: 0.4822
Loss at step 4230: 0.3638
Loss at step 4240: 0.3123
Loss at step 4250: 0.3762
Loss at step 4260: 0.7261
Loss at step 4270: 0.6180
Loss at step 4280: 0.3158
Loss at step 4290: 0.2878
Loss at step 4300: 0.3027
Loss at step 4310: 0.8456
Loss at step 4320: 0.2808
Loss at step 4330: 0.6011
Loss at step 4340: 0.2506
Loss at step 4350: 0.6138
Loss at step 4360: 0.5749
Loss at step 4370: 0.4736
Loss at step 4380: 0.2459
Loss at step 4390: 0.3283
Loss at step 4400: 0.4171
Loss at step 4410: 0.5896
Loss at step 4420: 0.6702
Loss at step 4430: 0.7371
Loss at step 4440: 0.4081
Loss at step 4450: 0.6635
Loss at step 4460: 0.6639
Loss at step 4470: 0.2472
Loss at step 4480: 0.5541
Loss at step 4490: 0.3689
Loss at step 4500: 0.4048
Loss at step 4510: 0.3167
Loss at step 4520: 0.2937
Loss at step 4530: 0.5391
Loss at step 4540: 0.2908
Loss at step 4550: 0.7247
Loss at step 4560: 0.3538
Loss at step 4570: 0.7176
Loss at step 4580: 0.7186
Loss at step 4590: 0.3713
Loss at step 4600: 0.3167
Loss at step 4610: 0.1741
Loss at step 4620: 0.6091
Loss at step 4630: 0.3581
Loss at step 4640: 1.2325
Loss at step 4650: 0.4267
Loss at step 4660: 0.3605
Loss at step 4670: 0.2210
Loss at step 4680: 0.5591
Loss at step 4690: 0.5186
Loss at step 4700: 0.4046
Loss at step 4710: 0.2894
Loss at step 4720: 0.1858
Loss at step 4730: 0.5856
Loss at step 4740: 0.7946
Loss at step 4750: 0.6020
Loss at step 4760: 1.2489
Loss at step 4770: 0.1914
Loss at step 4780: 0.6576
Loss at step 4790: 0.2845
Loss at step 4800: 0.2983
Loss at step 4810: 0.2074
Loss at step 4820: 0.7224
Loss at step 4830: 0.6053
Loss at step 4840: 0.4140
Loss at step 4850: 0.3935
Loss at step 4860: 0.1319
Loss at step 4870: 0.7842
Loss at step 4880: 0.7732
Loss at step 4890: 0.7131
Loss at step 4900: 0.2370
Loss at step 4910: 0.3007
Loss at step 4920: 0.2393
Loss at step 4930: 0.4406
Loss at step 4940: 0.7408
Loss at step 4950: 0.4529
Loss at step 4960: 0.3586
Loss at step 4970: 0.5630
Loss at step 4980: 0.1447
Loss at step 4990: 0.3634
Loss at step 5000: 0.4813
Loss at step 5010: 0.3455
Loss at step 5020: 0.3063
Loss at step 5030: 0.1928
Loss at step 5040: 0.2797
Loss at step 5050: 0.2635
Loss at step 5060: 0.2551
Loss at step 5070: 0.4869
Loss at step 5080: 0.2434
Loss at step 5090: 0.4252
Loss at step 5100: 0.4815
Loss at step 5110: 0.5095
Loss at step 5120: 0.5826
Loss at step 5130: 0.2429
Loss at step 5140: 0.3942
Loss at step 5150: 0.5601
Loss at step 5160: 0.2458
Loss at step 5170: 0.4730
Loss at step 5180: 0.2304
Loss at step 5190: 0.2779
Loss at step 5200: 0.2878
Loss at step 5210: 0.5692
Loss at step 5220: 0.2794
Loss at step 5230: 0.1759
Loss at step 5240: 0.4379
Loss at step 5250: 0.7587
Loss at step 5260: 0.3105
Loss at step 5270: 0.4613
Loss at step 5280: 0.2590
Loss at step 5290: 0.3423
Loss at step 5300: 0.2986
Loss at step 5310: 0.3898
Loss at step 5320: 0.4475
Loss at step 5330: 0.1752
Loss at step 5340: 0.2873
Loss at step 5350: 0.9711
Loss at step 5360: 0.3161
Loss at step 5370: 0.3138
Loss at step 5380: 0.3033
Loss at step 5390: 0.1804
Loss at step 5400: 0.7535
Loss at step 5410: 0.4895
Loss at step 5420: 0.0536
Loss at step 5430: 0.3255
Loss at step 5440: 0.4878
Loss at step 5450: 0.2725
Loss at step 5460: 0.4151
Loss at step 5470: 0.3617
Loss at step 5480: 0.3696
Loss at step 5490: 0.2565
Loss at step 5500: 0.7589
Loss at step 5510: 0.1342
Loss at step 5520: 0.1150
Loss at step 5530: 0.3294
Loss at step 5540: 0.3573
Loss at step 5550: 0.2297
Loss at step 5560: 0.2943
Loss at step 5570: 0.5687
Loss at step 5580: 0.4163
Loss at step 5590: 0.1394
Loss at step 5600: 0.4309
Loss at step 5610: 0.7252
Loss at step 5620: 0.4156
Loss at step 5630: 0.2385
Loss at step 5640: 0.3600
Loss at step 5650: 0.4821
Loss at step 5660: 0.3368
Loss at step 5670: 0.2193
Loss at step 5680: 0.3228
Loss at step 5690: 0.1700
Loss at step 5700: 1.0858
Loss at step 5710: 0.3088
Loss at step 5720: 0.4458
Loss at step 5730: 0.0228
Loss at step 5740: 0.4382
Loss at step 5750: 0.0762
Loss at step 5760: 0.2609
Loss at step 5770: 0.8109
Loss at step 5780: 0.0853
Loss at step 5790: 0.1397
Loss at step 5800: 0.1090
Loss at step 5810: 0.6034
Loss at step 5820: 0.3685
Loss at step 5830: 0.5587
Loss at step 5840: 0.2127
Loss at step 5850: 0.3654
Loss at step 5860: 0.5546
Loss at step 5870: 0.4252
Loss at step 5880: 0.1741
Loss at step 5890: 0.3039
Loss at step 5900: 0.6416
Loss at step 5910: 0.3559
Loss at step 5920: 0.2926
Loss at step 5930: 0.0972
Loss at step 5940: 0.3406
Loss at step 5950: 0.4120
Loss at step 5960: 0.2396
Loss at step 5970: 0.4369
Loss at step 5980: 0.3027
Loss at step 5990: 0.7391
Loss at step 6000: 0.3976
Loss at step 6010: 0.6573
Loss at step 6020: 0.2952
Loss at step 6030: 0.3419
Loss at step 6040: 0.1351
Loss at step 6050: 0.5741
Loss at step 6060: 0.2806
Loss at step 6070: 0.3010
Loss at step 6080: 0.4346
Loss at step 6090: 0.4427
Loss at step 6100: 0.2276
Loss at step 6110: 0.5630
Loss at step 6120: 0.6492
Loss at step 6130: 0.3377
Loss at step 6140: 0.7077
Loss at step 6150: 0.6896
Loss at step 6160: 1.0953
Loss at step 6170: 0.1298
Loss at step 6180: 0.3456
Loss at step 6190: 0.4387
Loss at step 6200: 0.4139
Loss at step 6210: 0.3457
Loss at step 6220: 0.5205
Loss at step 6230: 0.2681
Loss at step 6240: 0.5644
Loss at step 6250: 0.5744
Loss at step 6260: 0.2517
Loss at step 6270: 0.2627
Loss at step 6280: 0.0858
Loss at step 6290: 0.2004
Loss at step 6300: 0.4991
Loss at step 6310: 0.4372
Loss at step 6320: 0.3424
Loss at step 6330: 0.4249
Loss at step 6340: 0.4463
Loss at step 6350: 0.2254
Loss at step 6360: 0.2809
Loss at step 6370: 0.2916
Loss at step 6380: 0.3174
Loss at step 6390: 0.2672
Loss at step 6400: 0.5890
Loss at step 6410: 0.2921
Loss at step 6420: 0.3942
Loss at step 6430: 0.8706
Loss at step 6440: 0.3231
Loss at step 6450: 0.2251
Loss at step 6460: 0.4989
Loss at step 6470: 0.2338
Loss at step 6480: 0.6514
Loss at step 6490: 0.4835
Loss at step 6500: 0.5526
Loss at step 6510: 0.4577
Loss at step 6520: 0.4732
Loss at step 6530: 0.6390
Loss at step 6540: 0.3730
Loss at step 6550: 0.2569
Loss at step 6560: 0.4180
Loss at step 6570: 0.4658
Loss at step 6580: 0.3686
Loss at step 6590: 0.1996
Loss at step 6600: 0.4381
Loss at step 6610: 0.7245
Loss at step 6620: 0.1647
Loss at step 6630: 0.0930
Loss at step 6640: 0.4818
Loss at step 6650: 0.3077
Loss at step 6660: 0.0745
Loss at step 6670: 0.2818
Loss at step 6680: 0.3370
Loss at step 6690: 0.2880
Loss at step 6700: 0.2023
Loss at step 6710: 0.0976
Loss at step 6720: 0.6097
Loss at step 6730: 0.8541
Loss at step 6740: 0.3081
Loss at step 6750: 0.1965
Loss at step 6760: 0.0880
Loss at step 6770: 0.4443
Loss at step 6780: 0.2815
Loss at step 6790: 0.2970
Loss at step 6800: 0.4306
Loss at step 6810: 0.4324
Loss at step 6820: 0.4157
Loss at step 6830: 0.3354
Loss at step 6840: 0.4942
Loss at step 6850: 0.2088
Loss at step 6860: 0.3498
Loss at step 6870: 0.2634
Loss at step 6880: 0.1482
Loss at step 6890: 0.3378
Loss at step 6900: 0.3073
Loss at step 6910: 0.4011
Loss at step 6920: 0.7274
Loss at step 6930: 0.2001
Loss at step 6940: 0.3920
Loss at step 6950: 0.2658
Loss at step 6960: 0.3220
Loss at step 6970: 0.4236
Loss at step 6980: 0.2157
Loss at step 6990: 0.3110
Loss at step 7000: 0.1081
Loss at step 7010: 0.3417
Loss at step 7020: 0.4236
Loss at step 7030: 0.4650
Loss at step 7040: 0.1552
Loss at step 7050: 0.4737
Loss at step 7060: 0.0734
Loss at step 7070: 0.5386
Loss at step 7080: 0.2287
Loss at step 7090: 0.5901
Loss at step 7100: 0.5380
Loss at step 7110: 0.6358
Loss at step 7120: 0.1900
Loss at step 7130: 0.3288
Loss at step 7140: 0.1091
Loss at step 7150: 0.2656
Loss at step 7160: 0.2424
Loss at step 7170: 1.0288
Loss at step 7180: 0.2553
Loss at step 7190: 0.2573
Loss at step 7200: 0.4901
Loss at step 7210: 0.5812
Loss at step 7220: 0.1040
Loss at step 7230: 0.5003
Loss at step 7240: 0.9816
Loss at step 7250: 0.1651
Loss at step 7260: 0.8206
Loss at step 7270: 0.2752
Loss at step 7280: 0.2763
Loss at step 7290: 0.0299
Loss at step 7300: 0.3669
Loss at step 7310: 0.6499
Loss at step 7320: 0.3763
Loss at step 7330: 0.3218
Loss at step 7340: 0.4087
Loss at step 7350: 0.4541
Loss at step 7360: 0.2032
Loss at step 7370: 0.3056
Loss at step 7380: 0.2825
Loss at step 7390: 0.1227
Loss at step 7400: 0.1899
Loss at step 7410: 0.4684
Loss at step 7420: 0.0914
Loss at step 7430: 0.6904
Loss at step 7440: 0.2512
Loss at step 7450: 0.3204
Loss at step 7460: 0.3485
Loss at step 7470: 0.1780
Loss at step 7480: 0.3873
Loss at step 7490: 0.4139
Loss at step 7500: 0.3189
Loss at step 7510: 0.5113
Loss at step 7520: 0.4748
Loss at step 7530: 0.2752
Loss at step 7540: 0.4881
Loss at step 7550: 0.6450
Loss at step 7560: 0.3324
Loss at step 7570: 0.1592
Loss at step 7580: 0.2371
Loss at step 7590: 0.2220
Loss at step 7600: 0.2948
Loss at step 7610: 0.3510
Loss at step 7620: 0.2258
Loss at step 7630: 0.5527
Loss at step 7640: 0.1308
Loss at step 7650: 0.1840
Loss at step 7660: 0.6018
Loss at step 7670: 0.0977
Loss at step 7680: 0.3733
Loss at step 7690: 0.1855
Loss at step 7700: 0.4091
Loss at step 7710: 0.5913
Loss at step 7720: 0.2854
Loss at step 7730: 0.1499
Loss at step 7740: 0.4707
Loss at step 7750: 0.3250
Loss at step 7760: 0.5844
Loss at step 7770: 0.2548
Loss at step 7780: 0.2665
Loss at step 7790: 0.0437
Loss at step 7800: 0.4819
Loss at step 7810: 0.0609
Loss at step 7820: 0.3127
Loss at step 7830: 0.2992
Loss at step 7840: 0.5370
Loss at step 7850: 0.4247
Loss at step 7860: 0.4676
Loss at step 7870: 0.4155
Loss at step 7880: 0.8380
Loss at step 7890: 0.2805
Loss at step 7900: 0.7251
Loss at step 7910: 0.4921
Loss at step 7920: 0.2581
Loss at step 7930: 0.5762
Loss at step 7940: 0.4246
Loss at step 7950: 0.0657
Loss at step 7960: 0.1680
Loss at step 7970: 0.2454
Loss at step 7980: 0.4078
Loss at step 7990: 0.4189
Loss at step 8000: 0.3151
Loss at step 8010: 0.5204
Loss at step 8020: 0.6781
Loss at step 8030: 0.2487
Loss at step 8040: 0.2522
Loss at step 8050: 0.3313
Loss at step 8060: 0.5429
Loss at step 8070: 0.4369
Loss at step 8080: 0.3921
Loss at step 8090: 0.6197
Loss at step 8100: 0.2733
Loss at step 8110: 0.4296
Loss at step 8120: 0.5093
Loss at step 8130: 0.1039
Loss at step 8140: 0.3912
Loss at step 8150: 0.0169
Loss at step 8160: 0.1564
Loss at step 8170: 0.1874
Loss at step 8180: 0.3611
Loss at step 8190: 0.7272
Loss at step 8200: 0.4979
Loss at step 8210: 0.2696
Loss at step 8220: 0.2993
Loss at step 8230: 0.5026
Loss at step 8240: 0.6994
Loss at step 8250: 0.4442
Loss at step 8260: 0.2225
Loss at step 8270: 0.6055
Loss at step 8280: 0.4204
Loss at step 8290: 0.3738
Loss at step 8300: 0.2997
Loss at step 8310: 0.5875
Loss at step 8320: 0.0540
Loss at step 8330: 0.0081
Loss at step 8340: 0.1516
Loss at step 8350: 0.0842
Loss at step 8360: 0.2822
Loss at step 8370: 0.1334
Loss at step 8380: 0.2747
Loss at step 8390: 0.2752
Loss at step 8400: 0.1190
Loss at step 8410: 0.3373
Loss at step 8420: 0.2705
Loss at step 8430: 0.3659
Loss at step 8440: 0.4485
Loss at step 8450: 0.1596
Loss at step 8460: 0.1247
Loss at step 8470: 0.5827
Loss at step 8480: 0.3712
Loss at step 8490: 0.4394
Loss at step 8500: 0.1692
Loss at step 8510: 0.3957
Loss at step 8520: 0.2696
Loss at step 8530: 0.2204
Loss at step 8540: 1.0175
Loss at step 8550: 0.1878
Loss at step 8560: 0.4423
Loss at step 8570: 0.1537
Loss at step 8580: 0.4080
Loss at step 8590: 0.2061
Loss at step 8600: 0.2717
Loss at step 8610: 0.2568
Loss at step 8620: 0.4013
Loss at step 8630: 0.4898
Loss at step 8640: 0.4548
Loss at step 8650: 0.0705
Loss at step 8660: 0.1321
Loss at step 8670: 0.4237
Loss at step 8680: 0.0845
Loss at step 8690: 0.3118
Loss at step 8700: 0.5806
Loss at step 8710: 0.2586
Loss at step 8720: 0.5421
Loss at step 8730: 0.1898
Loss at step 8740: 0.3671
Loss at step 8750: 0.5729
Loss at step 8760: 0.4571
Loss at step 8770: 0.6655
Loss at step 8780: 0.4452
Loss at step 8790: 1.4576
Loss at step 8800: 0.4416
Loss at step 8810: 0.3099
Loss at step 8820: 0.2310
Loss at step 8830: 0.0498
Loss at step 8840: 0.1418
Loss at step 8850: 0.1209
Loss at step 8860: 0.3006
Loss at step 8870: 0.4260
Loss at step 8880: 0.2371
Loss at step 8890: 0.2954
Loss at step 8900: 0.4908
Loss at step 8910: 0.4180
Loss at step 8920: 0.2455
Loss at step 8930: 0.9014
Loss at step 8940: 1.3488
Loss at step 8950: 0.5913
Loss at step 8960: 0.0623
Loss at step 8970: 0.0310
Loss at step 8980: 0.9222
Loss at step 8990: 0.5008
Loss at step 9000: 0.0863
Loss at step 9010: 0.3150
Loss at step 9020: 0.8700
Loss at step 9030: 0.1628
Loss at step 9040: 0.2744
Loss at step 9050: 0.3178
Loss at step 9060: 0.3059
Loss at step 9070: 0.5816
Loss at step 9080: 0.1899
Loss at step 9090: 0.1795
Loss at step 9100: 0.1560
Loss at step 9110: 0.2828
Loss at step 9120: 0.5793
Loss at step 9130: 0.4634
Loss at step 9140: 0.2842
Loss at step 9150: 0.3992
Loss at step 9160: 0.4093
Loss at step 9170: 0.4197
Loss at step 9180: 0.3291
Loss at step 9190: 0.0896
Loss at step 9200: 0.2311
Loss at step 9210: 0.9713
Loss at step 9220: 0.3498
Loss at step 9230: 0.3608
Loss at step 9240: 0.6874
Loss at step 9250: 0.4205
Loss at step 9260: 0.8024
Loss at step 9270: 0.4054
Loss at step 9280: 0.3807
Loss at step 9290: 0.3417
Loss at step 9300: 0.2739
Loss at step 9310: 0.5443
Loss at step 9320: 0.1885
Loss at step 9330: 0.3342
Loss at step 9340: 0.3554
Loss at step 9350: 0.5339
Loss at step 9360: 0.3162
Loss at step 9370: 0.3500
Loss at step 9380: 0.3277
Loss at step 9390: 0.6442
Loss at step 9400: 0.0845
Loss at step 9410: 0.0358
Loss at step 9420: 0.4315
Loss at step 9430: 0.6743
Loss at step 9440: 0.0409
Loss at step 9450: 0.5966
Loss at step 9460: 0.1760
Loss at step 9470: 0.2632
Loss at step 9480: 0.0813
Loss at step 9490: 0.6225
Loss at step 9500: 0.2100
Loss at step 9510: 0.1893
Loss at step 9520: 0.4190
Loss at step 9530: 0.2669
Loss at step 9540: 0.1889
Loss at step 9550: 0.4315
Loss at step 9560: 0.1956
Loss at step 9570: 0.4059
Loss at step 9580: 0.3023
Loss at step 9590: 0.5813
Loss at step 9600: 0.5399
Loss at step 9610: 0.4020
Loss at step 9620: 0.2075
Loss at step 9630: 0.3839
Loss at step 9640: 0.4059
Loss at step 9650: 0.2409
Loss at step 9660: 0.7124
Loss at step 9670: 0.2962
Loss at step 9680: 0.5023
Loss at step 9690: 0.4791
Loss at step 9700: 0.1809
Loss at step 9710: 0.4742
Loss at step 9720: 0.2129
Loss at step 9730: 0.3821
Loss at step 9740: 0.3964
Loss at step 9750: 0.0798
Loss at step 9760: 0.1907
Loss at step 9770: 0.3941
Loss at step 9780: 0.5839
Loss at step 9790: 0.0435
Loss at step 9800: 0.2137
Loss at step 9810: 0.1817
Loss at step 9820: 0.2913
Loss at step 9830: 0.3041
Loss at step 9840: 0.3769
Loss at step 9850: 0.5631
Loss at step 9860: 0.0963
Loss at step 9870: 0.0489
Loss at step 9880: 0.2374
Loss at step 9890: 0.0696
Loss at step 9900: 0.5193
Loss at step 9910: 0.2495
Loss at step 9920: 0.2973
Loss at step 9930: 0.1320
Loss at step 9940: 0.0420
Loss at step 9950: 0.1107
Loss at step 9960: 0.4294
Loss at step 9970: 0.4505
Loss at step 9980: 0.4863
Loss at step 9990: 0.5918
Loss at step 10000: 0.4217
Loss at step 10010: 0.2185
Loss at step 10020: 0.0826
Loss at step 10030: 0.2293
Loss at step 10040: 0.2377
Loss at step 10050: 0.2911
Loss at step 10060: 0.8444
Loss at step 10070: 0.2436
Loss at step 10080: 0.0954
Loss at step 10090: 0.3989
Loss at step 10100: 0.3665
Loss at step 10110: 0.7387
Loss at step 10120: 0.2506
Loss at step 10130: 0.5894
Loss at step 10140: 0.3846
Loss at step 10150: 0.5759
Loss at step 10160: 0.4426
Loss at step 10170: 0.5079
Loss at step 10180: 0.2902
Loss at step 10190: 0.3347
Loss at step 10200: 0.1453
Loss at step 10210: 0.4817
Loss at step 10220: 0.6715
Loss at step 10230: 0.2550
Loss at step 10240: 0.4483
Loss at step 10250: 0.3600
Loss at step 10260: 0.5223
Loss at step 10270: 0.2165
Loss at step 10280: 0.4120
Loss at step 10290: 0.3373
Loss at step 10300: 0.7657
Loss at step 10310: 0.3757
Loss at step 10320: 0.2022
Loss at step 10330: 0.2940
Loss at step 10340: 0.4490
Loss at step 10350: 0.0587
Loss at step 10360: 0.4408
Loss at step 10370: 0.8617
Loss at step 10380: 0.2408
Loss at step 10390: 0.3266
Loss at step 10400: 0.5140
Loss at step 10410: 0.1190
Loss at step 10420: 0.5335
Loss at step 10430: 0.5218
Loss at step 10440: 0.2464
Loss at step 10450: 0.3854
Loss at step 10460: 0.1782
Loss at step 10470: 0.5325
Loss at step 10480: 0.2155
Loss at step 10490: 0.1752
Loss at step 10500: 0.5310
Loss at step 10510: 0.3510
Loss at step 10520: 0.3025
Loss at step 10530: 0.2714
Loss at step 10540: 0.1329
Loss at step 10550: 0.3043
Loss at step 10560: 0.4967
Loss at step 10570: 0.1583
Loss at step 10580: 0.5575
Loss at step 10590: 0.3191
Loss at step 10600: 0.4858
Loss at step 10610: 0.5309
Loss at step 10620: 0.1298
Loss at step 10630: 0.2817
Loss at step 10640: 0.5524
Loss at step 10650: 0.2178
Loss at step 10660: 0.1377
Loss at step 10670: 0.1232
Loss at step 10680: 0.1445
Loss at step 10690: 0.1519
Loss at step 10700: 0.1519
Loss at step 10710: 0.4112
Loss at step 10720: 0.3981
Loss at step 10730: 0.2983
Loss at step 10740: 0.6078
Loss at step 10750: 0.1994
Loss at step 10760: 0.2125
Loss at step 10770: 0.5306
Loss at step 10780: 0.5006
Loss at step 10790: 0.0609
Loss at step 10800: 0.2548
Loss at step 10810: 0.2887
Loss at step 10820: 0.2207
Loss at step 10830: 0.4231
Loss at step 10840: 0.3227
Loss at step 10850: 0.3827
Loss at step 10860: 0.4923
Loss at step 10870: 0.4827
Loss at step 10880: 0.1770
Loss at step 10890: 0.7301
Loss at step 10900: 0.1874
Loss at step 10910: 0.2218
Loss at step 10920: 0.0847
Loss at step 10930: 0.6232
Loss at step 10940: 0.1032
Loss at step 10950: 0.6318
Loss at step 10960: 0.4367
Loss at step 10970: 0.2402
Loss at step 10980: 0.2591
Loss at step 10990: 0.0761
Loss at step 11000: 0.1872
Loss at step 11010: 0.4140
Loss at step 11020: 0.5255
Loss at step 11030: 0.3827
Loss at step 11040: 0.2271
Loss at step 11050: 0.3781
Loss at step 11060: 0.3322
Loss at step 11070: 0.2651
Loss at step 11080: 0.0694
Loss at step 11090: 0.1113
Loss at step 11100: 0.3952
Loss at step 11110: 0.2591
Loss at step 11120: 0.3653
Loss at step 11130: 0.5811
Loss at step 11140: 0.3212
Loss at step 11150: 0.4641
Loss at step 11160: 0.0933
Loss at step 11170: 0.5119
Loss at step 11180: 0.3108
Loss at step 11190: 0.2788
Loss at step 11200: 0.1634
Loss at step 11210: 0.3300
Loss at step 11220: 0.3271
Loss at step 11230: 0.3589
Loss at step 11240: 0.2171
Loss at step 11250: 0.3454
Loss at step 11260: 0.2973
Loss at step 11270: 0.0626
Loss at step 11280: 0.1476
Loss at step 11290: 0.1969
Loss at step 11300: 0.2849
Loss at step 11310: 0.1843
Loss at step 11320: 0.4989
Loss at step 11330: 0.3807
Loss at step 11340: 0.2773
Loss at step 11350: 0.4024
Loss at step 11360: 0.0860
Loss at step 11370: 0.2858
Loss at step 11380: 0.2865
Loss at step 11390: 0.1183
Loss at step 11400: 0.5239
Loss at step 11410: 0.5292
Loss at step 11420: 0.5401
Loss at step 11430: 0.1553
Loss at step 11440: 0.1551
Loss at step 11450: 0.1555
Loss at step 11460: 0.6202
Loss at step 11470: 0.0623
Loss at step 11480: 0.1829
Loss at step 11490: 0.2875
Loss at step 11500: 0.1463
Loss at step 11510: 0.4217
Loss at step 11520: 0.2175
Loss at step 11530: 0.2775
Loss at step 11540: 0.3664
Loss at step 11550: 0.3699
Loss at step 11560: 0.3091
Loss at step 11570: 0.2273
Loss at step 11580: 0.2472
Loss at step 11590: 0.3783
Loss at step 11600: 0.4274
Loss at step 11610: 0.1982
Loss at step 11620: 0.1989
Loss at step 11630: 0.2217
Loss at step 11640: 0.3304
Loss at step 11650: 0.6723
Loss at step 11660: 0.1240
Loss at step 11670: 0.3620
Loss at step 11680: 0.5386
Loss at step 11690: 0.5155
Loss at step 11700: 0.1814
Loss at step 11710: 0.7955
Loss at step 11720: 0.2712
Loss at step 11730: 0.0514
Loss at step 11740: 0.2705
Loss at step 11750: 0.1384
Loss at step 11760: 0.1476
Loss at step 11770: 0.2488
Loss at step 11780: 0.4015
Loss at step 11790: 0.4294
Loss at step 11800: 0.2010
Loss at step 11810: 0.4837
Loss at step 11820: 0.4183
Loss at step 11830: 0.2227
Loss at step 11840: 0.4479
Loss at step 11850: 0.1710
Loss at step 11860: 0.2246
Loss at step 11870: 0.4420
Loss at step 11880: 0.5313
Loss at step 11890: 0.2160
Loss at step 11900: 0.0286
Loss at step 11910: 0.3378
Loss at step 11920: 0.4090
Loss at step 11930: 0.2093
Loss at step 11940: 0.3411
Loss at step 11950: 0.1885
Loss at step 11960: 0.1060
Loss at step 11970: 0.4052
Loss at step 11980: 0.1850
Loss at step 11990: 0.1682
Loss at step 12000: 0.3466
Loss at step 12010: 0.2627
Loss at step 12020: 0.1833
Loss at step 12030: 0.1271
Loss at step 12040: 0.1831
Loss at step 12050: 0.7548
Loss at step 12060: 0.5984
Loss at step 12070: 0.2291
Loss at step 12080: 0.4111
Loss at step 12090: 0.1830
Loss at step 12100: 0.3524
Loss at step 12110: 0.3608
Loss at step 12120: 0.4286
Loss at step 12130: 0.4087
Loss at step 12140: 0.4881
Loss at step 12150: 0.6085
Loss at step 12160: 0.4871
Loss at step 12170: 0.6441
Loss at step 12180: 0.5827
Loss at step 12190: 0.0896
Loss at step 12200: 0.6584
Loss at step 12210: 0.1890
Loss at step 12220: 0.5703
Loss at step 12230: 0.2828
Loss at step 12240: 0.3889
Loss at step 12250: 0.2478
Loss at step 12260: 0.4692
Loss at step 12270: 0.2095
Loss at step 12280: 0.1732
Loss at step 12290: 0.0619
Loss at step 12300: 0.0869
Loss at step 12310: 0.3180
Loss at step 12320: 0.1247
Loss at step 12330: 0.4478
Loss at step 12340: 0.1113
Loss at step 12350: 0.2427
Loss at step 12360: 0.2100
Loss at step 12370: 0.5630
Loss at step 12380: 0.6253
Loss at step 12390: 0.4619
Loss at step 12400: 0.1266
Loss at step 12410: 0.1337
Loss at step 12420: 0.8161
Loss at step 12430: 0.4689
Loss at step 12440: 0.1032
Loss at step 12450: 0.1657
Loss at step 12460: 0.4636
Loss at step 12470: 0.3081
Loss at step 12480: 0.2845
Loss at step 12490: 0.1302
Loss at step 12500: 0.2992
Loss at step 12510: 0.1015
Loss at step 12520: 0.5358
Loss at step 12530: 0.1988
Loss at step 12540: 0.2096
Loss at step 12550: 0.1330
Loss at step 12560: 0.1198
Loss at step 12570: 0.3805
Loss at step 12580: 0.3230
Loss at step 12590: 0.4984
Loss at step 12600: 0.4253
Loss at step 12610: 0.1708
Loss at step 12620: 0.3464
Loss at step 12630: 0.5729
Loss at step 12640: 0.5565
Loss at step 12650: 0.4390
Loss at step 12660: 0.3280
Loss at step 12670: 0.2361
Loss at step 12680: 0.1671
Loss at step 12690: 0.4760
Loss at step 12700: 0.3149
Loss at step 12710: 0.3086
Loss at step 12720: 0.1484
Loss at step 12730: 0.3068
Loss at step 12740: 0.3391
Loss at step 12750: 0.8325
Loss at step 12760: 0.1110
Loss at step 12770: 0.1083
Loss at step 12780: 0.3226
Loss at step 12790: 0.3551
Loss at step 12800: 0.5140
Loss at step 12810: 0.1841
Loss at step 12820: 0.2539
Loss at step 12830: 0.4656
Loss at step 12840: 0.5907
Loss at step 12850: 0.2584
Loss at step 12860: 0.4342
Loss at step 12870: 0.2288
Loss at step 12880: 0.1705
Loss at step 12890: 0.4176
Loss at step 12900: 0.2224
Loss at step 12910: 0.2800
Loss at step 12920: 0.4781
Loss at step 12930: 0.2282
Loss at step 12940: 0.1239
Loss at step 12950: 0.2170
Loss at step 12960: 0.9754
Loss at step 12970: 0.1147
Loss at step 12980: 0.5133
Loss at step 12990: 0.7023
Loss at step 13000: 0.1412
Loss at step 13010: 0.1440
Loss at step 13020: 0.2490
Loss at step 13030: 0.1779
Loss at step 13040: 0.2660
Loss at step 13050: 0.5175
Loss at step 13060: 0.1461
Loss at step 13070: 0.2080
Loss at step 13080: 0.2970
Loss at step 13090: 0.3562
Loss at step 13100: 0.3450
Loss at step 13110: 0.5270
Loss at step 13120: 0.1759
Loss at step 13130: 0.7125
Loss at step 13140: 0.2377
Loss at step 13150: 0.2398
Loss at step 13160: 0.2740
Loss at step 13170: 0.2359
Loss at step 13180: 0.2258
Loss at step 13190: 0.1779
Loss at step 13200: 0.2224
Loss at step 13210: 0.0500
Loss at step 13220: 0.4191
Loss at step 13230: 0.4159
Loss at step 13240: 0.3162
Loss at step 13250: 0.0298
Loss at step 13260: 0.2225
Loss at step 13270: 0.2143
Loss at step 13280: 0.9867
Loss at step 13290: 0.1790
Loss at step 13300: 0.2349
Loss at step 13310: 0.1116
Loss at step 13320: 0.2593
Loss at step 13330: 0.1011
Loss at step 13340: 0.2129
Loss at step 13350: 0.2856
Loss at step 13360: 0.4638
Loss at step 13370: 0.2761
Loss at step 13380: 0.7073
Loss at step 13390: 0.4440
Loss at step 13400: 0.1657
Loss at step 13410: 0.5429
Loss at step 13420: 0.4758
Loss at step 13430: 0.2176
Loss at step 13440: 0.2259
Loss at step 13450: 0.4138
Loss at step 13460: 0.1180
Loss at step 13470: 0.1182
Loss at step 13480: 0.2561
Loss at step 13490: 0.4858
Loss at step 13500: 0.6844
Loss at step 13510: 0.0994
Loss at step 13520: 1.0430
Loss at step 13530: 0.1696
Loss at step 13540: 0.3147
Loss at step 13550: 0.1540
Loss at step 13560: 0.4096
Loss at step 13570: 0.2410
Loss at step 13580: 0.1608
Loss at step 13590: 0.2199
Loss at step 13600: 0.4991
Loss at step 13610: 0.1322
Loss at step 13620: 0.2097
Loss at step 13630: 0.2045
Loss at step 13640: 0.5293
Loss at step 13650: 0.1093
Loss at step 13660: 0.2270
Loss at step 13670: 0.3053
Loss at step 13680: 0.2048
Loss at step 13690: 0.6531
Loss at step 13700: 0.2855
Loss at step 13710: 0.2487
Loss at step 13720: 0.3417
Loss at step 13730: 0.1118
Loss at step 13740: 0.1807
Loss at step 13750: 0.3683
Loss at step 13760: 0.1415
Loss at step 13770: 0.2363
Loss at step 13780: 0.1974
Loss at step 13790: 0.3751
Loss at step 13800: 0.1533
Loss at step 13810: 0.1886
Loss at step 13820: 0.2115
Loss at step 13830: 0.2720
Loss at step 13840: 0.7564
Loss at step 13850: 0.3000
Loss at step 13860: 0.2590
Loss at step 13870: 0.3615
Loss at step 13880: 0.6730
Loss at step 13890: 0.2872
Loss at step 13900: 0.2633
Loss at step 13910: 0.6350
Loss at step 13920: 0.2383
Loss at step 13930: 0.2355
Loss at step 13940: 0.2208
Loss at step 13950: 0.1448
Loss at step 13960: 0.5377
Loss at step 13970: 0.3138
Loss at step 13980: 0.3502
Loss at step 13990: 0.2350
Loss at step 14000: 0.0916
Loss at step 14010: 0.2317
Loss at step 14020: 0.1520
Loss at step 14030: 0.6308
Loss at step 14040: 0.1799
Loss at step 14050: 0.1073
Loss at step 14060: 0.1917
Loss at step 14070: 0.6528
Loss at step 14080: 0.1660
Loss at step 14090: 0.3465
Loss at step 14100: 0.3135
Loss at step 14110: 0.2370
Loss at step 14120: 0.1825
Loss at step 14130: 0.2007
Loss at step 14140: 0.1225
Loss at step 14150: 0.6090
Loss at step 14160: 0.6725
Loss at step 14170: 0.0858
Loss at step 14180: 0.1836
Loss at step 14190: 0.7938
Loss at step 14200: 0.3546
Loss at step 14210: 0.1451
Loss at step 14220: 0.4555
Loss at step 14230: 0.3822
Loss at step 14240: 0.1579
Loss at step 14250: 0.9466
Loss at step 14260: 0.2952
Loss at step 14270: 0.1338
Loss at step 14280: 0.1416
Loss at step 14290: 0.5529
Loss at step 14300: 0.2563
Loss at step 14310: 0.1976
Loss at step 14320: 0.3167
Loss at step 14330: 0.2547
Loss at step 14340: 0.3911
Loss at step 14350: 0.2430
Loss at step 14360: 0.3917
Loss at step 14370: 0.6432
Loss at step 14380: 0.2016
Loss at step 14390: 0.6265
Loss at step 14400: 0.1632
Loss at step 14410: 0.5110
Loss at step 14420: 0.1550
Loss at step 14430: 0.6612
Loss at step 14440: 0.2230
Loss at step 14450: 0.2816
Loss at step 14460: 0.0615
Loss at step 14470: 0.2001
Loss at step 14480: 0.1810
Loss at step 14490: 0.0553
Loss at step 14500: 0.1152
Loss at step 14510: 0.2821
Loss at step 14520: 0.1831
Loss at step 14530: 0.3286
Loss at step 14540: 0.4629
Loss at step 14550: 0.3880
Loss at step 14560: 0.4851
Loss at step 14570: 0.5954
Loss at step 14580: 0.1760
Loss at step 14590: 0.2408
Loss at step 14600: 0.6221
Loss at step 14610: 0.3430
Loss at step 14620: 0.0436
Loss at step 14630: 0.7030
Loss at step 14640: 0.0437
Loss at step 14650: 0.4776
Loss at step 14660: 0.2496
Loss at step 14670: 0.5693
Loss at step 14680: 0.3687
Loss at step 14690: 0.5217
Loss at step 14700: 0.2963
Loss at step 14710: 0.4176
Loss at step 14720: 0.3170
Loss at step 14730: 0.1889
Loss at step 14740: 0.2303
Loss at step 14750: 0.1581
Loss at step 14760: 0.3207
Loss at step 14770: 0.1335
Loss at step 14780: 0.3628
Loss at step 14790: 0.1225
Loss at step 14800: 0.1793
Loss at step 14810: 0.0986
Loss at step 14820: 0.0808
Loss at step 14830: 0.0799
Loss at step 14840: 0.2849
Loss at step 14850: 0.0419
Loss at step 14860: 0.2780
Loss at step 14870: 0.4900
Loss at step 14880: 0.3356
Loss at step 14890: 0.1477
Loss at step 14900: 0.2850
Loss at step 14910: 0.1268
Loss at step 14920: 0.4590
Loss at step 14930: 0.0537
Loss at step 14940: 0.0776
Loss at step 14950: 0.1242
Loss at step 14960: 0.0725
Loss at step 14970: 0.3906
Loss at step 14980: 0.2289
Loss at step 14990: 0.6427
Loss at step 15000: 0.2878
Loss at step 15010: 0.3970
Loss at step 15020: 0.3578
Loss at step 15030: 0.1666
Loss at step 15040: 0.3435
Loss at step 15050: 0.4922
Loss at step 15060: 0.7440
Loss at step 15070: 0.4703
Loss at step 15080: 0.4117
Loss at step 15090: 0.2588
Loss at step 15100: 0.2333
Loss at step 15110: 0.4567
Loss at step 15120: 0.2442
Loss at step 15130: 0.3959
Loss at step 15140: 0.4252
Loss at step 15150: 0.2163
Loss at step 15160: 0.2497
Loss at step 15170: 0.2588
Loss at step 15180: 0.3797
Loss at step 15190: 0.5987
Loss at step 15200: 0.1901
Loss at step 15210: 0.3911
Loss at step 15220: 0.2833
Loss at step 15230: 0.0180
Loss at step 15240: 0.1970
Loss at step 15250: 0.2803
Loss at step 15260: 0.5880
Loss at step 15270: 0.1701
Loss at step 15280: 0.3277
Loss at step 15290: 0.2373
Loss at step 15300: 0.3072
Loss at step 15310: 0.3496
Loss at step 15320: 0.5669
Loss at step 15330: 0.0856
Loss at step 15340: 0.0905
Loss at step 15350: 0.3469
Loss at step 15360: 0.7170
Loss at step 15370: 0.5446
Loss at step 15380: 0.3206
Loss at step 15390: 0.0546
Loss at step 15400: 0.0391
Loss at step 15410: 0.8039
Loss at step 15420: 0.4014
Loss at step 15430: 0.1374
Loss at step 15440: 0.4819
Loss at step 15450: 0.5439
Loss at step 15460: 0.2597
Loss at step 15470: 0.2417
Loss at step 15480: 0.1410
Loss at step 15490: 0.8148
Loss at step 15500: 0.4502
Loss at step 15510: 0.3245
Loss at step 15520: 0.1445
Loss at step 15530: 0.1031
Loss at step 15540: 0.2372
Loss at step 15550: 0.4578
Loss at step 15560: 0.1849
Loss at step 15570: 0.1678
Loss at step 15580: 0.0487
Loss at step 15590: 0.6226
Loss at step 15600: 0.5294
Loss at step 15610: 0.4828
Loss at step 15620: 0.0867
Loss at step 15630: 0.2758
Loss at step 15640: 0.1588
Loss at step 15650: 0.3626
Loss at step 15660: 0.0899
Loss at step 15670: 0.7766
Loss at step 15680: 0.8061
Loss at step 15690: 0.2946
Loss at step 15700: 0.1407
Loss at step 15710: 0.1798
Loss at step 15720: 0.4760
Loss at step 15730: 0.3022
Loss at step 15740: 0.0508
Loss at step 15750: 0.1659
Loss at step 15760: 0.1351
Loss at step 15770: 0.0614
Loss at step 15780: 0.0563
Loss at step 15790: 0.3356
Loss at step 15800: 0.3358
Loss at step 15810: 0.4194
Loss at step 15820: 0.0077
Loss at step 15830: 0.0316
Loss at step 15840: 0.4647
Loss at step 15850: 0.2395
Loss at step 15860: 0.5071
Loss at step 15870: 0.3204
Loss at step 15880: 0.5105
Loss at step 15890: 0.5146
Loss at step 15900: 0.1359
Loss at step 15910: 0.1715
Loss at step 15920: 0.7277
Loss at step 15930: 0.2709
Loss at step 15940: 0.5855
Loss at step 15950: 0.4102
Loss at step 15960: 0.1137
Loss at step 15970: 0.6348
Loss at step 15980: 0.6218
Loss at step 15990: 0.2769
Loss at step 16000: 0.2519
Loss at step 16010: 0.2197
Loss at step 16020: 0.1311
Loss at step 16030: 0.5695
Loss at step 16040: 0.1877
Loss at step 16050: 0.3159
Loss at step 16060: 0.1384
Loss at step 16070: 0.1757
Loss at step 16080: 0.1679
Loss at step 16090: 0.5851
Loss at step 16100: 0.1002
Loss at step 16110: 0.1673
Loss at step 16120: 0.6283
Loss at step 16130: 0.1097
Loss at step 16140: 0.1626
Loss at step 16150: 0.5196
Loss at step 16160: 1.0019
Loss at step 16170: 0.2240
Loss at step 16180: 0.1581
Loss at step 16190: 0.0967
Loss at step 16200: 0.1159
Loss at step 16210: 0.1403
Loss at step 16220: 0.1460
Loss at step 16230: 0.2186
Loss at step 16240: 0.1384
Loss at step 16250: 0.2432
Loss at step 16260: 0.2935
Loss at step 16270: 0.2873
Loss at step 16280: 0.2358
Loss at step 16290: 0.3667
Loss at step 16300: 0.3164
Loss at step 16310: 0.7317
Loss at step 16320: 0.1227
Loss at step 16330: 0.4301
Loss at step 16340: 0.2676
Loss at step 16350: 0.3857
Loss at step 16360: 0.2695
Loss at step 16370: 0.5396
Loss at step 16380: 0.0837
Loss at step 16390: 0.1847
Loss at step 16400: 0.7122
Loss at step 16410: 0.3546
Loss at step 16420: 0.2669
Loss at step 16430: 0.3295
Loss at step 16440: 0.4335
Loss at step 16450: 0.1840
Loss at step 16460: 0.0950
Loss at step 16470: 0.3184
Loss at step 16480: 0.5943
Loss at step 16490: 0.3458
Loss at step 16500: 0.4634
Loss at step 16510: 0.0810
Loss at step 16520: 0.2721
Loss at step 16530: 0.4509
Loss at step 16540: 0.3221
Loss at step 16550: 0.2689
Loss at step 16560: 0.1639
Loss at step 16570: 0.3546
Loss at step 16580: 0.1663
Loss at step 16590: 0.4124
Loss at step 16600: 0.3433
Loss at step 16610: 0.4868
Loss at step 16620: 0.4730
Loss at step 16630: 0.6586
Loss at step 16640: 0.1944
Loss at step 16650: 0.1262
Loss at step 16660: 0.5131
Loss at step 16670: 0.3549
Loss at step 16680: 0.4108
Loss at step 16690: 0.3453
Loss at step 16700: 0.2802
Loss at step 16710: 0.7271
Loss at step 16720: 0.2358
Loss at step 16730: 0.1432
Loss at step 16740: 0.8498
Loss at step 16750: 0.2118
Loss at step 16760: 0.3495
Loss at step 16770: 0.2486
Loss at step 16780: 0.2070
Loss at step 16790: 0.0728
Loss at step 16800: 0.2241
Loss at step 16810: 0.1663
Loss at step 16820: 0.3876
Loss at step 16830: 0.5707
Loss at step 16840: 0.2270
Loss at step 16850: 0.1731
Loss at step 16860: 0.0244
Loss at step 16870: 0.1719
Loss at step 16880: 0.1028
Loss at step 16890: 0.0438
Loss at step 16900: 0.3399
Loss at step 16910: 0.2818
Loss at step 16920: 0.4453
Loss at step 16930: 0.3580
Loss at step 16940: 0.3352
Loss at step 16950: 0.5755
Loss at step 16960: 0.0882
Loss at step 16970: 0.1604
Loss at step 16980: 0.2608
Loss at step 16990: 0.4517
Loss at step 17000: 0.2013
Loss at step 17010: 0.1116
Loss at step 17020: 0.2215
Loss at step 17030: 0.7545
Loss at step 17040: 0.0664
Loss at step 17050: 0.2101
Loss at step 17060: 0.3351
Loss at step 17070: 0.0976
Loss at step 17080: 0.2902
Loss at step 17090: 0.5614
Loss at step 17100: 0.2542
Loss at step 17110: 0.2915
Loss at step 17120: 0.2702
Loss at step 17130: 0.2149
Loss at step 17140: 0.1311
Loss at step 17150: 0.4621
Loss at step 17160: 0.1688
Loss at step 17170: 0.3766
Loss at step 17180: 0.1196
Loss at step 17190: 0.5548
Loss at step 17200: 0.0782
Loss at step 17210: 0.3922
Loss at step 17220: 0.0424
Loss at step 17230: 0.3799
Loss at step 17240: 0.0985
Loss at step 17250: 0.0842
Loss at step 17260: 0.1132
Loss at step 17270: 0.1724
Loss at step 17280: 0.2726
Loss at step 17290: 0.3191
Loss at step 17300: 0.2534
Loss at step 17310: 0.1149
Loss at step 17320: 0.1034
Loss at step 17330: 0.2368
Loss at step 17340: 0.6735
Loss at step 17350: 0.1169
Loss at step 17360: 0.3199
Loss at step 17370: 0.2777
Loss at step 17380: 0.1658
Loss at step 17390: 0.9396
Loss at step 17400: 0.1859
Loss at step 17410: 0.0411
Loss at step 17420: 0.0316
Loss at step 17430: 0.2914
Loss at step 17440: 0.7774
Loss at step 17450: 0.7667
Loss at step 17460: 0.3004
Loss at step 17470: 0.2624
Loss at step 17480: 0.3349
Loss at step 17490: 0.2508
Loss at step 17500: 0.2334
Loss at step 17510: 0.1131
Loss at step 17520: 0.6233
Loss at step 17530: 0.0843
Loss at step 17540: 0.1792
Loss at step 17550: 0.2066
Loss at step 17560: 0.5827
Loss at step 17570: 0.1377
Loss at step 17580: 0.2792
Loss at step 17590: 0.3140
Loss at step 17600: 0.1681
Loss at step 17610: 0.2786
Loss at step 17620: 0.4019
Loss at step 17630: 0.3828
Loss at step 17640: 0.1934
Loss at step 17650: 0.3622
Loss at step 17660: 0.3963
Loss at step 17670: 0.0201
Loss at step 17680: 0.0451
Loss at step 17690: 0.1639
Loss at step 17700: 0.0452
Loss at step 17710: 0.4765
Loss at step 17720: 0.2367
Loss at step 17730: 0.1599
Loss at step 17740: 0.1014
Loss at step 17750: 0.2151
Loss at step 17760: 0.2062
Loss at step 17770: 0.3977
Loss at step 17780: 0.6775
Loss at step 17790: 0.2373
Loss at step 17800: 0.7310
Loss at step 17810: 0.3438
Loss at step 17820: 0.3907
Loss at step 17830: 0.2834
Loss at step 17840: 0.1636
Loss at step 17850: 0.1639
Loss at step 17860: 0.0462
Loss at step 17870: 0.2160
Loss at step 17880: 0.4158
Loss at step 17890: 0.2241
Loss at step 17900: 0.1247
Loss at step 17910: 0.2433
Loss at step 17920: 0.3834
Loss at step 17930: 0.6287
Loss at step 17940: 0.1829
Loss at step 17950: 0.4722
Loss at step 17960: 0.6288
Loss at step 17970: 0.2802
Loss at step 17980: 0.4279
Loss at step 17990: 0.3107
Loss at step 18000: 0.3899
Loss at step 18010: 0.4584
Loss at step 18020: 0.1997
Loss at step 18030: 0.6494
Loss at step 18040: 0.0787
Loss at step 18050: 0.1835
Loss at step 18060: 0.1520
Loss at step 18070: 0.1819
Loss at step 18080: 0.3577
Loss at step 18090: 0.2122
Loss at step 18100: 0.2260
Loss at step 18110: 0.1712
Loss at step 18120: 0.4877
Loss at step 18130: 0.0413
Loss at step 18140: 0.1247
Loss at step 18150: 0.3262
Loss at step 18160: 0.2412
Loss at step 18170: 0.4240
Loss at step 18180: 0.2082
Loss at step 18190: 0.5703
Loss at step 18200: 0.3864
Loss at step 18210: 0.2160
Loss at step 18220: 0.0945
Loss at step 18230: 0.3311
Loss at step 18240: 0.1012
Loss at step 18250: 0.1615
Loss at step 18260: 0.3822
Loss at step 18270: 0.4107
Loss at step 18280: 0.2884
Loss at step 18290: 0.2847
Loss at step 18300: 0.3337
Loss at step 18310: 0.4738
Loss at step 18320: 0.3851
Loss at step 18330: 0.6816
Loss at step 18340: 0.0162
Loss at step 18350: 0.2037
Loss at step 18360: 0.4256
Loss at step 18370: 0.4395
Loss at step 18380: 0.0979
Loss at step 18390: 0.2038
Loss at step 18400: 0.0393
Loss at step 18410: 0.1938
Loss at step 18420: 0.1414
Loss at step 18430: 0.1427
Loss at step 18440: 0.0989
Loss at step 18450: 0.2240
Loss at step 18460: 0.1691
Loss at step 18470: 0.1447
Loss at step 18480: 0.2414
Loss at step 18490: 0.7409
Loss at step 18500: 0.4667
Loss at step 18510: 0.1069
Loss at step 18520: 0.3548
Loss at step 18530: 0.4422
Loss at step 18540: 0.4645
Loss at step 18550: 0.0431
Loss at step 18560: 0.5574
Loss at step 18570: 0.0541
Loss at step 18580: 0.4670
Loss at step 18590: 0.2140
Loss at step 18600: 0.3906
Loss at step 18610: 0.3507
Loss at step 18620: 0.2895
Loss at step 18630: 0.3181
Loss at step 18640: 0.3289
Loss at step 18650: 0.1680
Loss at step 18660: 0.0561
Loss at step 18670: 0.1204
Loss at step 18680: 0.2998
Loss at step 18690: 0.0632
Loss at step 18700: 0.5613
Loss at step 18710: 0.2470
Loss at step 18720: 0.3699
Loss at step 18730: 0.4688
Loss at step 18740: 0.2902
Loss at step 18750: 0.2325
Loss at step 18760: 0.4168
Loss at step 18770: 0.1240
Loss at step 18780: 0.1195
Loss at step 18790: 0.1053
Loss at step 18800: 0.0789
Loss at step 18810: 0.3391
Loss at step 18820: 0.7674
Loss at step 18830: 0.3225
Loss at step 18840: 0.4828
Loss at step 18850: 0.1976
Loss at step 18860: 0.0256
Loss at step 18870: 0.1025
Loss at step 18880: 0.1423
Loss at step 18890: 0.0902
Loss at step 18900: 0.3069
Loss at step 18910: 0.1912
Loss at step 18920: 0.0337
Loss at step 18930: 0.3780
Loss at step 18940: 0.2557
Loss at step 18950: 0.1076
Loss at step 18960: 0.1680
Loss at step 18970: 0.0467
Loss at step 18980: 0.2429
Loss at step 18990: 0.4006
Loss at step 19000: 0.2575
Loss at step 19010: 0.2512
Loss at step 19020: 0.2971
Loss at step 19030: 0.1097
Loss at step 19040: 0.0666
Loss at step 19050: 0.1557
Loss at step 19060: 0.0598
Loss at step 19070: 0.4027
Loss at step 19080: 0.2430
Loss at step 19090: 0.1422
Loss at step 19100: 0.4301
Loss at step 19110: 0.4374
Loss at step 19120: 0.3455
Loss at step 19130: 0.1303
Loss at step 19140: 0.2534
Loss at step 19150: 0.1015
Loss at step 19160: 0.4295
Loss at step 19170: 0.3305
Loss at step 19180: 0.2629
Loss at step 19190: 0.7636
Loss at step 19200: 0.3398
Loss at step 19210: 0.4036
Loss at step 19220: 0.0849
Loss at step 19230: 0.0617
Loss at step 19240: 0.0218
Loss at step 19250: 0.1143
Loss at step 19260: 0.3026
Loss at step 19270: 0.1447
Loss at step 19280: 0.0404
Loss at step 19290: 0.3705
Loss at step 19300: 0.4989
Loss at step 19310: 0.2699
Loss at step 19320: 0.1707
Loss at step 19330: 0.1949
Loss at step 19340: 0.1393
Loss at step 19350: 0.1514
Loss at step 19360: 0.2860
Loss at step 19370: 0.2111
Loss at step 19380: 0.1047
Loss at step 19390: 0.1388
Loss at step 19400: 0.0840
Loss at step 19410: 0.3677
Loss at step 19420: 0.1832
Loss at step 19430: 0.1978
Loss at step 19440: 0.3957
Loss at step 19450: 0.0631
Loss at step 19460: 0.2129
Loss at step 19470: 0.1345
Loss at step 19480: 0.1641
Loss at step 19490: 0.0877
Loss at step 19500: 0.0315
Loss at step 19510: 0.0550
Loss at step 19520: 0.2362
Loss at step 19530: 0.4561
Loss at step 19540: 0.1602
Loss at step 19550: 0.3034
Loss at step 19560: 0.5614
Loss at step 19570: 0.1301
Loss at step 19580: 0.4182
Loss at step 19590: 0.6039
Loss at step 19600: 0.1242
Loss at step 19610: 0.3026
Loss at step 19620: 0.4421
Loss at step 19630: 0.0818
Loss at step 19640: 0.0416
Loss at step 19650: 0.2463
Loss at step 19660: 0.4639
Loss at step 19670: 0.3485
Loss at step 19680: 0.0803
Loss at step 19690: 0.2618
Loss at step 19700: 0.2349
Loss at step 19710: 0.1647
Loss at step 19720: 0.0522
Loss at step 19730: 0.6771
Loss at step 19740: 0.0222
Loss at step 19750: 0.2792
Loss at step 19760: 0.3736
Loss at step 19770: 0.0360
Loss at step 19780: 0.1141
Loss at step 19790: 0.4306
Loss at step 19800: 0.0147
Loss at step 19810: 0.0412
Loss at step 19820: 0.4907
Loss at step 19830: 0.0978
Loss at step 19840: 0.2549
Loss at step 19850: 0.3602
Loss at step 19860: 0.4549
Loss at step 19870: 0.4839
Loss at step 19880: 0.0865
Loss at step 19890: 0.2278
Loss at step 19900: 0.1173
Loss at step 19910: 0.0911
Loss at step 19920: 0.2613
Loss at step 19930: 0.1860
Loss at step 19940: 0.1347
Loss at step 19950: 0.0040
Loss at step 19960: 0.5012
Loss at step 19970: 0.1036
Loss at step 19980: 0.4184
Loss at step 19990: 0.4010
Loss at step 20000: 0.0396
Loss at step 20010: 0.1521
Loss at step 20020: 0.1684
Loss at step 20030: 0.0598
Loss at step 20040: 0.2308
Loss at step 20050: 0.2116
Loss at step 20060: 0.1995
Loss at step 20070: 0.4268
Loss at step 20080: 0.1394
Loss at step 20090: 0.1428
Loss at step 20100: 0.3143
Loss at step 20110: 0.2753
Loss at step 20120: 0.0061
Loss at step 20130: 0.5265
Loss at step 20140: 0.3685
Loss at step 20150: 0.6171
Loss at step 20160: 0.1786
Loss at step 20170: 0.6921
Loss at step 20180: 0.3546
Loss at step 20190: 0.0845
Loss at step 20200: 0.0402
Loss at step 20210: 0.2536
Loss at step 20220: 0.5217
Loss at step 20230: 0.4089
Loss at step 20240: 0.1204
Loss at step 20250: 0.0546
Loss at step 20260: 0.1712
Loss at step 20270: 0.3744
Loss at step 20280: 0.5864
Loss at step 20290: 0.3250
Loss at step 20300: 0.2370
Loss at step 20310: 0.6591
Loss at step 20320: 0.0991
Loss at step 20330: 0.4764
Loss at step 20340: 0.3958
Loss at step 20350: 0.2288
Loss at step 20360: 0.4464
Loss at step 20370: 0.3686
Loss at step 20380: 0.1321
Loss at step 20390: 0.2240
Loss at step 20400: 0.0579
Loss at step 20410: 0.2883
Loss at step 20420: 0.2169
Loss at step 20430: 0.3346
Loss at step 20440: 0.3120
Loss at step 20450: 0.6741
Loss at step 20460: 0.1065
Loss at step 20470: 0.4321
Loss at step 20480: 0.3899
Loss at step 20490: 0.0712
Loss at step 20500: 0.3639
Loss at step 20510: 0.3600
Loss at step 20520: 0.2806
Loss at step 20530: 0.2474
Loss at step 20540: 0.3415
Loss at step 20550: 0.1816
Loss at step 20560: 0.1369
Loss at step 20570: 0.1942
Loss at step 20580: 0.1237
Loss at step 20590: 0.2807
Loss at step 20600: 0.2784
Loss at step 20610: 0.3430
Loss at step 20620: 0.2807
Loss at step 20630: 0.1674
Loss at step 20640: 0.9005
Loss at step 20650: 0.5388
Loss at step 20660: 0.5132
Loss at step 20670: 0.1297
Loss at step 20680: 0.2650
Loss at step 20690: 0.0623
Loss at step 20700: 0.1796
Loss at step 20710: 0.4969
Loss at step 20720: 0.0206
Loss at step 20730: 0.3136
Loss at step 20740: 0.3958
Loss at step 20750: 0.2793
Loss at step 20760: 0.1361
Loss at step 20770: 0.0785
Loss at step 20780: 0.0501
Loss at step 20790: 0.2236
Loss at step 20800: 0.1589
Loss at step 20810: 0.9587
Loss at step 20820: 0.1558
Loss at step 20830: 0.0400
Loss at step 20840: 0.3365
Loss at step 20850: 0.4689
Loss at step 20860: 0.2653
Loss at step 20870: 0.3503
Loss at step 20880: 0.4152
Loss at step 20890: 0.1771
Loss at step 20900: 0.4054
Loss at step 20910: 0.4332
Loss at step 20920: 0.3653
Loss at step 20930: 0.2722
Loss at step 20940: 0.1010
Loss at step 20950: 0.4158
Loss at step 20960: 0.0706
Loss at step 20970: 0.0809
Loss at step 20980: 0.2440
Loss at step 20990: 0.1855
Loss at step 21000: 0.1423
Loss at step 21010: 0.1266
Loss at step 21020: 0.0587
Loss at step 21030: 0.5176
Loss at step 21040: 0.2472
Loss at step 21050: 0.1208
Loss at step 21060: 0.1235
Loss at step 21070: 0.0299
Loss at step 21080: 0.2288
Loss at step 21090: 0.1972
Loss at step 21100: 0.4734
Loss at step 21110: 0.3910
Loss at step 21120: 0.4941
Loss at step 21130: 0.2803
Loss at step 21140: 0.0348
Loss at step 21150: 0.2963
Loss at step 21160: 0.2990
Loss at step 21170: 0.1638
Loss at step 21180: 0.1591
Loss at step 21190: 0.2180
Loss at step 21200: 0.2599
Loss at step 21210: 0.4035
Loss at step 21220: 0.2548
Loss at step 21230: 0.3350
Loss at step 21240: 0.0109
Loss at step 21250: 0.0950
Loss at step 21260: 0.2932
Loss at step 21270: 0.2710
Loss at step 21280: 0.4994
Loss at step 21290: 0.1358
Loss at step 21300: 0.1769
Loss at step 21310: 0.2657
Loss at step 21320: 0.4995
Loss at step 21330: 0.0899
Loss at step 21340: 0.1567
Loss at step 21350: 0.0255
Loss at step 21360: 0.3890
Loss at step 21370: 0.0554
Loss at step 21380: 0.3982
Loss at step 21390: 0.0527
Loss at step 21400: 0.3654
Loss at step 21410: 0.0845
Loss at step 21420: 0.4043
Loss at step 21430: 0.4180
Loss at step 21440: 0.2579
Loss at step 21450: 0.2496
Loss at step 21460: 0.1702
Loss at step 21470: 0.4043
Loss at step 21480: 0.0920
Loss at step 21490: 0.0325
Loss at step 21500: 0.1299
Loss at step 21510: 0.3586
Loss at step 21520: 0.1005
Loss at step 21530: 0.5532
Loss at step 21540: 0.2833
Loss at step 21550: 0.2131
Loss at step 21560: 0.4928
Loss at step 21570: 0.3603
Loss at step 21580: 0.3098
Loss at step 21590: 0.1301
Loss at step 21600: 0.2120
Loss at step 21610: 0.0330
Loss at step 21620: 0.2263
Loss at step 21630: 0.2924
Loss at step 21640: 0.4173
Loss at step 21650: 0.4028
Loss at step 21660: 0.5748
Loss at step 21670: 0.2207
Loss at step 21680: 0.0313
Loss at step 21690: 0.2035
Loss at step 21700: 0.1600
Loss at step 21710: 0.2318
Loss at step 21720: 0.4502
Loss at step 21730: 0.2816
Loss at step 21740: 0.2480
Loss at step 21750: 0.1644
Loss at step 21760: 0.3283
Loss at step 21770: 0.4137
Loss at step 21780: 0.1656
Loss at step 21790: 0.0176
Loss at step 21800: 0.0853
Loss at step 21810: 0.1606
Loss at step 21820: 0.0732
Loss at step 21830: 0.0435
Loss at step 21840: 0.1557
Loss at step 21850: 0.4273
Loss at step 21860: 0.1545
Loss at step 21870: 0.1763
Loss at step 21880: 0.1868
Loss at step 21890: 0.0153
Loss at step 21900: 0.1034
Loss at step 21910: 0.4186
Loss at step 21920: 0.4469
Loss at step 21930: 0.2313
Loss at step 21940: 0.2266
Loss at step 21950: 0.1501
Loss at step 21960: 0.1850
Loss at step 21970: 0.8122
Loss at step 21980: 0.1982
Loss at step 21990: 0.0734
Loss at step 22000: 0.6641
Loss at step 22010: 0.4842
Loss at step 22020: 0.0247
Loss at step 22030: 0.1824
Loss at step 22040: 0.0550
Loss at step 22050: 0.1474
Loss at step 22060: 0.1817
Loss at step 22070: 0.2011
Loss at step 22080: 0.0596
Loss at step 22090: 0.3025
Loss at step 22100: 0.3410
Loss at step 22110: 0.1798
Loss at step 22120: 0.2010
Loss at step 22130: 0.1501
Loss at step 22140: 0.6121
Loss at step 22150: 0.3883
Loss at step 22160: 0.5430
Loss at step 22170: 0.5022
Loss at step 22180: 0.0696
Loss at step 22190: 0.0316
Loss at step 22200: 0.0688
Loss at step 22210: 0.2783
Loss at step 22220: 0.1920
Loss at step 22230: 0.4862
Loss at step 22240: 0.0621
Loss at step 22250: 0.2379
Loss at step 22260: 0.2137
Loss at step 22270: 0.2536
Loss at step 22280: 0.1221
Loss at step 22290: 0.4654
Loss at step 22300: 0.1494
Loss at step 22310: 0.3120
Loss at step 22320: 0.2211
Loss at step 22330: 0.2395
Loss at step 22340: 0.2617
Loss at step 22350: 0.6006
Loss at step 22360: 0.0555
Loss at step 22370: 0.0153
Loss at step 22380: 0.2117
Loss at step 22390: 0.3337
Loss at step 22400: 0.1052
Loss at step 22410: 0.6238
Loss at step 22420: 0.0990
Loss at step 22430: 0.4551
Loss at step 22440: 0.0894
Loss at step 22450: 0.0171
Loss at step 22460: 0.0943
Loss at step 22470: 0.0787
Loss at step 22480: 0.3254
Loss at step 22490: 0.1561
Loss at step 22500: 0.0867
Loss at step 22510: 0.5562
Loss at step 22520: 0.9124
Loss at step 22530: 0.0256
Loss at step 22540: 0.0308
Loss at step 22550: 0.6074
Loss at step 22560: 0.5082
Loss at step 22570: 0.1133
Loss at step 22580: 0.5353
Loss at step 22590: 0.2067
Loss at step 22600: 0.0252
Loss at step 22610: 0.2998
Loss at step 22620: 0.3441
Loss at step 22630: 0.4911
Loss at step 22640: 0.1435
Loss at step 22650: 0.4142
Loss at step 22660: 0.1576
Loss at step 22670: 0.5704
Loss at step 22680: 0.2016
Loss at step 22690: 0.1869
Loss at step 22700: 0.6019
Loss at step 22710: 0.0273
Loss at step 22720: 0.0935
Loss at step 22730: 0.4287
Loss at step 22740: 0.7601
Loss at step 22750: 0.0558
Loss at step 22760: 0.4327
Loss at step 22770: 0.1847
Loss at step 22780: 0.0655
Loss at step 22790: 0.0174
Loss at step 22800: 0.0706
Loss at step 22810: 0.1977
Loss at step 22820: 0.0802
Loss at step 22830: 0.2679
Loss at step 22840: 0.2480
Loss at step 22850: 0.3501
Loss at step 22860: 0.1168
Loss at step 22870: 0.0516
Loss at step 22880: 0.2682
Loss at step 22890: 0.0795
Loss at step 22900: 0.2458
Loss at step 22910: 0.2432
Loss at step 22920: 0.3372
Loss at step 22930: 0.5315
Loss at step 22940: 0.0580
Loss at step 22950: 0.4774
Loss at step 22960: 0.0426
Loss at step 22970: 0.2502
Loss at step 22980: 0.2051
Loss at step 22990: 0.3048
Loss at step 23000: 0.0988
Loss at step 23010: 0.2997
Loss at step 23020: 0.1585
Loss at step 23030: 0.3622
Loss at step 23040: 0.1456
Loss at step 23050: 0.1702
Loss at step 23060: 0.1626
Loss at step 23070: 0.0388
Loss at step 23080: 0.2814
Loss at step 23090: 0.3117
Loss at step 23100: 0.4768
Loss at step 23110: 0.3818
Loss at step 23120: 0.3869
Loss at step 23130: 0.3780
Loss at step 23140: 0.0393
Loss at step 23150: 0.0629
Loss at step 23160: 0.1207
Loss at step 23170: 0.2867
Loss at step 23180: 0.1324
Loss at step 23190: 0.0597
Loss at step 23200: 0.3971
Loss at step 23210: 0.1639
Loss at step 23220: 0.2878
Loss at step 23230: 0.2749
Loss at step 23240: 0.0875
Loss at step 23250: 0.0733
Loss at step 23260: 0.4328
Loss at step 23270: 0.4443
Loss at step 23280: 0.5449
Loss at step 23290: 0.4510
Loss at step 23300: 0.0375
Loss at step 23310: 0.3910
Loss at step 23320: 0.4322
Loss at step 23330: 0.3121
Loss at step 23340: 0.0990
Loss at step 23350: 0.5270
Loss at step 23360: 0.0351
Loss at step 23370: 0.0789
Loss at step 23380: 0.0874
Loss at step 23390: 0.2405
Loss at step 23400: 0.2863
Loss at step 23410: 0.3564
Loss at step 23420: 0.3138
Loss at step 23430: 0.1703
Loss at step 23440: 0.1071
Loss at step 23450: 0.2421
Loss at step 23460: 0.1907
Loss at step 23470: 0.1920
Loss at step 23480: 0.1879
Loss at step 23490: 0.0883
Loss at step 23500: 0.1009
Loss at step 23510: 0.4062
Loss at step 23520: 0.5326
Loss at step 23530: 0.1883
Loss at step 23540: 0.1984
Loss at step 23550: 0.0326
Loss at step 23560: 0.4785
Loss at step 23570: 0.4360
Loss at step 23580: 0.0394
Loss at step 23590: 0.0639
Loss at step 23600: 0.2521
Loss at step 23610: 0.4939
Loss at step 23620: 0.1631
Loss at step 23630: 0.4085
Loss at step 23640: 0.5914
Loss at step 23650: 0.2982
Loss at step 23660: 0.0514
Loss at step 23670: 0.8956
Loss at step 23680: 0.4521
Loss at step 23690: 0.0600
Loss at step 23700: 0.0442
Loss at step 23710: 0.1735
Loss at step 23720: 0.3979
Loss at step 23730: 0.4232
Loss at step 23740: 0.0544
Loss at step 23750: 0.1210
Loss at step 23760: 0.4042
Loss at step 23770: 0.1199
Loss at step 23780: 0.0816
Loss at step 23790: 0.3805
Loss at step 23800: 0.0255
Loss at step 23810: 0.0388
Loss at step 23820: 0.3940
Loss at step 23830: 0.0741
Loss at step 23840: 0.1406
Loss at step 23850: 0.0205
Loss at step 23860: 0.1224
Loss at step 23870: 0.0050
Loss at step 23880: 0.3395
Loss at step 23890: 0.3007
Loss at step 23900: 0.0433
Loss at step 23910: 0.0893
Loss at step 23920: 0.4641
Loss at step 23930: 0.0417
Loss at step 23940: 0.1033
Loss at step 23950: 0.3555
Loss at step 23960: 0.0334
Loss at step 23970: 0.4737
Loss at step 23980: 0.4590
Loss at step 23990: 0.2265
Loss at step 24000: 0.3362
Loss at step 24010: 0.6017
Loss at step 24020: 0.3314
Loss at step 24030: 0.5169
Loss at step 24040: 0.5702
Loss at step 24050: 0.0221
Loss at step 24060: 0.1780
Loss at step 24070: 0.6954
Loss at step 24080: 0.3689
Loss at step 24090: 0.2708
Loss at step 24100: 0.0690
Loss at step 24110: 0.1233
Loss at step 24120: 0.5328
Loss at step 24130: 0.6757
Loss at step 24140: 0.0133
Loss at step 24150: 0.1929
Loss at step 24160: 0.0224
Loss at step 24170: 0.0647
Loss at step 24180: 0.2065
Loss at step 24190: 0.0451
Loss at step 24200: 0.2641
Loss at step 24210: 0.2934
Loss at step 24220: 0.2182
Loss at step 24230: 0.7672
Loss at step 24240: 0.0836
Loss at step 24250: 0.4180
Loss at step 24260: 0.0993
Loss at step 24270: 0.0656
Loss at step 24280: 0.0675
Loss at step 24290: 0.4113
Loss at step 24300: 0.0871
Loss at step 24310: 0.3222
Loss at step 24320: 0.6761
Loss at step 24330: 0.1236
Loss at step 24340: 0.2248
Loss at step 24350: 0.2263
Loss at step 24360: 0.0783
Loss at step 24370: 0.2781
Loss at step 24380: 0.2532
Loss at step 24390: 0.3875
Loss at step 24400: 0.1387
Loss at step 24410: 0.0222
Loss at step 24420: 0.0675
Loss at step 24430: 0.1948
Loss at step 24440: 0.0278
Loss at step 24450: 0.2265
Loss at step 24460: 0.3851
Loss at step 24470: 0.1497
Loss at step 24480: 0.4212
Loss at step 24490: 0.3385
Loss at step 24500: 0.0714
Loss at step 24510: 0.1587
Loss at step 24520: 0.0289
Loss at step 24530: 0.3918
Loss at step 24540: 0.6854
Loss at step 24550: 0.4734
Loss at step 24560: 0.1221
Loss at step 24570: 0.7808
Loss at step 24580: 0.0436
Loss at step 24590: 0.0492
Loss at step 24600: 0.1381
Loss at step 24610: 0.3146
Loss at step 24620: 0.2131
Loss at step 24630: 0.0519
Loss at step 24640: 0.0653
Loss at step 24650: 0.4937
Loss at step 24660: 0.2274
Loss at step 24670: 0.2581
Loss at step 24680: 0.5315
Loss at step 24690: 0.0291
Loss at step 24700: 0.5915
Loss at step 24710: 0.2831
Loss at step 24720: 0.2831
Loss at step 24730: 0.0639
Loss at step 24740: 0.5174
Loss at step 24750: 0.4087
Loss at step 24760: 0.2916
Loss at step 24770: 0.1744
Loss at step 24780: 0.0338
Loss at step 24790: 0.1788
Loss at step 24800: 0.1013
Loss at step 24810: 0.3068
Loss at step 24820: 0.3991
Loss at step 24830: 0.1244
Loss at step 24840: 0.4175
Loss at step 24850: 0.1103
Loss at step 24860: 0.2525
Loss at step 24870: 0.4495
Loss at step 24880: 0.4890
Loss at step 24890: 0.2253
Loss at step 24900: 0.2875
Loss at step 24910: 0.0543
Loss at step 24920: 0.1310
Loss at step 24930: 0.5678
Loss at step 24940: 0.0724
Loss at step 24950: 0.3425
Loss at step 24960: 0.2995
Loss at step 24970: 0.0770
Loss at step 24980: 0.2897
Loss at step 24990: 0.0285
Loss at step 25000: 0.2831
Loss at step 25010: 0.1099
Loss at step 25020: 0.0835
Loss at step 25030: 0.1963
Loss at step 25040: 0.3391
Loss at step 25050: 0.2934
Loss at step 25060: 0.2493
Loss at step 25070: 0.0658
Loss at step 25080: 0.0761
Loss at step 25090: 0.0451
Loss at step 25100: 0.0147
Loss at step 25110: 0.6664
Loss at step 25120: 0.5135
Loss at step 25130: 0.1140
Loss at step 25140: 0.2328
Loss at step 25150: 0.4030
Loss at step 25160: 0.3511
Loss at step 25170: 0.6014
Loss at step 25180: 0.2891
Loss at step 25190: 0.0116
Loss at step 25200: 0.0603
Loss at step 25210: 0.0647
Loss at step 25220: 0.0323
Loss at step 25230: 0.0241
Loss at step 25240: 0.0496
Loss at step 25250: 0.1362
Loss at step 25260: 0.3102
Loss at step 25270: 0.1719
Loss at step 25280: 0.0235
Loss at step 25290: 0.0619
Loss at step 25300: 0.2947
Loss at step 25310: 0.1015
Loss at step 25320: 0.0541
Loss at step 25330: 0.1919
Loss at step 25340: 0.1594
Loss at step 25350: 0.2104
Loss at step 25360: 0.4337
Loss at step 25370: 0.1724
Loss at step 25380: 0.4023
Loss at step 25390: 0.4846
Loss at step 25400: 0.0940
Loss at step 25410: 0.1002
Loss at step 25420: 0.1086
Loss at step 25430: 0.1787
Loss at step 25440: 0.3408
Loss at step 25450: 0.5238
Loss at step 25460: 0.0747
Loss at step 25470: 0.1756
Loss at step 25480: 0.0423
Loss at step 25490: 0.4239
Loss at step 25500: 0.0665
Loss at step 25510: 0.2021
Loss at step 25520: 0.2423
Loss at step 25530: 0.1512
Loss at step 25540: 0.7166
Loss at step 25550: 0.2688
Loss at step 25560: 0.1097
Loss at step 25570: 0.4006
Loss at step 25580: 0.0753
Loss at step 25590: 0.3473
Loss at step 25600: 0.3019
Loss at step 25610: 0.0108
Loss at step 25620: 0.2383
Loss at step 25630: 0.0216
Loss at step 25640: 0.1226
Loss at step 25650: 0.1974
Loss at step 25660: 0.2446
Loss at step 25670: 0.0538
Loss at step 25680: 0.2794
Loss at step 25690: 0.3344
Loss at step 25700: 0.0873
Loss at step 25710: 0.1063
Loss at step 25720: 0.5537
Loss at step 25730: 0.2314
Loss at step 25740: 0.1331
Loss at step 25750: 0.2089
Loss at step 25760: 0.2205
Loss at step 25770: 0.0756
Loss at step 25780: 0.0869
Loss at step 25790: 0.1390
Loss at step 25800: 0.0323
Loss at step 25810: 0.0470
Loss at step 25820: 0.0125
Loss at step 25830: 0.1048
Loss at step 25840: 0.0227
Loss at step 25850: 0.6123
Loss at step 25860: 0.0807
Loss at step 25870: 0.1948
Loss at step 25880: 0.2786
Loss at step 25890: 0.5540
Loss at step 25900: 0.1067
Loss at step 25910: 0.2337
Loss at step 25920: 0.0252
Loss at step 25930: 0.5190
Loss at step 25940: 0.3691
Loss at step 25950: 0.0779
Loss at step 25960: 0.1621
Loss at step 25970: 0.1534
Loss at step 25980: 0.0424
Loss at step 25990: 0.0564
Loss at step 26000: 0.2515
Loss at step 26010: 0.3552
Loss at step 26020: 0.2236
Loss at step 26030: 0.3024
Loss at step 26040: 0.2398
Loss at step 26050: 0.2885
Loss at step 26060: 0.3500
Loss at step 26070: 0.1808
Loss at step 26080: 0.0933
Loss at step 26090: 0.2058
Loss at step 26100: 0.0133
Loss at step 26110: 0.0967
Loss at step 26120: 0.2465
Loss at step 26130: 0.0524
Loss at step 26140: 0.0193
Loss at step 26150: 0.1923
Loss at step 26160: 0.3408
Loss at step 26170: 0.6165
Loss at step 26180: 0.1235
Loss at step 26190: 0.1326
Loss at step 26200: 0.5667
Loss at step 26210: 0.1075
Loss at step 26220: 0.2728
Loss at step 26230: 0.0728
Loss at step 26240: 0.2524
Loss at step 26250: 0.0342
Loss at step 26260: 0.0083
Loss at step 26270: 0.0169
Loss at step 26280: 0.0185
Loss at step 26290: 0.8894
Loss at step 26300: 0.1385
Loss at step 26310: 0.6448
Loss at step 26320: 0.2606
Loss at step 26330: 0.2684
Loss at step 26340: 0.2935
Loss at step 26350: 0.2898
Loss at step 26360: 0.0588
Loss at step 26370: 0.0942
Loss at step 26380: 0.1768
Loss at step 26390: 0.0300
Loss at step 26400: 0.1425
Loss at step 26410: 0.0512
Loss at step 26420: 0.0827
Loss at step 26430: 0.5037
Loss at step 26440: 0.5988
Loss at step 26450: 0.2327
Loss at step 26460: 0.1254
Loss at step 26470: 0.1685
Loss at step 26480: 0.5002
Loss at step 26490: 0.1985
Loss at step 26500: 0.5325
Loss at step 26510: 0.0954
Loss at step 26520: 0.4585
Loss at step 26530: 0.0868
Loss at step 26540: 0.0653
Loss at step 26550: 0.4479
Loss at step 26560: 0.1666
Loss at step 26570: 0.0094
Loss at step 26580: 0.2188
Loss at step 26590: 0.1405
Loss at step 26600: 0.1718
Loss at step 26610: 0.2124
Loss at step 26620: 0.0489
Loss at step 26630: 0.0903
Loss at step 26640: 0.2325
Loss at step 26650: 0.4736
Loss at step 26660: 0.4564
Loss at step 26670: 0.5714
Loss at step 26680: 0.2420
Loss at step 26690: 0.6200
Loss at step 26700: 0.4284
Loss at step 26710: 0.3412
Loss at step 26720: 0.2423
Loss at step 26730: 0.1779
Loss at step 26740: 0.0665
Loss at step 26750: 0.2354
Loss at step 26760: 0.3006
Loss at step 26770: 0.0570
Loss at step 26780: 0.2361
Loss at step 26790: 0.2494
Loss at step 26800: 0.1809
Loss at step 26810: 0.0933
Loss at step 26820: 0.0846
Loss at step 26830: 0.4351
Loss at step 26840: 0.1385
Loss at step 26850: 0.3903
Loss at step 26860: 0.1754
Loss at step 26870: 0.0685
Loss at step 26880: 0.1563
Loss at step 26890: 0.0598
Loss at step 26900: 0.0171
Loss at step 26910: 0.2861
Loss at step 26920: 0.3628
Loss at step 26930: 0.0587
Loss at step 26940: 0.0142
Loss at step 26950: 0.1730
Loss at step 26960: 0.3526
Loss at step 26970: 0.1582
Loss at step 26980: 0.0157
Loss at step 26990: 0.0369
Loss at step 27000: 0.2434
Loss at step 27010: 0.3021
Loss at step 27020: 0.4604
Loss at step 27030: 0.3713
Loss at step 27040: 0.2727
Loss at step 27050: 0.0721
Loss at step 27060: 0.1636
Loss at step 27070: 0.1061
Loss at step 27080: 0.0374
Loss at step 27090: 0.3087
Loss at step 27100: 0.1072
Loss at step 27110: 0.1272
Loss at step 27120: 0.3161
Loss at step 27130: 0.2256
Loss at step 27140: 0.1815
Loss at step 27150: 0.2376
Loss at step 27160: 0.6106
Loss at step 27170: 0.4694
Loss at step 27180: 0.0519
Loss at step 27190: 0.1753
Loss at step 27200: 0.1238
Loss at step 27210: 0.1549
Loss at step 27220: 0.0121
Loss at step 27230: 0.1346
Loss at step 27240: 0.2475
Loss at step 27250: 0.1293
Loss at step 27260: 0.2244
Loss at step 27270: 0.1163
Loss at step 27280: 0.0235
Loss at step 27290: 0.0511
Loss at step 27300: 0.1083
Loss at step 27310: 0.2914
Loss at step 27320: 0.2183
Loss at step 27330: 0.1967
Loss at step 27340: 0.5206
Loss at step 27350: 0.2184
Loss at step 27360: 0.1513
Loss at step 27370: 0.4575
Loss at step 27380: 0.2554
Loss at step 27390: 0.2958
Loss at step 27400: 0.2763
Loss at step 27410: 0.0115
Loss at step 27420: 0.6120
Loss at step 27430: 0.2380
Loss at step 27440: 0.0552
Loss at step 27450: 0.1507
Loss at step 27460: 0.2191
Loss at step 27470: 0.3156
Loss at step 27480: 0.0058
Loss at step 27490: 0.0278
Loss at step 27500: 0.2041
Loss at step 27510: 0.0331
Loss at step 27520: 0.4431
Loss at step 27530: 0.0548
Loss at step 27540: 0.1504
Loss at step 27550: 0.2225
Loss at step 27560: 0.0593
Loss at step 27570: 0.0836
Loss at step 27580: 0.7590
Loss at step 27590: 0.0330
Loss at step 27600: 0.1139
Loss at step 27610: 0.2796
Loss at step 27620: 0.3324
Loss at step 27630: 0.1320
Loss at step 27640: 0.0910
Loss at step 27650: 0.4387
Loss at step 27660: 0.3455
Loss at step 27670: 0.0900
Loss at step 27680: 0.5805
Loss at step 27690: 0.1904
Loss at step 27700: 0.4738
Loss at step 27710: 0.3681
Loss at step 27720: 0.0558
Loss at step 27730: 0.1817
Loss at step 27740: 0.2658
Loss at step 27750: 0.2992
Loss at step 27760: 0.4920
Loss at step 27770: 0.3550
Loss at step 27780: 0.1207
Loss at step 27790: 0.6238
Loss at step 27800: 0.3107
Loss at step 27810: 0.1966
Loss at step 27820: 0.4067
Loss at step 27830: 0.0172
Loss at step 27840: 0.2445
Loss at step 27850: 0.2955
Loss at step 27860: 0.0861
Loss at step 27870: 0.2538
Loss at step 27880: 0.6694
Loss at step 27890: 0.2783
Loss at step 27900: 0.0062
Loss at step 27910: 0.3127
Loss at step 27920: 0.3251
Loss at step 27930: 0.1481
Loss at step 27940: 0.1988
Loss at step 27950: 0.4239
Loss at step 27960: 0.3078
Loss at step 27970: 0.0521
Loss at step 27980: 0.1222
Loss at step 27990: 0.1221
Loss at step 28000: 0.0114
Loss at step 28010: 0.3656
Loss at step 28020: 0.4134
Loss at step 28030: 0.2945
Loss at step 28040: 0.2872
Loss at step 28050: 0.3015
Loss at step 28060: 0.2754
Loss at step 28070: 0.1582
Loss at step 28080: 0.1283
Loss at step 28090: 0.2771
Loss at step 28100: 0.1568
Loss at step 28110: 0.3382
Loss at step 28120: 0.4153
Loss at step 28130: 0.4153
Loss at step 28140: 0.4061
Loss at step 28150: 0.1150
Loss at step 28160: 0.1460
Loss at step 28170: 0.0268
Loss at step 28180: 0.0110
Loss at step 28190: 0.5202
Loss at step 28200: 0.0359
Loss at step 28210: 0.0189
Loss at step 28220: 0.2070
Loss at step 28230: 0.3026
Loss at step 28240: 0.2928
Loss at step 28250: 0.2962
Loss at step 28260: 0.2416
Loss at step 28270: 0.1507
Loss at step 28280: 0.1272
Loss at step 28290: 0.0150
Loss at step 28300: 0.5891
Loss at step 28310: 0.3908
Loss at step 28320: 0.3638
Loss at step 28330: 0.2557
Loss at step 28340: 0.2013
Loss at step 28350: 0.1152
Loss at step 28360: 0.0306
Loss at step 28370: 0.5366
Loss at step 28380: 0.0408
Loss at step 28390: 0.1029
Loss at step 28400: 0.3847
Loss at step 28410: 0.4076
Loss at step 28420: 0.5884
Loss at step 28430: 0.0203
Loss at step 28440: 0.2588
Loss at step 28450: 0.5006
Loss at step 28460: 0.0232
Loss at step 28470: 0.1808
Loss at step 28480: 0.0168
Loss at step 28490: 0.2373
Loss at step 28500: 0.0142
Loss at step 28510: 0.2060
Loss at step 28520: 0.5189
Loss at step 28530: 0.0724
Loss at step 28540: 0.0130
Loss at step 28550: 0.3899
Loss at step 28560: 0.4578
Loss at step 28570: 0.0942
Loss at step 28580: 0.2311
Loss at step 28590: 0.1383
Loss at step 28600: 0.0535
Loss at step 28610: 0.4409
Loss at step 28620: 0.1558
Loss at step 28630: 0.1916
Loss at step 28640: 0.1523
Loss at step 28650: 0.2320
Loss at step 28660: 0.6102
Loss at step 28670: 0.2678
Loss at step 28680: 0.2203
Loss at step 28690: 0.0564
Loss at step 28700: 0.4142
Loss at step 28710: 0.1831
Loss at step 28720: 0.0763
Loss at step 28730: 0.0199
Loss at step 28740: 0.1165
Loss at step 28750: 0.1605
Loss at step 28760: 0.2955
Loss at step 28770: 0.2592
Loss at step 28780: 0.2823
Loss at step 28790: 0.2830
Loss at step 28800: 0.0653
Loss at step 28810: 0.1809
Loss at step 28820: 0.1057
Loss at step 28830: 0.4529
Loss at step 28840: 0.3610
Loss at step 28850: 0.1467
Loss at step 28860: 0.6028
Loss at step 28870: 0.1683
Loss at step 28880: 0.0085
Loss at step 28890: 0.1087
Loss at step 28900: 0.2957
Loss at step 28910: 0.2380
Loss at step 28920: 0.3592
Loss at step 28930: 0.4862
Loss at step 28940: 0.0272
Loss at step 28950: 0.1427
Loss at step 28960: 0.4317
Loss at step 28970: 0.0348
Loss at step 28980: 0.0348
Loss at step 28990: 0.0416
Loss at step 29000: 0.1394
Loss at step 29010: 0.1091
Loss at step 29020: 0.6325
Loss at step 29030: 0.0741
Loss at step 29040: 0.4558
Loss at step 29050: 0.2657
Loss at step 29060: 0.1824
Loss at step 29070: 0.6957
Loss at step 29080: 0.0239
Loss at step 29090: 0.5236
Loss at step 29100: 0.2052
Loss at step 29110: 0.0843
Loss at step 29120: 0.2003
Loss at step 29130: 0.6273
Loss at step 29140: 0.1158
Loss at step 29150: 0.0418
Loss at step 29160: 0.1176
Loss at step 29170: 0.0863
Loss at step 29180: 0.0157
Loss at step 29190: 0.6212
Loss at step 29200: 0.0302
Loss at step 29210: 0.2231
Loss at step 29220: 0.2143
Loss at step 29230: 0.1251
Loss at step 29240: 0.1367
Loss at step 29250: 0.0902
Loss at step 29260: 0.0502
Loss at step 29270: 0.1454
Loss at step 29280: 0.4015
Loss at step 29290: 0.1256
Loss at step 29300: 0.1670
Loss at step 29310: 0.0948
Loss at step 29320: 0.0343
Loss at step 29330: 0.0358
Loss at step 29340: 0.0195
Loss at step 29350: 0.0129
Loss at step 29360: 0.3833
Loss at step 29370: 0.1809
Loss at step 29380: 0.0572
Loss at step 29390: 0.1105
Loss at step 29400: 0.0680
Loss at step 29410: 0.0174
Loss at step 29420: 0.2584
Loss at step 29430: 0.0500
Loss at step 29440: 0.2810
Loss at step 29450: 0.0177
Loss at step 29460: 0.4790
Loss at step 29470: 0.0126
Loss at step 29480: 0.2456
Loss at step 29490: 0.4920
Loss at step 29500: 0.2308
Loss at step 29510: 0.2221
Loss at step 29520: 0.3856
Loss at step 29530: 0.0141
Loss at step 29540: 0.6411
Loss at step 29550: 0.0191
Loss at step 29560: 0.5101
Loss at step 29570: 0.1220
Loss at step 29580: 0.0418
Loss at step 29590: 0.0492
Loss at step 29600: 0.2782
Loss at step 29610: 0.3333
Loss at step 29620: 0.3553
Loss at step 29630: 0.3504
Loss at step 29640: 0.0112
Loss at step 29650: 0.1239
Loss at step 29660: 0.0722
Loss at step 29670: 0.4947
Loss at step 29680: 0.1852
Loss at step 29690: 0.0359
Loss at step 29700: 0.2635
Loss at step 29710: 0.2508
Loss at step 29720: 0.1295
Loss at step 29730: 0.2184
Loss at step 29740: 0.3558
Loss at step 29750: 0.2632
Loss at step 29760: 0.2572
Loss at step 29770: 0.0312
Loss at step 29780: 0.1610
Loss at step 29790: 0.1361
Loss at step 29800: 0.2027
Loss at step 29810: 0.0340
Loss at step 29820: 0.0415
Loss at step 29830: 0.0200
Loss at step 29840: 0.4297
Loss at step 29850: 0.2081
Loss at step 29860: 0.1110
Loss at step 29870: 0.0335
Loss at step 29880: 0.3279
Loss at step 29890: 0.3191
Loss at step 29900: 0.1232
Loss at step 29910: 0.3303
Loss at step 29920: 0.2969
Loss at step 29930: 0.2726
Loss at step 29940: 0.0662
Loss at step 29950: 0.0673
Loss at step 29960: 0.2670
Loss at step 29970: 0.7074
Loss at step 29980: 0.0599
Loss at step 29990: 0.1543
Loss at step 30000: 0.0506
Loss at step 30010: 0.3134
Loss at step 30020: 0.0948
Loss at step 30030: 0.2482
Loss at step 30040: 0.0663
Loss at step 30050: 0.1859
Loss at step 30060: 0.2473
Loss at step 30070: 0.1911
Loss at step 30080: 0.2543
Loss at step 30090: 0.0604
Loss at step 30100: 0.5807
Loss at step 30110: 0.3075
Loss at step 30120: 0.2464
Loss at step 30130: 0.0698
Loss at step 30140: 0.0351
Loss at step 30150: 0.2502
Loss at step 30160: 0.1120
Loss at step 30170: 0.1813
Loss at step 30180: 0.3535
Loss at step 30190: 0.1990
Loss at step 30200: 0.0282
Loss at step 30210: 0.0843
Loss at step 30220: 0.2557
Loss at step 30230: 0.3246
Loss at step 30240: 0.3329
Loss at step 30250: 0.0653
Loss at step 30260: 0.0199
Loss at step 30270: 0.0931
Loss at step 30280: 0.0195
Loss at step 30290: 0.1028
Loss at step 30300: 0.0222
Loss at step 30310: 0.1663
Loss at step 30320: 0.1325
Loss at step 30330: 0.0828
Loss at step 30340: 0.2633
Loss at step 30350: 0.2313
Loss at step 30360: 0.2310
Loss at step 30370: 0.2151
Loss at step 30380: 0.0158
Loss at step 30390: 0.3332
Loss at step 30400: 0.3650
Loss at step 30410: 0.0227
Loss at step 30420: 0.1091
Loss at step 30430: 0.0062
Loss at step 30440: 0.6722
Loss at step 30450: 0.2787
Loss at step 30460: 0.0394
Loss at step 30470: 0.4221
Loss at step 30480: 0.2276
Loss at step 30490: 0.4098
Loss at step 30500: 0.1499
Loss at step 30510: 0.2910
Loss at step 30520: 0.3367
Loss at step 30530: 0.0400
Loss at step 30540: 0.2772
Loss at step 30550: 0.1372
Loss at step 30560: 0.3401
Loss at step 30570: 0.2729
Loss at step 30580: 0.0370
Loss at step 30590: 0.0373
Loss at step 30600: 0.0368
Loss at step 30610: 0.5354
Loss at step 30620: 0.2731
Loss at step 30630: 0.0719
Loss at step 30640: 0.1862
Loss at step 30650: 0.2526
Loss at step 30660: 0.3107
Loss at step 30670: 0.2819
Loss at step 30680: 0.4263
Loss at step 30690: 0.4947
Loss at step 30700: 0.0140
Loss at step 30710: 0.1006
Loss at step 30720: 0.3137
Loss at step 30730: 0.0107
Loss at step 30740: 0.3974
Loss at step 30750: 0.2325
Loss at step 30760: 0.4109
Loss at step 30770: 0.4316
Loss at step 30780: 0.1199
Loss at step 30790: 0.0256
Loss at step 30800: 0.0932
Loss at step 30810: 0.0336
Loss at step 30820: 0.0107
Loss at step 30830: 0.2722
Loss at step 30840: 0.2305
Loss at step 30850: 0.0355
Loss at step 30860: 0.2785
Loss at step 30870: 0.0140
Loss at step 30880: 0.2257
Loss at step 30890: 0.1075
Loss at step 30900: 0.2664
Loss at step 30910: 0.0177
Loss at step 30920: 0.1428
Loss at step 30930: 0.2055
Loss at step 30940: 0.0905
Loss at step 30950: 0.5704
Loss at step 30960: 0.0987
Loss at step 30970: 0.1016
Loss at step 30980: 0.0734
Loss at step 30990: 0.7736
Loss at step 31000: 0.0522
Loss at step 31010: 0.3015
Loss at step 31020: 0.2404
Loss at step 31030: 0.8136
Loss at step 31040: 0.2211
Loss at step 31050: 0.1279
Loss at step 31060: 0.0285
Loss at step 31070: 0.0398
Loss at step 31080: 0.4768
Loss at step 31090: 0.3277
Loss at step 31100: 0.0649
Loss at step 31110: 0.1201
Loss at step 31120: 0.1464
Loss at step 31130: 0.4071
Loss at step 31140: 0.0763
Loss at step 31150: 0.5620
Loss at step 31160: 0.0190
Loss at step 31170: 0.2192
Loss at step 31180: 0.0135
Loss at step 31190: 0.0955
Loss at step 31200: 0.1024
Loss at step 31210: 0.0245
Loss at step 31220: 0.3176
Loss at step 31230: 0.0046
Loss at step 31240: 0.4500
Loss at step 31250: 0.0669
Loss at step 31260: 0.1890
Loss at step 31270: 0.1947
Loss at step 31280: 0.1640
Loss at step 31290: 0.1315
Loss at step 31300: 0.0176
Loss at step 31310: 0.2981
Loss at step 31320: 0.0406
Loss at step 31330: 0.1016
Loss at step 31340: 0.2402
Loss at step 31350: 0.0092
Loss at step 31360: 0.5056
Loss at step 31370: 0.0128
Loss at step 31380: 0.0731
Loss at step 31390: 0.1021
Loss at step 31400: 0.1405
Loss at step 31410: 0.4023
Loss at step 31420: 0.0886
Loss at step 31430: 0.4067
Loss at step 31440: 0.3393
Loss at step 31450: 0.1146
Loss at step 31460: 1.0864
Loss at step 31470: 0.0445
Loss at step 31480: 0.0641
Loss at step 31490: 0.1366
Loss at step 31500: 0.0822
Loss at step 31510: 0.0288
Loss at step 31520: 0.3946
Loss at step 31530: 0.1354
Loss at step 31540: 0.3724
Loss at step 31550: 0.0461
Loss at step 31560: 0.2088
Loss at step 31570: 0.0575
Loss at step 31580: 0.1830
Loss at step 31590: 0.0088
Loss at step 31600: 0.1273
Loss at step 31610: 0.2392
Loss at step 31620: 0.4201
Loss at step 31630: 0.0777
Loss at step 31640: 0.0798
Loss at step 31650: 0.0344
Loss at step 31660: 0.6493
Loss at step 31670: 0.2247
Loss at step 31680: 0.4966
Loss at step 31690: 0.2868
Loss at step 31700: 0.0164
Loss at step 31710: 0.2249
Loss at step 31720: 0.9160
Loss at step 31730: 0.2058
Loss at step 31740: 0.0207
Loss at step 31750: 0.3691
Loss at step 31760: 0.2876
Loss at step 31770: 0.3506
Loss at step 31780: 0.2076
Loss at step 31790: 0.0331
Loss at step 31800: 0.0356
Loss at step 31810: 0.3164
Loss at step 31820: 0.0291
Loss at step 31830: 0.0547
Loss at step 31840: 0.4968
Loss at step 31850: 0.3958
Loss at step 31860: 0.0928
Loss at step 31870: 0.0965
Loss at step 31880: 0.1426
Loss at step 31890: 0.4079
Loss at step 31900: 0.2038
Loss at step 31910: 0.1256
Loss at step 31920: 0.5292
Loss at step 31930: 0.4253
Loss at step 31940: 0.2757
Loss at step 31950: 0.0230
Loss at step 31960: 0.0228
Loss at step 31970: 0.0035
Loss at step 31980: 0.0477
Loss at step 31990: 0.0165
Loss at step 32000: 0.3732
Loss at step 32010: 0.0702
Loss at step 32020: 0.1859
Loss at step 32030: 0.0271
Loss at step 32040: 0.0539
Loss at step 32050: 0.0472
Loss at step 32060: 0.0229
Loss at step 32070: 0.4086
Loss at step 32080: 0.4352
Loss at step 32090: 0.0923
Loss at step 32100: 0.1788
Loss at step 32110: 0.2374
Loss at step 32120: 0.4627
Loss at step 32130: 0.2095
Loss at step 32140: 0.2012
Loss at step 32150: 0.1097
Loss at step 32160: 0.0217
Loss at step 32170: 0.6854
Loss at step 32180: 0.0955
Loss at step 32190: 0.2068
Loss at step 32200: 0.2262
Loss at step 32210: 0.1935
Loss at step 32220: 0.1286
Loss at step 32230: 0.1253
Loss at step 32240: 0.0216
Loss at step 32250: 0.3290
Loss at step 32260: 0.1450
Loss at step 32270: 0.0537
Loss at step 32280: 0.0105
Loss at step 32290: 0.0104
Loss at step 32300: 0.0453
Loss at step 32310: 0.0294
Loss at step 32320: 0.0211
Loss at step 32330: 0.0096
Loss at step 32340: 0.3811
Loss at step 32350: 0.1936
Loss at step 32360: 0.5712
Loss at step 32370: 0.6674
Loss at step 32380: 0.5004
Loss at step 32390: 0.0850
Loss at step 32400: 0.2269
Loss at step 32410: 0.0034
Loss at step 32420: 0.1321
Loss at step 32430: 0.2042
Loss at step 32440: 0.3328
Loss at step 32450: 0.0087
Loss at step 32460: 0.0242
Loss at step 32470: 0.1064
Loss at step 32480: 0.1132
Loss at step 32490: 0.0122
Loss at step 32500: 0.5617
Loss at step 32510: 0.1541
Loss at step 32520: 0.1742
Loss at step 32530: 0.0176
Loss at step 32540: 0.0978
Loss at step 32550: 0.5386
Loss at step 32560: 0.0994
Loss at step 32570: 0.5277
Loss at step 32580: 0.3433
Loss at step 32590: 0.1738
Loss at step 32600: 0.3771
Loss at step 32610: 0.0446
Loss at step 32620: 0.2171
Loss at step 32630: 0.0235
Loss at step 32640: 0.0200
Loss at step 32650: 0.1403
Loss at step 32660: 0.0164
Loss at step 32670: 0.0126
Loss at step 32680: 0.0377
Loss at step 32690: 0.7325
Loss at step 32700: 0.6882
Loss at step 32710: 0.0209
Loss at step 32720: 0.0276
Loss at step 32730: 0.2452
Loss at step 32740: 0.3568
Loss at step 32750: 0.1460
Loss at step 32760: 0.0183
Loss at step 32770: 0.0171
Loss at step 32780: 0.4900
Loss at step 32790: 0.5164
Loss at step 32800: 0.0553
Loss at step 32810: 0.2207
Loss at step 32820: 0.1125
Loss at step 32830: 0.5544
Loss at step 32840: 0.1735
Loss at step 32850: 0.2663
Loss at step 32860: 0.0362
Loss at step 32870: 0.0558
Loss at step 32880: 0.0617
Loss at step 32890: 0.0348
Loss at step 32900: 0.0278
Loss at step 32910: 0.5738
Loss at step 32920: 0.0159
Loss at step 32930: 0.2475
Loss at step 32940: 0.0570
Loss at step 32950: 0.0248
Loss at step 32960: 0.5255
Loss at step 32970: 0.0952
Loss at step 32980: 0.0849
Loss at step 32990: 0.0456
Loss at step 33000: 0.1944
Loss at step 33010: 0.1889
Loss at step 33020: 0.0966
Loss at step 33030: 0.2994
Loss at step 33040: 0.4002
Loss at step 33050: 0.3221
Loss at step 33060: 0.0061
Loss at step 33070: 0.2521
Loss at step 33080: 0.3256
Loss at step 33090: 0.2410
Loss at step 33100: 0.1897
Loss at step 33110: 0.1867
Loss at step 33120: 0.2603
Loss at step 33130: 0.0807
Loss at step 33140: 0.4088
Loss at step 33150: 0.3150
Loss at step 33160: 0.1676
Loss at step 33170: 0.0997
Loss at step 33180: 0.2498
Loss at step 33190: 0.1740
Loss at step 33200: 0.1163
Loss at step 33210: 0.0515
Loss at step 33220: 0.0629
Loss at step 33230: 0.3275
Loss at step 33240: 0.0761
Loss at step 33250: 0.1165
Loss at step 33260: 0.0173
Loss at step 33270: 0.0246
Loss at step 33280: 0.3586
Loss at step 33290: 0.1024
Loss at step 33300: 0.2794
Loss at step 33310: 0.3438
Loss at step 33320: 0.0278
Loss at step 33330: 0.0447
Loss at step 33340: 0.1714
Loss at step 33350: 0.0114
Loss at step 33360: 0.1027
Loss at step 33370: 0.0294
Loss at step 33380: 0.0120
Loss at step 33390: 0.3477
Loss at step 33400: 0.0088
Loss at step 33410: 0.0223
Loss at step 33420: 0.0451
Loss at step 33430: 0.0768
Loss at step 33440: 0.1805
Loss at step 33450: 0.3543
Loss at step 33460: 0.4009
Loss at step 33470: 0.0630
Loss at step 33480: 0.3222
Loss at step 33490: 0.6714
Loss at step 33500: 0.1999
Loss at step 33510: 0.2622
Loss at step 33520: 0.5880
Loss at step 33530: 0.0356
Loss at step 33540: 0.0585
Loss at step 33550: 0.0357
Loss at step 33560: 0.1245
Loss at step 33570: 0.0344
Loss at step 33580: 0.2695
Loss at step 33590: 0.0679
Loss at step 33600: 0.2129
Loss at step 33610: 0.3144
Loss at step 33620: 0.0558
Loss at step 33630: 0.3381
Loss at step 33640: 0.3040
Loss at step 33650: 0.2377
Loss at step 33660: 0.0181
Loss at step 33670: 0.3489
Loss at step 33680: 0.2638
Loss at step 33690: 0.2839
Loss at step 33700: 0.0124
Loss at step 33710: 0.2336
Loss at step 33720: 0.7476
Loss at step 33730: 0.1485
Loss at step 33740: 0.0899
Loss at step 33750: 0.0104
Loss at step 33760: 0.4202
Loss at step 33770: 0.3629
Loss at step 33780: 0.0335
Loss at step 33790: 0.5938
Loss at step 33800: 0.5775
Loss at step 33810: 0.3411
Loss at step 33820: 0.1582
Loss at step 33830: 0.1278
Loss at step 33840: 0.0296
Loss at step 33850: 0.2466
Loss at step 33860: 0.4180
Loss at step 33870: 0.5392
Loss at step 33880: 0.0294
Loss at step 33890: 0.5075
Loss at step 33900: 0.1813
Loss at step 33910: 0.2393
Loss at step 33920: 0.4369
Loss at step 33930: 0.5130
Loss at step 33940: 0.5948
Loss at step 33950: 0.0185
Loss at step 33960: 0.0521
Loss at step 33970: 0.2965
Loss at step 33980: 0.2347
Loss at step 33990: 0.1147
Loss at step 34000: 0.1706
Loss at step 34010: 0.3878
Loss at step 34020: 0.1355
Loss at step 34030: 0.2190
Loss at step 34040: 0.4383
Loss at step 34050: 0.4270
Loss at step 34060: 0.4942
Loss at step 34070: 0.2732
Loss at step 34080: 0.0792
Loss at step 34090: 0.1501
Loss at step 34100: 0.2327
Loss at step 34110: 0.0659
Loss at step 34120: 0.3498
Loss at step 34130: 0.5969
Loss at step 34140: 0.1882
Loss at step 34150: 0.4416
Loss at step 34160: 0.2526
Loss at step 34170: 0.2214
Loss at step 34180: 0.1697
Loss at step 34190: 0.2827
Loss at step 34200: 0.2046
Loss at step 34210: 0.1926
Loss at step 34220: 0.1680
Loss at step 34230: 0.0871
Loss at step 34240: 0.1704
Loss at step 34250: 0.2698
Loss at step 34260: 0.0708
Loss at step 34270: 0.4945
Loss at step 34280: 0.6099
Loss at step 34290: 0.1865
Loss at step 34300: 0.2365
Loss at step 34310: 0.0207
Loss at step 34320: 0.0861
Loss at step 34330: 0.1629
Loss at step 34340: 0.0125
Loss at step 34350: 0.3975
Loss at step 34360: 0.3037
Loss at step 34370: 0.2408
Loss at step 34380: 0.0181
Loss at step 34390: 0.0360
Loss at step 34400: 0.3452
Loss at step 34410: 0.0853
Loss at step 34420: 0.0167
Loss at step 34430: 0.0078
Loss at step 34440: 0.3996
Loss at step 34450: 0.3035
Loss at step 34460: 0.0214
Loss at step 34470: 0.0627
Loss at step 34480: 0.0311
Loss at step 34490: 0.0411
Loss at step 34500: 0.1985
Loss at step 34510: 0.0715
Loss at step 34520: 0.3518
Loss at step 34530: 0.2726
Loss at step 34540: 0.0701
Loss at step 34550: 0.4608
Loss at step 34560: 0.0942
Loss at step 34570: 0.2227
Loss at step 34580: 0.2673
Loss at step 34590: 0.3428
Loss at step 34600: 0.0564
Loss at step 34610: 0.0147
Loss at step 34620: 0.4612
Loss at step 34630: 0.2472
Loss at step 34640: 0.0216
Loss at step 34650: 0.0083
Loss at step 34660: 0.3065
Loss at step 34670: 0.1573
Loss at step 34680: 0.2591
Loss at step 34690: 0.1054
Loss at step 34700: 0.0727
Loss at step 34710: 0.3472
Loss at step 34720: 0.1059
Loss at step 34730: 0.5170
Loss at step 34740: 0.3801
Loss at step 34750: 0.0605
Loss at step 34760: 0.1632
Loss at step 34770: 0.5017
Loss at step 34780: 0.2642
Loss at step 34790: 0.1754
Loss at step 34800: 0.4481
Loss at step 34810: 0.0232
Loss at step 34820: 0.1701
Loss at step 34830: 0.2709
Loss at step 34840: 0.5019
Loss at step 34850: 0.0209
Loss at step 34860: 0.4191
Loss at step 34870: 0.0048
Loss at step 34880: 0.0053
Loss at step 34890: 0.0969
Loss at step 34900: 0.2880
Loss at step 34910: 0.0650
Loss at step 34920: 0.2060
Loss at step 34930: 0.0088
Loss at step 34940: 0.0188
Loss at step 34950: 0.4172
Loss at step 34960: 0.1871
Loss at step 34970: 0.0167
Loss at step 34980: 0.1140
Loss at step 34990: 0.0056
Loss at step 35000: 0.6980
Loss at step 35010: 0.7226
Loss at step 35020: 0.0166
Loss at step 35030: 0.1053
Loss at step 35040: 0.1552
Loss at step 35050: 0.0922
Loss at step 35060: 0.0154
Loss at step 35070: 0.0704
Loss at step 35080: 0.1453
Loss at step 35090: 0.1159
Loss at step 35100: 0.0808
Loss at step 35110: 0.0109
Loss at step 35120: 0.2767
Loss at step 35130: 0.0543
Loss at step 35140: 0.2436
Loss at step 35150: 0.2224
Loss at step 35160: 0.0185
Loss at step 35170: 0.1658
Loss at step 35180: 0.3778
Loss at step 35190: 0.3420
Loss at step 35200: 0.5794
Loss at step 35210: 0.3950
Loss at step 35220: 0.5520
Loss at step 35230: 0.6833
Loss at step 35240: 0.1762
Loss at step 35250: 0.4348
Loss at step 35260: 0.0240
Loss at step 35270: 0.1743
Loss at step 35280: 0.2208
Loss at step 35290: 0.1340
Loss at step 35300: 0.1941
Loss at step 35310: 0.0164
Loss at step 35320: 0.0268
Loss at step 35330: 0.2144
Loss at step 35340: 0.1143
Loss at step 35350: 0.0604
Loss at step 35360: 0.2725
Loss at step 35370: 0.0221
Loss at step 35380: 0.0837
Loss at step 35390: 0.0066
Loss at step 35400: 0.0286
Loss at step 35410: 0.3858
Loss at step 35420: 0.4576
Loss at step 35430: 0.1693
Loss at step 35440: 0.2719
Loss at step 35450: 0.6470
Loss at step 35460: 0.0226
Loss at step 35470: 0.0870
Loss at step 35480: 0.1725
Loss at step 35490: 0.2093
Loss at step 35500: 0.1016
Loss at step 35510: 0.0851
Loss at step 35520: 0.0972
Loss at step 35530: 0.0422
Loss at step 35540: 0.0287
Loss at step 35550: 0.1119
Loss at step 35560: 0.2962
Loss at step 35570: 0.0361
Loss at step 35580: 0.0264
Loss at step 35590: 0.1244
Loss at step 35600: 0.2664
Loss at step 35610: 0.1834
Loss at step 35620: 0.4465
Loss at step 35630: 0.0221
Loss at step 35640: 0.1749
Loss at step 35650: 0.3493
Loss at step 35660: 0.0645
Loss at step 35670: 0.1607
Loss at step 35680: 0.0324
Loss at step 35690: 0.6257
Loss at step 35700: 0.2127
Loss at step 35710: 0.0049
Loss at step 35720: 0.0704
Loss at step 35730: 0.0513
Loss at step 35740: 0.1399
Loss at step 35750: 0.2228
Loss at step 35760: 0.0412
Loss at step 35770: 0.1409
Loss at step 35780: 0.0062
Loss at step 35790: 0.1021
Loss at step 35800: 0.1122
Loss at step 35810: 0.5402
Loss at step 35820: 0.3756
Loss at step 35830: 0.0576
Loss at step 35840: 0.4382
Loss at step 35850: 0.2097
Loss at step 35860: 0.0147
Loss at step 35870: 0.3020
Loss at step 35880: 0.3352
Loss at step 35890: 0.1067
Loss at step 35900: 0.2378
Loss at step 35910: 0.0450
Loss at step 35920: 0.3829
Loss at step 35930: 0.3076
Loss at step 35940: 0.2772
Loss at step 35950: 0.4764
Loss at step 35960: 0.2090
Loss at step 35970: 0.0162
Loss at step 35980: 0.0166
Loss at step 35990: 0.1424
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/recall/39d849ff49b976b6a0fd96ded18937147c0acfb9178109a493908b0275bbcc85 (last modified on Sat Nov 30 13:38:18 2024) since it couldn't be found locally at recall, or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/gaya/.cache/huggingface/modules/datasets_modules/metrics/f1/4f006eef192effdc533301c01aff7e4922b5a427fbdf53c50b3db69887dbdada (last modified on Sat Nov 30 13:38:19 2024) since it couldn't be found locally at f1, or remotely on the Hugging Face Hub.
***** Running testing *****
  Num examples = 82080
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.805702, 'precision': [0.902867, 0.362922, 0.33417], 'recall': [0.915475, 0.303216, 0.365], 'f1': [0.909127, 0.330393, 0.348905]}
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 107997
Loss at step 10: 0.6331
Loss at step 20: 0.6378
Loss at step 30: 0.5645
Loss at step 40: 0.6588
Loss at step 50: 0.3537
Loss at step 60: 0.6238
Loss at step 70: 0.3283
Loss at step 80: 0.4961
Loss at step 90: 0.7931
Loss at step 100: 0.5496
Loss at step 110: 0.7750
Loss at step 120: 0.2620
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Loss at step 10: 0.5434
Loss at step 20: 0.4950
Loss at step 30: 0.3822
Loss at step 40: 0.3828
Loss at step 50: 0.3816
Loss at step 60: 0.3651
Loss at step 70: 0.4811
Loss at step 80: 0.4698
Loss at step 90: 0.4223
Loss at step 100: 0.7105
Loss at step 110: 0.3897
Loss at step 120: 0.5301
Loss at step 130: 0.4084
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7c97963b94c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 140: 0.5155
Loss at step 150: 0.2966
Loss at step 160: 0.4836
Sample 490785 of the training set: {'input_ids': [101, 2622, 3208, 1024, 3100, 2021, 2059, 2017, 2017, 2064, 5245, 7910, 2070, 2060, 6462, 2004, 2092, 1012, 102, 2622, 3208, 2001, 3241, 2008, 2009, 2052, 3828, 2769, 2006, 1996, 6556, 1012, 2059, 2002, 2052, 2693, 2006, 2000, 1996, 5446, 2112, 2000, 2156, 2065, 2009, 2052, 2022, 2035, 2825, 1012, 2059, 2002, 2052, 2693, 2006, 2000, 1996, 5446, 2112, 1010, 2029, 2003, 3492, 10990, 2000, 2156, 2065, 2009, 2001, 2035, 2825, 1012, 2059, 2002, 2052, 2031, 1037, 3492, 2524, 2051, 5155, 2023, 2005, 4376, 1998, 1037, 2431, 19329, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 13500
Loss at step 170: 0.5936
Loss at step 180: 0.3866
Loss at step 190: 0.4872
Loss at step 200: 0.4172
Loss at step 210: 0.8119
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x7841c9d7b4c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 220: 0.5128
Loss at step 230: 0.2815
Loss at step 240: 0.4490
Sample 382554 of the training set: {'input_ids': [101, 5310, 8278, 1024, 2057, 2031, 1996, 3715, 2099, 2061, 2009, 1005, 1055, 2053, 3291, 1012, 102, 3919, 5859, 2245, 2008, 1996, 4613, 5038, 2323, 2022, 2204, 2005, 4966, 2867, 2000, 1996, 6556, 2491, 1012, 5310, 8278, 4081, 2008, 2009, 2071, 2025, 2022, 3733, 2000, 2224, 1996, 27662, 3898, 1998, 3919, 5859, 1005, 1055, 5448, 1012, 2622, 3208, 3530, 1010, 1996, 2136, 2787, 2000, 2224, 4613, 5038, 3853, 1998, 5310, 8278, 3530, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 128
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 1
  Total optimization steps = 13500
Loss at step 250: 0.5697
Loss at step 260: 0.3423
Loss at step 270: 0.4575
Loss at step 280: 0.3611
Loss at step 290: 0.3745
Loss at step 300: 0.5886
Loss at step 310: 0.3907
Loss at step 320: 0.4297
Loss at step 330: 0.5103
Loss at step 340: 0.3000
Loss at step 350: 0.3995
Loss at step 360: 0.4051
Loss at step 370: 0.3867
Loss at step 380: 0.3441
Loss at step 390: 0.3032
Loss at step 400: 0.5393
Loss at step 410: 0.4117
Loss at step 420: 0.4853
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x70fb58dfb4c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 430: 0.4202
Loss at step 440: 0.2961
Loss at step 450: 0.5101
Sample 382554 of the training set: {'input_ids': [101, 5310, 8278, 1024, 2057, 2031, 1996, 3715, 2099, 2061, 2009, 1005, 1055, 2053, 3291, 1012, 102, 3919, 5859, 2245, 2008, 1996, 4613, 5038, 2323, 2022, 2204, 2005, 4966, 2867, 2000, 1996, 6556, 2491, 1012, 5310, 8278, 4081, 2008, 2009, 2071, 2025, 2022, 3733, 2000, 2224, 1996, 27662, 3898, 1998, 3919, 5859, 1005, 1055, 5448, 1012, 2622, 3208, 3530, 1010, 1996, 2136, 2787, 2000, 2224, 4613, 5038, 3853, 1998, 5310, 8278, 3530, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Loss at step 460: 0.3806
Loss at step 470: 0.3397
Loss at step 480: 0.4738
Loss at step 490: 0.4664
Loss at step 500: 0.3795
Loss at step 510: 0.3614
Loss at step 520: 0.4490
Loss at step 530: 0.4804
Parameter 'function'=<function get_omission_datasets.<locals>.pair_func at 0x71794adbb4c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Loss at step 540: 0.4539
Loss at step 550: 0.5988
Loss at step 560: 0.4231
Loss at step 570: 0.3583
Sample 382554 of the training set: {'input_ids': [101, 5310, 8278, 1024, 2057, 2031, 1996, 3715, 2099, 2061, 2009, 1005, 1055, 2053, 3291, 1012, 102, 3919, 5859, 2245, 2008, 1996, 4613, 5038, 2323, 2022, 2204, 2005, 4966, 2867, 2000, 1996, 6556, 2491, 1012, 5310, 8278, 4081, 2008, 2009, 2071, 2025, 2022, 3733, 2000, 2224, 1996, 27662, 3898, 1998, 3919, 5859, 1005, 1055, 5448, 1012, 2622, 3208, 3530, 1010, 1996, 2136, 2787, 2000, 2224, 4613, 5038, 3853, 1998, 5310, 8278, 3530, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Loss at step 580: 0.5543
Loss at step 590: 0.2869
Loss at step 600: 0.5473
Loss at step 610: 0.4408
Loss at step 620: 0.4356
Loss at step 630: 0.4831
Loss at step 640: 0.4973
Loss at step 650: 0.4487
Loss at step 660: 0.4201
Loss at step 670: 0.4762
Loss at step 680: 0.3538
Loss at step 690: 0.4536
Loss at step 700: 0.5313
Loss at step 710: 0.3669
Loss at step 720: 0.5211
Loss at step 730: 0.3085
Loss at step 740: 0.4538
Loss at step 750: 0.3984
Loss at step 760: 0.4332
Loss at step 770: 0.5337
Loss at step 780: 0.5546
Loss at step 790: 0.4399
Loss at step 800: 0.3885
Loss at step 810: 0.6107
Loss at step 820: 0.3885
Loss at step 830: 0.3821
Loss at step 840: 0.4208
Loss at step 850: 0.3005
Loss at step 860: 0.4308
Loss at step 870: 0.4506
Loss at step 880: 0.3531
Loss at step 890: 0.5052
Loss at step 900: 0.5207
Loss at step 910: 0.3076
Loss at step 920: 0.2192
Loss at step 930: 0.2922
Loss at step 940: 0.3168
Loss at step 950: 0.4900
Loss at step 960: 0.3734
Loss at step 970: 0.3389
Loss at step 980: 0.6148
Loss at step 990: 0.2712
Loss at step 1000: 0.5115
Loss at step 1010: 0.3165
Loss at step 1020: 0.3938
Loss at step 1030: 0.4695
Loss at step 1040: 0.3953
Loss at step 1050: 0.5161
Loss at step 1060: 0.4188
Loss at step 1070: 0.3843
Loss at step 1080: 0.3527
Loss at step 1090: 0.4101
Loss at step 1100: 0.4465
Loss at step 1110: 0.4785
Loss at step 1120: 0.3514
Loss at step 1130: 0.4485
Loss at step 1140: 0.3159
Loss at step 1150: 0.2596
Loss at step 1160: 0.5168
Loss at step 1170: 0.3609
Loss at step 1180: 0.2464
Loss at step 1190: 0.5920
Loss at step 1200: 0.2070
Loss at step 1210: 0.3786
Loss at step 1220: 0.4402
Loss at step 1230: 0.4089
Loss at step 1240: 0.3531
Loss at step 1250: 0.4421
Loss at step 1260: 0.3585
Loss at step 1270: 0.2333
Loss at step 1280: 0.3325
Loss at step 1290: 0.2346
Loss at step 1300: 0.2799
Loss at step 1310: 0.2510
Loss at step 1320: 0.2969
Loss at step 1330: 0.2593
Loss at step 1340: 0.3370
Loss at step 1350: 0.4196
Loss at step 1360: 0.4168
Loss at step 1370: 0.3107
Loss at step 1380: 0.4047
Loss at step 1390: 0.3266
Loss at step 1400: 0.5124
Loss at step 1410: 0.2534
Loss at step 1420: 0.3445
Loss at step 1430: 0.2643
Loss at step 1440: 0.4816
Loss at step 1450: 0.3452
Loss at step 1460: 0.2650
Loss at step 1470: 0.3885
Loss at step 1480: 0.2409
Loss at step 1490: 0.3019
Loss at step 1500: 0.2402
Loss at step 1510: 0.2119
Loss at step 1520: 0.3769
Loss at step 1530: 0.3705
Loss at step 1540: 0.5333
Loss at step 1550: 0.3081
Loss at step 1560: 0.4519
Loss at step 1570: 0.2319
Loss at step 1580: 0.2901
Loss at step 1590: 0.4199
Loss at step 1600: 0.5060
Loss at step 1610: 0.3711
Loss at step 1620: 0.3621
Loss at step 1630: 0.3616
Loss at step 1640: 0.3546
Loss at step 1650: 0.3438
Loss at step 1660: 0.2774
Loss at step 1670: 0.3059
Loss at step 1680: 0.5613
Loss at step 1690: 0.4914
Loss at step 1700: 0.2923
Loss at step 1710: 0.3595
Loss at step 1720: 0.3329
Loss at step 1730: 0.3020
Loss at step 1740: 0.2664
Loss at step 1750: 0.2476
Loss at step 1760: 0.2803
Loss at step 1770: 0.4396
Loss at step 1780: 0.4156
Loss at step 1790: 0.3555
Loss at step 1800: 0.4413
Loss at step 1810: 0.5093
Loss at step 1820: 0.5151
Loss at step 1830: 0.2807
Loss at step 1840: 0.4220
Loss at step 1850: 0.2166
Loss at step 1860: 0.3220
Loss at step 1870: 0.3824
Loss at step 1880: 0.4176
Loss at step 1890: 0.2603
Loss at step 1900: 0.3588
Loss at step 1910: 0.2552
Loss at step 1920: 0.3214
Loss at step 1930: 0.2171
Loss at step 1940: 0.4327
Loss at step 1950: 0.3585
Loss at step 1960: 0.4417
Loss at step 1970: 0.3257
Loss at step 1980: 0.2923
Loss at step 1990: 0.3483
Loss at step 2000: 0.5502
Loss at step 2010: 0.2216
Loss at step 2020: 0.3694
Loss at step 2030: 0.2802
Loss at step 2040: 0.2308
Loss at step 2050: 0.3449
Loss at step 2060: 0.2625
Loss at step 2070: 0.3543
Loss at step 2080: 0.2837
Loss at step 2090: 0.2291
Loss at step 2100: 0.3855
Loss at step 2110: 0.4087
Loss at step 2120: 0.2714
Loss at step 2130: 0.3218
Loss at step 2140: 0.3428
Loss at step 2150: 0.3007
Loss at step 2160: 0.3403
Loss at step 2170: 0.2565
Loss at step 2180: 0.3940
Loss at step 2190: 0.2412
Loss at step 2200: 0.3199
Loss at step 2210: 0.2967
Loss at step 2220: 0.3340
Loss at step 2230: 0.1460
Loss at step 2240: 0.2908
Loss at step 2250: 0.2503
Loss at step 2260: 0.3472
Loss at step 2270: 0.2822
Loss at step 2280: 0.4071
Loss at step 2290: 0.3426
Loss at step 2300: 0.2584
Loss at step 2310: 0.1983
Loss at step 2320: 0.3962
Loss at step 2330: 0.1246
Loss at step 2340: 0.1950
Loss at step 2350: 0.2026
Loss at step 2360: 0.1647
Loss at step 2370: 0.3235
Loss at step 2380: 0.2307
Loss at step 2390: 0.3326
Loss at step 2400: 0.3396
Loss at step 2410: 0.2269
Loss at step 2420: 0.2272
Loss at step 2430: 0.3755
Loss at step 2440: 0.2373
Loss at step 2450: 0.4406
Loss at step 2460: 0.3779
Loss at step 2470: 0.2581
Loss at step 2480: 0.4222
Loss at step 2490: 0.4082
Loss at step 2500: 0.3016
Loss at step 2510: 0.2962
Loss at step 2520: 0.1793
Loss at step 2530: 0.2585
Loss at step 2540: 0.2902
Loss at step 2550: 0.3770
Loss at step 2560: 0.3055
Loss at step 2570: 0.3519
Loss at step 2580: 0.2676
Loss at step 2590: 0.2960
Loss at step 2600: 0.4301
Loss at step 2610: 0.3308
Loss at step 2620: 0.3604
Loss at step 2630: 0.3169
Loss at step 2640: 0.3437
Loss at step 2650: 0.3492
Loss at step 2660: 0.3916
Loss at step 2670: 0.2735
Loss at step 2680: 0.2489
Loss at step 2690: 0.3062
Loss at step 2700: 0.3081
Loss at step 2710: 0.2633
Loss at step 2720: 0.3609
Loss at step 2730: 0.1670
Loss at step 2740: 0.3435
Loss at step 2750: 0.2351
Loss at step 2760: 0.3366
Loss at step 2770: 0.3284
Loss at step 2780: 0.3823
Loss at step 2790: 0.2419
Loss at step 2800: 0.2148
Loss at step 2810: 0.4268
Loss at step 2820: 0.2937
Loss at step 2830: 0.2188
Loss at step 2840: 0.2930
Loss at step 2850: 0.3383
Loss at step 2860: 0.2321
Loss at step 2870: 0.2237
Loss at step 2880: 0.1952
Loss at step 2890: 0.3479
Loss at step 2900: 0.2000
Loss at step 2910: 0.2512
Loss at step 2920: 0.2825
Loss at step 2930: 0.3341
Loss at step 2940: 0.2033
Loss at step 2950: 0.2082
Loss at step 2960: 0.3112
Loss at step 2970: 0.3328
Loss at step 2980: 0.3750
Loss at step 2990: 0.3200
Loss at step 3000: 0.2636
Loss at step 3010: 0.2311
Loss at step 3020: 0.2773
Loss at step 3030: 0.4165
Loss at step 3040: 0.3217
Loss at step 3050: 0.3552
Loss at step 3060: 0.2621
Loss at step 3070: 0.1792
Loss at step 3080: 0.2686
Loss at step 3090: 0.2817
Loss at step 3100: 0.2691
Loss at step 3110: 0.3449
Loss at step 3120: 0.4498
Loss at step 3130: 0.3297
Loss at step 3140: 0.2282
Loss at step 3150: 0.3006
Loss at step 3160: 0.3198
Loss at step 3170: 0.2984
Loss at step 3180: 0.2403
Loss at step 3190: 0.1733
Loss at step 3200: 0.2258
Loss at step 3210: 0.2654
Loss at step 3220: 0.1418
Loss at step 3230: 0.2701
Loss at step 3240: 0.2686
Loss at step 3250: 0.2162
Loss at step 3260: 0.2087
Loss at step 3270: 0.2383
Loss at step 3280: 0.3190
Loss at step 3290: 0.2998
Loss at step 3300: 0.1129
Loss at step 3310: 0.1256
Loss at step 3320: 0.3874
Loss at step 3330: 0.2722
Loss at step 3340: 0.2991
Loss at step 3350: 0.2781
Loss at step 3360: 0.1988
Loss at step 3370: 0.2197
Loss at step 3380: 0.4824
Loss at step 3390: 0.2847
Loss at step 3400: 0.2226
Loss at step 3410: 0.2966
Loss at step 3420: 0.1827
Loss at step 3430: 0.3556
Loss at step 3440: 0.2486
Loss at step 3450: 0.4845
Loss at step 3460: 0.3074
Loss at step 3470: 0.3247
Loss at step 3480: 0.1807
Loss at step 3490: 0.3945
Loss at step 3500: 0.1764
Loss at step 3510: 0.3482
Loss at step 3520: 0.2153
Loss at step 3530: 0.3077
Loss at step 3540: 0.2913
Loss at step 3550: 0.2454
Loss at step 3560: 0.2189
Loss at step 3570: 0.3360
Loss at step 3580: 0.2141
Loss at step 3590: 0.4058
Loss at step 3600: 0.2166
Loss at step 3610: 0.1354
Loss at step 3620: 0.1344
Loss at step 3630: 0.2463
Loss at step 3640: 0.2790
Loss at step 3650: 0.2604
Loss at step 3660: 0.2857
Loss at step 3670: 0.1779
Loss at step 3680: 0.3030
Loss at step 3690: 0.2179
Loss at step 3700: 0.1549
Loss at step 3710: 0.0985
Loss at step 3720: 0.1597
Loss at step 3730: 0.2207
Loss at step 3740: 0.2967
Loss at step 3750: 0.3434
Loss at step 3760: 0.2730
Loss at step 3770: 0.1793
Loss at step 3780: 0.1508
Loss at step 3790: 0.2176
Loss at step 3800: 0.0903
Loss at step 3810: 0.4281
Loss at step 3820: 0.1469
Loss at step 3830: 0.2425
Loss at step 3840: 0.1793
Loss at step 3850: 0.2148
Loss at step 3860: 0.1602
Loss at step 3870: 0.2029
Loss at step 3880: 0.3497
Loss at step 3890: 0.2847
Loss at step 3900: 0.2105
Loss at step 3910: 0.1380
Loss at step 3920: 0.3145
Loss at step 3930: 0.3757
Loss at step 3940: 0.2857
Loss at step 3950: 0.1251
Loss at step 3960: 0.3148
Loss at step 3970: 0.3414
Loss at step 3980: 0.3436
Loss at step 3990: 0.2422
Loss at step 4000: 0.1220
Loss at step 4010: 0.2650
Loss at step 4020: 0.2048
Loss at step 4030: 0.3509
Loss at step 4040: 0.4799
Loss at step 4050: 0.1998
Loss at step 4060: 0.2194
Loss at step 4070: 0.1988
Loss at step 4080: 0.1107
Loss at step 4090: 0.3314
Loss at step 4100: 0.3367
Loss at step 4110: 0.2220
Loss at step 4120: 0.1662
Loss at step 4130: 0.1553
Loss at step 4140: 0.1612
Loss at step 4150: 0.1921
Loss at step 4160: 0.2433
Loss at step 4170: 0.1775
Loss at step 4180: 0.2771
Loss at step 4190: 0.2851
Loss at step 4200: 0.2982
Loss at step 4210: 0.2649
Loss at step 4220: 0.1240
Loss at step 4230: 0.2814
Loss at step 4240: 0.1479
Loss at step 4250: 0.2043
Loss at step 4260: 0.1655
Loss at step 4270: 0.1440
Loss at step 4280: 0.1373
Loss at step 4290: 0.2896
Loss at step 4300: 0.3180
Loss at step 4310: 0.2223
Loss at step 4320: 0.2694
Loss at step 4330: 0.1383
Loss at step 4340: 0.1456
Loss at step 4350: 0.2951
Loss at step 4360: 0.3985
Loss at step 4370: 0.1988
Loss at step 4380: 0.2901
Loss at step 4390: 0.1580
Loss at step 4400: 0.1101
Loss at step 4410: 0.2161
Loss at step 4420: 0.1232
Loss at step 4430: 0.1143
Loss at step 4440: 0.3932
Loss at step 4450: 0.3038
Loss at step 4460: 0.1554
Loss at step 4470: 0.2905
Loss at step 4480: 0.1890
Loss at step 4490: 0.3787
Loss at step 4500: 0.2200
Loss at step 4510: 0.2311
Loss at step 4520: 0.2894
Loss at step 4530: 0.3582
Loss at step 4540: 0.1191
Loss at step 4550: 0.1488
Loss at step 4560: 0.1228
Loss at step 4570: 0.1348
Loss at step 4580: 0.2281
Loss at step 4590: 0.2752
Loss at step 4600: 0.2992
Loss at step 4610: 0.2114
Loss at step 4620: 0.1601
Loss at step 4630: 0.2235
Loss at step 4640: 0.1907
Loss at step 4650: 0.1810
Loss at step 4660: 0.1713
Loss at step 4670: 0.3018
Loss at step 4680: 0.3308
Loss at step 4690: 0.2690
Loss at step 4700: 0.2900
Loss at step 4710: 0.1717
Loss at step 4720: 0.2255
Loss at step 4730: 0.2130
Loss at step 4740: 0.0722
Loss at step 4750: 0.1124
Loss at step 4760: 0.1340
Loss at step 4770: 0.1168
Loss at step 4780: 0.2510
Loss at step 4790: 0.2144
Loss at step 4800: 0.2270
Loss at step 4810: 0.1140
Loss at step 4820: 0.1519
Loss at step 4830: 0.1309
Loss at step 4840: 0.1389
Loss at step 4850: 0.5948
Loss at step 4860: 0.1990
Loss at step 4870: 0.2334
Loss at step 4880: 0.1729
Loss at step 4890: 0.3274
Loss at step 4900: 0.1889
Loss at step 4910: 0.1009
Loss at step 4920: 0.2069
Loss at step 4930: 0.1575
Loss at step 4940: 0.1778
Loss at step 4950: 0.0833
Loss at step 4960: 0.1340
Loss at step 4970: 0.2332
Loss at step 4980: 0.2969
Loss at step 4990: 0.1853
Loss at step 5000: 0.2229
Loss at step 5010: 0.2520
Loss at step 5020: 0.2863
Loss at step 5030: 0.1568
Loss at step 5040: 0.1163
Loss at step 5050: 0.0937
Loss at step 5060: 0.1454
Loss at step 5070: 0.2789
Loss at step 5080: 0.0831
Loss at step 5090: 0.2360
Loss at step 5100: 0.1405
Loss at step 5110: 0.2433
Loss at step 5120: 0.1305
Loss at step 5130: 0.1562
Loss at step 5140: 0.2126
Loss at step 5150: 0.3294
Loss at step 5160: 0.2134
Loss at step 5170: 0.1705
Loss at step 5180: 0.1615
Loss at step 5190: 0.2175
Loss at step 5200: 0.1614
Loss at step 5210: 0.3433
Loss at step 5220: 0.2233
Loss at step 5230: 0.2515
Loss at step 5240: 0.1624
Loss at step 5250: 0.1364
Loss at step 5260: 0.1576
Loss at step 5270: 0.2618
Loss at step 5280: 0.2559
Loss at step 5290: 0.1181
Loss at step 5300: 0.3218
Loss at step 5310: 0.1785
Loss at step 5320: 0.1807
Loss at step 5330: 0.3485
Loss at step 5340: 0.2317
Loss at step 5350: 0.1397
Loss at step 5360: 0.3054
Loss at step 5370: 0.2637
Loss at step 5380: 0.0702
Loss at step 5390: 0.3098
Loss at step 5400: 0.2053
Loss at step 5410: 0.2560
Loss at step 5420: 0.1394
Loss at step 5430: 0.1990
Loss at step 5440: 0.1195
Loss at step 5450: 0.1202
Loss at step 5460: 0.2243
Loss at step 5470: 0.1111
Loss at step 5480: 0.2102
Loss at step 5490: 0.2801
Loss at step 5500: 0.3009
Loss at step 5510: 0.1257
Loss at step 5520: 0.1957
Loss at step 5530: 0.1835
Loss at step 5540: 0.1628
Loss at step 5550: 0.1510
Loss at step 5560: 0.1331
Loss at step 5570: 0.2900
Loss at step 5580: 0.1530
Loss at step 5590: 0.3521
Loss at step 5600: 0.2292
Loss at step 5610: 0.2948
Loss at step 5620: 0.1676
Loss at step 5630: 0.3671
Loss at step 5640: 0.2189
Loss at step 5650: 0.1336
Loss at step 5660: 0.2314
Loss at step 5670: 0.1310
Loss at step 5680: 0.1573
Loss at step 5690: 0.1095
Loss at step 5700: 0.2631
Loss at step 5710: 0.1385
Loss at step 5720: 0.1578
Loss at step 5730: 0.1774
Loss at step 5740: 0.2547
Loss at step 5750: 0.0705
Loss at step 5760: 0.1657
Loss at step 5770: 0.0657
Loss at step 5780: 0.2470
Loss at step 5790: 0.1658
Loss at step 5800: 0.2959
Loss at step 5810: 0.1010
Loss at step 5820: 0.1788
Loss at step 5830: 0.1669
Loss at step 5840: 0.1602
Loss at step 5850: 0.2645
Loss at step 5860: 0.1260
Loss at step 5870: 0.2238
Loss at step 5880: 0.2418
Loss at step 5890: 0.3895
Loss at step 5900: 0.2362
Loss at step 5910: 0.2710
Loss at step 5920: 0.1920
Loss at step 5930: 0.2866
Loss at step 5940: 0.2993
Loss at step 5950: 0.2342
Loss at step 5960: 0.1181
Loss at step 5970: 0.2993
Loss at step 5980: 0.1972
Loss at step 5990: 0.1256
Loss at step 6000: 0.1682
Loss at step 6010: 0.1596
Loss at step 6020: 0.2271
Loss at step 6030: 0.1783
Loss at step 6040: 0.2017
Loss at step 6050: 0.2940
Loss at step 6060: 0.1120
Loss at step 6070: 0.1381
Loss at step 6080: 0.3986
Loss at step 6090: 0.1353
Loss at step 6100: 0.2206
Loss at step 6110: 0.1341
Loss at step 6120: 0.0887
Loss at step 6130: 0.0963
Loss at step 6140: 0.2040
Loss at step 6150: 0.1124
Loss at step 6160: 0.1117
Loss at step 6170: 0.1463
Loss at step 6180: 0.2377
Loss at step 6190: 0.1605
Loss at step 6200: 0.1928
Loss at step 6210: 0.1969
Loss at step 6220: 0.1918
Loss at step 6230: 0.1806
Loss at step 6240: 0.2558
Loss at step 6250: 0.2189
Loss at step 6260: 0.2010
Loss at step 6270: 0.1860
Loss at step 6280: 0.3691
Loss at step 6290: 0.3544
Loss at step 6300: 0.0898
Loss at step 6310: 0.1527
Loss at step 6320: 0.2299
Loss at step 6330: 0.2258
Loss at step 6340: 0.2469
Loss at step 6350: 0.0809
Loss at step 6360: 0.1612
Loss at step 6370: 0.0979
Loss at step 6380: 0.1588
Loss at step 6390: 0.1999
Loss at step 6400: 0.1906
Loss at step 6410: 0.1551
Loss at step 6420: 0.1751
Loss at step 6430: 0.2331
Loss at step 6440: 0.2132
Loss at step 6450: 0.2099
Loss at step 6460: 0.3367
Loss at step 6470: 0.1425
Loss at step 6480: 0.1473
Loss at step 6490: 0.2780
Loss at step 6500: 0.1008
Loss at step 6510: 0.2622
Loss at step 6520: 0.2713
Loss at step 6530: 0.1483
Loss at step 6540: 0.2351
Loss at step 6550: 0.1832
Loss at step 6560: 0.1312
Loss at step 6570: 0.1431
Loss at step 6580: 0.1387
Loss at step 6590: 0.1459
Loss at step 6600: 0.1658
Loss at step 6610: 0.2237
Loss at step 6620: 0.2244
Loss at step 6630: 0.1670
Loss at step 6640: 0.3015
Loss at step 6650: 0.2307
Loss at step 6660: 0.2424
Loss at step 6670: 0.1950
Loss at step 6680: 0.1272
Loss at step 6690: 0.2311
Loss at step 6700: 0.2157
Loss at step 6710: 0.1202
Loss at step 6720: 0.2229
Loss at step 6730: 0.0867
Loss at step 6740: 0.1854
Loss at step 6750: 0.1785
Loss at step 6760: 0.1863
Loss at step 6770: 0.1353
Loss at step 6780: 0.1568
Loss at step 6790: 0.1535
Loss at step 6800: 0.1045
Loss at step 6810: 0.2628
Loss at step 6820: 0.1672
Loss at step 6830: 0.0960
Loss at step 6840: 0.1555
Loss at step 6850: 0.0845
Loss at step 6860: 0.3015
Loss at step 6870: 0.1114
Loss at step 6880: 0.3140
Loss at step 6890: 0.1571
Loss at step 6900: 0.1176
Loss at step 6910: 0.1909
Loss at step 6920: 0.2141
Loss at step 6930: 0.1925
Loss at step 6940: 0.1445
Loss at step 6950: 0.1427
Loss at step 6960: 0.1208
Loss at step 6970: 0.2150
Loss at step 6980: 0.2621
Loss at step 6990: 0.1493
Loss at step 7000: 0.0835
Loss at step 7010: 0.1906
Loss at step 7020: 0.0992
Loss at step 7030: 0.1524
Loss at step 7040: 0.2015
Loss at step 7050: 0.1779
Loss at step 7060: 0.3514
Loss at step 7070: 0.2823
Loss at step 7080: 0.1772
Loss at step 7090: 0.1066
Loss at step 7100: 0.2028
Loss at step 7110: 0.1857
Loss at step 7120: 0.0960
Loss at step 7130: 0.1520
Loss at step 7140: 0.1962
Loss at step 7150: 0.1490
Loss at step 7160: 0.1147
Loss at step 7170: 0.0966
Loss at step 7180: 0.1958
Loss at step 7190: 0.3060
Loss at step 7200: 0.0934
Loss at step 7210: 0.1464
Loss at step 7220: 0.1642
Loss at step 7230: 0.1766
Loss at step 7240: 0.1928
Loss at step 7250: 0.1049
Loss at step 7260: 0.3353
Loss at step 7270: 0.3127
Loss at step 7280: 0.1689
Loss at step 7290: 0.1640
Loss at step 7300: 0.0625
Loss at step 7310: 0.0762
Loss at step 7320: 0.2063
Loss at step 7330: 0.0890
Loss at step 7340: 0.1732
Loss at step 7350: 0.1478
Loss at step 7360: 0.0843
Loss at step 7370: 0.2132
Loss at step 7380: 0.0509
Loss at step 7390: 0.1701
Loss at step 7400: 0.3200
Loss at step 7410: 0.0926
Loss at step 7420: 0.2736
Loss at step 7430: 0.1302
Loss at step 7440: 0.2043
Loss at step 7450: 0.2352
Loss at step 7460: 0.3433
Loss at step 7470: 0.1802
Loss at step 7480: 0.1041
Loss at step 7490: 0.3112
Loss at step 7500: 0.1204
Loss at step 7510: 0.1201
Loss at step 7520: 0.1892
Loss at step 7530: 0.1033
Loss at step 7540: 0.2090
Loss at step 7550: 0.0447
Loss at step 7560: 0.3076
Loss at step 7570: 0.1189
Loss at step 7580: 0.1457
Loss at step 7590: 0.0442
Loss at step 7600: 0.1631
Loss at step 7610: 0.2114
Loss at step 7620: 0.0893
Loss at step 7630: 0.1879
Loss at step 7640: 0.1258
Loss at step 7650: 0.1306
Loss at step 7660: 0.1714
Loss at step 7670: 0.1350
Loss at step 7680: 0.2911
Loss at step 7690: 0.2689
Loss at step 7700: 0.0992
Loss at step 7710: 0.1603
Loss at step 7720: 0.1634
Loss at step 7730: 0.0418
Loss at step 7740: 0.0785
Loss at step 7750: 0.1631
Loss at step 7760: 0.1288
Loss at step 7770: 0.1769
Loss at step 7780: 0.1192
Loss at step 7790: 0.1534
Loss at step 7800: 0.3045
Loss at step 7810: 0.1592
Loss at step 7820: 0.1115
Loss at step 7830: 0.1896
Loss at step 7840: 0.1621
Loss at step 7850: 0.1573
Loss at step 7860: 0.1132
Loss at step 7870: 0.1024
Loss at step 7880: 0.1489
Loss at step 7890: 0.1762
Loss at step 7900: 0.1933
Loss at step 7910: 0.1462
Loss at step 7920: 0.2333
Loss at step 7930: 0.4626
Loss at step 7940: 0.2397
Loss at step 7950: 0.1444
Loss at step 7960: 0.1428
Loss at step 7970: 0.1712
Loss at step 7980: 0.2399
Loss at step 7990: 0.0431
Loss at step 8000: 0.1101
Loss at step 8010: 0.2067
Loss at step 8020: 0.1161
Loss at step 8030: 0.2276
Loss at step 8040: 0.1489
Loss at step 8050: 0.2147
Loss at step 8060: 0.1788
Loss at step 8070: 0.1480
Loss at step 8080: 0.0751
Loss at step 8090: 0.1237
Loss at step 8100: 0.1747
Loss at step 8110: 0.2790
Loss at step 8120: 0.1208
Loss at step 8130: 0.2587
Loss at step 8140: 0.0563
Loss at step 8150: 0.0804
Loss at step 8160: 0.1289
Loss at step 8170: 0.1480
Loss at step 8180: 0.1041
Loss at step 8190: 0.1925
Loss at step 8200: 0.1558
Loss at step 8210: 0.1577
Loss at step 8220: 0.2340
Loss at step 8230: 0.1160
Loss at step 8240: 0.1169
Loss at step 8250: 0.1509
Loss at step 8260: 0.0989
Loss at step 8270: 0.0920
Loss at step 8280: 0.2104
Loss at step 8290: 0.2394
Loss at step 8300: 0.1440
Loss at step 8310: 0.0360
Loss at step 8320: 0.3658
Loss at step 8330: 0.1531
Loss at step 8340: 0.1720
Loss at step 8350: 0.2630
Loss at step 8360: 0.1050
Loss at step 8370: 0.1861
Loss at step 8380: 0.2500
Loss at step 8390: 0.1509
Loss at step 8400: 0.1350
Loss at step 8410: 0.1444
Loss at step 8420: 0.1136
Loss at step 8430: 0.1852
Loss at step 8440: 0.2208
Loss at step 8450: 0.1016
Loss at step 8460: 0.1198
Loss at step 8470: 0.0601
Loss at step 8480: 0.1138
Loss at step 8490: 0.0814
Loss at step 8500: 0.0890
Loss at step 8510: 0.0247
Loss at step 8520: 0.0902
Loss at step 8530: 0.1078
Loss at step 8540: 0.2441
Loss at step 8550: 0.2585
Loss at step 8560: 0.1662
Loss at step 8570: 0.1520
Loss at step 8580: 0.2070
Loss at step 8590: 0.4095
Loss at step 8600: 0.2333
Loss at step 8610: 0.1870
Loss at step 8620: 0.1330
Loss at step 8630: 0.0844
Loss at step 8640: 0.0638
Loss at step 8650: 0.0888
Loss at step 8660: 0.1083
Loss at step 8670: 0.3244
Loss at step 8680: 0.2537
Loss at step 8690: 0.1122
Loss at step 8700: 0.2625
Loss at step 8710: 0.0562
Loss at step 8720: 0.1403
Loss at step 8730: 0.2565
Loss at step 8740: 0.1732
Loss at step 8750: 0.1830
Loss at step 8760: 0.1871
Loss at step 8770: 0.1094
Loss at step 8780: 0.2560
Loss at step 8790: 0.1455
Loss at step 8800: 0.1070
Loss at step 8810: 0.3420
Loss at step 8820: 0.1544
Loss at step 8830: 0.1626
Loss at step 8840: 0.1926
Loss at step 8850: 0.1655
Loss at step 8860: 0.0648
Loss at step 8870: 0.2186
Loss at step 8880: 0.2185
Loss at step 8890: 0.1122
Loss at step 8900: 0.1417
Loss at step 8910: 0.0813
Loss at step 8920: 0.1538
Loss at step 8930: 0.0673
Loss at step 8940: 0.0487
Loss at step 8950: 0.2842
Loss at step 8960: 0.1021
Loss at step 8970: 0.1055
Loss at step 8980: 0.1484
Loss at step 8990: 0.0913
Loss at step 9000: 0.1317
***** Running testing *****
  Num examples = 82080
  Instantaneous batch size per device = 16
  Total eval batch size = 16
{'accuracy': 0.813645, 'precision': [0.905492, 0.381339, 0.392134], 'recall': [0.916794, 0.349595, 0.387581], 'f1': [0.911108, 0.364778, 0.389844]}
{'accuracy': 0.908358, 'precision': 0.392134, 'recall': 0.387581, 'f1': 0.389844, 'WordR': 0.511674}
Loss at step 9010: 0.1988
Loss at step 9020: 0.1322
Loss at step 9030: 0.1331
Loss at step 9040: 0.1364
Loss at step 9050: 0.1176
Loss at step 9060: 0.1346
Loss at step 9070: 0.1136
Loss at step 9080: 0.0588
Loss at step 9090: 0.0872
Loss at step 9100: 0.1544
Loss at step 9110: 0.3087
Loss at step 9120: 0.1324
Loss at step 9130: 0.1280
Loss at step 9140: 0.0790
Loss at step 9150: 0.0427
Loss at step 9160: 0.1338
Loss at step 9170: 0.0993
Loss at step 9180: 0.0271
Loss at step 9190: 0.1266
Loss at step 9200: 0.1276
Loss at step 9210: 0.2285
Loss at step 9220: 0.1429
Loss at step 9230: 0.2732
Loss at step 9240: 0.1727
Loss at step 9250: 0.0892
Loss at step 9260: 0.1532
Loss at step 9270: 0.0533
Loss at step 9280: 0.1407
Loss at step 9290: 0.1410
Loss at step 9300: 0.2363
Loss at step 9310: 0.0416
Loss at step 9320: 0.1211
Loss at step 9330: 0.1553
Loss at step 9340: 0.1341
Loss at step 9350: 0.0619
Loss at step 9360: 0.0296
Loss at step 9370: 0.1895
Loss at step 9380: 0.0478
Loss at step 9390: 0.1204
Loss at step 9400: 0.0686
Loss at step 9410: 0.0277
Loss at step 9420: 0.0881
Loss at step 9430: 0.1414
Loss at step 9440: 0.1565
Loss at step 9450: 0.0334
Loss at step 9460: 0.2061
Loss at step 9470: 0.1117
Loss at step 9480: 0.1322
Loss at step 9490: 0.1645
Loss at step 9500: 0.1782
Loss at step 9510: 0.0914
Loss at step 9520: 0.0774
Loss at step 9530: 0.1469
Loss at step 9540: 0.0445
Loss at step 9550: 0.0981
Loss at step 9560: 0.0710
Loss at step 9570: 0.0875
Loss at step 9580: 0.0598
Loss at step 9590: 0.0774
Loss at step 9600: 0.1748
Loss at step 9610: 0.1330
Loss at step 9620: 0.0158
Loss at step 9630: 0.1041
Loss at step 9640: 0.1808
Loss at step 9650: 0.0855
Loss at step 9660: 0.0484
Loss at step 9670: 0.0289
Loss at step 9680: 0.1352
Loss at step 9690: 0.0393
Loss at step 9700: 0.1287
Loss at step 9710: 0.0873
Loss at step 9720: 0.0545
Loss at step 9730: 0.2880
Loss at step 9740: 0.0346
Loss at step 9750: 0.0795
Loss at step 9760: 0.0260
Loss at step 9770: 0.1849
Loss at step 9780: 0.1052
Loss at step 9790: 0.0286
Loss at step 9800: 0.1072
Loss at step 9810: 0.2039
Loss at step 9820: 0.2524
Loss at step 9830: 0.0832
Loss at step 9840: 0.1851
Loss at step 9850: 0.1408
Loss at step 9860: 0.1007
Loss at step 9870: 0.1639
Loss at step 9880: 0.1200
Loss at step 9890: 0.1039
Loss at step 9900: 0.2778
Loss at step 9910: 0.0686
Loss at step 9920: 0.0882
Loss at step 9930: 0.1165
Loss at step 9940: 0.0711
Loss at step 9950: 0.2436
Loss at step 9960: 0.1577
Loss at step 9970: 0.2954
Loss at step 9980: 0.1872
Loss at step 9990: 0.0692
Loss at step 10000: 0.1628
Loss at step 10010: 0.0917
Loss at step 10020: 0.0972
Loss at step 10030: 0.1440
Loss at step 10040: 0.0741
Loss at step 10050: 0.1209
Loss at step 10060: 0.1312
Loss at step 10070: 0.0239
Loss at step 10080: 0.2441
Loss at step 10090: 0.2280
Loss at step 10100: 0.0567
Loss at step 10110: 0.0882
Loss at step 10120: 0.0312
Loss at step 10130: 0.0800
Loss at step 10140: 0.0332
Loss at step 10150: 0.0395
Loss at step 10160: 0.0576
Loss at step 10170: 0.0794
Loss at step 10180: 0.1586
Loss at step 10190: 0.1091
Loss at step 10200: 0.0877
Loss at step 10210: 0.0713
Loss at step 10220: 0.1935
Loss at step 10230: 0.1133
Loss at step 10240: 0.0513
Loss at step 10250: 0.2073
Loss at step 10260: 0.2783
Loss at step 10270: 0.1438
Loss at step 10280: 0.0848
Loss at step 10290: 0.1714
Loss at step 10300: 0.1615
Loss at step 10310: 0.1402
Loss at step 10320: 0.3221
Loss at step 10330: 0.0613
Loss at step 10340: 0.1052
Loss at step 10350: 0.1058
Loss at step 10360: 0.1449
Loss at step 10370: 0.0786
Loss at step 10380: 0.2033
Loss at step 10390: 0.1250
Loss at step 10400: 0.1643
Loss at step 10410: 0.0449
Loss at step 10420: 0.1209
Loss at step 10430: 0.0926
Loss at step 10440: 0.1410
Loss at step 10450: 0.0568
Loss at step 10460: 0.0917
Loss at step 10470: 0.0769
Loss at step 10480: 0.1917
Loss at step 10490: 0.0887
Loss at step 10500: 0.0540
Loss at step 10510: 0.0813
Loss at step 10520: 0.0684
Loss at step 10530: 0.1960
Loss at step 10540: 0.1338
Loss at step 10550: 0.1355
Loss at step 10560: 0.1605
Loss at step 10570: 0.0552
Loss at step 10580: 0.1762
Loss at step 10590: 0.0946
Loss at step 10600: 0.0595
Loss at step 10610: 0.1005
Loss at step 10620: 0.0387
Loss at step 10630: 0.1739
Loss at step 10640: 0.2746
Loss at step 10650: 0.1566
Loss at step 10660: 0.0460
Loss at step 10670: 0.2242
Loss at step 10680: 0.1566
Loss at step 10690: 0.0613
Loss at step 10700: 0.1315
Loss at step 10710: 0.0890
Loss at step 10720: 0.1002
Loss at step 10730: 0.0777
Loss at step 10740: 0.0445
Loss at step 10750: 0.1012
Loss at step 10760: 0.0904
Loss at step 10770: 0.1177
Loss at step 10780: 0.1757
Loss at step 10790: 0.1899
Loss at step 10800: 0.1431
Loss at step 10810: 0.1848
Loss at step 10820: 0.0320
Loss at step 10830: 0.0790
Loss at step 10840: 0.0350
Loss at step 10850: 0.1644
Loss at step 10860: 0.0604
Loss at step 10870: 0.1125
Loss at step 10880: 0.1525
Loss at step 10890: 0.0756
Loss at step 10900: 0.0699
Loss at step 10910: 0.0694
Loss at step 10920: 0.2510
Loss at step 10930: 0.0747
Loss at step 10940: 0.0202
Loss at step 10950: 0.1442
Loss at step 10960: 0.0599
Loss at step 10970: 0.1286
Loss at step 10980: 0.1890
Loss at step 10990: 0.1992
Loss at step 11000: 0.0984
Loss at step 11010: 0.1153
Loss at step 11020: 0.0807
Loss at step 11030: 0.0640
Loss at step 11040: 0.0640
Loss at step 11050: 0.1161
Loss at step 11060: 0.0314
Loss at step 11070: 0.1167
Loss at step 11080: 0.0335
Loss at step 11090: 0.1965
Loss at step 11100: 0.0959
Loss at step 11110: 0.0644
Loss at step 11120: 0.0937
Loss at step 11130: 0.0740
Loss at step 11140: 0.2298
Loss at step 11150: 0.0871
Loss at step 11160: 0.0360
Loss at step 11170: 0.0809
Loss at step 11180: 0.1180
Loss at step 11190: 0.0819
Loss at step 11200: 0.1033
Loss at step 11210: 0.0713
Loss at step 11220: 0.1596
Loss at step 11230: 0.1113
Loss at step 11240: 0.0729
Loss at step 11250: 0.2618
Loss at step 11260: 0.0828
Loss at step 11270: 0.1786
Loss at step 11280: 0.0587
Loss at step 11290: 0.1551
Loss at step 11300: 0.0770
Loss at step 11310: 0.0977
Loss at step 11320: 0.0718
Loss at step 11330: 0.1214
Loss at step 11340: 0.0636
Loss at step 11350: 0.1073
Loss at step 11360: 0.1260
Loss at step 11370: 0.0906
Loss at step 11380: 0.2125
Loss at step 11390: 0.0501
Loss at step 11400: 0.1205
Loss at step 11410: 0.0366
Loss at step 11420: 0.0737
Loss at step 11430: 0.1271
Loss at step 11440: 0.3210
Loss at step 11450: 0.0579
Loss at step 11460: 0.1014
Loss at step 11470: 0.0209
Loss at step 11480: 0.2498
Loss at step 11490: 0.2131
Loss at step 11500: 0.1699
Loss at step 11510: 0.0535
Loss at step 11520: 0.1418
Loss at step 11530: 0.1187
Loss at step 11540: 0.0571
Loss at step 11550: 0.0780
Loss at step 11560: 0.1253
Loss at step 11570: 0.1730
Loss at step 11580: 0.1443
Loss at step 11590: 0.0910
Loss at step 11600: 0.0874
Loss at step 11610: 0.1692
Loss at step 11620: 0.0325
Loss at step 11630: 0.2004
Loss at step 11640: 0.1539
Loss at step 11650: 0.1215
Loss at step 11660: 0.1001
Loss at step 11670: 0.1527
Loss at step 11680: 0.1141
Loss at step 11690: 0.1660
Loss at step 11700: 0.0681
Loss at step 11710: 0.2140
Loss at step 11720: 0.0824
Loss at step 11730: 0.0669
Loss at step 11740: 0.0981
Loss at step 11750: 0.1445
Loss at step 11760: 0.1786
Loss at step 11770: 0.0795
Loss at step 11780: 0.0595
Loss at step 11790: 0.0702
Loss at step 11800: 0.1568
Loss at step 11810: 0.1654
Loss at step 11820: 0.0752
Loss at step 11830: 0.2297
Loss at step 11840: 0.0929
Loss at step 11850: 0.1829
Loss at step 11860: 0.1196
Loss at step 11870: 0.0331
Loss at step 11880: 0.0409
Loss at step 11890: 0.0558
Loss at step 11900: 0.0704
Loss at step 11910: 0.1034
Loss at step 11920: 0.0996
Loss at step 11930: 0.0324
Loss at step 11940: 0.0550
Loss at step 11950: 0.1696
Loss at step 11960: 0.0371
Loss at step 11970: 0.0786
Loss at step 11980: 0.1726
Loss at step 11990: 0.0752
Loss at step 12000: 0.1045
Loss at step 12010: 0.0491
Loss at step 12020: 0.0675
Loss at step 12030: 0.0663
Loss at step 12040: 0.0714
Loss at step 12050: 0.0624
Loss at step 12060: 0.1786
Loss at step 12070: 0.2129
Loss at step 12080: 0.0722
Loss at step 12090: 0.1504
Loss at step 12100: 0.1921
Loss at step 12110: 0.1530
Loss at step 12120: 0.0410
Loss at step 12130: 0.1564
Loss at step 12140: 0.1523
Loss at step 12150: 0.1801
Loss at step 12160: 0.0674
Loss at step 12170: 0.0343
Loss at step 12180: 0.1227
Loss at step 12190: 0.1326
Loss at step 12200: 0.0343
Loss at step 12210: 0.0854
Loss at step 12220: 0.1018
Loss at step 12230: 0.1326
Loss at step 12240: 0.1546
Loss at step 12250: 0.1490
Loss at step 12260: 0.1222
Loss at step 12270: 0.2073
Loss at step 12280: 0.0971
Loss at step 12290: 0.0895
Loss at step 12300: 0.0629
Loss at step 12310: 0.1442
Loss at step 12320: 0.1254
Loss at step 12330: 0.1034
Loss at step 12340: 0.1675
Loss at step 12350: 0.1632
Loss at step 12360: 0.2096
Loss at step 12370: 0.0561
Loss at step 12380: 0.1310
Loss at step 12390: 0.0504
Loss at step 12400: 0.2738
Loss at step 12410: 0.0637
Loss at step 12420: 0.0864
Loss at step 12430: 0.1804
Loss at step 12440: 0.0390
Loss at step 12450: 0.0692
Loss at step 12460: 0.0597
Loss at step 12470: 0.1634
Loss at step 12480: 0.0860
Loss at step 12490: 0.2148
Loss at step 12500: 0.0870
Loss at step 12510: 0.0808
Loss at step 12520: 0.0677
Loss at step 12530: 0.0347
Loss at step 12540: 0.0588
Loss at step 12550: 0.1901
Loss at step 12560: 0.1123
Loss at step 12570: 0.0448
Loss at step 12580: 0.0586
Loss at step 12590: 0.0691
Loss at step 12600: 0.0966
Loss at step 12610: 0.0709
Loss at step 12620: 0.1736
Loss at step 12630: 0.0578
Loss at step 12640: 0.1732
Loss at step 12650: 0.0214
Loss at step 12660: 0.0397
Loss at step 12670: 0.1080
Loss at step 12680: 0.0173
Loss at step 12690: 0.0911
Loss at step 12700: 0.0137
Loss at step 12710: 0.0450
Loss at step 12720: 0.0560
Loss at step 12730: 0.1238
Loss at step 12740: 0.0630
Loss at step 12750: 0.2022
Loss at step 12760: 0.0752
Loss at step 12770: 0.1146
Loss at step 12780: 0.1122
Loss at step 12790: 0.0968
Loss at step 12800: 0.0741
Loss at step 12810: 0.1283
Loss at step 12820: 0.0803
Loss at step 12830: 0.0402
Loss at step 12840: 0.1502
Loss at step 12850: 0.1659
Loss at step 12860: 0.0307
Loss at step 12870: 0.0576
Loss at step 12880: 0.0272
Loss at step 12890: 0.0836
Loss at step 12900: 0.1129
Loss at step 12910: 0.0462
Loss at step 12920: 0.0560
Loss at step 12930: 0.0990
Loss at step 12940: 0.0501
Loss at step 12950: 0.0289
Loss at step 12960: 0.0518
Loss at step 12970: 0.1547
Loss at step 12980: 0.1036
Loss at step 12990: 0.0139
Loss at step 13000: 0.0752
Loss at step 13010: 0.0662
Loss at step 13020: 0.0591
Loss at step 13030: 0.1924
Loss at step 13040: 0.0973
Loss at step 13050: 0.1045
Loss at step 13060: 0.0413
Loss at step 13070: 0.1085
Loss at step 13080: 0.0277
Loss at step 13090: 0.0576
Loss at step 13100: 0.0500
Loss at step 13110: 0.0073
Loss at step 13120: 0.0698
Loss at step 13130: 0.0677
Loss at step 13140: 0.0576
Loss at step 13150: 0.2809
Loss at step 13160: 0.0476
Loss at step 13170: 0.1524
Loss at step 13180: 0.0812
Loss at step 13190: 0.1096
Loss at step 13200: 0.2238
Loss at step 13210: 0.0330
Loss at step 13220: 0.1419
Loss at step 13230: 0.1235
Loss at step 13240: 0.1357
Loss at step 13250: 0.0805
Loss at step 13260: 0.1308
Loss at step 13270: 0.0538
Loss at step 13280: 0.0718
Loss at step 13290: 0.0445
Loss at step 13300: 0.0738
Loss at step 13310: 0.1065
Loss at step 13320: 0.1194
Loss at step 13330: 0.1599
Loss at step 13340: 0.1198
Loss at step 13350: 0.1696
Loss at step 13360: 0.1662
Loss at step 13370: 0.1582
Loss at step 13380: 0.1062
Loss at step 13390: 0.0307
Loss at step 13400: 0.1200
Loss at step 13410: 0.0453
Loss at step 13420: 0.1000
Loss at step 13430: 0.0378
Loss at step 13440: 0.1937
Loss at step 13450: 0.0505
Loss at step 13460: 0.1301
Loss at step 13470: 0.0780
Loss at step 13480: 0.0414
Loss at step 13490: 0.1066
Loss at step 13500: 0.0569
Loss at step 13510: 0.0915
Loss at step 13520: 0.0838
Loss at step 13530: 0.0766
Loss at step 13540: 0.0513
Loss at step 13550: 0.0322
Loss at step 13560: 0.0412
Loss at step 13570: 0.0873
Loss at step 13580: 0.1614
Loss at step 13590: 0.0246
Loss at step 13600: 0.0893
Loss at step 13610: 0.0932
Loss at step 13620: 0.0815
Loss at step 13630: 0.1055
Loss at step 13640: 0.0995
Loss at step 13650: 0.1450
Loss at step 13660: 0.0552
Loss at step 13670: 0.0823
Loss at step 13680: 0.1717
Loss at step 13690: 0.0574
Loss at step 13700: 0.0365
Loss at step 13710: 0.0830
Loss at step 13720: 0.1292
Loss at step 13730: 0.1119
Loss at step 13740: 0.2806
Loss at step 13750: 0.0466
Loss at step 13760: 0.0932
Loss at step 13770: 0.0988
Loss at step 13780: 0.1298
Loss at step 13790: 0.0602
Loss at step 13800: 0.1615
Loss at step 13810: 0.0523
Loss at step 13820: 0.0834
Loss at step 13830: 0.0379
Loss at step 13840: 0.0891
Loss at step 13850: 0.1206
Loss at step 13860: 0.1107
Loss at step 13870: 0.1131
Loss at step 13880: 0.1574
Loss at step 13890: 0.0995
Loss at step 13900: 0.0745
Loss at step 13910: 0.0971
Loss at step 13920: 0.1980
Loss at step 13930: 0.1537
Loss at step 13940: 0.1902
Loss at step 13950: 0.1646
Loss at step 13960: 0.1903
Loss at step 13970: 0.2189
Loss at step 13980: 0.0830
Loss at step 13990: 0.1375
Loss at step 14000: 0.0352
Loss at step 14010: 0.1321
Loss at step 14020: 0.1825
Loss at step 14030: 0.0810
Loss at step 14040: 0.0709
Loss at step 14050: 0.0162
Loss at step 14060: 0.0935
Loss at step 14070: 0.0682
Loss at step 14080: 0.1603
Loss at step 14090: 0.0498
Loss at step 14100: 0.2417
Loss at step 14110: 0.0333
Loss at step 14120: 0.1441
Loss at step 14130: 0.0485
Loss at step 14140: 0.0838
Loss at step 14150: 0.0281
Loss at step 14160: 0.0573
Loss at step 14170: 0.0434
Loss at step 14180: 0.1437
Loss at step 14190: 0.0937
Loss at step 14200: 0.0596
Loss at step 14210: 0.3109
Loss at step 14220: 0.1032
Loss at step 14230: 0.0505
Loss at step 14240: 0.0763
Loss at step 14250: 0.0782
Loss at step 14260: 0.0739
Loss at step 14270: 0.0795
Loss at step 14280: 0.0287
Loss at step 14290: 0.1087
Loss at step 14300: 0.0613
Loss at step 14310: 0.0753
Loss at step 14320: 0.0200
Loss at step 14330: 0.0552
Loss at step 14340: 0.0151
Loss at step 14350: 0.0698
Loss at step 14360: 0.0226
Loss at step 14370: 0.0582
Loss at step 14380: 0.1035
Loss at step 14390: 0.0157
Loss at step 14400: 0.1635
Loss at step 14410: 0.1444
Loss at step 14420: 0.0212
Loss at step 14430: 0.0375
Loss at step 14440: 0.0165
Loss at step 14450: 0.0954
Loss at step 14460: 0.1287
Loss at step 14470: 0.0987
Loss at step 14480: 0.0224
Loss at step 14490: 0.1127
Loss at step 14500: 0.0962
Loss at step 14510: 0.0367
Loss at step 14520: 0.0386
Loss at step 14530: 0.0247
Loss at step 14540: 0.1495
Loss at step 14550: 0.0261
Loss at step 14560: 0.1274
Loss at step 14570: 0.1850
Loss at step 14580: 0.0237
Loss at step 14590: 0.1355
Loss at step 14600: 0.0616
Loss at step 14610: 0.1178
Loss at step 14620: 0.1372
Loss at step 14630: 0.1227
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
Sample 116739 of the training set: {'input_ids': [101, 5821, 1024, 2057, 2031, 2005, 1996, 3945, 3853, 1012, 102, 1996, 2177, 2787, 2006, 1996, 8773, 1997, 1996, 6556, 2491, 1999, 1037, 5415, 6556, 2491, 2029, 2001, 3733, 2000, 2224, 2009, 1998, 2009, 2009, 3733, 2000, 2022, 2881, 1012, 2036, 1010, 1996, 5310, 8278, 2245, 2027, 2323, 2069, 2031, 1037, 2843, 1997, 27662, 3898, 1012, 2027, 2036, 3373, 2008, 2027, 2071, 2224, 2019, 4469, 3853, 2029, 2071, 2022, 2881, 2005, 2547, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 575970
  Num Epochs = 3
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 27000
