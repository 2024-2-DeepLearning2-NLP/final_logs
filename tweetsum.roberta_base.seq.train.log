Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x79094d51eee0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 544, 147, 51, 32, 3276, 7, 1514, 4591, 8, 67, 4496, 14, 42, 16, 45, 447, 4, 50118, 45443, 27620, 13, 5, 1254, 8, 553, 7, 18695, 455, 766, 8, 1286, 33000, 4, 2, 2, 0, 100, 23126, 216, 596, 381, 6526, 2812, 20524, 16, 5, 129, 248, 18941, 2214, 38, 64, 465, 15, 787, 1225, 4432, 4652, 2, 0, 1039, 37401, 20695, 11468, 96, 25685, 328, 2615, 47, 1137, 201, 99, 247, 110, 1316, 16, 278, 7, 116, 166, 581, 1649, 383, 66, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 315, 532, 328, 2, 0, 1039, 37401, 20695, 274, 35997, 7344, 52, 581, 28, 441, 7, 33, 70, 9, 49, 3686, 1010, 6, 53, 89, 18, 8574, 59, 13703, 1383, 259, 35, 1205, 640, 90, 4, 876, 73, 288, 118, 398, 534, 642, 757, 257, 32259, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 20227, 1942, 594, 328, 2, 0, 1039, 37401, 20695, 12487, 13095, 328, 370, 216, 147, 7, 465, 201, 114, 89, 18, 932, 1493, 47, 240, 244, 19, 4, 1205, 640, 90, 4, 876, 73, 257, 717, 330, 7083, 282, 46073, 30083, 17841, 23171, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 3837, 47, 328, 17841, 14285, 2, 0, 1039, 37401, 20695, 370, 214, 460, 2814, 328, 17841, 23171, 1589, 3764, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.0836
Loss at step 20: 1.0351
Loss at step 30: 1.0434
Loss at step 40: 1.1058
Loss at step 50: 1.1191
Loss at step 60: 1.0741
Loss at step 70: 1.1018
Loss at step 80: 1.1172
Loss at step 90: 1.0986
Loss at step 100: 1.1006
Loss at step 110: 1.1470
Loss at step 120: 1.1196
Loss at step 130: 1.1133
Loss at step 140: 1.1357
Loss at step 150: 1.0644
Loss at step 160: 1.0965
Loss at step 170: 1.0954
Loss at step 180: 1.0850
Loss at step 190: 1.0936
Loss at step 200: 1.1228
Loss at step 210: 1.0687
Loss at step 220: 1.0868
Loss at step 230: 1.1023
Loss at step 240: 1.1183
Loss at step 250: 1.0414
Loss at step 260: 1.0735
Loss at step 270: 1.0968
Loss at step 280: 1.1182
Loss at step 290: 1.0736
Loss at step 300: 1.0729
Loss at step 310: 1.1516
Loss at step 320: 1.0836
Loss at step 330: 1.0956
Loss at step 340: 1.1318
Loss at step 350: 1.0658
Loss at step 360: 1.0825
Loss at step 370: 1.0577
Loss at step 380: 1.0866
Loss at step 390: 1.0535
Loss at step 400: 1.0456
Loss at step 410: 1.0732
Loss at step 420: 1.1040
Loss at step 430: 1.0593
Loss at step 440: 1.0692
Loss at step 450: 1.0958
Loss at step 460: 1.0384
Loss at step 470: 1.1536
Loss at step 480: 1.1237
Loss at step 490: 1.0723
Loss at step 500: 1.0905
Loss at step 510: 1.0526
Loss at step 520: 1.0844
Loss at step 530: 1.0985
Loss at step 540: 1.1370
Loss at step 550: 1.1036
Loss at step 560: 1.1183
Loss at step 570: 1.0826
Loss at step 580: 1.0973
Loss at step 590: 1.0726
Loss at step 600: 1.0901
Loss at step 610: 1.0303
Loss at step 620: 1.0696
Loss at step 630: 1.0799
Loss at step 640: 1.1041
Loss at step 650: 1.0792
Loss at step 660: 1.1043
Loss at step 670: 1.1169
Loss at step 680: 1.0592
Loss at step 690: 1.0580
Loss at step 700: 1.0931
Loss at step 710: 1.1097
Loss at step 720: 1.0835
Loss at step 730: 1.0747
Loss at step 740: 1.1139
Loss at step 750: 1.0752
Loss at step 760: 1.0929
Loss at step 770: 1.0501
Loss at step 780: 1.1003
Loss at step 790: 1.1072
Loss at step 800: 1.1330
Loss at step 810: 1.1273
Loss at step 820: 1.0982
Loss at step 830: 1.0988
Loss at step 840: 1.0879
Loss at step 850: 1.0962
Loss at step 860: 1.0683
Loss at step 870: 1.1317
Loss at step 880: 1.0909
Loss at step 890: 1.0752
Loss at step 900: 1.0613
Loss at step 910: 1.0892
Loss at step 920: 1.0741
Loss at step 930: 1.0833
Loss at step 940: 1.0590
Loss at step 950: 1.0590
Loss at step 960: 1.0762
Loss at step 970: 1.0791
Loss at step 980: 1.1017
Loss at step 990: 1.1016
Loss at step 1000: 1.0729
Loss at step 1010: 1.0946
Loss at step 1020: 1.1049
Loss at step 1030: 1.0925
Loss at step 1040: 1.0843
Loss at step 1050: 1.1123
Loss at step 1060: 1.0754
Loss at step 1070: 1.0972
Loss at step 1080: 1.0916
Loss at step 1090: 1.1001
Loss at step 1100: 1.1054
Loss at step 1110: 1.0776
Loss at step 1120: 1.0888
Loss at step 1130: 1.0659
Loss at step 1140: 1.0939
Loss at step 1150: 1.1140
Loss at step 1160: 1.1044
Loss at step 1170: 1.1112
Loss at step 1180: 1.0689
Loss at step 1190: 1.0931
Loss at step 1200: 1.0970
Loss at step 1210: 1.0651
Loss at step 1220: 1.1085
Loss at step 1230: 1.1184
Loss at step 1240: 1.0876
Loss at step 1250: 1.0666
Loss at step 1260: 1.1024
Loss at step 1270: 1.1511
Loss at step 1280: 1.0638
Loss at step 1290: 1.1009
Loss at step 1300: 1.0810
Loss at step 1310: 1.1085
Loss at step 1320: 1.1049
Loss at step 1330: 1.0900
Loss at step 1340: 1.0654
Loss at step 1350: 1.0982
Loss at step 1360: 1.1011
Loss at step 1370: 1.0691
Loss at step 1380: 1.0724
Loss at step 1390: 1.1219
Loss at step 1400: 1.1043
Loss at step 1410: 1.0945
Loss at step 1420: 1.1053
Loss at step 1430: 1.0722
Loss at step 1440: 1.0814
Loss at step 1450: 1.1153
Loss at step 1460: 1.0975
Loss at step 1470: 1.1208
Loss at step 1480: 1.0822
Loss at step 1490: 1.0698
Loss at step 1500: 1.0969
Loss at step 1510: 1.0519
Loss at step 1520: 1.1150
Loss at step 1530: 1.0631
Loss at step 1540: 1.0960
Loss at step 1550: 1.1062
Loss at step 1560: 1.0898
Loss at step 1570: 1.0944
Loss at step 1580: 1.0912
Loss at step 1590: 1.0954
Loss at step 1600: 1.1146
Loss at step 1610: 1.1002
Loss at step 1620: 1.1276
Loss at step 1630: 1.0700
Loss at step 1640: 1.0940
Loss at step 1650: 1.0951
Loss at step 1660: 1.0994
Loss at step 1670: 1.0677
Loss at step 1680: 1.1089
Loss at step 1690: 1.0942
Loss at step 1700: 1.0849
Loss at step 1710: 1.0809
Loss at step 1720: 1.1109
Loss at step 1730: 1.0824
Loss at step 1740: 1.0923
Loss at step 1750: 1.0860
Loss at step 1760: 1.1128
Loss at step 1770: 1.0783
Loss at step 1780: 1.0917
Loss at step 1790: 1.0735
Loss at step 1800: 1.1231
Loss at step 1810: 1.1143
Loss at step 1820: 1.0438
Loss at step 1830: 1.0678
Loss at step 1840: 1.0977
Loss at step 1850: 1.0953
Loss at step 1860: 1.0706
Loss at step 1870: 1.1225
Loss at step 1880: 1.0737
Loss at step 1890: 1.0642
Loss at step 1900: 1.1075
Loss at step 1910: 1.0875
Loss at step 1920: 1.0750
Loss at step 1930: 1.1031
Loss at step 1940: 1.0704
Loss at step 1950: 1.0875
Loss at step 1960: 1.0748
Loss at step 1970: 1.0955
Loss at step 1980: 1.0782
Loss at step 1990: 1.1135
Loss at step 2000: 1.0680
Loss at step 2010: 1.0735
Loss at step 2020: 1.0821
Loss at step 2030: 1.0683
Loss at step 2040: 1.0728
Loss at step 2050: 1.0711
Loss at step 2060: 1.0896
Loss at step 2070: 1.0782
Loss at step 2080: 1.0557
Loss at step 2090: 1.0897
Loss at step 2100: 1.0977
Loss at step 2110: 1.0283
Loss at step 2120: 1.1073
Loss at step 2130: 1.0970
Loss at step 2140: 1.0472
Loss at step 2150: 1.0953
Loss at step 2160: 1.0653
Loss at step 2170: 1.1308
Loss at step 2180: 1.0592
Loss at step 2190: 1.1103
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.370304, 'precision': [0.0, 0.370304, 0.0], 'recall': [0.0, 1.0, 0.0], 'f1': [0.0, 0.540469, 0.0]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x75aa27b09d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 4092 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 544, 147, 51, 32, 3276, 7, 1514, 4591, 8, 67, 4496, 14, 42, 16, 45, 447, 4, 50118, 45443, 27620, 13, 5, 1254, 8, 553, 7, 18695, 455, 766, 8, 1286, 33000, 4, 2, 2, 0, 100, 23126, 216, 596, 381, 6526, 2812, 20524, 16, 5, 129, 248, 18941, 2214, 38, 64, 465, 15, 787, 1225, 4432, 4652, 2, 0, 1039, 37401, 20695, 11468, 96, 25685, 328, 2615, 47, 1137, 201, 99, 247, 110, 1316, 16, 278, 7, 116, 166, 581, 1649, 383, 66, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 315, 532, 328, 2, 0, 1039, 37401, 20695, 274, 35997, 7344, 52, 581, 28, 441, 7, 33, 70, 9, 49, 3686, 1010, 6, 53, 89, 18, 8574, 59, 13703, 1383, 259, 35, 1205, 640, 90, 4, 876, 73, 288, 118, 398, 534, 642, 757, 257, 32259, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 20227, 1942, 594, 328, 2, 0, 1039, 37401, 20695, 12487, 13095, 328, 370, 216, 147, 7, 465, 201, 114, 89, 18, 932, 1493, 47, 240, 244, 19, 4, 1205, 640, 90, 4, 876, 73, 257, 717, 330, 7083, 282, 46073, 30083, 17841, 23171, 1589, 3764, 2, 0, 1039, 32110, 4591, 347, 5347, 3837, 47, 328, 17841, 14285, 2, 0, 1039, 37401, 20695, 370, 214, 460, 2814, 328, 17841, 23171, 1589, 3764, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 0, 2, 0, 1, 0, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.0836
Loss at step 20: 1.0351
Loss at step 30: 1.0434
Loss at step 40: 1.1058
Loss at step 50: 1.1191
Loss at step 60: 1.0741
Loss at step 70: 1.1018
Loss at step 80: 1.1172
Loss at step 90: 1.0986
Loss at step 100: 1.1006
Loss at step 110: 1.1470
Loss at step 120: 1.1196
Loss at step 130: 1.1133
Loss at step 140: 1.1357
Loss at step 150: 1.0644
Loss at step 160: 1.0965
Loss at step 170: 1.0954
Loss at step 180: 1.0850
Loss at step 190: 1.0936
Loss at step 200: 1.1228
Loss at step 210: 1.0687
Loss at step 220: 1.0868
Loss at step 230: 1.1023
Loss at step 240: 1.1183
Loss at step 250: 1.0414
Loss at step 260: 1.0735
Loss at step 270: 1.0968
Loss at step 280: 1.1182
Loss at step 290: 1.0736
Loss at step 300: 1.0729
Loss at step 310: 1.1516
Loss at step 320: 1.0836
Loss at step 330: 1.0956
Loss at step 340: 1.1318
Loss at step 350: 1.0658
Loss at step 360: 1.0825
Loss at step 370: 1.0577
Loss at step 380: 1.0866
Loss at step 390: 1.0535
Loss at step 400: 1.0456
Loss at step 410: 1.0732
Loss at step 420: 1.1040
Loss at step 430: 1.0593
Loss at step 440: 1.0692
Loss at step 450: 1.0958
Loss at step 460: 1.0384
Loss at step 470: 1.1536
Loss at step 480: 1.1237
Loss at step 490: 1.0723
Loss at step 500: 1.0905
Loss at step 510: 1.0526
Loss at step 520: 1.0844
Loss at step 530: 1.0985
Loss at step 540: 1.1370
Loss at step 550: 1.1036
Loss at step 560: 1.1183
Loss at step 570: 1.0826
Loss at step 580: 1.0973
Loss at step 590: 1.0726
Loss at step 600: 1.0901
Loss at step 610: 1.0303
Loss at step 620: 1.0696
Loss at step 630: 1.0799
Loss at step 640: 1.1041
Loss at step 650: 1.0792
Loss at step 660: 1.1043
Loss at step 670: 1.1169
Loss at step 680: 1.0592
Loss at step 690: 1.0580
Loss at step 700: 1.0931
Loss at step 710: 1.1097
Loss at step 720: 1.0835
Loss at step 730: 1.0747
Loss at step 740: 1.1139
Loss at step 750: 1.0752
Loss at step 760: 1.0929
Loss at step 770: 1.0501
Loss at step 780: 1.1003
Loss at step 790: 1.1072
Loss at step 800: 1.1330
Loss at step 810: 1.1273
Loss at step 820: 1.0982
Loss at step 830: 1.0988
Loss at step 840: 1.0879
Loss at step 850: 1.0962
Loss at step 860: 1.0683
Loss at step 870: 1.1317
Loss at step 880: 1.0909
Loss at step 890: 1.0752
Loss at step 900: 1.0613
Loss at step 910: 1.0892
Loss at step 920: 1.0741
Loss at step 930: 1.0833
Loss at step 940: 1.0590
Loss at step 950: 1.0590
Loss at step 960: 1.0762
Loss at step 970: 1.0791
Loss at step 980: 1.1017
Loss at step 990: 1.1016
Loss at step 1000: 1.0729
Loss at step 1010: 1.0946
Loss at step 1020: 1.1049
Loss at step 1030: 1.0925
Loss at step 1040: 1.0843
Loss at step 1050: 1.1123
Loss at step 1060: 1.0754
Loss at step 1070: 1.0972
Loss at step 1080: 1.0916
Loss at step 1090: 1.1001
Loss at step 1100: 1.1054
Loss at step 1110: 1.0776
Loss at step 1120: 1.0888
Loss at step 1130: 1.0659
Loss at step 1140: 1.0939
Loss at step 1150: 1.1140
Loss at step 1160: 1.1044
Loss at step 1170: 1.1112
Loss at step 1180: 1.0689
Loss at step 1190: 1.0931
Loss at step 1200: 1.0970
Loss at step 1210: 1.0651
Loss at step 1220: 1.1085
Loss at step 1230: 1.1184
Loss at step 1240: 1.0876
Loss at step 1250: 1.0666
Loss at step 1260: 1.1024
Loss at step 1270: 1.1511
Loss at step 1280: 1.0638
Loss at step 1290: 1.1009
Loss at step 1300: 1.0810
Loss at step 1310: 1.1085
Loss at step 1320: 1.1049
Loss at step 1330: 1.0900
Loss at step 1340: 1.0654
Loss at step 1350: 1.0982
Loss at step 1360: 1.1011
Loss at step 1370: 1.0691
Loss at step 1380: 1.0724
Loss at step 1390: 1.1219
Loss at step 1400: 1.1043
Loss at step 1410: 1.0945
Loss at step 1420: 1.1053
Loss at step 1430: 1.0722
Loss at step 1440: 1.0814
Loss at step 1450: 1.1153
Loss at step 1460: 1.0975
Loss at step 1470: 1.1208
Loss at step 1480: 1.0822
Loss at step 1490: 1.0698
Loss at step 1500: 1.0969
Loss at step 1510: 1.0519
Loss at step 1520: 1.1150
Loss at step 1530: 1.0631
Loss at step 1540: 1.0960
Loss at step 1550: 1.1062
Loss at step 1560: 1.0898
Loss at step 1570: 1.0944
Loss at step 1580: 1.0912
Loss at step 1590: 1.0954
Loss at step 1600: 1.1146
Loss at step 1610: 1.1002
Loss at step 1620: 1.1276
Loss at step 1630: 1.0700
Loss at step 1640: 1.0940
Loss at step 1650: 1.0951
Loss at step 1660: 1.0994
Loss at step 1670: 1.0677
Loss at step 1680: 1.1089
Loss at step 1690: 1.0942
Loss at step 1700: 1.0849
Loss at step 1710: 1.0809
Loss at step 1720: 1.1109
Loss at step 1730: 1.0824
Loss at step 1740: 1.0923
Loss at step 1750: 1.0860
Loss at step 1760: 1.1128
Loss at step 1770: 1.0783
Loss at step 1780: 1.0917
Loss at step 1790: 1.0735
Loss at step 1800: 1.1231
Loss at step 1810: 1.1143
Loss at step 1820: 1.0438
Loss at step 1830: 1.0678
Loss at step 1840: 1.0977
Loss at step 1850: 1.0953
Loss at step 1860: 1.0706
Loss at step 1870: 1.1225
Loss at step 1880: 1.0737
Loss at step 1890: 1.0642
Loss at step 1900: 1.1075
Loss at step 1910: 1.0875
Loss at step 1920: 1.0750
Loss at step 1930: 1.1031
Loss at step 1940: 1.0704
Loss at step 1950: 1.0875
Loss at step 1960: 1.0748
Loss at step 1970: 1.0955
Loss at step 1980: 1.0782
Loss at step 1990: 1.1135
Loss at step 2000: 1.0680
Loss at step 2010: 1.0735
Loss at step 2020: 1.0821
Loss at step 2030: 1.0683
Loss at step 2040: 1.0728
Loss at step 2050: 1.0711
Loss at step 2060: 1.0896
Loss at step 2070: 1.0782
Loss at step 2080: 1.0557
Loss at step 2090: 1.0897
Loss at step 2100: 1.0977
Loss at step 2110: 1.0283
Loss at step 2120: 1.1073
Loss at step 2130: 1.0970
Loss at step 2140: 1.0472
Loss at step 2150: 1.0953
Loss at step 2160: 1.0653
Loss at step 2170: 1.1308
Loss at step 2180: 1.0592
Loss at step 2190: 1.1103
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.370304, 'precision': [0.0, 0.370304, 0.0], 'recall': [0.0, 1.0, 0.0], 'f1': [0.0, 0.540469, 0.0]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x747df47b1160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 44799, 16, 13689, 59, 5, 2373, 544, 147, 37, 56, 5, 177, 8, 67, 4496, 14, 37, 56, 41, 2935, 4, 50118, 45443, 5034, 7, 458, 5, 1254, 59, 5, 576, 3104, 8, 3496, 14, 51, 40, 33, 10, 2789, 356, 4, 2, 2, 0, 1039, 22960, 31297, 653, 1102, 7, 195, 4377, 23, 1764, 506, 4926, 116, 2, 0, 1039, 4390, 1549, 6668, 787, 22960, 31297, 854, 108, 10913, 21766, 6, 1437, 2, 0, 243, 21, 10, 6174, 1437, 2, 0, 133, 6174, 34, 1249, 2, 0, 1039, 11674, 6905, 1215, 16431, 787, 22960, 31297, 653, 21, 145, 7715, 116, 2, 0, 1039, 4390, 1549, 6668, 787, 22960, 31297, 1681, 414, 19489, 9, 195, 534, 11742, 23, 1764, 506, 2, 0, 1039, 11674, 6905, 1215, 16431, 787, 22960, 31297, 38, 33976, 206, 14, 18, 10, 6174, 4, 653, 18, 110, 8515, 9, 6174, 116, 2, 0, 1039, 11674, 6905, 1215, 16431, 787, 22960, 31297, 318, 195, 4377, 21, 5, 46964, 21438, 3069, 204, 22900, 2982, 8515, 24, 839, 58, 1622, 1331, 3226, 5, 544, 787, 10, 86, 5120, 4, 318, 1717, 885, 254, 46964, 179, 24, 890, 202, 741, 22345, 23200, 1205, 640, 90, 4, 876, 73, 763, 139, 8105, 417, 246, 329, 306, 282, 2, 0, 1039, 11674, 6905, 1215, 16431, 787, 22960, 31297, 3401, 52, 236, 24, 124, 328, 22, 35396, 6457, 113, 456, 328, 27785, 2, 0, 1039, 4390, 1549, 6668, 787, 22960, 31297, 2647, 829, 4, 3837, 13, 110, 773, 8, 10791, 45868, 2, 0, 1039, 11674, 6905, 1215, 16431, 787, 22960, 31297, 30020, 62, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 0, 1, 2, 0, 0, 1, 1, 2, 1, 0]}.
***** Running training *****
  Num examples = 8790
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 6594
Loss at step 10: 1.0838
Loss at step 20: 1.0808
Loss at step 30: 1.1485
Loss at step 40: 1.0675
Loss at step 50: 1.1039
Loss at step 60: 1.1047
Loss at step 70: 1.0936
Loss at step 80: 1.1330
Loss at step 90: 1.1085
Loss at step 100: 1.0887
Loss at step 110: 1.0642
Loss at step 120: 1.0338
Loss at step 130: 1.0944
Loss at step 140: 0.9935
Loss at step 150: 1.0563
Loss at step 160: 1.1394
Loss at step 170: 1.0140
Loss at step 180: 0.9327
Loss at step 190: 1.0661
Loss at step 200: 0.8914
Loss at step 210: 1.1231
Loss at step 220: 0.9505
Loss at step 230: 0.7884
Loss at step 240: 0.9851
Loss at step 250: 0.9437
Loss at step 260: 1.0748
Loss at step 270: 0.9333
Loss at step 280: 0.8160
Loss at step 290: 0.8930
Loss at step 300: 0.8546
Loss at step 310: 0.8551
Loss at step 320: 0.9243
Loss at step 330: 0.9546
Loss at step 340: 0.7807
Loss at step 350: 1.0424
Loss at step 360: 1.1735
Loss at step 370: 0.8760
Loss at step 380: 0.8504
Loss at step 390: 1.0999
Loss at step 400: 0.9921
Loss at step 410: 0.7634
Loss at step 420: 0.8460
Loss at step 430: 0.8962
Loss at step 440: 0.9460
Loss at step 450: 1.1315
Loss at step 460: 0.8803
Loss at step 470: 0.6394
Loss at step 480: 0.9814
Loss at step 490: 1.0410
Loss at step 500: 0.7995
Loss at step 510: 0.6967
Loss at step 520: 0.8437
Loss at step 530: 0.7528
Loss at step 540: 0.6738
Loss at step 550: 1.1081
Loss at step 560: 0.7648
Loss at step 570: 0.8034
Loss at step 580: 0.8051
Loss at step 590: 0.9126
Loss at step 600: 0.9129
Loss at step 610: 0.5854
Loss at step 620: 0.7851
Loss at step 630: 0.8189
Loss at step 640: 0.8796
Loss at step 650: 0.9672
Loss at step 660: 0.7531
Loss at step 670: 0.9267
Loss at step 680: 0.6619
Loss at step 690: 0.5806
Loss at step 700: 1.0402
Loss at step 710: 0.8347
Loss at step 720: 0.8252
Loss at step 730: 0.5335
Loss at step 740: 0.6211
Loss at step 750: 0.8421
Loss at step 760: 0.5792
Loss at step 770: 0.5563
Loss at step 780: 0.8796
Loss at step 790: 0.4683
Loss at step 800: 0.7741
Loss at step 810: 0.8010
Loss at step 820: 0.6828
Loss at step 830: 0.5182
Loss at step 840: 0.5918
Loss at step 850: 0.7869
Loss at step 860: 0.6678
Loss at step 870: 0.7299
Loss at step 880: 0.7024
Loss at step 890: 0.6811
Loss at step 900: 0.4288
Loss at step 910: 0.6132
Loss at step 920: 1.0179
Loss at step 930: 0.6117
Loss at step 940: 1.0750
Loss at step 950: 0.5642
Loss at step 960: 0.4399
Loss at step 970: 0.5328
Loss at step 980: 0.5603
Loss at step 990: 0.7616
Loss at step 1000: 0.4515
Loss at step 1010: 0.7036
Loss at step 1020: 0.6198
Loss at step 1030: 0.6475
Loss at step 1040: 0.6220
Loss at step 1050: 0.5033
Loss at step 1060: 0.5866
Loss at step 1070: 0.5795
Loss at step 1080: 0.6595
Loss at step 1090: 0.4708
Loss at step 1100: 0.4334
Loss at step 1110: 0.6622
Loss at step 1120: 0.3138
Loss at step 1130: 0.5469
Loss at step 1140: 0.5088
Loss at step 1150: 0.4802
Loss at step 1160: 0.5150
Loss at step 1170: 0.4114
Loss at step 1180: 0.5793
Loss at step 1190: 0.5584
Loss at step 1200: 0.4205
Loss at step 1210: 0.3612
Loss at step 1220: 0.6513
Loss at step 1230: 0.5989
Loss at step 1240: 0.2653
Loss at step 1250: 0.5888
Loss at step 1260: 0.3843
Loss at step 1270: 0.4751
Loss at step 1280: 0.4472
Loss at step 1290: 0.5403
Loss at step 1300: 0.5374
Loss at step 1310: 0.2906
Loss at step 1320: 0.5298
Loss at step 1330: 0.2741
Loss at step 1340: 0.3984
Loss at step 1350: 0.5642
Loss at step 1360: 0.3689
Loss at step 1370: 0.2409
Loss at step 1380: 0.6607
Loss at step 1390: 0.3727
Loss at step 1400: 0.6482
Loss at step 1410: 0.3383
Loss at step 1420: 0.6861
Loss at step 1430: 0.3118
Loss at step 1440: 0.5037
Loss at step 1450: 0.3566
Loss at step 1460: 0.2875
Loss at step 1470: 0.3921
Loss at step 1480: 0.4197
Loss at step 1490: 0.7417
Loss at step 1500: 0.2737
Loss at step 1510: 0.5958
Loss at step 1520: 0.3167
Loss at step 1530: 0.3694
Loss at step 1540: 0.3735
Loss at step 1550: 0.1685
Loss at step 1560: 0.4618
Loss at step 1570: 0.1215
Loss at step 1580: 0.3710
Loss at step 1590: 0.2823
Loss at step 1600: 0.7136
Loss at step 1610: 0.2845
Loss at step 1620: 0.1309
Loss at step 1630: 0.4385
Loss at step 1640: 0.3846
Loss at step 1650: 0.4728
Loss at step 1660: 0.2685
Loss at step 1670: 0.5293
Loss at step 1680: 0.4858
Loss at step 1690: 0.3786
Loss at step 1700: 0.3401
Loss at step 1710: 0.2045
Loss at step 1720: 0.2053
Loss at step 1730: 0.3198
Loss at step 1740: 0.3109
Loss at step 1750: 0.1837
Loss at step 1760: 0.3332
Loss at step 1770: 0.6071
Loss at step 1780: 0.1807
Loss at step 1790: 0.2352
Loss at step 1800: 0.3135
Loss at step 1810: 0.6417
Loss at step 1820: 0.2055
Loss at step 1830: 0.5778
Loss at step 1840: 0.1774
Loss at step 1850: 0.5561
Loss at step 1860: 0.3649
Loss at step 1870: 0.2968
Loss at step 1880: 0.2073
Loss at step 1890: 0.4378
Loss at step 1900: 0.2163
Loss at step 1910: 0.4573
Loss at step 1920: 0.3731
Loss at step 1930: 0.2878
Loss at step 1940: 0.2813
Loss at step 1950: 0.2936
Loss at step 1960: 0.1895
Loss at step 1970: 0.2743
Loss at step 1980: 0.4203
Loss at step 1990: 0.6166
Loss at step 2000: 0.5141
Loss at step 2010: 0.0819
Loss at step 2020: 0.3065
Loss at step 2030: 0.3079
Loss at step 2040: 0.2110
Loss at step 2050: 0.2004
Loss at step 2060: 0.2012
Loss at step 2070: 0.4698
Loss at step 2080: 0.4627
Loss at step 2090: 0.2723
Loss at step 2100: 0.1885
Loss at step 2110: 0.1010
Loss at step 2120: 0.4303
Loss at step 2130: 0.1047
Loss at step 2140: 0.1657
Loss at step 2150: 0.4087
Loss at step 2160: 0.3735
Loss at step 2170: 0.2753
Loss at step 2180: 0.1927
Loss at step 2190: 0.2164
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.574391, 'precision': [0.698801, 0.523211, 0.525413], 'recall': [0.544458, 0.658685, 0.496815], 'f1': [0.612049, 0.583184, 0.510714]}
{'accuracy': 0.752931, 'precision': 0.525413, 'recall': 0.496815, 'f1': 0.510714, 'WordR': 0.336112}
Loss at step 2200: 0.1491
Loss at step 2210: 0.3313
Loss at step 2220: 0.3523
Loss at step 2230: 0.0851
Loss at step 2240: 0.0770
Loss at step 2250: 0.1887
Loss at step 2260: 0.0983
Loss at step 2270: 0.2295
Loss at step 2280: 0.2810
Loss at step 2290: 0.1798
Loss at step 2300: 0.1499
Loss at step 2310: 0.5100
Loss at step 2320: 0.0453
Loss at step 2330: 0.1782
Loss at step 2340: 0.2554
Loss at step 2350: 0.2168
Loss at step 2360: 0.4073
Loss at step 2370: 0.6616
Loss at step 2380: 0.2183
Loss at step 2390: 0.1212
Loss at step 2400: 0.2627
Loss at step 2410: 0.0765
Loss at step 2420: 0.2013
Loss at step 2430: 0.4981
Loss at step 2440: 0.2736
Loss at step 2450: 0.0781
Loss at step 2460: 0.0745
Loss at step 2470: 0.1037
Loss at step 2480: 0.2040
Loss at step 2490: 0.1365
Loss at step 2500: 0.7121
Loss at step 2510: 0.1519
Loss at step 2520: 0.0797
Loss at step 2530: 0.3990
Loss at step 2540: 0.1775
Loss at step 2550: 0.1544
Loss at step 2560: 0.0938
Loss at step 2570: 0.0447
Loss at step 2580: 0.1123
Loss at step 2590: 0.4441
Loss at step 2600: 0.2141
Loss at step 2610: 0.1090
Loss at step 2620: 0.0578
Loss at step 2630: 0.0166
Loss at step 2640: 0.5511
Loss at step 2650: 0.0747
Loss at step 2660: 0.1232
Loss at step 2670: 0.1543
Loss at step 2680: 0.2337
Loss at step 2690: 0.0449
Loss at step 2700: 0.2470
Loss at step 2710: 0.0486
Loss at step 2720: 0.0407
Loss at step 2730: 0.3796
Loss at step 2740: 0.1021
Loss at step 2750: 0.1742
Loss at step 2760: 0.2894
Loss at step 2770: 0.2503
Loss at step 2780: 0.1495
Loss at step 2790: 0.1162
Loss at step 2800: 0.3228
Loss at step 2810: 0.1266
Loss at step 2820: 0.2384
Loss at step 2830: 0.0271
Loss at step 2840: 0.0563
Loss at step 2850: 0.0395
Loss at step 2860: 0.1446
Loss at step 2870: 0.3476
Loss at step 2880: 0.0768
Loss at step 2890: 0.2384
Loss at step 2900: 0.0442
Loss at step 2910: 0.0870
Loss at step 2920: 0.1015
Loss at step 2930: 0.2181
Loss at step 2940: 0.1356
Loss at step 2950: 0.1407
Loss at step 2960: 0.1399
Loss at step 2970: 0.0865
Loss at step 2980: 0.0371
Loss at step 2990: 0.0698
Loss at step 3000: 0.2387
Loss at step 3010: 0.2625
Loss at step 3020: 0.2787
Loss at step 3030: 0.3137
Loss at step 3040: 0.2937
Loss at step 3050: 0.1358
Loss at step 3060: 0.2814
Loss at step 3070: 0.0675
Loss at step 3080: 0.0768
Loss at step 3090: 0.1978
Loss at step 3100: 0.4824
Loss at step 3110: 0.0452
Loss at step 3120: 0.2420
Loss at step 3130: 0.1080
Loss at step 3140: 0.3762
Loss at step 3150: 0.0301
Loss at step 3160: 0.0702
Loss at step 3170: 0.1268
Loss at step 3180: 0.0396
Loss at step 3190: 0.1904
Loss at step 3200: 0.3540
Loss at step 3210: 0.1351
Loss at step 3220: 0.1238
Loss at step 3230: 0.1640
Loss at step 3240: 0.1901
Loss at step 3250: 0.1153
Loss at step 3260: 0.0116
Loss at step 3270: 0.1175
Loss at step 3280: 0.1401
Loss at step 3290: 0.0488
Loss at step 3300: 0.0711
Loss at step 3310: 0.2372
Loss at step 3320: 0.1809
Loss at step 3330: 0.1285
Loss at step 3340: 0.1485
Loss at step 3350: 0.0204
Loss at step 3360: 0.2536
Loss at step 3370: 0.0889
Loss at step 3380: 0.3431
Loss at step 3390: 0.0599
Loss at step 3400: 0.2663
Loss at step 3410: 0.2761
Loss at step 3420: 0.0943
Loss at step 3430: 0.1199
Loss at step 3440: 0.1298
Loss at step 3450: 0.0429
Loss at step 3460: 0.1666
Loss at step 3470: 0.1073
Loss at step 3480: 0.2975
Loss at step 3490: 0.0205
Loss at step 3500: 0.0309
Loss at step 3510: 0.1872
Loss at step 3520: 0.1976
Loss at step 3530: 0.0592
Loss at step 3540: 0.0812
Loss at step 3550: 0.0837
Loss at step 3560: 0.1548
Loss at step 3570: 0.0654
Loss at step 3580: 0.1033
Loss at step 3590: 0.1116
Loss at step 3600: 0.0695
Loss at step 3610: 0.0588
Loss at step 3620: 0.1241
Loss at step 3630: 0.0547
Loss at step 3640: 0.1608
Loss at step 3650: 0.1827
Loss at step 3660: 0.1534
Loss at step 3670: 0.2032
Loss at step 3680: 0.3726
Loss at step 3690: 0.1153
Loss at step 3700: 0.0282
Loss at step 3710: 0.0072
Loss at step 3720: 0.0629
Loss at step 3730: 0.1000
Loss at step 3740: 0.0087
Loss at step 3750: 0.2083
Loss at step 3760: 0.1224
Loss at step 3770: 0.2065
Loss at step 3780: 0.2272
Loss at step 3790: 0.1098
Loss at step 3800: 0.0108
Loss at step 3810: 0.1102
Loss at step 3820: 0.1549
Loss at step 3830: 0.0584
Loss at step 3840: 0.1874
Loss at step 3850: 0.0499
Loss at step 3860: 0.2194
Loss at step 3870: 0.0735
Loss at step 3880: 0.2153
Loss at step 3890: 0.1847
Loss at step 3900: 0.0093
Loss at step 3910: 0.0281
Loss at step 3920: 0.1037
Loss at step 3930: 0.1874
Loss at step 3940: 0.0257
Loss at step 3950: 0.1725
Loss at step 3960: 0.0220
Loss at step 3970: 0.3063
Loss at step 3980: 0.0934
Loss at step 3990: 0.1897
Loss at step 4000: 0.1766
Loss at step 4010: 0.1326
Loss at step 4020: 0.1536
Loss at step 4030: 0.0577
Loss at step 4040: 0.0201
Loss at step 4050: 0.2206
Loss at step 4060: 0.0133
Loss at step 4070: 0.2302
Loss at step 4080: 0.0654
Loss at step 4090: 0.0138
Loss at step 4100: 0.1971
Loss at step 4110: 0.0966
Loss at step 4120: 0.0343
Loss at step 4130: 0.1845
Loss at step 4140: 0.1902
Loss at step 4150: 0.0958
Loss at step 4160: 0.0342
Loss at step 4170: 0.1162
Loss at step 4180: 0.1109
Loss at step 4190: 0.2321
Loss at step 4200: 0.1377
Loss at step 4210: 0.0643
Loss at step 4220: 0.0120
Loss at step 4230: 0.0263
Loss at step 4240: 0.0817
Loss at step 4250: 0.2261
Loss at step 4260: 0.0420
Loss at step 4270: 0.0419
Loss at step 4280: 0.2438
Loss at step 4290: 0.0647
Loss at step 4300: 0.1401
Loss at step 4310: 0.0956
Loss at step 4320: 0.2655
Loss at step 4330: 0.1209
Loss at step 4340: 0.0506
Loss at step 4350: 0.0859
Loss at step 4360: 0.0083
Loss at step 4370: 0.0481
Loss at step 4380: 0.0516
Loss at step 4390: 0.0038
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.570033, 'precision': [0.651413, 0.527749, 0.53141], 'recall': [0.589525, 0.613636, 0.480023], 'f1': [0.618926, 0.567461, 0.504411]}
{'accuracy': 0.755185, 'precision': 0.53141, 'recall': 0.480023, 'f1': 0.504411, 'WordR': 0.177855}
Loss at step 4400: 0.1043
Loss at step 4410: 0.1380
Loss at step 4420: 0.0031
Loss at step 4430: 0.0027
Loss at step 4440: 0.1140
Loss at step 4450: 0.0878
Loss at step 4460: 0.0034
Loss at step 4470: 0.0758
Loss at step 4480: 0.0719
Loss at step 4490: 0.0096
Loss at step 4500: 0.0686
Loss at step 4510: 0.0385
Loss at step 4520: 0.3272
Loss at step 4530: 0.1111
Loss at step 4540: 0.0607
Loss at step 4550: 0.0572
Loss at step 4560: 0.0046
Loss at step 4570: 0.0099
Loss at step 4580: 0.1014
Loss at step 4590: 0.0091
Loss at step 4600: 0.1117
Loss at step 4610: 0.0379
Loss at step 4620: 0.0642
Loss at step 4630: 0.0073
Loss at step 4640: 0.0968
Loss at step 4650: 0.0589
Loss at step 4660: 0.0889
Loss at step 4670: 0.1348
Loss at step 4680: 0.0027
Loss at step 4690: 0.0020
Loss at step 4700: 0.0028
Loss at step 4710: 0.1932
Loss at step 4720: 0.0067
Loss at step 4730: 0.0888
Loss at step 4740: 0.0453
Loss at step 4750: 0.1644
Loss at step 4760: 0.0125
Loss at step 4770: 0.1434
Loss at step 4780: 0.0026
Loss at step 4790: 0.1400
Loss at step 4800: 0.0874
Loss at step 4810: 0.3381
Loss at step 4820: 0.0073
Loss at step 4830: 0.0329
Loss at step 4840: 0.1739
Loss at step 4850: 0.0076
Loss at step 4860: 0.0045
Loss at step 4870: 0.0522
Loss at step 4880: 0.0073
Loss at step 4890: 0.0194
Loss at step 4900: 0.0338
Loss at step 4910: 0.0025
Loss at step 4920: 0.0050
Loss at step 4930: 0.0056
Loss at step 4940: 0.0031
Loss at step 4950: 0.0193
Loss at step 4960: 0.1947
Loss at step 4970: 0.0420
Loss at step 4980: 0.1807
Loss at step 4990: 0.0085
Loss at step 5000: 0.0593
Loss at step 5010: 0.0072
Loss at step 5020: 0.1796
Loss at step 5030: 0.0645
Loss at step 5040: 0.0987
Loss at step 5050: 0.1108
Loss at step 5060: 0.0047
Loss at step 5070: 0.0074
Loss at step 5080: 0.0082
Loss at step 5090: 0.0014
Loss at step 5100: 0.0332
Loss at step 5110: 0.1848
Loss at step 5120: 0.2463
Loss at step 5130: 0.1065
Loss at step 5140: 0.0193
Loss at step 5150: 0.0072
Loss at step 5160: 0.2052
Loss at step 5170: 0.0019
Loss at step 5180: 0.0216
Loss at step 5190: 0.0153
Loss at step 5200: 0.0055
Loss at step 5210: 0.0018
Loss at step 5220: 0.0281
Loss at step 5230: 0.1309
Loss at step 5240: 0.3428
Loss at step 5250: 0.0795
Loss at step 5260: 0.0109
Loss at step 5270: 0.1163
Loss at step 5280: 0.0011
Loss at step 5290: 0.0024
Loss at step 5300: 0.0069
Loss at step 5310: 0.1383
Loss at step 5320: 0.1039
Loss at step 5330: 0.1713
Loss at step 5340: 0.0028
Loss at step 5350: 0.1980
Loss at step 5360: 0.0337
Loss at step 5370: 0.0110
Loss at step 5380: 0.1073
Loss at step 5390: 0.0939
Loss at step 5400: 0.0035
Loss at step 5410: 0.0531
Loss at step 5420: 0.0145
Loss at step 5430: 0.1361
Loss at step 5440: 0.0899
Loss at step 5450: 0.0756
Loss at step 5460: 0.0057
Loss at step 5470: 0.0066
Loss at step 5480: 0.3855
Loss at step 5490: 0.0041
Loss at step 5500: 0.1583
Loss at step 5510: 0.0928
Loss at step 5520: 0.0017
Loss at step 5530: 0.0823
Loss at step 5540: 0.0088
Loss at step 5550: 0.0094
Loss at step 5560: 0.1213
Loss at step 5570: 0.0850
Loss at step 5580: 0.0033
Loss at step 5590: 0.0031
Loss at step 5600: 0.2171
Loss at step 5610: 0.1218
Loss at step 5620: 0.0027
Loss at step 5630: 0.0040
Loss at step 5640: 0.0253
Loss at step 5650: 0.0019
Loss at step 5660: 0.1357
Loss at step 5670: 0.1677
Loss at step 5680: 0.0062
Loss at step 5690: 0.0025
Loss at step 5700: 0.1069
Loss at step 5710: 0.1141
Loss at step 5720: 0.0938
Loss at step 5730: 0.1515
Loss at step 5740: 0.1147
Loss at step 5750: 0.0055
Loss at step 5760: 0.0072
Loss at step 5770: 0.0492
Loss at step 5780: 0.0057
Loss at step 5790: 0.0848
Loss at step 5800: 0.0038
Loss at step 5810: 0.1003
Loss at step 5820: 0.0159
Loss at step 5830: 0.1173
Loss at step 5840: 0.0049
Loss at step 5850: 0.1340
Loss at step 5860: 0.0019
Loss at step 5870: 0.0123
Loss at step 5880: 0.0943
Loss at step 5890: 0.0015
Loss at step 5900: 0.0645
Loss at step 5910: 0.2324
Loss at step 5920: 0.0033
Loss at step 5930: 0.0051
Loss at step 5940: 0.0238
Loss at step 5950: 0.0046
Loss at step 5960: 0.0718
Loss at step 5970: 0.0179
Loss at step 5980: 0.0357
Loss at step 5990: 0.2062
Loss at step 6000: 0.0038
Loss at step 6010: 0.0069
Loss at step 6020: 0.0031
Loss at step 6030: 0.0131
Loss at step 6040: 0.0078
Loss at step 6050: 0.0068
Loss at step 6060: 0.1320
Loss at step 6070: 0.0030
Loss at step 6080: 0.0255
Loss at step 6090: 0.1911
Loss at step 6100: 0.3172
Loss at step 6110: 0.0357
Loss at step 6120: 0.0807
Loss at step 6130: 0.0330
Loss at step 6140: 0.0524
Loss at step 6150: 0.0016
Loss at step 6160: 0.0422
Loss at step 6170: 0.0360
Loss at step 6180: 0.0039
Loss at step 6190: 0.3839
Loss at step 6200: 0.0022
Loss at step 6210: 0.1529
Loss at step 6220: 0.1130
Loss at step 6230: 0.0017
Loss at step 6240: 0.0082
Loss at step 6250: 0.0061
Loss at step 6260: 0.1591
Loss at step 6270: 0.0056
Loss at step 6280: 0.0023
Loss at step 6290: 0.1713
Loss at step 6300: 0.0037
Loss at step 6310: 0.0023
Loss at step 6320: 0.0066
Loss at step 6330: 0.0588
Loss at step 6340: 0.0018
Loss at step 6350: 0.0925
Loss at step 6360: 0.1279
Loss at step 6370: 0.0014
Loss at step 6380: 0.0176
Loss at step 6390: 0.0012
Loss at step 6400: 0.0027
Loss at step 6410: 0.0336
Loss at step 6420: 0.1481
Loss at step 6430: 0.0017
Loss at step 6440: 0.1616
Loss at step 6450: 0.0835
Loss at step 6460: 0.1160
Loss at step 6470: 0.0264
Loss at step 6480: 0.2386
Loss at step 6490: 0.0032
Loss at step 6500: 0.0137
Loss at step 6510: 0.2274
Loss at step 6520: 0.0023
Loss at step 6530: 0.0015
Loss at step 6540: 0.2616
Loss at step 6550: 0.0015
Loss at step 6560: 0.0337
Loss at step 6570: 0.1329
Loss at step 6580: 0.0020
Loss at step 6590: 0.0021
***** Running testing *****
  Num examples = 660
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.579952, 'precision': [0.667755, 0.535775, 0.542767], 'recall': [0.580999, 0.635146, 0.49971], 'f1': [0.621363, 0.581244, 0.52035]}
{'accuracy': 0.760896, 'precision': 0.542767, 'recall': 0.49971, 'f1': 0.52035, 'WordR': 0.177855}
