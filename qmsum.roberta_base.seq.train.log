Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7c25611b6280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7c639e476280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x781607b7c280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x79c7498b6280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7714a6dbc280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
Parameter 'function'=<function get_omission_datasets.<locals>.seq_func at 0x7db8c83b6280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Sample 7668 of the training set: {'input_ids': [0, 17297, 495, 230, 3978, 5, 165, 14, 5, 5122, 40511, 2225, 56, 57, 3903, 8, 5, 1019, 74, 185, 317, 11, 83, 337, 28659, 10060, 11, 772, 4, 50118, 133, 165, 21, 442, 2017, 15, 15, 12, 1902, 2340, 1938, 6, 442, 24, 74, 1157, 13, 5, 414, 4, 2, 2, 0, 2264, 222, 15221, 211, 206, 59, 226, 4794, 116, 47385, 2, 0, 17297, 495, 211, 35, 150, 84, 467, 16, 855, 23, 707, 135, 479, 12698, 2156, 53, 99, 2594, 67, 16, 14, 114, 38, 4161, 7, 5, 2156, 7252, 25522, 417, 32062, 6920, 254, 24303, 25522, 13424, 31375, 1536, 9834, 24303, 10, 769, 111, 33689, 1538, 1732, 9, 5, 1901, 8, 25522, 44970, 24303, 38, 769, 111, 33689, 1538, 42, 634, 10, 1104, 6496, 14, 128, 29, 33459, 30, 10, 226, 4794, 2156, 37463, 2156, 14929, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 12698, 2156, 157, 2156, 47, 64, 5848, 2156, 14, 2156, 37463, 25522, 417, 32062, 6920, 254, 24303, 14, 42, 16, 45, 1901, 2156, 2, 0, 44385, 163, 35, 8976, 479, 2, 0, 17297, 495, 211, 35, 98, 5, 5567, 16, 45, 5389, 7, 5281, 42, 479, 125, 579, 888, 24, 2369, 101, 25522, 44970, 24303, 42759, 2156, 98, 52, 32, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 2647, 2156, 38, 1266, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 17297, 495, 211, 35, 35670, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 163, 35, 345, 128, 29, 80, 1272, 89, 479, 38, 1266, 25522, 417, 32062, 6920, 254, 24303, 38, 1266, 2156, 98, 25522, 417, 32062, 6920, 254, 24303, 98, 5, 78, 16, 25522, 31375, 1536, 9834, 24303, 14, 30, 608, 226, 4794, 111, 11971, 19, 33689, 1538, 1901, 885, 101, 47, 128, 241, 584, 2156, 37463, 2156, 24, 128, 29, 25522, 417, 32062, 6920, 254, 24303, 25522, 31375, 1536, 9834, 24303, 939, 939, 47, 128, 241, 25522, 417, 32062, 6920, 254, 24303, 47, 128, 241, 1271, 97, 30223, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 5143, 17487, 407, 24, 128, 29, 45, 95, 5, 6496, 53, 47, 128, 241, 1271, 11, 754, 103, 30223, 142, 24, 128, 29, 129, 41, 46194, 479, 12698, 2156, 8, 5, 200, 631, 16, 25522, 417, 32062, 6920, 254, 24303, 61, 16, 475, 2085, 55, 2679, 25522, 417, 32062, 6920, 254, 24303, 16, 14, 2156, 7252, 2156, 25522, 44951, 24303, 25522, 31375, 1536, 9834, 24303, 114, 47, 109, 24, 19, 39217, 1901, 2156, 47, 120, 42, 346, 479, 653, 114, 47, 56, 25522, 44970, 24303, 626, 1966, 25522, 44951, 24303, 769, 111, 37423, 8, 551, 5, 3242, 25, 157, 17487, 41881, 17487, 407, 122, 47, 342, 5, 3242, 11, 479, 2, 0, 17297, 495, 211, 35, 23129, 111, 41437, 479, 2, 0, 44385, 163, 35, 653, 74, 5, 3164, 28, 172, 17487, 2, 0, 17297, 495, 211, 35, 12698, 25522, 417, 32062, 6920, 254, 24303, 2, 0, 44385, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}.
***** Running training *****
  Num examples = 10950
  Num Epochs = 3
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 8214
Loss at step 10: 0.7649
Loss at step 20: 0.6826
Loss at step 30: 0.8718
Loss at step 40: 0.6538
Loss at step 50: 0.7714
Loss at step 60: 0.5158
Loss at step 70: 0.6176
Loss at step 80: 0.5886
Loss at step 90: 0.5922
Loss at step 100: 0.7683
Loss at step 110: 0.7458
Loss at step 120: 0.6733
Loss at step 130: 0.9445
Loss at step 140: 0.6860
Loss at step 150: 0.6576
Loss at step 160: 0.5479
Loss at step 170: 0.5251
Loss at step 180: 0.6474
Loss at step 190: 0.7167
Loss at step 200: 0.6891
Loss at step 210: 0.5354
Loss at step 220: 0.5556
Loss at step 230: 0.6459
Loss at step 240: 0.6016
Loss at step 250: 0.8221
Loss at step 260: 0.7095
Loss at step 270: 0.5088
Loss at step 280: 0.6446
Loss at step 290: 0.5658
Loss at step 300: 0.5520
Loss at step 310: 0.6180
Loss at step 320: 0.7342
Loss at step 330: 0.7086
Loss at step 340: 0.5080
Loss at step 350: 0.6348
Loss at step 360: 0.7737
Loss at step 370: 0.5482
Loss at step 380: 0.5015
Loss at step 390: 0.6724
Loss at step 400: 0.6869
Loss at step 410: 0.6508
Loss at step 420: 0.5645
Loss at step 430: 0.5975
Loss at step 440: 0.8453
Loss at step 450: 0.5136
Loss at step 460: 0.9281
Loss at step 470: 0.6435
Loss at step 480: 0.5821
Loss at step 490: 0.5441
Loss at step 500: 0.4419
Loss at step 510: 0.9848
Loss at step 520: 0.6103
Loss at step 530: 0.4515
Loss at step 540: 0.4375
Loss at step 550: 0.7143
Loss at step 560: 0.6061
Loss at step 570: 0.5241
Loss at step 580: 0.7461
Loss at step 590: 0.6157
Loss at step 600: 0.4757
Loss at step 610: 0.5234
Loss at step 620: 0.5996
Loss at step 630: 0.4883
Loss at step 640: 0.6456
Loss at step 650: 0.5956
Loss at step 660: 0.5132
Loss at step 670: 0.4186
Loss at step 680: 0.9782
Loss at step 690: 0.5741
Loss at step 700: 0.5136
Loss at step 710: 0.4315
Loss at step 720: 0.5347
Loss at step 730: 0.4577
Loss at step 740: 0.7435
Loss at step 750: 0.5532
Loss at step 760: 0.7110
Loss at step 770: 0.5015
Loss at step 780: 0.4016
Loss at step 790: 0.5705
Loss at step 800: 0.5523
Loss at step 810: 0.5231
Loss at step 820: 0.6545
Loss at step 830: 0.6232
Loss at step 840: 0.5275
Loss at step 850: 0.5410
Loss at step 860: 0.6923
Loss at step 870: 0.4283
Loss at step 880: 0.4171
Loss at step 890: 0.5072
Loss at step 900: 0.4989
Loss at step 910: 0.4926
Loss at step 920: 0.8048
Loss at step 930: 0.5120
Loss at step 940: 0.3730
Loss at step 950: 0.6516
Loss at step 960: 0.3834
Loss at step 970: 0.4902
Loss at step 980: 0.5493
Loss at step 990: 0.3926
Loss at step 1000: 0.5890
Loss at step 1010: 0.4372
Loss at step 1020: 0.5737
Loss at step 1030: 0.6599
Loss at step 1040: 0.7133
Loss at step 1050: 0.3891
Loss at step 1060: 0.5807
Loss at step 1070: 0.6351
Loss at step 1080: 0.3109
Loss at step 1090: 0.3713
Loss at step 1100: 0.3489
Loss at step 1110: 0.3352
Loss at step 1120: 0.6246
Loss at step 1130: 0.4534
Loss at step 1140: 0.3127
Loss at step 1150: 0.3979
Loss at step 1160: 0.5103
Loss at step 1170: 0.6461
Loss at step 1180: 0.4951
Loss at step 1190: 0.3353
Loss at step 1200: 0.6066
Loss at step 1210: 0.5167
Loss at step 1220: 0.2441
Loss at step 1230: 0.4857
Loss at step 1240: 0.5011
Loss at step 1250: 0.4577
Loss at step 1260: 0.5938
Loss at step 1270: 0.4701
Loss at step 1280: 0.5071
Loss at step 1290: 0.3409
Loss at step 1300: 0.3079
Loss at step 1310: 0.5281
Loss at step 1320: 0.4198
Loss at step 1330: 0.4533
Loss at step 1340: 0.3814
Loss at step 1350: 0.3146
Loss at step 1360: 0.4223
Loss at step 1370: 0.2896
Loss at step 1380: 0.8305
Loss at step 1390: 0.3629
Loss at step 1400: 0.4674
Loss at step 1410: 0.4241
Loss at step 1420: 0.4132
Loss at step 1430: 0.2433
Loss at step 1440: 0.4712
Loss at step 1450: 0.4573
Loss at step 1460: 0.5830
Loss at step 1470: 0.3640
Loss at step 1480: 0.4268
Loss at step 1490: 0.2867
Loss at step 1500: 0.2502
Loss at step 1510: 0.6071
Loss at step 1520: 0.2814
Loss at step 1530: 0.5106
Loss at step 1540: 0.3479
Loss at step 1550: 0.5506
Loss at step 1560: 0.4552
Loss at step 1570: 0.5453
Loss at step 1580: 0.3295
Loss at step 1590: 0.4908
Loss at step 1600: 0.4683
Loss at step 1610: 0.4208
Loss at step 1620: 0.1913
Loss at step 1630: 0.3578
Loss at step 1640: 0.5337
Loss at step 1650: 0.3440
Loss at step 1660: 0.3695
Loss at step 1670: 0.2877
Loss at step 1680: 0.3183
Loss at step 1690: 0.4809
Loss at step 1700: 0.2532
Loss at step 1710: 0.3596
Loss at step 1720: 0.3208
Loss at step 1730: 0.2429
Loss at step 1740: 0.4838
Loss at step 1750: 0.2075
Loss at step 1760: 0.2668
Loss at step 1770: 0.2776
Loss at step 1780: 0.3648
Loss at step 1790: 0.5241
Loss at step 1800: 0.4116
Loss at step 1810: 0.3287
Loss at step 1820: 0.3831
Loss at step 1830: 0.4179
Loss at step 1840: 0.5922
Loss at step 1850: 0.4472
Loss at step 1860: 0.3410
Loss at step 1870: 0.3141
Loss at step 1880: 0.1559
Loss at step 1890: 0.3235
Loss at step 1900: 0.3519
Loss at step 1910: 0.2724
Loss at step 1920: 0.1988
Loss at step 1930: 0.4525
Loss at step 1940: 0.2273
Loss at step 1950: 0.3516
Loss at step 1960: 0.3697
Loss at step 1970: 0.2232
Loss at step 1980: 0.1013
Loss at step 1990: 0.2239
Loss at step 2000: 0.1123
Loss at step 2010: 0.2045
Loss at step 2020: 0.2909
Loss at step 2030: 0.1487
Loss at step 2040: 0.3162
Loss at step 2050: 0.2628
Loss at step 2060: 0.2704
Loss at step 2070: 0.4853
Loss at step 2080: 0.3319
Loss at step 2090: 0.2712
Loss at step 2100: 0.1493
Loss at step 2110: 0.2733
Loss at step 2120: 0.3582
Loss at step 2130: 0.3057
Loss at step 2140: 0.3078
Loss at step 2150: 0.2755
Loss at step 2160: 0.2494
Loss at step 2170: 0.1848
Loss at step 2180: 0.2511
Loss at step 2190: 0.5719
Loss at step 2200: 0.2166
Loss at step 2210: 0.2366
Loss at step 2220: 0.2245
Loss at step 2230: 0.3127
Loss at step 2240: 0.1857
Loss at step 2250: 0.2000
Loss at step 2260: 0.1815
Loss at step 2270: 0.1901
Loss at step 2280: 0.0796
Loss at step 2290: 0.1419
Loss at step 2300: 0.4474
Loss at step 2310: 0.4984
Loss at step 2320: 0.3144
Loss at step 2330: 0.3747
Loss at step 2340: 0.4729
Loss at step 2350: 0.2829
Loss at step 2360: 0.3904
Loss at step 2370: 0.2755
Loss at step 2380: 0.3809
Loss at step 2390: 0.2720
Loss at step 2400: 0.1897
Loss at step 2410: 0.2284
Loss at step 2420: 0.4492
Loss at step 2430: 0.2917
Loss at step 2440: 0.4275
Loss at step 2450: 0.2980
Loss at step 2460: 0.2585
Loss at step 2470: 0.2727
Loss at step 2480: 0.0776
Loss at step 2490: 0.1590
Loss at step 2500: 0.0953
Loss at step 2510: 0.3427
Loss at step 2520: 0.1843
Loss at step 2530: 0.4311
Loss at step 2540: 0.5687
Loss at step 2550: 0.1926
Loss at step 2560: 0.2484
Loss at step 2570: 0.0971
Loss at step 2580: 0.2389
Loss at step 2590: 0.3000
Loss at step 2600: 0.3885
Loss at step 2610: 0.3029
Loss at step 2620: 0.2324
Loss at step 2630: 0.2875
Loss at step 2640: 0.2650
Loss at step 2650: 0.1672
Loss at step 2660: 0.1786
Loss at step 2670: 0.5955
Loss at step 2680: 0.4311
Loss at step 2690: 0.2446
Loss at step 2700: 0.4265
Loss at step 2710: 0.3148
Loss at step 2720: 0.1879
Loss at step 2730: 0.3878
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725912, 'precision': [0.857265, 0.452699, 0.491377], 'recall': [0.860709, 0.402026, 0.55129], 'f1': [0.858984, 0.42586, 0.519612]}
{'accuracy': 0.843775, 'precision': 0.491377, 'recall': 0.55129, 'f1': 0.519612, 'WordR': 0.130783}
Loss at step 2740: 0.2429
Loss at step 2750: 0.2124
Loss at step 2760: 0.2601
Loss at step 2770: 0.1226
Loss at step 2780: 0.3155
Loss at step 2790: 0.1795
Loss at step 2800: 0.5138
Loss at step 2810: 0.1851
Loss at step 2820: 0.2387
Loss at step 2830: 0.0962
Loss at step 2840: 0.1328
Loss at step 2850: 0.1644
Loss at step 2860: 0.1039
Loss at step 2870: 0.1679
Loss at step 2880: 0.4835
Loss at step 2890: 0.0570
Loss at step 2900: 0.2763
Loss at step 2910: 0.1677
Loss at step 2920: 0.2139
Loss at step 2930: 0.3046
Loss at step 2940: 0.2001
Loss at step 2950: 0.1994
Loss at step 2960: 0.1937
Loss at step 2970: 0.1189
Loss at step 2980: 0.2685
Loss at step 2990: 0.3892
Loss at step 3000: 0.0982
Loss at step 3010: 0.1100
Loss at step 3020: 0.2160
Loss at step 3030: 0.4610
Loss at step 3040: 0.1641
Loss at step 3050: 0.1817
Loss at step 3060: 0.3560
Loss at step 3070: 0.1235
Loss at step 3080: 0.2989
Loss at step 3090: 0.2299
Loss at step 3100: 0.0499
Loss at step 3110: 0.2223
Loss at step 3120: 0.2139
Loss at step 3130: 0.1060
Loss at step 3140: 0.1475
Loss at step 3150: 0.2108
Loss at step 3160: 0.1437
Loss at step 3170: 0.2685
Loss at step 3180: 0.1273
Loss at step 3190: 0.1921
Loss at step 3200: 0.1759
Loss at step 3210: 0.5129
Loss at step 3220: 0.0927
Loss at step 3230: 0.2100
Loss at step 3240: 0.0556
Loss at step 3250: 0.1960
Loss at step 3260: 0.0885
Loss at step 3270: 0.3656
Loss at step 3280: 0.0473
Loss at step 3290: 0.1057
Loss at step 3300: 0.1392
Loss at step 3310: 0.3249
Loss at step 3320: 0.2011
Loss at step 3330: 0.0933
Loss at step 3340: 0.2456
Loss at step 3350: 0.0766
Loss at step 3360: 0.2694
Loss at step 3370: 0.0930
Loss at step 3380: 0.1775
Loss at step 3390: 0.1987
Loss at step 3400: 0.2596
Loss at step 3410: 0.5152
Loss at step 3420: 0.2524
Loss at step 3430: 0.5326
Loss at step 3440: 0.2389
Loss at step 3450: 0.4557
Loss at step 3460: 0.1668
Loss at step 3470: 0.2871
Loss at step 3480: 0.0972
Loss at step 3490: 0.3517
Loss at step 3500: 0.1636
Loss at step 3510: 0.0882
Loss at step 3520: 0.1195
Loss at step 3530: 0.1892
Loss at step 3540: 0.2719
Loss at step 3550: 0.1763
Loss at step 3560: 0.2019
Loss at step 3570: 0.1877
Loss at step 3580: 0.0735
Loss at step 3590: 0.1474
Loss at step 3600: 0.0449
Loss at step 3610: 0.0618
Loss at step 3620: 0.2237
Loss at step 3630: 0.0298
Loss at step 3640: 0.2162
Loss at step 3650: 0.0883
Loss at step 3660: 0.0530
Loss at step 3670: 0.4373
Loss at step 3680: 0.2186
Loss at step 3690: 0.3142
Loss at step 3700: 0.0777
Loss at step 3710: 0.0784
Loss at step 3720: 0.1781
Loss at step 3730: 0.0275
Loss at step 3740: 0.2754
Loss at step 3750: 0.0453
Loss at step 3760: 0.1683
Loss at step 3770: 0.1319
Loss at step 3780: 0.1081
Loss at step 3790: 0.0462
Loss at step 3800: 0.2126
Loss at step 3810: 0.1715
Loss at step 3820: 0.1605
Loss at step 3830: 0.1542
Loss at step 3840: 0.0299
Loss at step 3850: 0.1015
Loss at step 3860: 0.2632
Loss at step 3870: 0.1347
Loss at step 3880: 0.1283
Loss at step 3890: 0.2924
Loss at step 3900: 0.1962
Loss at step 3910: 0.0529
Loss at step 3920: 0.1632
Loss at step 3930: 0.0284
Loss at step 3940: 0.0605
Loss at step 3950: 0.0813
Loss at step 3960: 0.0350
Loss at step 3970: 0.2705
Loss at step 3980: 0.0450
Loss at step 3990: 0.0472
Loss at step 4000: 0.1130
Loss at step 4010: 0.1235
Loss at step 4020: 0.0686
Loss at step 4030: 0.0558
Loss at step 4040: 0.2279
Loss at step 4050: 0.0406
Loss at step 4060: 0.0912
Loss at step 4070: 0.0828
Loss at step 4080: 0.3038
Loss at step 4090: 0.1694
Loss at step 4100: 0.2498
Loss at step 4110: 0.2777
Loss at step 4120: 0.1226
Loss at step 4130: 0.0657
Loss at step 4140: 0.0501
Loss at step 4150: 0.0402
Loss at step 4160: 0.3578
Loss at step 4170: 0.1577
Loss at step 4180: 0.1372
Loss at step 4190: 0.0619
Loss at step 4200: 0.1176
Loss at step 4210: 0.3652
Loss at step 4220: 0.1456
Loss at step 4230: 0.0758
Loss at step 4240: 0.1203
Loss at step 4250: 0.2509
Loss at step 4260: 0.0557
Loss at step 4270: 0.1957
Loss at step 4280: 0.1238
Loss at step 4290: 0.1927
Loss at step 4300: 0.0578
Loss at step 4310: 0.2223
Loss at step 4320: 0.0134
Loss at step 4330: 0.1694
Loss at step 4340: 0.0950
Loss at step 4350: 0.1316
Loss at step 4360: 0.2429
Loss at step 4370: 0.0576
Loss at step 4380: 0.0621
Loss at step 4390: 0.1627
Loss at step 4400: 0.0732
Loss at step 4410: 0.0227
Loss at step 4420: 0.2868
Loss at step 4430: 0.0605
Loss at step 4440: 0.0898
Loss at step 4450: 0.0947
Loss at step 4460: 0.0649
Loss at step 4470: 0.2780
Loss at step 4480: 0.1834
Loss at step 4490: 0.1238
Loss at step 4500: 0.0508
Loss at step 4510: 0.0940
Loss at step 4520: 0.0193
Loss at step 4530: 0.0227
Loss at step 4540: 0.2613
Loss at step 4550: 0.0903
Loss at step 4560: 0.2097
Loss at step 4570: 0.0302
Loss at step 4580: 0.2730
Loss at step 4590: 0.1568
Loss at step 4600: 0.2605
Loss at step 4610: 0.0712
Loss at step 4620: 0.3100
Loss at step 4630: 0.0964
Loss at step 4640: 0.1575
Loss at step 4650: 0.1961
Loss at step 4660: 0.1331
Loss at step 4670: 0.1813
Loss at step 4680: 0.0331
Loss at step 4690: 0.1601
Loss at step 4700: 0.1866
Loss at step 4710: 0.0869
Loss at step 4720: 0.0407
Loss at step 4730: 0.0751
Loss at step 4740: 0.0727
Loss at step 4750: 0.0535
Loss at step 4760: 0.1439
Loss at step 4770: 0.1184
Loss at step 4780: 0.0651
Loss at step 4790: 0.0964
Loss at step 4800: 0.1550
Loss at step 4810: 0.0164
Loss at step 4820: 0.0930
Loss at step 4830: 0.0112
Loss at step 4840: 0.2088
Loss at step 4850: 0.0254
Loss at step 4860: 0.2471
Loss at step 4870: 0.3731
Loss at step 4880: 0.0623
Loss at step 4890: 0.0203
Loss at step 4900: 0.0373
Loss at step 4910: 0.3151
Loss at step 4920: 0.1387
Loss at step 4930: 0.1280
Loss at step 4940: 0.0966
Loss at step 4950: 0.0657
Loss at step 4960: 0.1119
Loss at step 4970: 0.0519
Loss at step 4980: 0.2517
Loss at step 4990: 0.2171
Loss at step 5000: 0.1374
Loss at step 5010: 0.1102
Loss at step 5020: 0.1166
Loss at step 5030: 0.0857
Loss at step 5040: 0.0907
Loss at step 5050: 0.1125
Loss at step 5060: 0.3754
Loss at step 5070: 0.1457
Loss at step 5080: 0.2484
Loss at step 5090: 0.2697
Loss at step 5100: 0.1196
Loss at step 5110: 0.1379
Loss at step 5120: 0.2171
Loss at step 5130: 0.0453
Loss at step 5140: 0.0363
Loss at step 5150: 0.1403
Loss at step 5160: 0.0819
Loss at step 5170: 0.0559
Loss at step 5180: 0.1684
Loss at step 5190: 0.1282
Loss at step 5200: 0.1069
Loss at step 5210: 0.1136
Loss at step 5220: 0.0939
Loss at step 5230: 0.3172
Loss at step 5240: 0.0318
Loss at step 5250: 0.0722
Loss at step 5260: 0.0416
Loss at step 5270: 0.0454
Loss at step 5280: 0.0791
Loss at step 5290: 0.0403
Loss at step 5300: 0.0838
Loss at step 5310: 0.1019
Loss at step 5320: 0.0808
Loss at step 5330: 0.2151
Loss at step 5340: 0.0728
Loss at step 5350: 0.1787
Loss at step 5360: 0.0883
Loss at step 5370: 0.0931
Loss at step 5380: 0.2519
Loss at step 5390: 0.0438
Loss at step 5400: 0.0435
Loss at step 5410: 0.0775
Loss at step 5420: 0.0725
Loss at step 5430: 0.2800
Loss at step 5440: 0.2265
Loss at step 5450: 0.0432
Loss at step 5460: 0.0451
Loss at step 5470: 0.0448
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.725253, 'precision': [0.867882, 0.443616, 0.500456], 'recall': [0.850452, 0.470472, 0.505835], 'f1': [0.859078, 0.456649, 0.503131]}
{'accuracy': 0.846882, 'precision': 0.500456, 'recall': 0.505835, 'f1': 0.503131, 'WordR': 0.130783}
Loss at step 5480: 0.2114
Loss at step 5490: 0.0644
Loss at step 5500: 0.0563
Loss at step 5510: 0.1007
Loss at step 5520: 0.1131
Loss at step 5530: 0.2686
Loss at step 5540: 0.0431
Loss at step 5550: 0.0286
Loss at step 5560: 0.1004
Loss at step 5570: 0.1661
Loss at step 5580: 0.0678
Loss at step 5590: 0.0123
Loss at step 5600: 0.0142
Loss at step 5610: 0.1362
Loss at step 5620: 0.0491
Loss at step 5630: 0.0833
Loss at step 5640: 0.3065
Loss at step 5650: 0.1319
Loss at step 5660: 0.0095
Loss at step 5670: 0.0144
Loss at step 5680: 0.1267
Loss at step 5690: 0.0747
Loss at step 5700: 0.0311
Loss at step 5710: 0.0121
Loss at step 5720: 0.0725
Loss at step 5730: 0.0817
Loss at step 5740: 0.0069
Loss at step 5750: 0.1079
Loss at step 5760: 0.0655
Loss at step 5770: 0.0063
Loss at step 5780: 0.0127
Loss at step 5790: 0.0241
Loss at step 5800: 0.2470
Loss at step 5810: 0.1034
Loss at step 5820: 0.0780
Loss at step 5830: 0.0373
Loss at step 5840: 0.2075
Loss at step 5850: 0.1148
Loss at step 5860: 0.0417
Loss at step 5870: 0.1220
Loss at step 5880: 0.0779
Loss at step 5890: 0.0424
Loss at step 5900: 0.1401
Loss at step 5910: 0.1984
Loss at step 5920: 0.1633
Loss at step 5930: 0.2125
Loss at step 5940: 0.0334
Loss at step 5950: 0.0623
Loss at step 5960: 0.1600
Loss at step 5970: 0.0834
Loss at step 5980: 0.0994
Loss at step 5990: 0.0206
Loss at step 6000: 0.1011
Loss at step 6010: 0.1958
Loss at step 6020: 0.1428
Loss at step 6030: 0.0278
Loss at step 6040: 0.0921
Loss at step 6050: 0.0950
Loss at step 6060: 0.0136
Loss at step 6070: 0.1532
Loss at step 6080: 0.0074
Loss at step 6090: 0.0069
Loss at step 6100: 0.2453
Loss at step 6110: 0.0269
Loss at step 6120: 0.0768
Loss at step 6130: 0.3071
Loss at step 6140: 0.0746
Loss at step 6150: 0.0922
Loss at step 6160: 0.1938
Loss at step 6170: 0.0038
Loss at step 6180: 0.1788
Loss at step 6190: 0.0894
Loss at step 6200: 0.1190
Loss at step 6210: 0.0116
Loss at step 6220: 0.0904
Loss at step 6230: 0.1572
Loss at step 6240: 0.0530
Loss at step 6250: 0.0760
Loss at step 6260: 0.0977
Loss at step 6270: 0.0117
Loss at step 6280: 0.0142
Loss at step 6290: 0.0529
Loss at step 6300: 0.0835
Loss at step 6310: 0.0597
Loss at step 6320: 0.0348
Loss at step 6330: 0.0182
Loss at step 6340: 0.0408
Loss at step 6350: 0.0152
Loss at step 6360: 0.2612
Loss at step 6370: 0.1214
Loss at step 6380: 0.1032
Loss at step 6390: 0.0958
Loss at step 6400: 0.0150
Loss at step 6410: 0.0148
Loss at step 6420: 0.0840
Loss at step 6430: 0.1560
Loss at step 6440: 0.0918
Loss at step 6450: 0.0054
Loss at step 6460: 0.0038
Loss at step 6470: 0.0082
Loss at step 6480: 0.0045
Loss at step 6490: 0.1104
Loss at step 6500: 0.0835
Loss at step 6510: 0.2131
Loss at step 6520: 0.2462
Loss at step 6530: 0.0676
Loss at step 6540: 0.1095
Loss at step 6550: 0.0217
Loss at step 6560: 0.0666
Loss at step 6570: 0.1906
Loss at step 6580: 0.0427
Loss at step 6590: 0.1266
Loss at step 6600: 0.1072
Loss at step 6610: 0.1594
Loss at step 6620: 0.1442
Loss at step 6630: 0.0101
Loss at step 6640: 0.1155
Loss at step 6650: 0.0263
Loss at step 6660: 0.0974
Loss at step 6670: 0.0992
Loss at step 6680: 0.0585
Loss at step 6690: 0.0781
Loss at step 6700: 0.0699
Loss at step 6710: 0.0484
Loss at step 6720: 0.0810
Loss at step 6730: 0.0079
Loss at step 6740: 0.0867
Loss at step 6750: 0.1801
Loss at step 6760: 0.1297
Loss at step 6770: 0.0554
Loss at step 6780: 0.1682
Loss at step 6790: 0.0570
Loss at step 6800: 0.0043
Loss at step 6810: 0.1385
Loss at step 6820: 0.2704
Loss at step 6830: 0.0889
Loss at step 6840: 0.0159
Loss at step 6850: 0.0396
Loss at step 6860: 0.0572
Loss at step 6870: 0.4483
Loss at step 6880: 0.0207
Loss at step 6890: 0.0904
Loss at step 6900: 0.0049
Loss at step 6910: 0.1632
Loss at step 6920: 0.0325
Loss at step 6930: 0.0071
Loss at step 6940: 0.1352
Loss at step 6950: 0.0141
Loss at step 6960: 0.0261
Loss at step 6970: 0.1394
Loss at step 6980: 0.0810
Loss at step 6990: 0.0247
Loss at step 7000: 0.0498
Loss at step 7010: 0.0043
Loss at step 7020: 0.0668
Loss at step 7030: 0.0215
Loss at step 7040: 0.0074
Loss at step 7050: 0.0023
Loss at step 7060: 0.0844
Loss at step 7070: 0.0070
Loss at step 7080: 0.0040
Loss at step 7090: 0.1878
Loss at step 7100: 0.0306
Loss at step 7110: 0.3817
Loss at step 7120: 0.0204
Loss at step 7130: 0.0557
Loss at step 7140: 0.0032
Loss at step 7150: 0.0684
Loss at step 7160: 0.1311
Loss at step 7170: 0.0914
Loss at step 7180: 0.1777
Loss at step 7190: 0.1825
Loss at step 7200: 0.1230
Loss at step 7210: 0.1397
Loss at step 7220: 0.0142
Loss at step 7230: 0.0314
Loss at step 7240: 0.1222
Loss at step 7250: 0.0250
Loss at step 7260: 0.1315
Loss at step 7270: 0.1870
Loss at step 7280: 0.0057
Loss at step 7290: 0.0886
Loss at step 7300: 0.1068
Loss at step 7310: 0.0105
Loss at step 7320: 0.0400
Loss at step 7330: 0.0403
Loss at step 7340: 0.0412
Loss at step 7350: 0.0776
Loss at step 7360: 0.1448
Loss at step 7370: 0.0032
Loss at step 7380: 0.0421
Loss at step 7390: 0.1352
Loss at step 7400: 0.1148
Loss at step 7410: 0.0526
Loss at step 7420: 0.0252
Loss at step 7430: 0.0222
Loss at step 7440: 0.1890
Loss at step 7450: 0.0792
Loss at step 7460: 0.0131
Loss at step 7470: 0.2000
Loss at step 7480: 0.1902
Loss at step 7490: 0.0782
Loss at step 7500: 0.0271
Loss at step 7510: 0.0079
Loss at step 7520: 0.0069
Loss at step 7530: 0.0153
Loss at step 7540: 0.0652
Loss at step 7550: 0.1298
Loss at step 7560: 0.0312
Loss at step 7570: 0.0959
Loss at step 7580: 0.1534
Loss at step 7590: 0.2382
Loss at step 7600: 0.0061
Loss at step 7610: 0.0476
Loss at step 7620: 0.0673
Loss at step 7630: 0.1230
Loss at step 7640: 0.0190
Loss at step 7650: 0.0102
Loss at step 7660: 0.0628
Loss at step 7670: 0.0545
Loss at step 7680: 0.0481
Loss at step 7690: 0.0445
Loss at step 7700: 0.0278
Loss at step 7710: 0.0142
Loss at step 7720: 0.0430
Loss at step 7730: 0.0044
Loss at step 7740: 0.0760
Loss at step 7750: 0.0082
Loss at step 7760: 0.0683
Loss at step 7770: 0.0883
Loss at step 7780: 0.0201
Loss at step 7790: 0.3637
Loss at step 7800: 0.1251
Loss at step 7810: 0.0613
Loss at step 7820: 0.0938
Loss at step 7830: 0.0630
Loss at step 7840: 0.0367
Loss at step 7850: 0.0150
Loss at step 7860: 0.0285
Loss at step 7870: 0.1203
Loss at step 7880: 0.0280
Loss at step 7890: 0.0032
Loss at step 7900: 0.0461
Loss at step 7910: 0.0948
Loss at step 7920: 0.0008
Loss at step 7930: 0.0254
Loss at step 7940: 0.0077
Loss at step 7950: 0.0016
Loss at step 7960: 0.0211
Loss at step 7970: 0.0014
Loss at step 7980: 0.4465
Loss at step 7990: 0.1411
Loss at step 8000: 0.1245
Loss at step 8010: 0.0551
Loss at step 8020: 0.0045
Loss at step 8030: 0.0482
Loss at step 8040: 0.0572
Loss at step 8050: 0.0282
Loss at step 8060: 0.2137
Loss at step 8070: 0.0850
Loss at step 8080: 0.1405
Loss at step 8090: 0.0418
Loss at step 8100: 0.0565
Loss at step 8110: 0.1064
Loss at step 8120: 0.0302
Loss at step 8130: 0.0468
Loss at step 8140: 0.2688
Loss at step 8150: 0.0909
Loss at step 8160: 0.0857
Loss at step 8170: 0.1107
Loss at step 8180: 0.0090
Loss at step 8190: 0.0714
Loss at step 8200: 0.0227
Loss at step 8210: 0.1701
***** Running testing *****
  Num examples = 1422
  Instantaneous batch size per device = 4
  Total eval batch size = 4
{'accuracy': 0.7269, 'precision': [0.876449, 0.445098, 0.498812], 'recall': [0.846148, 0.485792, 0.515971], 'f1': [0.861032, 0.464556, 0.507246]}
{'accuracy': 0.846364, 'precision': 0.498812, 'recall': 0.515971, 'f1': 0.507246, 'WordR': 0.258629}
